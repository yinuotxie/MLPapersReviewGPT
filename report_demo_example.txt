[TITLE]
Academic GPT: Leveraging Large Language Models for the Review of Machine Learning Research Papers

[ABSTRACT]
In the field of machine learning, the literature review process is crucial for maintaining the quality of academic conferences. This process, however, is challenged by the overwhelming number of submissions and a shortage of qualified reviewers. The surge in scholarly output and the increasing complexity of specialized knowledge further strain traditional review mechanisms, making high-quality peer reviews increasingly rare. This scarcity disproportionately affects junior researchers and those from under-resourced backgrounds, who may struggle to obtain timely and constructive feedback. Recent advancements in Large Language Models (LLMs), such as GPT-4, have highlighted their potential to automate the provision of scientific feedback on research manuscripts. In response, this project developed an automated pipeline using LLMs to review complete PDFs of machine learning papers. We utilized Chat-GPT 3.5, GPT-4, and a fine-tuned Mistral 7B model, which was trained on a corpus of 15,000 machine learning papers and their peer reviews. Our evaluation, based on a hit-rate metric, revealed that GPT-3.5 achieved a hit rate of 32%, GPT-4 reached 43%, and Mistral 7B scored 55%. Notably, Mistral 7B exhibited higher hit rates than GPT-3.5 in 84.21% of cases, and GPT-4 in 63.16% of cases. These findings suggest that LLMs, particularly when fine-tuned, can significantly aid the research review process. While there are accuracy limitations, LLMs can serve as valuable tools for researchers to preliminarily review their papers before submission. This could potentially improve acceptance rates and alleviate the burden on human reviewers by providing preliminary feedback, especially during the early stages of manuscript preparation.