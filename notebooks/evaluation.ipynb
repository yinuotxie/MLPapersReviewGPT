{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../reviews/notes.jsonl') as f:\n",
    "    reviews = [json.loads(line) for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, List, Any\n",
    "\n",
    "def extract_review_messages(review: Dict[str, Any]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Extracts the review messages from the given review data structure.\n",
    "\n",
    "    The function navigates through the nested structure of the input dictionary,\n",
    "    specifically looking for a 'reviews_msg' key. It then iterates over its contents,\n",
    "    extracting the 'review' field from each 'content' sub-dictionary.\n",
    "\n",
    "    Args:\n",
    "        review (Dict[str, Any]): A dictionary containing review information, \n",
    "        where the key 'reviews_msg' is expected to be a list of dictionaries\n",
    "        with a 'content' key, which in turn should contain a 'review' key with the actual message.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of review messages extracted from the input dictionary. \n",
    "        If no relevant messages are found, returns an empty list.\n",
    "    \"\"\"\n",
    "    review_messages = []\n",
    "    if 'reviews_msg' in review:\n",
    "        for message in review['reviews_msg']:\n",
    "            if 'content' in message and 'review' in message['content']:\n",
    "                review_messages.append(message['content']['review'])\n",
    "\n",
    "    return review_messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_messages = extract_review_messages(reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of review messages: 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of review messages:\", len(review_messages))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_review_messages = review_messages[:2]\n",
    "generated_review_messages = review_messages[2:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROUGE-L and Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "ROUGEL_SCORER = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "COSINE_SIM_EMBED = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_rouge_l(human_reviews: List[str], generated_reviews: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the ROUGE-L score between two sets of reviews, typically human-written and generated.\n",
    "\n",
    "    ROUGE-L is a measure of the longest common subsequence and is used to assess the quality\n",
    "    of generated text in comparison to reference text. This function computes the ROUGE-L\n",
    "    fmeasure for each pair of human and generated reviews and returns the average score.\n",
    "\n",
    "    Args:\n",
    "        human_reviews (List[str]): A list of reference reviews written by humans.\n",
    "        generated_reviews (List[str]): A list of reviews generated by a model.\n",
    "\n",
    "    Returns:\n",
    "        float: The average ROUGE-L fmeasure score across all pairs of human and generated reviews.\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    for h_review in human_reviews:\n",
    "        for g_review in generated_reviews:\n",
    "            scores.append(ROUGEL_SCORER.score(h_review, g_review)['rougeL'].fmeasure)\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "def calculate_cosine_similarity(human_reviews: List[str], generated_reviews: List[str]) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the cosine similarity between the embeddings of two sets of reviews.\n",
    "\n",
    "    This function first encodes the reviews into embeddings using a predefined embedding model,\n",
    "    then calculates the cosine similarity between each pair of human and generated review embeddings,\n",
    "    and finally returns the average similarity score.\n",
    "\n",
    "    Args:\n",
    "        human_reviews (List[str]): A list of reference reviews written by humans.\n",
    "        generated_reviews (List[str]): A list of reviews generated by a model.\n",
    "\n",
    "    Returns:\n",
    "        float: The average cosine similarity score between the embeddings of human and generated reviews.\n",
    "    \"\"\"\n",
    "    human_reviews_embeddings = COSINE_SIM_EMBED.encode(human_reviews)\n",
    "    generated_reviews_embeddings = COSINE_SIM_EMBED.encode(generated_reviews)\n",
    "\n",
    "    scores = []\n",
    "    for h_review, g_review in zip(human_reviews_embeddings, generated_reviews_embeddings):\n",
    "        scores.append(cosine_similarity([h_review], [g_review])[0][0])\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rouge-L Score: 0.1803172009459371\n"
     ]
    }
   ],
   "source": [
    "rouge_score = calculate_rouge_l(human_review_messages, generated_review_messages)\n",
    "print(\"Rouge-L Score:\", rouge_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Score: 0.5914192\n"
     ]
    }
   ],
   "source": [
    "cosine_similarity_score = calculate_cosine_similarity(human_review_messages, generated_review_messages)\n",
    "print(\"Cosine Similarity Score:\", cosine_similarity_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "api_key = \"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\"\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt_template = \"\"\"\n",
    "Your task is to carefully analyze and accurately match the key concerns raised in two reviews, ensuring a strong correspondence between the matched points. Examine the verbatim closely.\n",
    "\n",
    "=====Review A: \n",
    "{Review_A}\n",
    "\n",
    "===== \n",
    "\n",
    "=====Review B: \n",
    "{Review_B}\n",
    "\n",
    "===== \n",
    "\n",
    "Please follow the example JSON format below for matching points. For instance, if point from review A is nearly identical to point from review B, it should look like this:\n",
    "{{ \n",
    "\"A3-B2\": {{\n",
    "    \"rationale\": \"/explain why A3 and B2 are nearly identical/\",\n",
    "    \"similarity\": \"/5-10, only an integer/\"\n",
    "}},\n",
    "...\n",
    "}}\n",
    "\n",
    "**Note that you should only match points with a significant degree of similarity in their concerns. Refrain from matching points with only superficial similarities or weak connections.** For each matched pair, rate the similarity on a scale of 5-10:\n",
    "- 5 Somewhat Related: Points address similar themes but from different angles.\n",
    "- 6 Moderately Related: Points share a common theme but with different perspectives or suggestions.\n",
    "- 7 Strongly Related: Points are largely aligned but differ in some details or nuances.\n",
    "- 8 Very Strongly Related: Points offer similar suggestions or concerns, with slight differences.\n",
    "- 9 Almost Identical: Points are nearly the same, with minor differences in wording or presentation.\n",
    "- 10 Identical: Points are exactly the same in terms of concerns, suggestions, or praises.\n",
    "\n",
    "If no match is found, output an empty JSON object. Provide your output as JSON only.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from typing import List, Tuple\n",
    "\n",
    "def concatenate_reviews(reviews: List[str], review_type: str) -> str:\n",
    "    \"\"\"\n",
    "    Concatenates a list of reviews into a single string, with each review prefixed by its type and index.\n",
    "\n",
    "    Args:\n",
    "        reviews (List[str]): A list of review strings.\n",
    "        review_type (str): A character ('A' or 'B') indicating the type of the reviews.\n",
    "\n",
    "    Returns:\n",
    "        str: A single string containing all reviews, each prefixed by its type and index.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the review type is not 'A' or 'B'.\n",
    "    \"\"\"\n",
    "    if review_type not in ['A', 'B']:\n",
    "        raise ValueError(\"type must be either 'A' or 'B'\")\n",
    "    \n",
    "    concatenate_msg = \"\"\n",
    "    for i, review in enumerate(reviews):\n",
    "        concatenate_msg += f\"{review_type}{i+1}: {review}\\n\"\n",
    "    \n",
    "    return concatenate_msg\n",
    "\n",
    "def does_key_match_pattern(key: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a given key matches the pattern 'A<integer>-B<integer>' or 'B<integer>-A<integer>'.\n",
    "\n",
    "    Args:\n",
    "        key (str): The key string to check.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the key matches the pattern, False otherwise.\n",
    "    \"\"\"\n",
    "    pattern = r'\\b(A\\d+-B\\d+|B\\d+-A\\d+)\\b$'\n",
    "    return bool(re.fullmatch(pattern, key))\n",
    "\n",
    "def count_hit(output_content: str) -> int:\n",
    "    \"\"\"\n",
    "    Counts the number of keys in a JSON string that match a specific pattern and have a 'similarity' score within a certain range.\n",
    "\n",
    "    Args:\n",
    "        output_content (str): A JSON string containing the output data.\n",
    "\n",
    "    Returns:\n",
    "        int: The count of keys that meet the criteria.\n",
    "    \"\"\"\n",
    "    content = json.loads(output_content)\n",
    "    hit_cnt = 0\n",
    "    for key in content.keys():\n",
    "        if does_key_match_pattern(key) and 'similarity' in content[key]:\n",
    "            score = int(content[key]['similarity'])\n",
    "            if 7 <= score <= 10:\n",
    "                hit_cnt += 1\n",
    "    return hit_cnt\n",
    "\n",
    "def calculate_hit_rate(human_review_messages: List[str], generated_review_messages: List[str]) -> Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Calculates various statistical measures based on the similarity scores between human and generated review messages.\n",
    "\n",
    "    Args:\n",
    "        human_review_messages (List[str]): A list of human-written review messages.\n",
    "        generated_review_messages (List[str]): A list of review messages generated by a model.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[float, float, float, float]: A tuple containing the hit rate, Szymkiewicz–Simpson coefficient, \n",
    "        Jaccard index, and Sørensen–Dice coefficient.\n",
    "    \"\"\"\n",
    "    reviews_A = concatenate_reviews(human_review_messages, \"A\")\n",
    "    reviews_B = concatenate_reviews(generated_review_messages, \"B\")\n",
    "    \n",
    "    system_prompt = system_prompt_template.format(Review_A=reviews_A, Review_B=reviews_B)\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4-0125-preview\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    hit_cnt = count_hit(completion.choices[0].message)\n",
    "    total_human_reviews = len(human_review_messages)\n",
    "    total_generated_reviews = len(generated_review_messages)\n",
    "\n",
    "    hit_rate = hit_cnt / total_human_reviews\n",
    "    szymkiewicz_simpson_coefficient = hit_cnt / min(total_human_reviews, total_generated_reviews)\n",
    "    jaccard_index = hit_cnt / (total_human_reviews + total_generated_reviews)\n",
    "    sorensen_dice_coefficient = 2 * hit_cnt / (total_human_reviews + total_generated_reviews)\n",
    "\n",
    "    return hit_rate, szymkiewicz_simpson_coefficient, jaccard_index, sorensen_dice_coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hit Rate: 0.5\n",
      "Szymkiewicz–Simpson Overlap Coefficient: 1.0\n",
      "Jaccard Index: 0.3333333333333333\n",
      "Sørensen–Dice Coefficient: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "hit_rate, szymkiewicz_simpson_coefficient, jaccard_index, sorensen_dice_coefficient = calculate_hit_rate(human_review_messages, generated_review_messages)\n",
    "print(\"Hit Rate:\", hit_rate)\n",
    "print(\"Szymkiewicz–Simpson Overlap Coefficient:\", szymkiewicz_simpson_coefficient)\n",
    "print(\"Jaccard Index:\", jaccard_index)\n",
    "print(\"Sørensen–Dice Coefficient:\", sorensen_dice_coefficient)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
