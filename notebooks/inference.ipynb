{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Purpose\n",
    "\n",
    "This notebook demonstrates how to generate reviews for a given paper using various language models. We utilize open-source Large Language Models (LLMs), our fine-tuned Mistral 7b model, and GPT to produce these reviews. The process includes the following steps:\n",
    "\n",
    "1. **Model Selection:** Overview of the models used, including open-source LLMs, Mistral 7b, and GPT.\n",
    "2. **Review Generation:** Description of how each model generates reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipdf\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from prompts import SYSTEM_PROMPT\n",
    "from pdf_parser import parse_pdf_content, parse_pdf_abstract, generate_input \n",
    "from model_review import extract_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf = scipdf.parse_pdf_to_dict('../demo/Transformers.pdf') # change the path to the pdf file you want to parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import AutoPeftModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 24 13:12:09 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.161.08             Driver Version: 535.161.08   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L4                      On  | 00000000:38:00.0 Off |                    0 |\n",
      "| N/A   49C    P0              28W /  72W |  20956MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA L4                      On  | 00000000:3A:00.0 Off |                    0 |\n",
      "| N/A   43C    P0              27W /  72W |    346MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA L4                      On  | 00000000:3C:00.0 Off |                    0 |\n",
      "| N/A   49C    P0              29W /  72W |    346MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA L4                      On  | 00000000:3E:00.0 Off |                    0 |\n",
      "| N/A   46C    P0              29W /  72W |    346MiB / 23034MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A     12866      C   java                                          0MiB |\n",
      "|    1   N/A  N/A     12866      C   java                                          0MiB |\n",
      "|    2   N/A  N/A     12866      C   java                                          0MiB |\n",
      "|    3   N/A  N/A     12866      C   java                                          0MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mistral 7b Model\n",
    "In this section, we utilize the Mistral 7b model to generate reviews for the specified paper. This model is selected for its specific capabilities and performance characteristics in review generation tasks. If needed, you can replace the Mistral 7b model with any other model of your choice to compare different outcomes or performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcdfebd60a36415aac1569b7b39359bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# model_id = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id,\n",
    "#     device_map=\"auto\",\n",
    "#     torch_dtype=torch.bfloat16,\n",
    "#     quantization_config=bnb_config\n",
    "# )\n",
    "# tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Fine-tuned Model\n",
    "You can also use our fine-tuned Mistral 7b model to generate reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load directly from the huggingface model hub\n",
    "model_id = \"travis0103/mistral_7b_paper_review_lora\"\n",
    "model = AutoPeftModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=device,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Reviews Using Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_content = parse_pdf_abstract(pdf)\n",
    "abstract_input = generate_input(abstract_content)\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": SYSTEM_PROMPT + '\\n\\n' + abstract_content},\n",
    "]\n",
    "encoded_input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "generated_ids = model.generate(encoded_input, max_new_tokens=1024, do_sample=True, pad_token_id=tokenizer.eos_token_id, eos_token_id=tokenizer.eos_token_id)\n",
    "decoded_output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extract_output(decoded_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPT Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "client = openai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_content = parse_pdf_content(pdf)\n",
    "full_input = generate_input(full_content)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "      {\"role\": \"user\", \"content\": full_input},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Significance and novelty]\n",
      "The \"Transformer\" paper introduces a novel architecture that challenges the dominant sequence transduction models which rely on recurrent neural networks (RNNs) and convolutional neural networks (CNNs). The main contributions and innovations of this work include the introduction of an architecture based entirely on attention mechanisms, eliminating the need for recurrence or convolution within the model, which represents a significant shift in the approach to machine translation and potentially other sequence modeling tasks.\n",
      "\n",
      "- 'Elimination of recurrence and convolution': The proposed Transformer model is novel in its exclusive reliance on attention mechanisms, avoiding traditional RNNs and CNNs which can be computationally expensive and slow due to their inherent sequential nature.\n",
      "- 'Optimization for Parallelization': The Transformer is designed to maximize parallel computational capabilities, reducing training time significantly compared to existing models.\n",
      "- 'Introduction of Multi-Head Attention': This paper is the first to implement Multi-Head Attention, allowing the model to jointly process information from different representation subspaces at different positions, which enhances the model's ability to capture complex dependencies in the data.\n",
      "\n",
      "[Potential reasons for acceptance]\n",
      "The Transformer model demonstrates state-of-the-art performance on standard machine translation tasks, significant improvements in training efficiency, and detailed experimental analysis, all of which strongly support the acceptance of this paper.\n",
      "\n",
      "- 'Superior Performance': The model outperforms all previous models, including ensembles, on the WMT 2014 English-to-German and English-to-French translation tasks, with substantial improvements in BLEU scores.\n",
      "- 'Efficiency in Training': It is noted that the Transformer requires substantially less time to train—up to 3.5 days on eight GPUs, making it more practical and accessible for use in different settings.\n",
      "- 'Extensive Experimental Validation': The authors provide a comprehensive set of experiments and ablations that demonstrate the effectiveness of different components of the Transformer, such as variations in the number of attention heads and the use of sinusoidal vs. learned positional encodings.\n",
      "\n",
      "[Potential reasons for rejection]\n",
      "Despite its strengths, there are a few aspects that might concern reviewers, mainly related to the generalizability and the computational demand of the model.\n",
      "\n",
      "- 'Heavy Computational Resources': While more efficient than some prior models, the Transformer still requires significant computational power (e.g., multiple GPUs), which might not be feasible for all research or practical applications.\n",
      "- 'Limited Evaluation Scope': The application of the Transformer is demonstrated primarily on machine translation tasks. Its performance and utility on other NLP tasks or in non-NLP domains remain unexplored in this paper.\n",
      "- 'Potential Overfitting in Smaller Datasets': The model's complexity and capacity might lead to overfitting when applied to smaller datasets or less resource-rich languages, a point that isn't addressed thoroughly.\n",
      "\n",
      "[Suggestions for improvement]\n",
      "To strengthen the paper further, the authors might consider addressing the generalizability of the Transformer model and experiment with computational optimizations.\n",
      "\n",
      "- 'Broader Evaluation on Various Tasks': Expanding the evaluation to include a variety of other tasks such as summarization, text generation, and beyond NLP tasks can demonstrate the adaptability and broad utility of the Transformer model.\n",
      "- 'Investigation into Model Compression': Research into model compression techniques or more efficient attention mechanisms could help in reducing the computational load, making the Transformer feasible on lower-resource settings.\n",
      "- 'Experiments on Smaller and Diverse Datasets': Additional experiments on smaller or low-resource datasets could help understand the model's performance limitations and improvements in handling overfitting.\n",
      "\n",
      "By addressing these aspects, the paper could appeal to a broader audience and offer insights into the scalability and versatility of the Transformer architecture in various domains and applications.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "review_gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
