{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip -q install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oi5bJvhn8_iw",
        "outputId": "817fa3ce-80f9-4562-a438-79f45a48b99c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m262.4/262.4 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68M8X5v1_TNx",
        "outputId": "3cf138c4-0830-4795-bc60-2a15283fa9b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m810.5/810.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m54.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m269.1/269.1 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.6/71.6 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.0/53.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "%set_env OPENAI_API_KEY="
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_p1_qyCU9D2K",
        "outputId": "eb312673-6049-418d-cccc-0ecf3491a247"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "env: OPENAI_API_KEY=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from openai import OpenAI\n",
        "\n",
        "from langchain.prompts.prompt import PromptTemplate"
      ],
      "metadata": {
        "id": "3bpxmjOe9FgS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZV4Be8Y4-JBS",
        "outputId": "d8f159a8-029e-470c-ed1e-3351398dc76e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "toy_data = pd.read_csv(\"/content/drive/My Drive//CIS6200/toy_dataset.csv\")\n",
        "toy_data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "mcRvrG_E-MKM",
        "outputId": "bed11ed3-1774-4bef-baa9-a7301cd2b683"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0            Pre-Training by Completing Point Clouds   \n",
              "1            Pre-Training by Completing Point Clouds   \n",
              "2            Pre-Training by Completing Point Clouds   \n",
              "3            Pre-Training by Completing Point Clouds   \n",
              "4  GamePad: A Learning Environment for Theorem Pr...   \n",
              "\n",
              "                                         url  \\\n",
              "0  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "1  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "2  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "3  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "4   https://openreview.net/pdf?id=r1xwKoR9Y7   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  There has recently been a flurry of exciting a...   \n",
              "1  There has recently been a flurry of exciting a...   \n",
              "2  There has recently been a flurry of exciting a...   \n",
              "3  There has recently been a flurry of exciting a...   \n",
              "4  In this paper, we introduce a system called Ga...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "1  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "2  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "3  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "4  Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya ...   \n",
              "\n",
              "                                        review_title  \\\n",
              "0                                         A good one   \n",
              "1  paper shows promising results using point clou...   \n",
              "2              Limited novelty and weak improvements   \n",
              "3  Since the idea itself is simple enough, the re...   \n",
              "4  An intriguing integration of ML and automated ...   \n",
              "\n",
              "                                              review  \\\n",
              "0  This paper proposes a better pre-trained prior...   \n",
              "1  The paper considers the problem of training ne...   \n",
              "2  The authors propose completing an occluded poi...   \n",
              "3  The idea of this paper is simple but fascinati...   \n",
              "4  Summary: This paper mixes automated theorem pr...   \n",
              "\n",
              "                                     rating  \n",
              "0                     7: Good paper, accept  \n",
              "1                     7: Good paper, accept  \n",
              "2     4: Ok but not good enough - rejection  \n",
              "3  5: Marginally below acceptance threshold  \n",
              "4                     7: Good paper, accept  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-34ae8212-7250-4d41-a82b-89a5e8f9e728\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>review_title</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>A good one</td>\n",
              "      <td>This paper proposes a better pre-trained prior...</td>\n",
              "      <td>7: Good paper, accept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>paper shows promising results using point clou...</td>\n",
              "      <td>The paper considers the problem of training ne...</td>\n",
              "      <td>7: Good paper, accept</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>Limited novelty and weak improvements</td>\n",
              "      <td>The authors propose completing an occluded poi...</td>\n",
              "      <td>4: Ok but not good enough - rejection</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>Since the idea itself is simple enough, the re...</td>\n",
              "      <td>The idea of this paper is simple but fascinati...</td>\n",
              "      <td>5: Marginally below acceptance threshold</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GamePad: A Learning Environment for Theorem Pr...</td>\n",
              "      <td>https://openreview.net/pdf?id=r1xwKoR9Y7</td>\n",
              "      <td>In this paper, we introduce a system called Ga...</td>\n",
              "      <td>Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya ...</td>\n",
              "      <td>An intriguing integration of ML and automated ...</td>\n",
              "      <td>Summary: This paper mixes automated theorem pr...</td>\n",
              "      <td>7: Good paper, accept</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-34ae8212-7250-4d41-a82b-89a5e8f9e728')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-34ae8212-7250-4d41-a82b-89a5e8f9e728 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-34ae8212-7250-4d41-a82b-89a5e8f9e728');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-cbb40134-aea7-4634-8de9-6798798b1e2c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cbb40134-aea7-4634-8de9-6798798b1e2c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-cbb40134-aea7-4634-8de9-6798798b1e2c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "toy_data",
              "summary": "{\n  \"name\": \"toy_data\",\n  \"rows\": 22,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Pre-Training by Completing Point Clouds\",\n          \"GamePad: A Learning Environment for Theorem Proving\",\n          \"C-Learning: Learning to Achieve Goals via Recursive Classification\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"https://openreview.net/pdf?id=jPSYH47QSZL\",\n          \"https://openreview.net/pdf?id=r1xwKoR9Y7\",\n          \"https://openreview.net/pdf?id=tc5qisoB-C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.\",\n          \"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.\",\n          \"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,Matt Kusner\",\n          \"Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya Sutskever\",\n          \"Benjamin Eysenbach,Ruslan Salakhutdinov,Sergey Levine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"A good one\",\n          \"Official Blind Review #5\",\n          \"Interesting and relevant to UOM paper but relevance and connection is not discussed. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"This paper proposes a better pre-trained prior for a variety of downstream applications in point cloud analysis. The workflow of the pre-training mechanism is to first 1) generate occluded points that result from view occlusion and then 2) optimize the encoder to learn how to complete the occluded points from the partial point cloud. In downstream applications, the obtained encoder will be used as the initial weights in the network training. Empirical experiments have shown that such a pre-train mechanism can improve initialization over prior baselines and benefit a variety of tasks even with a large domain gap.\\r\\n\\r\\nPros:\\r\\n1. The experimental results have shown a steady improvement in performance by using the proposed pre-training approach in different encoder architectures and different downstream applications. That provides strong support for validating the effectiveness of the proposed approach.\\r\\n2. I also like the result that the initialization is only pre-trained on the occlusions generated from the ModelNet40 but still work in another dataset. And yet, the pre-training is done in a self-supervised manner. This is a great plus for this approach as it indicates that it could be a general-purpose booster for a wide range of applications without spending too much effort in collecting special-purpose dataset for pre-training. \\r\\n3. The paper is well written and presented.\\r\\n\\r\\nCons:\\r\\n1. The improvement, as shown in the statistics, is very incremental in most cases. I understand it is difficult to achieve better results on well-established benchmarks, but it somehow indicates the improvement is limited.\\r\\n2. Though the paper already stated some nice explanation of the idea behind this approach, I would appreciate it if a more in-depth analysis of why such a pre-training mechanism could work is provided. Specifically, I would like more analysis of why such a pre-training method can adapt to different datasets? What are the common features that OcCo captures across different datasets? \\r\\nSome visualization similar to Figure 3 would be helpful.\\r\\n\\r\\n---- Final Rating ----\\r\\n\\r\\nThe authors' response has resolved my concerns. I would keep my positive rating.\",\n          \"Summary:\\r\\n\\r\\nThis paper introduces PERIL, a meta RL method that combines demonstration trajectories and trajectories collected by the policy, in order to adapt to a new task. To this end, the authors combine ideas from metaRL (specifically from PEARL (Rakelly et al. 2019) and Humplik et al (2019)) where a set encoder is used to encode trajectories to a latent vector describing the task, with imitation learning techniques by (a) training this encoder also with demonstrations (b) initialising the latent vector at test time by feeding demonstrations through the encoder, and (c) having additional losses inspired by metaIL techniques. The motivation is that using demonstrations allows us to learn tasks that are difficult otherwise, for example because the rewards (at test time) are sparse. \\r\\n\\r\\nOverall impression:\\r\\n\\r\\nI like the idea of using demonstrations for metaRL when tasks are sparse. Many metaRL methods do not work well in sparse reward tasks, and using expert demonstrations is a nice way of guiding the agent towards behaviour that can solve the task. Empirically, the proposed method PERIL outperforms the baselines PEARL and MetaIL, so that is promising. The authors provide analysis of the latent space which nicely illustrates what the method has learned. However, PERIL is quite complex since it consists of many different parts and loss terms (six if I counted correctly), and it needs demonstrations + interactions + (sparse) reward signals at test time. I found it hard to keep track of everything and make sense of how these parts fit together. From both the text and the empirical results, it is not clear to me why all the parts are necessary / what they do, and I am left wondering if a simpler approach would work as well. The notation and mathematical formulation in the paper is not polished enough (there are inconsistencies, variable name clashes, some parts of the objective function not properly introduced and explained) which added to my confusion. Therefore, even though the idea seems promising, I think the paper is not quite ready for publication.\\r\\n\\r\\nQuestions:\\r\\n- In the introduction you say that MetaIL methods have the drawback that \\\"after adaptation, they cannot continue to improve the policy in the way that RL methods can\\\". You say that you method PERIL \\\"allows for continual policy improvement through further exploration of the task\\\". I have a few questions about this.\\r\\n - Since only the latent embedding is updated, doesn't PERIL also suffer from the fact that the policy cannot be improved in the way that RL methods can (but instead, all adaptation is that within the limits of task inference)? \\r\\n - Why is additional exploration at test time even necessary, if we have expert rollouts and the policy itself isn't actually updated (the only thing that's adapted at test time is the latent embedding)? If all the demos + trajectories are used for is task inference, then shouldn't the expert demos always be sufficient?\\r\\n - You motivate your approach by saying that at test time, it is useful if the expert does not have to provide a shaped reward. However, you do make use of a shaped reward during training - this is a limitation that should be discussed in the paper. In addition, you still need (sparse) rewards at test time. Are those really necessary, given that you have a demonstration of the task? Did you test PERIL without those sparse reward inputs to the encoder?\\r\\n- Table 1, how was the agent trained? Was it with number of adaptation trajectories k>0? If so, what if you would train the agent with k=0? On the other hand, can you get good zero-shot adaptation performance by just increasing the number of demonstrations? \\r\\n- You say $d_\\\\lambda$ is a VAE, but if I understand your setup correctly then $d_\\\\lambda$ is only the decoder of a VAE right? And Eq 8 is the reconstruction loss? Which also means it's not technically a VAE, because in encodes and decodes different things (encodes trajectories, decodes task descriptors - Humplik et al. 2019 describe this as an information bottleneck). Shouldn't there also be a KL term somewhere here?\\r\\n- Where is $L_{bc}$ used? It's not part of Eq (7), but I also can't find it anywhere else except in Algo 1 and Fig 2. And what about $L_{mi}$, it's only in Algo 1 but nowhere else? Fig 2 has $L_{KL}$, where is that from? It would really help my understanding of piecing everything together if there was one single equation somewhere, that includes all loss terms. For each loss term, I as the reader want to clearly understand where it comes from, and why it is necessary (see suggestions for additional baselines/ablations below).\\r\\n\\r\\nSuggestions / Feedback:\\r\\n- The problem formulation and the proposed solution don't match. In your problem setting you say you're in a general POMDP where the true state may be partially observed, but in your algorithm you rely on the fact that it's a POMDP only w.r.t. the task (i.e., reward and transition function) and *not* w.r.t. the environment state. That's an important difference! To explain that in more detail: in the introduction and problem setting you say $z$ models the true underlying state $s$ which can change at any moment: your transition function is $p(o',z'|o,a,z)$ where $o$ are observations. However, the entire formulation in your algorithm relies on the fact that $z$ does _not_ change over time, but instead describes a fixed task. That's also what PEARL does, which is what your formulation is based on. I think there's two ways to resolve this: (A) Either change the problem setting such that $z$ is fixed throughout time and define the transition function as $p(s'|s,a,z)$ where the environment state $s$ is now fully observable, or (B) change the algorithm to actually model a belief over a latent $z$ that can change over time. Option (A) is probably an easier fix, but then you might also have to change some of the environments (if I understand correctly, in Key2D the state of the handle is unobserved and can change the unobserved environment state).\\r\\n- Section 3.1, I would add explicitly what the objective of the policy is (both in writing and in a mathematical expression). You aim to maximise the return of a policy that conditions on $K_d$ demonstrations, and which has interacted with the environment for $K_r$ rollouts (changing the notation here to make the distinction clear). From there it is easy for the reader to see what happens if you set $K_d=0$ (you get something more similar to PEARL), and what happens when you set $K_r=0$ (which is the zero-shot case). It's good to contrast this for the reader, and explain / show empirically why and when $K_d>0$ and $K_r>0$ is necessary. (See my comment on baselines below.) \\r\\n- To understand PERIL better, I would suggest to add a few baselines. \\r\\n - PEARL with a pre-initialised buffer that contains the demonstrations. The encoder and policy will be trained as normal, but there's additional data coming from the buffer that contains expert trajectories. Since PEARL uses an off-policy algorithm it is possible to train the policy with this data. I think this is an important comparison, because it's a very simple way to incorporate demonstrations into PEARL and it would be good to understand if/when/why this works/doesn't work.\\r\\n - In addition to the above, use the demonstrations at test time to initialise the context in PEARL. This is very close to the setting in PERIL, except that some parts of the objective function are missing ($L_{info}$, $L_{aux}$, $L_{bc}$, $L_{mi}$ - I think).  This would give insight into whether those additional losses are truly necessary (currently you only have ablations on $L_{aux}$).\\r\\n - Zero-Shot PERIL. There is some analysis of this in Table 1, but I think it would still be helpful to add this baseline. Does it work well for within-task-distribution adaptation (Fig 4) and not so well for settings that require more generalisation (Fig 5)? What if we just throw in more demonstrations, is that sufficient to do zero-shot adaptation or do we really need the policy rollouts? I think this is a central question that should be very clearly answered in the paper. Table 1 is a good start but this analysis can be expanded.\\r\\n - Humplik et al. (2019), but with additional demonstration data to train the encoder/decoder. Again, this is the simplest way to incorporate the demonstration data into this method without explicitly making use of it at test time. This comparison would tell us something about why the demonstrations are necessary - are they necessary during training but not at test time, or the other way around, or are they necessary both during training and testing?\\r\\n - Not sure I got everything, there's still $L_{bc}$ and $L_{mi}$ which I'm not entirely sure where they come from and if they are necessary. But basically, I think it's really important to analyse which parts are necessary - and make the method as simple as possible if you find some parts are not necessary.\\r\\n\\r\\nSmaller comments (didn't influence my score):\\r\\n- There's a clash between using the variable k/K for the demonstrations (e.g., Sec 3.1 \\\"primal inference\\\", Sec 4.1 first sentence, Fig 7), and for the number of policy rollouts (e.g., algorithm 1 line 5, Table 1). This is confusing, so I strongly suggest using two distinct variable names (something like $k_d$ and $k_r$ also works).\\r\\n- Similarly it would help if you use two separate notations for the trajectories $\\\\tau$ that come from the demonstrations, and the ones that come from the policy. Throughout Section 3 I don't always understand which one of the two you are talking about.\\r\\n- Your references need fixing. Some of them are without a year, and some say technical report even though they were published at a conference (e.g. Finn et al., Rakelly et al.). Your \\\"Wang\\\" reference for RL2 also seems wrong (first sentence in related work)? It should be Jane Wang et al. \\\"Learning to reinforcement learn\\\". The way I always get my bibtex entries is via scholar.google.com: search for the paper there; click on the \\\"cite\\\" button under it and then \\\"BibTeX\\\" (Double check whether the paper was published somewhere though, google scholar often only puts the arXiv link then actually the paper was published somewhere. Sometimes the authors also put the correct bibtex comment on their homepage/github with the code).\\r\\n- Fig 1 left, there's a typo: \\\"learn to lean\\\" -> \\\"learn to learn\\\"\\r\\n- For your experiments, I would call PERIL-A only \\\"PERIL\\\" (since this is your full method, including all losses), and then call the *ablations* different, so for example PERIL-noAux when you remove the auxiliary loss.\\r\\n- All figures should have some form of indication of the error/std/confidence interval (using shaded regions around the mean for example).\\r\\n- Sec 4.2, explain what the multi-task family setting is and why it is challenging. \\r\\n\",\n          \"This paper attempts to explain the generalization in a deep network using separability of the features at various layers. The paper is motivated via a perceptron classifying a Gaussian distribution.\\r\\n\\r\\nOverall, I think the experimental evidence in the paper is not rigorous enough to draw the conclusions that the authors wish to draw. The paper will benefit from more careful experiments, I am listing down some suggestions below.\\r\\n\\r\\nWhat is the connection between the result in Section 2 with those in Section 3? What is the variance on the plots in Fig. 2?\\r\\nOn lines 103-110, max-pooling should reduce variability because it collapses spatial variability in the input. I think there is confounding occurring here due to the presence of batch-norm which (at least in theory) de-correlates the features. De-correlated features are easier to separate using a linear classifier. Further, the features of well-performing deep networks are not easily separable, e.g., https://arxiv.org/abs/1902.01889.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"4: Ok but not good enough - rejection\",\n          \"3: Clear rejection\",\n          \"7: Good paper, accept\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = OpenAI(\n",
        "    api_key=os.environ.get(\"OPENAI_API_KEY\"),\n",
        ")\n",
        "def send_prompt(prompt):\n",
        "  response = client.chat.completions.create(\n",
        "      messages=[\n",
        "          {\n",
        "              \"role\": \"user\",\n",
        "              \"content\": prompt,\n",
        "          }\n",
        "      ],\n",
        "      # model=\"gpt-4-0125-preview\",\n",
        "      model=\"gpt-3.5-turbo\",\n",
        "      temperature=1,\n",
        "  )\n",
        "  return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "GCFzb1N49H8r"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "gZt9BEYY87wm"
      },
      "outputs": [],
      "source": [
        "template = \"\"\"\n",
        "You are a professional machine learning conference reviewer who reviews a given paper and considers 4 criteria: **importance and novelty**, **potential reasons for acceptance**, **potential reasons for rejection**, and **suggestions for improvement**.\n",
        "\n",
        "You just need to use the following JSON format for output, but don't output opinions that don't exist in the original reviews. if you're not sure, return an empty dict:\n",
        "\n",
        "{\n",
        "'Significance and novelty': List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
        "\n",
        "'Potential reasons for acceptance': List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
        "\n",
        "\"Potential reasons for rejection\": List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
        "\n",
        "'Suggestions for improvement': List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
        "}\n",
        "\n",
        "The given paper is as follows:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# PROMPT = PromptTemplate(input_variables=[\"input\"], template=template)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample\n",
        "\n",
        "paper_abstract = toy_data[\"abstract\"][0]\n",
        "paper_abstract"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "yXxE_pFx_q_7",
        "outputId": "730c1788-409d-4e04-a5f3-48372e2fd483"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = template + paper_abstract\n",
        "print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQR0qqqDAOjm",
        "outputId": "91cef950-d6c6-42b8-a48f-ae377a4fc062"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "You are a professional machine learning conference reviewer who reviews a given paper and considers 4 criteria: **importance and novelty**, **potential reasons for acceptance**, **potential reasons for rejection**, and **suggestions for improvement**.\n",
            "\n",
            "You just need to use the following JSON format for output, but don't output opinions that don't exist in the original reviews. if you're not sure, return an empty dict:\n",
            "\n",
            "{\n",
            "'Significance and novelty': List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
            "\n",
            "'Potential reasons for acceptance': List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
            "\n",
            "\"Potential reasons for rejection\": List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
            "\n",
            "'Suggestions for improvement': List multiple items by using Dict, The key is a brief description of the item, and the value is a detailed description of the item.\n",
            "}\n",
            "\n",
            "The given paper is as follows:\n",
            "\n",
            "There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = send_prompt(prompt)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3GZUB1t9NJd",
        "outputId": "9487e060-e807-490d-b9ae-814759094054"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "\"Significance and novelty\": {\n",
            "\"Mask-based pre-training\": \"The use of mask-based pre-training in the context of point clouds is a novel approach that draws inspiration from the natural language processing community.\",\n",
            "\"Occlusion Completion (OcCo)\": \"The proposal of OcCo as a method for pre-training on point clouds through completion of occluded points is a unique and innovative contribution to the field.\"\n",
            "},\n",
            "\"Potential reasons for acceptance\": {\n",
            "\"Increased semantic understanding\": \"The paper demonstrates that the proposed method improves semantic understanding of point clouds compared to previous techniques.\",\n",
            "\"Generalization on downstream tasks\": \"The method shows improved generalization on downstream tasks, which is a crucial aspect of any machine learning model.\"\n",
            "},\n",
            "\"Potential reasons for rejection\": {},\n",
            "\"Suggestions for improvement\": {\n",
            "\"Experimental validation\": \"Providing more thorough experimental validation on a wider range of datasets could strengthen the paper's claims and results.\",\n",
            "\"Comparison with existing methods\": \"Including a more detailed comparison with existing methods in the field would help in understanding the effectiveness and uniqueness of the proposed approach.\"\n",
            "}\n",
            "}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_gpt_response(paper_abstract):\n",
        "  prompt = template + paper_abstract\n",
        "  response = send_prompt(prompt)\n",
        "  return response\n",
        "\n",
        "toy_data[\"gpt-3.5-turbo-reviews\"] = toy_data[\"abstract\"].apply(get_gpt_response)"
      ],
      "metadata": {
        "id": "8_wFHbCiA6ws"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "toy_data.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "Woy4J5hpCQqH",
        "outputId": "5b4aa6f0-00a0-4ec0-cf5d-c03109a1811c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                               title  \\\n",
              "0            Pre-Training by Completing Point Clouds   \n",
              "1            Pre-Training by Completing Point Clouds   \n",
              "2            Pre-Training by Completing Point Clouds   \n",
              "3            Pre-Training by Completing Point Clouds   \n",
              "4  GamePad: A Learning Environment for Theorem Pr...   \n",
              "\n",
              "                                         url  \\\n",
              "0  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "1  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "2  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "3  https://openreview.net/pdf?id=jPSYH47QSZL   \n",
              "4   https://openreview.net/pdf?id=r1xwKoR9Y7   \n",
              "\n",
              "                                            abstract  \\\n",
              "0  There has recently been a flurry of exciting a...   \n",
              "1  There has recently been a flurry of exciting a...   \n",
              "2  There has recently been a flurry of exciting a...   \n",
              "3  There has recently been a flurry of exciting a...   \n",
              "4  In this paper, we introduce a system called Ga...   \n",
              "\n",
              "                                             authors  \\\n",
              "0  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "1  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "2  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "3  Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...   \n",
              "4  Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya ...   \n",
              "\n",
              "                                        review_title  \\\n",
              "0                                         A good one   \n",
              "1  paper shows promising results using point clou...   \n",
              "2              Limited novelty and weak improvements   \n",
              "3  Since the idea itself is simple enough, the re...   \n",
              "4  An intriguing integration of ML and automated ...   \n",
              "\n",
              "                                              review  \\\n",
              "0  This paper proposes a better pre-trained prior...   \n",
              "1  The paper considers the problem of training ne...   \n",
              "2  The authors propose completing an occluded poi...   \n",
              "3  The idea of this paper is simple but fascinati...   \n",
              "4  Summary: This paper mixes automated theorem pr...   \n",
              "\n",
              "                                     rating  \\\n",
              "0                     7: Good paper, accept   \n",
              "1                     7: Good paper, accept   \n",
              "2     4: Ok but not good enough - rejection   \n",
              "3  5: Marginally below acceptance threshold   \n",
              "4                     7: Good paper, accept   \n",
              "\n",
              "                               gpt-3.5-turbo-reviews  \n",
              "0  {\\n    \"Significance and novelty\": {\\n        ...  \n",
              "1  {\\n\"Significance and novelty\": {\\n    \"Use of ...  \n",
              "2  {\\n    \"Significance and novelty\": {\\n        ...  \n",
              "3  {\\n\"Significance and novelty\": {\\n    \"Mask-ba...  \n",
              "4  {\\n  \"Significance and novelty\": {\\n    \"Explo...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-2a7ab963-4a86-4314-9828-a73aab9d9d2e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>title</th>\n",
              "      <th>url</th>\n",
              "      <th>abstract</th>\n",
              "      <th>authors</th>\n",
              "      <th>review_title</th>\n",
              "      <th>review</th>\n",
              "      <th>rating</th>\n",
              "      <th>gpt-3.5-turbo-reviews</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>A good one</td>\n",
              "      <td>This paper proposes a better pre-trained prior...</td>\n",
              "      <td>7: Good paper, accept</td>\n",
              "      <td>{\\n    \"Significance and novelty\": {\\n        ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>paper shows promising results using point clou...</td>\n",
              "      <td>The paper considers the problem of training ne...</td>\n",
              "      <td>7: Good paper, accept</td>\n",
              "      <td>{\\n\"Significance and novelty\": {\\n    \"Use of ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>Limited novelty and weak improvements</td>\n",
              "      <td>The authors propose completing an occluded poi...</td>\n",
              "      <td>4: Ok but not good enough - rejection</td>\n",
              "      <td>{\\n    \"Significance and novelty\": {\\n        ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Pre-Training by Completing Point Clouds</td>\n",
              "      <td>https://openreview.net/pdf?id=jPSYH47QSZL</td>\n",
              "      <td>There has recently been a flurry of exciting a...</td>\n",
              "      <td>Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,M...</td>\n",
              "      <td>Since the idea itself is simple enough, the re...</td>\n",
              "      <td>The idea of this paper is simple but fascinati...</td>\n",
              "      <td>5: Marginally below acceptance threshold</td>\n",
              "      <td>{\\n\"Significance and novelty\": {\\n    \"Mask-ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GamePad: A Learning Environment for Theorem Pr...</td>\n",
              "      <td>https://openreview.net/pdf?id=r1xwKoR9Y7</td>\n",
              "      <td>In this paper, we introduce a system called Ga...</td>\n",
              "      <td>Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya ...</td>\n",
              "      <td>An intriguing integration of ML and automated ...</td>\n",
              "      <td>Summary: This paper mixes automated theorem pr...</td>\n",
              "      <td>7: Good paper, accept</td>\n",
              "      <td>{\\n  \"Significance and novelty\": {\\n    \"Explo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a7ab963-4a86-4314-9828-a73aab9d9d2e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2a7ab963-4a86-4314-9828-a73aab9d9d2e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2a7ab963-4a86-4314-9828-a73aab9d9d2e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6ad7279b-4cfb-4377-87f7-17e08cd8141c\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6ad7279b-4cfb-4377-87f7-17e08cd8141c')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6ad7279b-4cfb-4377-87f7-17e08cd8141c button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "toy_data",
              "summary": "{\n  \"name\": \"toy_data\",\n  \"rows\": 22,\n  \"fields\": [\n    {\n      \"column\": \"title\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Pre-Training by Completing Point Clouds\",\n          \"GamePad: A Learning Environment for Theorem Proving\",\n          \"C-Learning: Learning to Achieve Goals via Recursive Classification\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"url\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"https://openreview.net/pdf?id=jPSYH47QSZL\",\n          \"https://openreview.net/pdf?id=r1xwKoR9Y7\",\n          \"https://openreview.net/pdf?id=tc5qisoB-C\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"abstract\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.\",\n          \"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.\",\n          \"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"authors\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 7,\n        \"samples\": [\n          \"Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,Matt Kusner\",\n          \"Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya Sutskever\",\n          \"Benjamin Eysenbach,Ruslan Salakhutdinov,Sergey Levine\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review_title\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 20,\n        \"samples\": [\n          \"A good one\",\n          \"Official Blind Review #5\",\n          \"Interesting and relevant to UOM paper but relevance and connection is not discussed. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"review\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"This paper proposes a better pre-trained prior for a variety of downstream applications in point cloud analysis. The workflow of the pre-training mechanism is to first 1) generate occluded points that result from view occlusion and then 2) optimize the encoder to learn how to complete the occluded points from the partial point cloud. In downstream applications, the obtained encoder will be used as the initial weights in the network training. Empirical experiments have shown that such a pre-train mechanism can improve initialization over prior baselines and benefit a variety of tasks even with a large domain gap.\\r\\n\\r\\nPros:\\r\\n1. The experimental results have shown a steady improvement in performance by using the proposed pre-training approach in different encoder architectures and different downstream applications. That provides strong support for validating the effectiveness of the proposed approach.\\r\\n2. I also like the result that the initialization is only pre-trained on the occlusions generated from the ModelNet40 but still work in another dataset. And yet, the pre-training is done in a self-supervised manner. This is a great plus for this approach as it indicates that it could be a general-purpose booster for a wide range of applications without spending too much effort in collecting special-purpose dataset for pre-training. \\r\\n3. The paper is well written and presented.\\r\\n\\r\\nCons:\\r\\n1. The improvement, as shown in the statistics, is very incremental in most cases. I understand it is difficult to achieve better results on well-established benchmarks, but it somehow indicates the improvement is limited.\\r\\n2. Though the paper already stated some nice explanation of the idea behind this approach, I would appreciate it if a more in-depth analysis of why such a pre-training mechanism could work is provided. Specifically, I would like more analysis of why such a pre-training method can adapt to different datasets? What are the common features that OcCo captures across different datasets? \\r\\nSome visualization similar to Figure 3 would be helpful.\\r\\n\\r\\n---- Final Rating ----\\r\\n\\r\\nThe authors' response has resolved my concerns. I would keep my positive rating.\",\n          \"Summary:\\r\\n\\r\\nThis paper introduces PERIL, a meta RL method that combines demonstration trajectories and trajectories collected by the policy, in order to adapt to a new task. To this end, the authors combine ideas from metaRL (specifically from PEARL (Rakelly et al. 2019) and Humplik et al (2019)) where a set encoder is used to encode trajectories to a latent vector describing the task, with imitation learning techniques by (a) training this encoder also with demonstrations (b) initialising the latent vector at test time by feeding demonstrations through the encoder, and (c) having additional losses inspired by metaIL techniques. The motivation is that using demonstrations allows us to learn tasks that are difficult otherwise, for example because the rewards (at test time) are sparse. \\r\\n\\r\\nOverall impression:\\r\\n\\r\\nI like the idea of using demonstrations for metaRL when tasks are sparse. Many metaRL methods do not work well in sparse reward tasks, and using expert demonstrations is a nice way of guiding the agent towards behaviour that can solve the task. Empirically, the proposed method PERIL outperforms the baselines PEARL and MetaIL, so that is promising. The authors provide analysis of the latent space which nicely illustrates what the method has learned. However, PERIL is quite complex since it consists of many different parts and loss terms (six if I counted correctly), and it needs demonstrations + interactions + (sparse) reward signals at test time. I found it hard to keep track of everything and make sense of how these parts fit together. From both the text and the empirical results, it is not clear to me why all the parts are necessary / what they do, and I am left wondering if a simpler approach would work as well. The notation and mathematical formulation in the paper is not polished enough (there are inconsistencies, variable name clashes, some parts of the objective function not properly introduced and explained) which added to my confusion. Therefore, even though the idea seems promising, I think the paper is not quite ready for publication.\\r\\n\\r\\nQuestions:\\r\\n- In the introduction you say that MetaIL methods have the drawback that \\\"after adaptation, they cannot continue to improve the policy in the way that RL methods can\\\". You say that you method PERIL \\\"allows for continual policy improvement through further exploration of the task\\\". I have a few questions about this.\\r\\n - Since only the latent embedding is updated, doesn't PERIL also suffer from the fact that the policy cannot be improved in the way that RL methods can (but instead, all adaptation is that within the limits of task inference)? \\r\\n - Why is additional exploration at test time even necessary, if we have expert rollouts and the policy itself isn't actually updated (the only thing that's adapted at test time is the latent embedding)? If all the demos + trajectories are used for is task inference, then shouldn't the expert demos always be sufficient?\\r\\n - You motivate your approach by saying that at test time, it is useful if the expert does not have to provide a shaped reward. However, you do make use of a shaped reward during training - this is a limitation that should be discussed in the paper. In addition, you still need (sparse) rewards at test time. Are those really necessary, given that you have a demonstration of the task? Did you test PERIL without those sparse reward inputs to the encoder?\\r\\n- Table 1, how was the agent trained? Was it with number of adaptation trajectories k>0? If so, what if you would train the agent with k=0? On the other hand, can you get good zero-shot adaptation performance by just increasing the number of demonstrations? \\r\\n- You say $d_\\\\lambda$ is a VAE, but if I understand your setup correctly then $d_\\\\lambda$ is only the decoder of a VAE right? And Eq 8 is the reconstruction loss? Which also means it's not technically a VAE, because in encodes and decodes different things (encodes trajectories, decodes task descriptors - Humplik et al. 2019 describe this as an information bottleneck). Shouldn't there also be a KL term somewhere here?\\r\\n- Where is $L_{bc}$ used? It's not part of Eq (7), but I also can't find it anywhere else except in Algo 1 and Fig 2. And what about $L_{mi}$, it's only in Algo 1 but nowhere else? Fig 2 has $L_{KL}$, where is that from? It would really help my understanding of piecing everything together if there was one single equation somewhere, that includes all loss terms. For each loss term, I as the reader want to clearly understand where it comes from, and why it is necessary (see suggestions for additional baselines/ablations below).\\r\\n\\r\\nSuggestions / Feedback:\\r\\n- The problem formulation and the proposed solution don't match. In your problem setting you say you're in a general POMDP where the true state may be partially observed, but in your algorithm you rely on the fact that it's a POMDP only w.r.t. the task (i.e., reward and transition function) and *not* w.r.t. the environment state. That's an important difference! To explain that in more detail: in the introduction and problem setting you say $z$ models the true underlying state $s$ which can change at any moment: your transition function is $p(o',z'|o,a,z)$ where $o$ are observations. However, the entire formulation in your algorithm relies on the fact that $z$ does _not_ change over time, but instead describes a fixed task. That's also what PEARL does, which is what your formulation is based on. I think there's two ways to resolve this: (A) Either change the problem setting such that $z$ is fixed throughout time and define the transition function as $p(s'|s,a,z)$ where the environment state $s$ is now fully observable, or (B) change the algorithm to actually model a belief over a latent $z$ that can change over time. Option (A) is probably an easier fix, but then you might also have to change some of the environments (if I understand correctly, in Key2D the state of the handle is unobserved and can change the unobserved environment state).\\r\\n- Section 3.1, I would add explicitly what the objective of the policy is (both in writing and in a mathematical expression). You aim to maximise the return of a policy that conditions on $K_d$ demonstrations, and which has interacted with the environment for $K_r$ rollouts (changing the notation here to make the distinction clear). From there it is easy for the reader to see what happens if you set $K_d=0$ (you get something more similar to PEARL), and what happens when you set $K_r=0$ (which is the zero-shot case). It's good to contrast this for the reader, and explain / show empirically why and when $K_d>0$ and $K_r>0$ is necessary. (See my comment on baselines below.) \\r\\n- To understand PERIL better, I would suggest to add a few baselines. \\r\\n - PEARL with a pre-initialised buffer that contains the demonstrations. The encoder and policy will be trained as normal, but there's additional data coming from the buffer that contains expert trajectories. Since PEARL uses an off-policy algorithm it is possible to train the policy with this data. I think this is an important comparison, because it's a very simple way to incorporate demonstrations into PEARL and it would be good to understand if/when/why this works/doesn't work.\\r\\n - In addition to the above, use the demonstrations at test time to initialise the context in PEARL. This is very close to the setting in PERIL, except that some parts of the objective function are missing ($L_{info}$, $L_{aux}$, $L_{bc}$, $L_{mi}$ - I think).  This would give insight into whether those additional losses are truly necessary (currently you only have ablations on $L_{aux}$).\\r\\n - Zero-Shot PERIL. There is some analysis of this in Table 1, but I think it would still be helpful to add this baseline. Does it work well for within-task-distribution adaptation (Fig 4) and not so well for settings that require more generalisation (Fig 5)? What if we just throw in more demonstrations, is that sufficient to do zero-shot adaptation or do we really need the policy rollouts? I think this is a central question that should be very clearly answered in the paper. Table 1 is a good start but this analysis can be expanded.\\r\\n - Humplik et al. (2019), but with additional demonstration data to train the encoder/decoder. Again, this is the simplest way to incorporate the demonstration data into this method without explicitly making use of it at test time. This comparison would tell us something about why the demonstrations are necessary - are they necessary during training but not at test time, or the other way around, or are they necessary both during training and testing?\\r\\n - Not sure I got everything, there's still $L_{bc}$ and $L_{mi}$ which I'm not entirely sure where they come from and if they are necessary. But basically, I think it's really important to analyse which parts are necessary - and make the method as simple as possible if you find some parts are not necessary.\\r\\n\\r\\nSmaller comments (didn't influence my score):\\r\\n- There's a clash between using the variable k/K for the demonstrations (e.g., Sec 3.1 \\\"primal inference\\\", Sec 4.1 first sentence, Fig 7), and for the number of policy rollouts (e.g., algorithm 1 line 5, Table 1). This is confusing, so I strongly suggest using two distinct variable names (something like $k_d$ and $k_r$ also works).\\r\\n- Similarly it would help if you use two separate notations for the trajectories $\\\\tau$ that come from the demonstrations, and the ones that come from the policy. Throughout Section 3 I don't always understand which one of the two you are talking about.\\r\\n- Your references need fixing. Some of them are without a year, and some say technical report even though they were published at a conference (e.g. Finn et al., Rakelly et al.). Your \\\"Wang\\\" reference for RL2 also seems wrong (first sentence in related work)? It should be Jane Wang et al. \\\"Learning to reinforcement learn\\\". The way I always get my bibtex entries is via scholar.google.com: search for the paper there; click on the \\\"cite\\\" button under it and then \\\"BibTeX\\\" (Double check whether the paper was published somewhere though, google scholar often only puts the arXiv link then actually the paper was published somewhere. Sometimes the authors also put the correct bibtex comment on their homepage/github with the code).\\r\\n- Fig 1 left, there's a typo: \\\"learn to lean\\\" -> \\\"learn to learn\\\"\\r\\n- For your experiments, I would call PERIL-A only \\\"PERIL\\\" (since this is your full method, including all losses), and then call the *ablations* different, so for example PERIL-noAux when you remove the auxiliary loss.\\r\\n- All figures should have some form of indication of the error/std/confidence interval (using shaded regions around the mean for example).\\r\\n- Sec 4.2, explain what the multi-task family setting is and why it is challenging. \\r\\n\",\n          \"This paper attempts to explain the generalization in a deep network using separability of the features at various layers. The paper is motivated via a perceptron classifying a Gaussian distribution.\\r\\n\\r\\nOverall, I think the experimental evidence in the paper is not rigorous enough to draw the conclusions that the authors wish to draw. The paper will benefit from more careful experiments, I am listing down some suggestions below.\\r\\n\\r\\nWhat is the connection between the result in Section 2 with those in Section 3? What is the variance on the plots in Fig. 2?\\r\\nOn lines 103-110, max-pooling should reduce variability because it collapses spatial variability in the input. I think there is confounding occurring here due to the presence of batch-norm which (at least in theory) de-correlates the features. De-correlated features are easier to separate using a linear classifier. Further, the features of well-performing deep networks are not easily separable, e.g., https://arxiv.org/abs/1902.01889.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rating\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 8,\n        \"samples\": [\n          \"4: Ok but not good enough - rejection\",\n          \"3: Clear rejection\",\n          \"7: Good paper, accept\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"gpt-3.5-turbo-reviews\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 22,\n        \"samples\": [\n          \"{\\n    \\\"Significance and novelty\\\": {\\n        \\\"Introduction of mask-based pre-training for point clouds\\\": \\\"The paper introduces a novel pre-training mechanism based on mask-based pre-training, inspired by natural language processing techniques, for point clouds which is a unique contribution in the field.\\\",\\n        \\\"OcCo method for point cloud completion\\\": \\\"The proposed Occlusion Completion (OcCo) method is innovative and aims to address the challenges of label inefficiency and time-consuming annotation by learning representations through point cloud completion.\\\"\\n    },\\n    \\\"Potential reasons for acceptance\\\": {\\n        \\\"Innovative approach\\\": \\\"The paper presents a novel approach for pre-training point cloud models using occlusion completion, which can potentially lead to significant advancements in the field.\\\",\\n        \\\"Improved semantic understanding and generalization\\\": \\\"The OcCo method demonstrates improved semantic understandings and generalization on downstream tasks compared to prior methods, showcasing its potential for acceptance.\\\"\\n    },\\n    \\\"Potential reasons for rejection\\\": {\\n        \\\"Lack of comparative analysis\\\": \\\"The paper should provide a more detailed comparative analysis with existing methods in the field to further validate the effectiveness of the OcCo method.\\\",\\n        \\\"Limited experimental results\\\": \\\"More extensive experimental results and evaluation on various datasets are needed to fully demonstrate the effectiveness of the proposed method.\\\"\\n    },\\n    \\\"Suggestions for improvement\\\": {\\n        \\\"Comparative analysis\\\": \\\"Include a more comprehensive comparative analysis with existing methods to highlight the advantages of the OcCo method.\\\",\\n        \\\"Additional experiments\\\": \\\"Conduct additional experiments on different datasets and scenarios to provide a deeper evaluation of the proposed method.\\\"\\n    }\\n}\",\n          \"{\\n\\\"Significance and novelty\\\": {\\n    \\\"Combining imitation learning with meta reinforcement learning (meta-RL)\\\": \\\"Introducing a method, PERIL, that combines imitation learning with meta-RL to improve adaptation rates in unseen tasks.\\\",\\n    \\\"Dual inference strategies for exploration policies\\\": \\\"Using dual inference strategies in PERIL to precondition exploration policies on demonstrations, enhancing adaptation rates.\\\",\\n    \\\"Robustness to task alterations and uncertainties\\\": \\\"Contrasting with pure imitation learning, PERIL is capable of exploring beyond demonstrations, making it robust to task alterations and uncertainties.\\\",\\n    \\\"Interpolating from previously learned dynamics\\\": \\\"Showing how PERIL can adapt to unseen tasks and task families by interpolating from within previously learnt dynamics under sparse rewards.\\\"\\n},\\n\\n\\\"Potential reasons for acceptance\\\": {\\n    \\\"Innovative approach\\\": \\\"Introducing a new method, PERIL, which combines imitation learning and meta-RL to improve adaptation to new tasks.\\\",\\n    \\\"Demonstrated improvement in adaptation rates\\\": \\\"Showing that PERIL improves adaptation rates in unseen tasks by preconditioning exploration policies on demonstrations.\\\",\\n    \\\"Robustness and flexibility\\\": \\\"Highlighting the robustness of PERIL to task alterations and uncertainties, as well as its flexibility in adapting to unseen tasks and task families.\\\"\\n},\\n\\n\\\"Potential reasons for rejection\\\": {\\n    \\\"Lack of empirical evaluation\\\": \\\"The paper may be rejected if it lacks thorough empirical evaluation of the proposed method PERIL.\\\",\\n    \\\"Limited comparison with existing methods\\\": \\\"There may be grounds for rejection if the paper does not provide adequate comparison with existing methods in the field.\\\",\\n    \\\"Unclear implementation details\\\": \\\"Insufficient details on the implementation of PERIL could lead to rejection.\\\"\\n},\\n\\n\\\"Suggestions for improvement\\\": {\\n    \\\"Include more empirical results\\\": \\\"Adding more empirical results and experiments to further validate the performance of PERIL.\\\",\\n    \\\"Perform a comprehensive comparison\\\": \\\"Conducting a thorough comparison with existing methods to demonstrate the advantages of PERIL.\\\",\\n    \\\"Provide more implementation details\\\": \\\"Offering more detailed information on the implementation of PERIL to aid reproducibility.\\\"\\n}\\n}\",\n          \"{\\n\\\"Significance and novelty\\\": {\\n    \\\"Consideration of data geometry for generalization\\\": \\\"The paper proposes considering the geometry of data to better explain generalization achieved in deep learning, which is a novel approach.\\\",\\n    \\\"Linear separability in high dimensions\\\": \\\"The paper shows that the separability of data can explain generalization in high dimensions, which is an important finding.\\\",\\n    \\\"Sequential increase in linear separability within CNNs\\\": \\\"The paper demonstrates how layers within CNNs sequentially increase the linear separability of data, which is a significant contribution.\\\"\\n},\\n\\n\\\"Potential reasons for acceptance\\\": {\\n    \\\"Innovative approach\\\": \\\"The paper takes an innovative approach by focusing on data geometry for explaining generalization in deep learning.\\\",\\n    \\\"Relevance to current deep learning research\\\": \\\"The findings on linear separability and CNN layers are relevant and contribute to the current understanding of deep learning models.\\\",\\n    \\\"Clear presentation of results\\\": \\\"The paper presents the results on data separability and CNN layers clearly, making it easy to follow and understand.\\\"\\n},\\n\\n\\\"Potential reasons for rejection\\\": {},\\n\\n\\\"Suggestions for improvement\\\": {\\n    \\\"Comparison with existing methods\\\": \\\"It would be beneficial to compare the proposed approach with existing methods for explaining generalization in deep learning.\\\",\\n    \\\"Additional experiments\\\": \\\"Including more experiments to further validate the findings on data separability and CNN layers would strengthen the paper's conclusions.\\\"\\n}\\n}\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = '/content/toy_data_w_gpt_reviews.csv'  # Define your file path here\n",
        "toy_data.to_csv(csv_file_path, index=False)"
      ],
      "metadata": {
        "id": "kuszM0GlC85o"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(csv_file_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "ZhjPLsPJDEux",
        "outputId": "1acbd218-b23b-4e85-f4b5-088317045683"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d922e6c8-2aa4-43d2-9998-9d61798e726b\", \"toy_data_w_gpt_reviews.csv\", 135719)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}