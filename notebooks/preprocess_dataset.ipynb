{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/paper_single_round_1w5_update.jsonl') as file:\n",
    "    dataset = json.load(file)\n",
    "    \n",
    "dataset = [data['conversation'][0] for data in dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of papers in the dataset:  15565\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of papers in the dataset: \", len(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each dataset contains 3 columns: `instruction`, `input`, and `output`. The instruction contains the system prompt for the model, and the input is the paper content. The output is the summarized reviews of the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'system': \"You are a professional machine learning conference reviewer who reviews a given paper and considers 4 criteria: [Significance and novelty], [Potential reasons for acceptance], [Potential reasons for rejection], and [Suggestions for improvement]. For each criterion, provide random number of supporting points derived from the paper's content. And for each supporting point, use the format: '<title of supporting point>' followed by a detailed explanation. Your response should only include your reviews only, which means always start with [Significance and novelty], dont' repeat the given paper and output things other than your reviews in required format. The paper is given as follows:\",\n",
       " 'input': '[TITLE]\\nImage Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks\\n\\n[ABSTRACT]\\nWe propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature. Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions. We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function. We then provide a set of systematic experiments that explore its hyper-parameters. We show that each of the energy function\\'s components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned. We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.\\n\\n[CAPTIONS]\\nFigure 1: From left to right, the images are the original image, a contrast stretched image, an image with impulsive noise contamination, and a Gaussian smoothed image. Although these images differ greatly in quality, they all have the same MSE from the original image (about 400), suggesting that MSE is a limited technique for measuring image quality.\\nFigure 2: Comparison of the gradient (edges in the image) for models 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, A is the autoencoded image, OG is the gradient of the original image, AG is the gradient of the autoencoded image, and S is the gradient magnitude similarity score for the discriminator (D) and generator (G). White equals greater similarity (better performance) and black equals lower similarity for the final column.\\nFigure 3: Comparison of the chrominance for models 9 (BEGAN+GMSM+Chrom), 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, OC is the original image in the corresponding color space, A is the autoencoded image in the color space, and S is the chrominance similarity score. I and Q indicate the (blue-red) and (green-purple) color dimensions, respectively. All images were normalized relative to their maximum value to increase luminance. Note that pink and purple approximate a similarity of 1, and green and blue approximate a similarity of 0 for I and Q dimensions, respectively. The increased gradient \\'speckling\\' of model 12Q suggests an inverse relationship between the GMSM and chrominance distance functions.\\nTable 1: Models and their corresponding model distance function parameters. The l 1 , GMSM, and Chrom parameters are their respective β d values from Equation 8.\\nTable 2: Lists the models, their discriminator mean error scores, and their standard deviations for the l 1 , GMSM, and chrominance distance functions over all training epochs. Bold values show the best scores for similar models. Double lines separate sets of similar models. Values that are both bold and italic indicate the best scores overall, excluding models that suffered from modal collapse. These results suggest that model training should be customized to emphasize the relevant components.\\n\\n[CONTENT]\\nSection Title: INTRODUCTION\\n  INTRODUCTION\\n\\nSection Title: IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING\\n  IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING   Radford et al. (2015)  demonstrated that Generative Adversarial Networks (GANs) are a good unsu- pervised technique for learning representations of images for the generative modeling of 2D images. Since then, a number of improvements have been made. First,  Zhao et al. (2016)  modified the error signal of the deep neural network from the original, single parameter criterion to a multi-parameter criterion using auto-encoder reconstruction loss.  Berthelot et al. (2017)  then further modified the loss function from a hinge loss to the Wasserstein distance between loss distributions. For each modification, the proposed changes improved the resulting output to visual inspection (see Ap- pendix A  Figure 4 , Row 1 for the output of the most recent, BEGAN model). We propose a new loss function, building on the changes of the BEGAN model (called the scaled BEGAN GMSM) that further modifies the loss function to handle a broader range of image features within its internal representation.\\n\\nSection Title: GENERATIVE ADVERSARIAL NETWORKS\\n  GENERATIVE ADVERSARIAL NETWORKS Generative Adversarial Networks are a form of two-sample or hypothesis testing that uses a classi- fier, called a discriminator, to distinguish between observed (training) data and data generated by the model or generator. Training is then simplified to a competing (i.e., adversarial) objective between the discriminator and generator, where the discriminator is trained to better differentiate training from generated data, and the generator is trained to better trick the discriminator into thinking its generated data is real. The convergence of a GAN is achieved when the generator and discriminator reach a Nash equilibrium, from a game theory point of view (Zhao et al., 2016). In the original GAN specification, the task is to learn the generator\\'s distribution p G over data x ( Goodfellow et al., 2014 ). To accomplish this, one defines a generator function G(z; θ G ), which produces an image using a noise vector z as input, and G is a differentiable function with param- eters θ G . The discriminator is then specified as a second function D(x; θ D ) that outputs a scalar representing the probability that x came from the data rather than p G . D is then trained to maxi- mize the probability of assigning the correct labels to the data and the image output of G while G is trained to minimize the probability that D assigns its output to the fake class, or 1 − D(G(z)). Although G and D can be any differentiable functions, we will only consider deep convolutional neural networks in what follows.  Zhao et al. (2016)  initially proposed a shift from the original single-dimensional criterion-the scalar class probability-to a multidimensional criterion by constructing D as an autoencoder. The image output by the autoencoder can then be directly compared to the output of G using one of the many standard distance functions (e.g., l 1 norm, mean square error). However,  Zhao et al. (2016)  also proposed a new interpretation of the underlying GAN architecture in terms of an energy-based model ( LeCun et al., 2006 ).\\n\\nSection Title: ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS\\n  ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS The basic idea of energy-based models (EBMs) is to map an input space to a single scalar or set of scalars (called its \"energy\") via the construction of a function ( LeCun et al., 2006 ). Learning in this framework modifies the energy surface such that desirable pairings get low energies while undesir- able pairings get high energies. This framework allows for the interpretation of the discriminator (D) as an energy function that lacks any explicit probabilistic interpretation (Zhao et al., 2016). In this view, the discriminator is a trainable cost function for the generator that assigns low energy val- ues to regions of high data density and high energy to the opposite. The generator is then interpreted as a trainable parameterized function that produces samples in regions assigned low energy by the discriminator. To accomplish this setup,  Zhao et al. (2016)  first define the discriminator\\'s energy function as the mean square error of the reconstruction loss of the autoencoder, or:   Zhao et al. (2016)  then define the loss function for their discriminator using a form of margin loss. L D (x, z) = E D (x) + [m − E D (G(z))] + (2) where m is a constant and [·] + = max(0, ·). They define the loss function for their generator: The authors then prove that, if the system reaches a Nash equilibrium, then the generator will pro- duce samples that cannot be distinguished from the dataset. Problematically, simple visual inspec- tion can easily distinguish the generated images from the dataset.\\n\\nSection Title: DEFINING THE PROBLEM\\n  DEFINING THE PROBLEM It is clear that, despite the mathematical proof of  Zhao et al. (2016) , humans can distinguish the images generated by energy-based models from real images. There are two direct approaches that could provide insight into this problem, both of which are outlined in the original paper. The first approach that is discussed by  Zhao et al. (2016)  changes Equation 2 to allow for better approxima- tions than m. The BEGAN model takes this approach. The second approach addresses Equation 1, but was only implicitly addressed when (Zhao et al., 2016) chose to change the original GAN to use the reconstruction error of an autoencoder instead of a binary logistic energy function. We chose to take the latter approach while building on the work of BEGAN. Our main contributions are as follows: • An energy-based formulation of BEGAN\\'s solution to the visual problem. • An energy-based formulation of the problems with Equation 1. • Experiments that explore the different hyper-parameters of the new energy function. • Evaluations that provide greater detail into the learned representations of the model. • A demonstration that scaled BEGAN+GMSM can be used to generate better quality images from the CelebA dataset at 128x128 pixel resolution than the original BEGAN model in quantifiable ways.\\n\\nSection Title: BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS\\n  BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS The Boundary Equilibrium Generative Adversarial Network (BEGAN) makes a number of modi- fications to the original energy-based approach. However, the most important contribution can be summarized in its changes to Equation 2. In place of the hinge loss,  Berthelot et al. (2017)  use the Wasserstein distance between the autoencoder reconstruction loss distributions of G and D. They also add three new hyper-parameters in place of m: k t , λ k , and γ. Using an energy-based approach, we get the following new equation: The value of k t is then defined as: k t+1 = k t + λ k (γE D (x) − E D (G(z))) for each t (5) where k t ∈ [0, 1] is the emphasis put on E(G(z)) at training step t for the gradient of E D , λ k is the learning rate for k, and γ ∈ [0, 1]. Both Equations 2 and 4 are describing the same phenomenon: the discriminator is doing well if either 1) it is properly reconstructing the real images or 2) it is detecting errors in the reconstruction of the generated images. Equation 4 just changes how the model achieves that goal. In the original equation (Equation 2), we punish the discriminator (L D → ∞) when the generated input is doing well (E D (G(z)) → 0). In Equation 4, we reward the discriminator (L D → 0) when the generated input is doing poorly (E D (G(z)) → ∞). What is also different between Equations 2 and 4 is the way their boundaries function. In Equation 2, m only acts as a one directional boundary that removes the impact of the generated input on the discriminator if E D (G(z)) > m. In Equation 5, γE D (x) functions in a similar but more complex way by adding a dependency to E D (x). Instead of 2 conditions on either side of the boundary m, there are now four: The optimal condition is condition 1  Berthelot et al. (2017) . Thus, the BEGAN model tries to keep the energy of the generated output approaching the limit of the energy of the real images. As the latter will change over the course of learning, the resulting boundary dynamically establishes an equilibrium between the energy state of the real and generated input. It is not particularly surprising that these modifications to Equation 2 show improvements.  Zhao et al. (2016)  devote an appendix section to the correct selection of m and explicitly mention that the \"balance between... real and fake samples[s]\" (italics theirs) is crucial to the correct selection of m. Unsurprisingly, a dynamically updated parameter that accounts for this balance is likely to be the best instantiation of the authors\\' intuitions and visual inspection of the resulting output supports this (see  Berthelot et al., 2017 ). We chose a slightly different approach to improving the proposed loss function by changing the original energy function (Equation 1).\\n\\nSection Title: FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT\\n  FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT In the original description of the energy-based approach to GANs, the energy function was defined as the mean square error (MSE) of the reconstruction loss of the autoencoder (Equation 1). Our first insight was a trivial generalization of Equation 1: E(x) = δ(D(x), x) (6) where δ is some distance function. This more general equation suggests that there are many possible distance functions that could be used to describe the reconstruction error and that the selection of δ is itself a design decision for the resulting energy and loss functions. Not surprisingly, an entire field of study exists that focuses on the construction of similar δ functions in the image domain: the field of image quality assessment (IQA). The field of IQA focuses on evaluating the quality of digital images ( Wang & Bovik, 2006 ). IQA is a rich and diverse field that merits substantial further study. However, for the sake of this paper, we want to emphasize three important findings from this field. First, distance functions like δ are called full-reference IQA (or FR-IQA) functions because the reconstruction (D(x)) has a \\'true\\' or undistorted reference image (x) which it can be evaluated from  Wang et al. (2004) . Second, IQA researchers have known for a long time that MSE is a poor indicator of image quality ( Wang & Bovik, 2006 ). And third, there are numerous other functions that are better able to indicate image quality. We explain each of these points below. One way to view the FR-IQA approach is in terms of a reference and distortion vector. In this view, an image is represented as a vector whose dimensions correspond with the pixels of the image. The reference image sets up the initial vector from the origin, which defines the original, perfect image. The distorted image is then defined as another vector defined from the origin. The vector that maps the reference image to the distorted image is called the distortion vector and FR-IQA studies how to evaluate different types of distortion vectors. In terms of our energy-based approach and Equation 6, the distortion vector is measured by δ and it defines the surface of the energy function. MSE is one of the ways to measure distortion vectors. It is based in a paradigm that views the loss of quality in an image in terms of the visibility of an error signal, which MSE quantifies. Problem- atically, it has been shown that MSE actually only defines the length of a distortion vector not its type ( Wang & Bovik, 2006 ). For any given reference image vector, there are an entire hypersphere of other image vectors that can be reached by a distortion vector of a given size (i.e., that all have the same MSE from the reference image; see  Figure 1 ). A number of different measurement techniques have been created that improve upon MSE (for a review, see  Chandler, 2013 ). Often these techniques are defined in terms of the similarity (S) between the reference and distorted image, where δ = 1−S. One of the most notable improvements is the Structural Similarity Index (SSIM), which measures the similarity of the luminance, contrast, and structure of the reference and distorted image using the following similarity function: 2 S(v d , v r ) = 2v d v r + C v 2 d + v 2 r + C (7) where v d is the distorted image vector, v r is the reference image vector, C is a constant, and all multiplications occur element-wise  Wang & Bovik (2006) . 3 This function has a number of desirable features. It is symmetric (i.e., S(v d , v r ) = S(v r , v d ), bounded by 1 (and 0 for x > 0), and it has a unique maximum of 1 only when v d = v r . Although we chose not to use SSIM as our energy function (δ) as it can only handle black-and-white images, its similarity function (Equation 7) informs our chosen technique. The above discussion provides some insights into why visual inspection fails to show this correspon- dence between real and generated output of the resulting models, even though  Zhao et al. (2016)  proved that the generator should produce samples that cannot be distinguished from the dataset. The original proof by  Zhao et al. (2016)  did not account for Equation 1. Thus, when  Zhao et al. (2016)  show that their generated output should be indistinguishable from real images, what they are actu- ally showing is that it should be indistinguishable from the real images plus some residual distortion vector described by δ. Yet, we have just shown that MSE (the author\\'s chosen δ) can only constrain the length of the distortion vector, not its type. Consequently, it is entirely possible for two systems using MSE for δ to have both reached a Nash equilibrium, have the same energy distribution, and yet have radically different internal representations of the learned images. The energy function is as important as the loss function for defining the data distribution.\\n\\nSection Title: A NEW ENERGY FUNCTION\\n  A NEW ENERGY FUNCTION Rather than assume that any one distance function would suffice to represent all of the various features of real images, we chose to use a multi-component approach for defining δ. In place of the luminance, contrast, and structural similarity of SSIM, we chose to evaluate the l 1 norm, the gradient magnitude similarity score (GMS), and a chrominance similarity score (Chrom). We outline the latter two in more detail below. The GMS score and chrom scores derive from an FR-IQA model called the color Quality Score (cQS;  Gupta et al., 2017 ). The cQS uses GMS and chrom as its two components. First, it converts images to the YIQ color space model. In this model, the three channels correspond to the luminance information (Y) and the chrominance information (I and Q). Second, GMS is used to evaluate the local gradients across the reference and distorted images on the luminance dimension in order to compare their edges. This is performed by convolving a 3 × 3 Sobel filter in both the horizontal and vertical directions of each image to get the corresponding gradients. The horizontal and vertical gradients are then collapsed to the gradient magnitude of each image using the Euclidean distance. 4 The similarity between the gradient magnitudes of the reference and distorted image are then com- pared using Equation 7. Third, Equation 7 is used to directly compute the similarity between the I and Q color dimensions of each image. The mean is then taken of the GMS score (resulting in the GMSM score) and the combined I and Q scores (resulting in the Chrom score). In order to experimentally evaluate how each of the different components contribute to the underly- ing image representations, we defined the following, multi-component energy function: E D = δ∈D δ(D(x), x)β d δ∈D β d (8) where β d is the weight that determines the proportion of each δ to include for a given model, and D includes the l 1 norm, GMSM, and the chrominance part of cQS as individual δs. In what follows, we experimentally evaluate each of the energy function components(β) and some of their combinations.\\n\\nSection Title: EXPERIMENTS\\n  EXPERIMENTS\\n\\nSection Title: METHOD\\n  METHOD We conducted extensive quantitative and qualitative evaluation on the CelebA dataset of face images  Liu et al. (2015) . This dataset has been used frequently in the past for evaluating GANs  Radford et al. (2015) ;  Zhao et al. (2016) ;  Chen et al. (2016) ;  Liu & Tuzel (2016) . We evaluated 12 different models in a number of combinations (see  Table 1 ). They are as follows. Models 1, 7, and 11 are the original BEGAN model. Models 2 and 3 only use the GMSM and chrominance distance functions, respectively. Models 4 and 8 are the BEGAN model plus GMSM. Models 5 and 9 use all three Under review as a conference paper at ICLR 2018 distance functions (BEGAN+GMSM+Chrom). Models 6, 10, and 12 use a \\'scaled\\' BEGAN model (β l1 = 2) with GMSM. All models with different model numbers but the same β d values differ in their γ values or the output image size.\\n\\nSection Title: SETUP\\n  SETUP All of the models we evaluate in this paper are based on the architecture of the BEGAN model  Berthelot et al. (2017) . 5 We trained the models using Adam with a batch size of 16, β 1 of 0.9, β 2 of 0.999, and an initial learning rate of 0.00008, which decayed by a factor of 2 every 100,000 epochs. Parameters k t and k 0 were set at 0.001 and 0, respectively (see Equation 5). The γ parameter was set relative to the model (see  Table 1 ). Most of our experiments were performed on 64 × 64 pixel images with a single set of tests run on 128 × 128 images. The number of convolution layers were 3 and 4, respectively, with a constant down-sampled size of 8 × 8. We found that the original size of 64 for the input vector (N z ) and hidden state (N h ) resulted in modal collapse for the models using GMSM. However, we found that this was fixed by increasing the input size to 128 and 256 for the 64 and 128 pixel images, respectively. We used N z = 128 for all models except 12 (scaled BEGAN+GMSM), which used 256. N z always equaled N h in all experiments. Models 2-3 were run for 18,000 epochs, 1 and 4-10 were run for 100,000 epochs, and 11-12 were run for 300,000 epochs. Models 2-4 suffered from modal collapse immediately and 5 (BE- GAN+GMSM+Chrom) collapsed around epoch 65,000 (see Appendix A Figure 4 rows 2-5).\\n\\nSection Title: EVALUATIONS\\n  EVALUATIONS We performed two evaluations. First, to evaluate whether and to what extent the models were able to capture the relevant properties of each associated distance function, we compared the mean and standard deviation of the error scores. We calculated them for each distance function over all epochs of all models. We chose to use the mean rather than the minimum score as we were interested in how each model performs as a whole, rather than at some specific epoch. All calculations use the distance, or one minus the corresponding similarity score, for both the gradient magnitude and chrominance values. Reduced pixelation is an artifact of the intensive scaling for image presentation (up to 4×). All images in the qualitative evaluations were upscaled from their original sizes using cubic image sampling so that they can be viewed at larger sizes. Consequently, the apparent smoothness of the scaled images is not a property of the model.\\n\\nSection Title: RESULTS\\n  RESULTS GANs are used to generate different types of images. Which image components are important depends on the domain of these images. Our results suggest that models used in any particular GAN application should be customized to emphasize the relevant components-there is not a one-size- fits-all component choice. We discuss the results of our four evaluations below.\\n\\nSection Title: MEANS AND STANDARD DEVIATIONS OF ERROR SCORES\\n  MEANS AND STANDARD DEVIATIONS OF ERROR SCORES Results were as expected: the three different distance functions captured different features of the underlying image representations. We compared all of the models in terms of their means and standard deviations of the error score of the associated distance functions (see  Table 2 ). In particular, each of models 1-3 only used one of the distance functions and had the lowest error for the associated function (e.g., model 2 was trained with GMSM and has the lowest GMSM error score). Models 4-6 expanded on the first three models by examining the distance functions in different combinations. Model 5 (BEGAN+GMSM+Chrom) had the lowest chrominance error score and Model 6 (scaled BEGAN+GMSM) had the lowest scores for l 1 and GMSM of any model using a γ of 0.5. For the models with γ set at 0.7, models 7-9 showed similar results to the previous scores. Model 8 (BEGAN+GMSM) scored the lowest GMSM score overall and model 9 (BEGAN+GMSM+Chrom) scored the lowest chrominance score of the models that did not suffer from modal collapse. For the two models that were trained to generate 128 × 128 pixel images, model 12 (scaled BE- GAN+GMSM) had the lowest error scores for l 1 and GMSM, and model 11 (BEGAN) had the lowest score for chrominance. Model 12 had the lowest l 1 score, overall.\\n\\nSection Title: VISUAL COMPARISON OF SIMILARITY SCORES\\n  VISUAL COMPARISON OF SIMILARITY SCORES Subjective visual comparison of the gradient magnitudes in column S of  Figure 2  shows there are more black pixels for model 11 (row 11D) when comparing real images before and after autoencod- ing. This indicates a lower similarity in the autoencoder. Model 12 (row 12D) has a higher similarity between the original and autoencoded real images as indicated by fewer black pixels. This pattern continues for the generator output (rows 11G and 12G), but with greater similarity between the gra- dients of the original and autoencoded images than the real images (i.e., fewer black pixels overall). The visual comparison of chrominance and related similarity score also weakly supported our hy- potheses (see  Figure 3 ). All of the models show a strong ability to capture the I dimension (blue-red) of the YIQ color space, but only model 9 (BEGAN+GMSM+Chrom) is able to accurately capture the relevant information in the Q dimension (green-purple).\\n\\nSection Title: OUTLOOK\\n  OUTLOOK We bring an energy-based formulation to the BEGAN model and some of the problems of the en- ergy function originally proposed in  Zhao et al. (2016) . We proposed a new, multi-component energy function on the basis of research from the Image Quality Assessment literature. The scaled BEGAN+GMSM model produces better image representations than its competitors in ways that can be measured using subjective evaluations of the associated features (e.g., luminance gradient simi- larity, chrominance similarity). For future work, we would like to extend this research to encompass other datasets and FR-IQA energy functions.\\n\\nSection Title: Annex Figures\\n  Annex Figures   fig_3 Figure 4:   Figure 4: Four outputs of each of the generators of all 12 models. The best images for each model were hand-picked. The first row is model 1, which corresponds with the original BEGAN model. Rows 2-12 represent our experiments. Each cell represents the output of a random sample.  \\n',\n",
       " 'output': \"[Significance and novelty]\\n<Incremental nature of the paper> Reviewers highlighted that the paper's proposed approach was somewhat incremental, and its significance and novelty were questioned. The modifications and additions to the existing BEGAN framework were not perceived as significantly contributing to the field.\\n<Limited novelty of energy function components> The paper's novelty was deemed to be restricted, particularly in the choice and justification of the energy function components. Reviewers pointed out that the selected components were borrowed from existing image quality assessment techniques, and their application was not seen as a significant contribution to GAN theories or algorithm designs.\\n\\n[Potential reasons for acceptance]\\n<No related terms> -\\n\\n[Potential reasons for rejection]\\n<Weak experiments and results analysis> Reviewers noted that the experimental section was weak, with reliance on visual evaluations and a limited range of datasets (primarily CelebA). This led to concerns about the thoroughness and robustness of the paper's findings.\\n<Unclear significance and contribution> The paper was criticized for its lack of clarity in motivating its contributions and presenting results. Reviewers found it difficult to estimate the significance of the proposed model and understand how its results differed from baseline models.\\n\\n[Suggestions for improvement]\\n<Broaden experimental testing> To strengthen the paper, reviewers suggested broadening the experimental testing to include different datasets involving natural images, beyond the single CelebA dataset. This would provide a more comprehensive evaluation of the proposed techniques.\\n<Clarify and justify design choices> Improvements in the paper's clarity and justification were recommended, specifically in clarifying the design choices made for the energy function components. Providing clear justifications for the modifications and additions to the BEGAN framework would enhance the paper's credibility and significance.\\n\\n\"}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Number of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of words in the dataset:  5679.193639575972\n"
     ]
    }
   ],
   "source": [
    "lengths = []\n",
    "for data in dataset:\n",
    "    instruction = data['system']\n",
    "    input = data['input']\n",
    "    output = data['output']\n",
    "    \n",
    "    len_instruction = len(instruction.split())\n",
    "    len_input = len(input.split())\n",
    "    len_output = len(output.split())\n",
    "    \n",
    "    sum_len = len_instruction + len_input + len_output\n",
    "    lengths.append(sum_len)\n",
    "    \n",
    "print(\"Average number of words in the dataset: \", sum(lengths)/len(lengths))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the rule of thumb is that one token generally corresponds to ~4 characters of text for common English text. This translates to roughly ¾ of a word (so 100 tokens ~= 75 words). We can use this to estimate the average number of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of tokens in the dataset:  7572.258186101296\n",
      "Max number of tokens in the dataset:  10488.0\n"
     ]
    }
   ],
   "source": [
    "token2word_ratio = 4/3\n",
    "print(\"Average number of tokens in the dataset: \", sum(lengths)/len(lengths) * token2word_ratio)\n",
    "print(\"Max number of tokens in the dataset: \", max(lengths) * token2word_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# draw a histogram of the lengths\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(lengths, bins=20, color='skyblue', edgecolor='black')\n",
    "plt.xlabel('Length of the paper (pages)', fontsize=14)\n",
    "plt.ylabel('Number of papers', fontsize=14)\n",
    "plt.title('Histogram of the Lengths of Academic Papers', fontsize=16)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = Dataset.from_list(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_conversation(sample):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sample['system']},\n",
    "            {\"role\": \"user\", \"content\": sample[\"input\"]},\n",
    "            {\"role\": \"assistant\", \"content\": sample[\"output\"]}\n",
    "        ]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features, batched=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': \"You are a professional machine learning conference reviewer who reviews a given paper and considers 4 criteria: [Significance and novelty], [Potential reasons for acceptance], [Potential reasons for rejection], and [Suggestions for improvement]. For each criterion, provide random number of supporting points derived from the paper's content. And for each supporting point, use the format: '<title of supporting point>' followed by a detailed explanation. Your response should only include your reviews only, which means always start with [Significance and novelty], dont' repeat the given paper and output things other than your reviews in required format. The paper is given as follows:\", 'role': 'system'}, {'content': '[TITLE]\\nImage Quality Assessment Techniques Improve Training and Evaluation of Energy-Based Generative Adversarial Networks\\n\\n[ABSTRACT]\\nWe propose a new, multi-component energy function for energy-based Generative Adversarial Networks (GANs) based on methods from the image quality assessment literature. Our approach expands on the Boundary Equilibrium Generative Adversarial Network (BEGAN) by outlining some of the short-comings of the original energy and loss functions. We address these short-comings by incorporating an l1 score, the Gradient Magnitude Similarity score, and a chrominance score into the new energy function. We then provide a set of systematic experiments that explore its hyper-parameters. We show that each of the energy function\\'s components is able to represent a slightly different set of features, which require their own evaluation criteria to assess whether they have been adequately learned. We show that models using the new energy function are able to produce better image representations than the BEGAN model in predicted ways.\\n\\n[CAPTIONS]\\nFigure 1: From left to right, the images are the original image, a contrast stretched image, an image with impulsive noise contamination, and a Gaussian smoothed image. Although these images differ greatly in quality, they all have the same MSE from the original image (about 400), suggesting that MSE is a limited technique for measuring image quality.\\nFigure 2: Comparison of the gradient (edges in the image) for models 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, A is the autoencoded image, OG is the gradient of the original image, AG is the gradient of the autoencoded image, and S is the gradient magnitude similarity score for the discriminator (D) and generator (G). White equals greater similarity (better performance) and black equals lower similarity for the final column.\\nFigure 3: Comparison of the chrominance for models 9 (BEGAN+GMSM+Chrom), 11 (BEGAN) and 12 (scaled BEGAN+GMSM), where O is the original image, OC is the original image in the corresponding color space, A is the autoencoded image in the color space, and S is the chrominance similarity score. I and Q indicate the (blue-red) and (green-purple) color dimensions, respectively. All images were normalized relative to their maximum value to increase luminance. Note that pink and purple approximate a similarity of 1, and green and blue approximate a similarity of 0 for I and Q dimensions, respectively. The increased gradient \\'speckling\\' of model 12Q suggests an inverse relationship between the GMSM and chrominance distance functions.\\nTable 1: Models and their corresponding model distance function parameters. The l 1 , GMSM, and Chrom parameters are their respective β d values from Equation 8.\\nTable 2: Lists the models, their discriminator mean error scores, and their standard deviations for the l 1 , GMSM, and chrominance distance functions over all training epochs. Bold values show the best scores for similar models. Double lines separate sets of similar models. Values that are both bold and italic indicate the best scores overall, excluding models that suffered from modal collapse. These results suggest that model training should be customized to emphasize the relevant components.\\n\\n[CONTENT]\\nSection Title: INTRODUCTION\\n  INTRODUCTION\\n\\nSection Title: IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING\\n  IMPROVING LEARNED REPRESENTATIONS FOR GENERATIVE MODELING   Radford et al. (2015)  demonstrated that Generative Adversarial Networks (GANs) are a good unsu- pervised technique for learning representations of images for the generative modeling of 2D images. Since then, a number of improvements have been made. First,  Zhao et al. (2016)  modified the error signal of the deep neural network from the original, single parameter criterion to a multi-parameter criterion using auto-encoder reconstruction loss.  Berthelot et al. (2017)  then further modified the loss function from a hinge loss to the Wasserstein distance between loss distributions. For each modification, the proposed changes improved the resulting output to visual inspection (see Ap- pendix A  Figure 4 , Row 1 for the output of the most recent, BEGAN model). We propose a new loss function, building on the changes of the BEGAN model (called the scaled BEGAN GMSM) that further modifies the loss function to handle a broader range of image features within its internal representation.\\n\\nSection Title: GENERATIVE ADVERSARIAL NETWORKS\\n  GENERATIVE ADVERSARIAL NETWORKS Generative Adversarial Networks are a form of two-sample or hypothesis testing that uses a classi- fier, called a discriminator, to distinguish between observed (training) data and data generated by the model or generator. Training is then simplified to a competing (i.e., adversarial) objective between the discriminator and generator, where the discriminator is trained to better differentiate training from generated data, and the generator is trained to better trick the discriminator into thinking its generated data is real. The convergence of a GAN is achieved when the generator and discriminator reach a Nash equilibrium, from a game theory point of view (Zhao et al., 2016). In the original GAN specification, the task is to learn the generator\\'s distribution p G over data x ( Goodfellow et al., 2014 ). To accomplish this, one defines a generator function G(z; θ G ), which produces an image using a noise vector z as input, and G is a differentiable function with param- eters θ G . The discriminator is then specified as a second function D(x; θ D ) that outputs a scalar representing the probability that x came from the data rather than p G . D is then trained to maxi- mize the probability of assigning the correct labels to the data and the image output of G while G is trained to minimize the probability that D assigns its output to the fake class, or 1 − D(G(z)). Although G and D can be any differentiable functions, we will only consider deep convolutional neural networks in what follows.  Zhao et al. (2016)  initially proposed a shift from the original single-dimensional criterion-the scalar class probability-to a multidimensional criterion by constructing D as an autoencoder. The image output by the autoencoder can then be directly compared to the output of G using one of the many standard distance functions (e.g., l 1 norm, mean square error). However,  Zhao et al. (2016)  also proposed a new interpretation of the underlying GAN architecture in terms of an energy-based model ( LeCun et al., 2006 ).\\n\\nSection Title: ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS\\n  ENERGY-BASED GENERATIVE ADVERSARIAL NETWORKS The basic idea of energy-based models (EBMs) is to map an input space to a single scalar or set of scalars (called its \"energy\") via the construction of a function ( LeCun et al., 2006 ). Learning in this framework modifies the energy surface such that desirable pairings get low energies while undesir- able pairings get high energies. This framework allows for the interpretation of the discriminator (D) as an energy function that lacks any explicit probabilistic interpretation (Zhao et al., 2016). In this view, the discriminator is a trainable cost function for the generator that assigns low energy val- ues to regions of high data density and high energy to the opposite. The generator is then interpreted as a trainable parameterized function that produces samples in regions assigned low energy by the discriminator. To accomplish this setup,  Zhao et al. (2016)  first define the discriminator\\'s energy function as the mean square error of the reconstruction loss of the autoencoder, or:   Zhao et al. (2016)  then define the loss function for their discriminator using a form of margin loss. L D (x, z) = E D (x) + [m − E D (G(z))] + (2) where m is a constant and [·] + = max(0, ·). They define the loss function for their generator: The authors then prove that, if the system reaches a Nash equilibrium, then the generator will pro- duce samples that cannot be distinguished from the dataset. Problematically, simple visual inspec- tion can easily distinguish the generated images from the dataset.\\n\\nSection Title: DEFINING THE PROBLEM\\n  DEFINING THE PROBLEM It is clear that, despite the mathematical proof of  Zhao et al. (2016) , humans can distinguish the images generated by energy-based models from real images. There are two direct approaches that could provide insight into this problem, both of which are outlined in the original paper. The first approach that is discussed by  Zhao et al. (2016)  changes Equation 2 to allow for better approxima- tions than m. The BEGAN model takes this approach. The second approach addresses Equation 1, but was only implicitly addressed when (Zhao et al., 2016) chose to change the original GAN to use the reconstruction error of an autoencoder instead of a binary logistic energy function. We chose to take the latter approach while building on the work of BEGAN. Our main contributions are as follows: • An energy-based formulation of BEGAN\\'s solution to the visual problem. • An energy-based formulation of the problems with Equation 1. • Experiments that explore the different hyper-parameters of the new energy function. • Evaluations that provide greater detail into the learned representations of the model. • A demonstration that scaled BEGAN+GMSM can be used to generate better quality images from the CelebA dataset at 128x128 pixel resolution than the original BEGAN model in quantifiable ways.\\n\\nSection Title: BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS\\n  BOUNDARY EQUILIBRIUM GENERATIVE ADVERSARIAL NETWORKS The Boundary Equilibrium Generative Adversarial Network (BEGAN) makes a number of modi- fications to the original energy-based approach. However, the most important contribution can be summarized in its changes to Equation 2. In place of the hinge loss,  Berthelot et al. (2017)  use the Wasserstein distance between the autoencoder reconstruction loss distributions of G and D. They also add three new hyper-parameters in place of m: k t , λ k , and γ. Using an energy-based approach, we get the following new equation: The value of k t is then defined as: k t+1 = k t + λ k (γE D (x) − E D (G(z))) for each t (5) where k t ∈ [0, 1] is the emphasis put on E(G(z)) at training step t for the gradient of E D , λ k is the learning rate for k, and γ ∈ [0, 1]. Both Equations 2 and 4 are describing the same phenomenon: the discriminator is doing well if either 1) it is properly reconstructing the real images or 2) it is detecting errors in the reconstruction of the generated images. Equation 4 just changes how the model achieves that goal. In the original equation (Equation 2), we punish the discriminator (L D → ∞) when the generated input is doing well (E D (G(z)) → 0). In Equation 4, we reward the discriminator (L D → 0) when the generated input is doing poorly (E D (G(z)) → ∞). What is also different between Equations 2 and 4 is the way their boundaries function. In Equation 2, m only acts as a one directional boundary that removes the impact of the generated input on the discriminator if E D (G(z)) > m. In Equation 5, γE D (x) functions in a similar but more complex way by adding a dependency to E D (x). Instead of 2 conditions on either side of the boundary m, there are now four: The optimal condition is condition 1  Berthelot et al. (2017) . Thus, the BEGAN model tries to keep the energy of the generated output approaching the limit of the energy of the real images. As the latter will change over the course of learning, the resulting boundary dynamically establishes an equilibrium between the energy state of the real and generated input. It is not particularly surprising that these modifications to Equation 2 show improvements.  Zhao et al. (2016)  devote an appendix section to the correct selection of m and explicitly mention that the \"balance between... real and fake samples[s]\" (italics theirs) is crucial to the correct selection of m. Unsurprisingly, a dynamically updated parameter that accounts for this balance is likely to be the best instantiation of the authors\\' intuitions and visual inspection of the resulting output supports this (see  Berthelot et al., 2017 ). We chose a slightly different approach to improving the proposed loss function by changing the original energy function (Equation 1).\\n\\nSection Title: FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT\\n  FINDING A NEW ENERGY FUNCTION VIA IMAGE QUALITY ASSESSMENT In the original description of the energy-based approach to GANs, the energy function was defined as the mean square error (MSE) of the reconstruction loss of the autoencoder (Equation 1). Our first insight was a trivial generalization of Equation 1: E(x) = δ(D(x), x) (6) where δ is some distance function. This more general equation suggests that there are many possible distance functions that could be used to describe the reconstruction error and that the selection of δ is itself a design decision for the resulting energy and loss functions. Not surprisingly, an entire field of study exists that focuses on the construction of similar δ functions in the image domain: the field of image quality assessment (IQA). The field of IQA focuses on evaluating the quality of digital images ( Wang & Bovik, 2006 ). IQA is a rich and diverse field that merits substantial further study. However, for the sake of this paper, we want to emphasize three important findings from this field. First, distance functions like δ are called full-reference IQA (or FR-IQA) functions because the reconstruction (D(x)) has a \\'true\\' or undistorted reference image (x) which it can be evaluated from  Wang et al. (2004) . Second, IQA researchers have known for a long time that MSE is a poor indicator of image quality ( Wang & Bovik, 2006 ). And third, there are numerous other functions that are better able to indicate image quality. We explain each of these points below. One way to view the FR-IQA approach is in terms of a reference and distortion vector. In this view, an image is represented as a vector whose dimensions correspond with the pixels of the image. The reference image sets up the initial vector from the origin, which defines the original, perfect image. The distorted image is then defined as another vector defined from the origin. The vector that maps the reference image to the distorted image is called the distortion vector and FR-IQA studies how to evaluate different types of distortion vectors. In terms of our energy-based approach and Equation 6, the distortion vector is measured by δ and it defines the surface of the energy function. MSE is one of the ways to measure distortion vectors. It is based in a paradigm that views the loss of quality in an image in terms of the visibility of an error signal, which MSE quantifies. Problem- atically, it has been shown that MSE actually only defines the length of a distortion vector not its type ( Wang & Bovik, 2006 ). For any given reference image vector, there are an entire hypersphere of other image vectors that can be reached by a distortion vector of a given size (i.e., that all have the same MSE from the reference image; see  Figure 1 ). A number of different measurement techniques have been created that improve upon MSE (for a review, see  Chandler, 2013 ). Often these techniques are defined in terms of the similarity (S) between the reference and distorted image, where δ = 1−S. One of the most notable improvements is the Structural Similarity Index (SSIM), which measures the similarity of the luminance, contrast, and structure of the reference and distorted image using the following similarity function: 2 S(v d , v r ) = 2v d v r + C v 2 d + v 2 r + C (7) where v d is the distorted image vector, v r is the reference image vector, C is a constant, and all multiplications occur element-wise  Wang & Bovik (2006) . 3 This function has a number of desirable features. It is symmetric (i.e., S(v d , v r ) = S(v r , v d ), bounded by 1 (and 0 for x > 0), and it has a unique maximum of 1 only when v d = v r . Although we chose not to use SSIM as our energy function (δ) as it can only handle black-and-white images, its similarity function (Equation 7) informs our chosen technique. The above discussion provides some insights into why visual inspection fails to show this correspon- dence between real and generated output of the resulting models, even though  Zhao et al. (2016)  proved that the generator should produce samples that cannot be distinguished from the dataset. The original proof by  Zhao et al. (2016)  did not account for Equation 1. Thus, when  Zhao et al. (2016)  show that their generated output should be indistinguishable from real images, what they are actu- ally showing is that it should be indistinguishable from the real images plus some residual distortion vector described by δ. Yet, we have just shown that MSE (the author\\'s chosen δ) can only constrain the length of the distortion vector, not its type. Consequently, it is entirely possible for two systems using MSE for δ to have both reached a Nash equilibrium, have the same energy distribution, and yet have radically different internal representations of the learned images. The energy function is as important as the loss function for defining the data distribution.\\n\\nSection Title: A NEW ENERGY FUNCTION\\n  A NEW ENERGY FUNCTION Rather than assume that any one distance function would suffice to represent all of the various features of real images, we chose to use a multi-component approach for defining δ. In place of the luminance, contrast, and structural similarity of SSIM, we chose to evaluate the l 1 norm, the gradient magnitude similarity score (GMS), and a chrominance similarity score (Chrom). We outline the latter two in more detail below. The GMS score and chrom scores derive from an FR-IQA model called the color Quality Score (cQS;  Gupta et al., 2017 ). The cQS uses GMS and chrom as its two components. First, it converts images to the YIQ color space model. In this model, the three channels correspond to the luminance information (Y) and the chrominance information (I and Q). Second, GMS is used to evaluate the local gradients across the reference and distorted images on the luminance dimension in order to compare their edges. This is performed by convolving a 3 × 3 Sobel filter in both the horizontal and vertical directions of each image to get the corresponding gradients. The horizontal and vertical gradients are then collapsed to the gradient magnitude of each image using the Euclidean distance. 4 The similarity between the gradient magnitudes of the reference and distorted image are then com- pared using Equation 7. Third, Equation 7 is used to directly compute the similarity between the I and Q color dimensions of each image. The mean is then taken of the GMS score (resulting in the GMSM score) and the combined I and Q scores (resulting in the Chrom score). In order to experimentally evaluate how each of the different components contribute to the underly- ing image representations, we defined the following, multi-component energy function: E D = δ∈D δ(D(x), x)β d δ∈D β d (8) where β d is the weight that determines the proportion of each δ to include for a given model, and D includes the l 1 norm, GMSM, and the chrominance part of cQS as individual δs. In what follows, we experimentally evaluate each of the energy function components(β) and some of their combinations.\\n\\nSection Title: EXPERIMENTS\\n  EXPERIMENTS\\n\\nSection Title: METHOD\\n  METHOD We conducted extensive quantitative and qualitative evaluation on the CelebA dataset of face images  Liu et al. (2015) . This dataset has been used frequently in the past for evaluating GANs  Radford et al. (2015) ;  Zhao et al. (2016) ;  Chen et al. (2016) ;  Liu & Tuzel (2016) . We evaluated 12 different models in a number of combinations (see  Table 1 ). They are as follows. Models 1, 7, and 11 are the original BEGAN model. Models 2 and 3 only use the GMSM and chrominance distance functions, respectively. Models 4 and 8 are the BEGAN model plus GMSM. Models 5 and 9 use all three Under review as a conference paper at ICLR 2018 distance functions (BEGAN+GMSM+Chrom). Models 6, 10, and 12 use a \\'scaled\\' BEGAN model (β l1 = 2) with GMSM. All models with different model numbers but the same β d values differ in their γ values or the output image size.\\n\\nSection Title: SETUP\\n  SETUP All of the models we evaluate in this paper are based on the architecture of the BEGAN model  Berthelot et al. (2017) . 5 We trained the models using Adam with a batch size of 16, β 1 of 0.9, β 2 of 0.999, and an initial learning rate of 0.00008, which decayed by a factor of 2 every 100,000 epochs. Parameters k t and k 0 were set at 0.001 and 0, respectively (see Equation 5). The γ parameter was set relative to the model (see  Table 1 ). Most of our experiments were performed on 64 × 64 pixel images with a single set of tests run on 128 × 128 images. The number of convolution layers were 3 and 4, respectively, with a constant down-sampled size of 8 × 8. We found that the original size of 64 for the input vector (N z ) and hidden state (N h ) resulted in modal collapse for the models using GMSM. However, we found that this was fixed by increasing the input size to 128 and 256 for the 64 and 128 pixel images, respectively. We used N z = 128 for all models except 12 (scaled BEGAN+GMSM), which used 256. N z always equaled N h in all experiments. Models 2-3 were run for 18,000 epochs, 1 and 4-10 were run for 100,000 epochs, and 11-12 were run for 300,000 epochs. Models 2-4 suffered from modal collapse immediately and 5 (BE- GAN+GMSM+Chrom) collapsed around epoch 65,000 (see Appendix A Figure 4 rows 2-5).\\n\\nSection Title: EVALUATIONS\\n  EVALUATIONS We performed two evaluations. First, to evaluate whether and to what extent the models were able to capture the relevant properties of each associated distance function, we compared the mean and standard deviation of the error scores. We calculated them for each distance function over all epochs of all models. We chose to use the mean rather than the minimum score as we were interested in how each model performs as a whole, rather than at some specific epoch. All calculations use the distance, or one minus the corresponding similarity score, for both the gradient magnitude and chrominance values. Reduced pixelation is an artifact of the intensive scaling for image presentation (up to 4×). All images in the qualitative evaluations were upscaled from their original sizes using cubic image sampling so that they can be viewed at larger sizes. Consequently, the apparent smoothness of the scaled images is not a property of the model.\\n\\nSection Title: RESULTS\\n  RESULTS GANs are used to generate different types of images. Which image components are important depends on the domain of these images. Our results suggest that models used in any particular GAN application should be customized to emphasize the relevant components-there is not a one-size- fits-all component choice. We discuss the results of our four evaluations below.\\n\\nSection Title: MEANS AND STANDARD DEVIATIONS OF ERROR SCORES\\n  MEANS AND STANDARD DEVIATIONS OF ERROR SCORES Results were as expected: the three different distance functions captured different features of the underlying image representations. We compared all of the models in terms of their means and standard deviations of the error score of the associated distance functions (see  Table 2 ). In particular, each of models 1-3 only used one of the distance functions and had the lowest error for the associated function (e.g., model 2 was trained with GMSM and has the lowest GMSM error score). Models 4-6 expanded on the first three models by examining the distance functions in different combinations. Model 5 (BEGAN+GMSM+Chrom) had the lowest chrominance error score and Model 6 (scaled BEGAN+GMSM) had the lowest scores for l 1 and GMSM of any model using a γ of 0.5. For the models with γ set at 0.7, models 7-9 showed similar results to the previous scores. Model 8 (BEGAN+GMSM) scored the lowest GMSM score overall and model 9 (BEGAN+GMSM+Chrom) scored the lowest chrominance score of the models that did not suffer from modal collapse. For the two models that were trained to generate 128 × 128 pixel images, model 12 (scaled BE- GAN+GMSM) had the lowest error scores for l 1 and GMSM, and model 11 (BEGAN) had the lowest score for chrominance. Model 12 had the lowest l 1 score, overall.\\n\\nSection Title: VISUAL COMPARISON OF SIMILARITY SCORES\\n  VISUAL COMPARISON OF SIMILARITY SCORES Subjective visual comparison of the gradient magnitudes in column S of  Figure 2  shows there are more black pixels for model 11 (row 11D) when comparing real images before and after autoencod- ing. This indicates a lower similarity in the autoencoder. Model 12 (row 12D) has a higher similarity between the original and autoencoded real images as indicated by fewer black pixels. This pattern continues for the generator output (rows 11G and 12G), but with greater similarity between the gra- dients of the original and autoencoded images than the real images (i.e., fewer black pixels overall). The visual comparison of chrominance and related similarity score also weakly supported our hy- potheses (see  Figure 3 ). All of the models show a strong ability to capture the I dimension (blue-red) of the YIQ color space, but only model 9 (BEGAN+GMSM+Chrom) is able to accurately capture the relevant information in the Q dimension (green-purple).\\n\\nSection Title: OUTLOOK\\n  OUTLOOK We bring an energy-based formulation to the BEGAN model and some of the problems of the en- ergy function originally proposed in  Zhao et al. (2016) . We proposed a new, multi-component energy function on the basis of research from the Image Quality Assessment literature. The scaled BEGAN+GMSM model produces better image representations than its competitors in ways that can be measured using subjective evaluations of the associated features (e.g., luminance gradient simi- larity, chrominance similarity). For future work, we would like to extend this research to encompass other datasets and FR-IQA energy functions.\\n\\nSection Title: Annex Figures\\n  Annex Figures   fig_3 Figure 4:   Figure 4: Four outputs of each of the generators of all 12 models. The best images for each model were hand-picked. The first row is model 1, which corresponds with the original BEGAN model. Rows 2-12 represent our experiments. Each cell represents the output of a random sample.  \\n', 'role': 'user'}, {'content': \"[Significance and novelty]\\n<Incremental nature of the paper> Reviewers highlighted that the paper's proposed approach was somewhat incremental, and its significance and novelty were questioned. The modifications and additions to the existing BEGAN framework were not perceived as significantly contributing to the field.\\n<Limited novelty of energy function components> The paper's novelty was deemed to be restricted, particularly in the choice and justification of the energy function components. Reviewers pointed out that the selected components were borrowed from existing image quality assessment techniques, and their application was not seen as a significant contribution to GAN theories or algorithm designs.\\n\\n[Potential reasons for acceptance]\\n<No related terms> -\\n\\n[Potential reasons for rejection]\\n<Weak experiments and results analysis> Reviewers noted that the experimental section was weak, with reliance on visual evaluations and a limited range of datasets (primarily CelebA). This led to concerns about the thoroughness and robustness of the paper's findings.\\n<Unclear significance and contribution> The paper was criticized for its lack of clarity in motivating its contributions and presenting results. Reviewers found it difficult to estimate the significance of the proposed model and understand how its results differed from baseline models.\\n\\n[Suggestions for improvement]\\n<Broaden experimental testing> To strengthen the paper, reviewers suggested broadening the experimental testing to include different datasets involving natural images, beyond the single CelebA dataset. This would provide a more comprehensive evaluation of the proposed techniques.\\n<Clarify and justify design choices> Improvements in the paper's clarity and justification were recommended, specifically in clarifying the design choices made for the energy function components. Providing clear justifications for the modifications and additions to the BEGAN framework would enhance the paper's credibility and significance.\\n\\n\", 'role': 'assistant'}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset[0][\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split to Train, Validation, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid_dataset = dataset.train_test_split(test_size=0.2, seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 12452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 3113\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset = train_valid_dataset['test'].train_test_split(test_size=0.2, seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 2490\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['messages'],\n",
       "        num_rows: 623\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['messages'],\n",
       "    num_rows: 2490\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bcf6dec016648acb9abc51eef985f5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/13 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9053f9df634f40a99a6ae23f3c688740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48a2a64148b40bb8b33ca06f0cfeeec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "22182584"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid_dataset['train'].to_json(\"../data/train_dataset.json\", orient=\"records\", lines=True)\n",
    "valid_dataset['train'].to_json(\"../data/valid_dataset.json\", orient=\"records\", lines=True)\n",
    "valid_dataset['test'].to_json(\"../data/test_dataset.json\", orient=\"records\", lines=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cis6200",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
