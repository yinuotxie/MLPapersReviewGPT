{"title": "AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS", "authors": "", "pub_date": "", "abstract": "Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.", "sections": [{"heading": "INTRODUCTION", "text": "Layer depth is one of the decisive factors of the success of Deep Neural Networks (DNNs). For example, image classification accuracy keeps improving as the depth of network models grows (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014;Szegedy et al., 2015;He et al., 2016;Huang et al., 2017). Although shallow networks cannot ensure high accuracy, DNNs composed of too many layers may suffer from over-fitting and convergence difficulty in training. How to obtain the optimal depth for a DNN still remains mysterious. For instance, ResNet-152 (He et al., 2016) uses 3, 8, 36 and 3 residual blocks under output sizes of 56 \u00d7 56, 28 \u00d7 28, 14 \u00d7 14 and 7 \u00d7 7, respectively, which don't show an obvious quantitative relation. In practice, people usually reply on some heuristic trials and tests to obtain the depth of a network: they first design a DNN with a specific depth and then train and evaluate the network on a given dataset; finally, they change the depth and repeat the procedure until the accuracy meets the requirement. Besides the high computational cost induced by the iteration process, such trial & test iterations must be repeated whenever dataset changes. In this paper, we propose AutoGrow that can automate depth discovery given a layer architecture. We will show that AutoGrow generalizes to different datasets and layer architectures.\nThere are some previous works which add or morph layers to increase the depth in DNNs. Vg-gNet (Simonyan & Zisserman, 2014) and DropIn (Smith et al., 2016) added new layers into shallower DNNs; Network Morphism (Wei et al., 2016;Chen et al., 2015) morphed each layer to multiple layers to increase the depth meanwhile preserving the function of the shallower net. Table 1 summarizes differences in this work. Their goal was to overcome difficulty of training deeper DNNs or accelerate it. Our goal is to automatically find an optimal depth. Moreover, previous works applied layer growth by once or a few times at pre-defined locations to grow a pre-defined number of layers; in contrast, ours automatically learns the number of new layers and growth locations without limiting growing times. We will summarize more related works in Section 4. Figure 1 illustrates an example of AutoGrow. It starts from the shallowest backbone network and gradually grows sub-modules (A sub-module can be one or more layers, e.g., a residual block); the growth stops once a stopping policy is satisfied. We studied multiple initializers of new layers and multiple growing policies, and surprisingly find that: (1) a random initializer works equally or better than complicated Network Morphism; (2) it is more effective to grow before a shallow net converges. We hypothesize that this is because a converged shallow net is an inadequate initialization for training deeper net, while random initialization can help to escape from a bad starting point.\nMotivated by this, we intentionally avoid full convergence during the growing by using (1) random initialization of new layers, (2) a constant large learning rate, and (3) a short growing interval.  (3) We challenge the idea of Network Morphism, as random initialization works equally or better when growing layers. (4) We find that it is beneficial to rapidly grow layers before a shallower net converge, contradicting previous intuition.\n2 AutoGrow -A DEPTH GROWING ALGORITHM Algorithm 1 AutoGrow Algorithm.\nInput :\nA seed shallow network g(X0) composed of M sub-networks F = {fi (\u2022; Wi) : i = 0 . . . M \u2212 1},\nwhere each sub-network has only one sub-module (a dimension reduction sub-module); an epoch interval K to check growing and stopping policies; the number of fine-tuning epochs N after growing.", "n_publication_ref": 10, "n_figure_ref": 1}, {"heading": "Initialization:", "text": "A Circular Linked List of sub-networks under growing:\nsubNetList = f0 (\u2022; W0) \u2192 \u2022 \u2022 \u2022 \u2192 f M \u22121 (\u2022; W M \u22121 ) \u2190 \u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212\u2212 \u2212 ;\nThe current growing sub-network: growingSubNet = subNetList.head() = f0 (\u2022; W0);\nThe recent grown sub-network: grownSubNet = None; Process : A trained neural network g(X0) with learned depth.\n#\nFigure 1 gives an overview of the proposed AutoGrow. In this paper, we use network, sub-networks, sub-modules and layers to describe the architecture hierarchy. A network is composed of a cascade of sub-networks. A sub-network is composed of sub-modules, which typical share the same output size. A sub-module (e.g. a residual block) is an elementary growing block composed of one or a few layers. In this section, we rigorously formulate a generic version of AutoGrow which will be materialized in subsections. A deep convolutional network g(X 0 ) is a cascade of sub-networks by composing functions as g(X\n0 ) = l (f M \u22121 (f M \u22122 (\u2022 \u2022 \u2022 f 1 (f 0 (X 0 )) \u2022 \u2022 \u2022 )))\n, where X 0 is an input image, M is the number of sub-networks, l(\u2022) is a loss function, and X i+1 = f i (X i ) is a sub-network that operates on an input image or a feature tensor X i \u2208 R ci\u00d7hi\u00d7wi . Here, c i is the number of channels, and h i and w i are spatial dimensions. f i (X i ) is a simplified notation of f i (X i ; W i ), where W i is a set of sub-modules' parameters within the i-th sub-network. Thus W = {W i : i = 0 . . . M \u2212 1} denotes the whole set of parameters in the DNN. To facilitate growing, the following properties are supported within a sub-network: (1) the first sub-module usually reduces the size of input feature maps, e.g., using pooling or convolution with a stride; and (2) all sub-modules in a sub-network maintain the same output size. As such, our framework can support popular networks, including VggNet-like plain networks (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2015), ResNets (He et al., 2016) and DenseNets (Huang et al., 2017). In this paper, we select ResNets and VggNet-like nets as representatives of DNNs with and without shortcuts, respectively.\nWith above notations, Algorithm 1 rigorously describes the AutoGrow algorithm. In brief, AutoGrow starts with the shallowest net where every sub-network has only one sub-module for spatial dimension reduction. AutoGrow loops over all growing sub-networks in order. For each sub-network, AutoGrow stacks a new sub-module. When the new sub-module does not improve the accuracy, the growth in corresponding sub-network will be permanently stopped. The details of our method will be materialized in the following subsections.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "SEED SHALLOW NETWORKS AND SUB-MODULES", "text": "In this paper, in all datasets except ImageNet, we explore growing depth for four types of DNNs: (1) Basic3ResNet: the same ResNet used for CIFAR10 in He et al. (2016) In AutoGrow, the architectures of seed shallow networks and sub-modules are pre-defined. In plain DNNs, a sub-module is a stack of convolution, Batch Normalization and ReLU; in residual DNNs, a sub-module is a residual block. In AutoGrow, a sub-network is a stack of all sub-modules with the same output spatial size. Unlike He et al. (2016) which manually designed the depth, AutoGrow starts from a seed architecture in which each sub-network has only one sub-module and automatically learns the number of sub-modules.\nOn ImageNet, we apply the same backbones in He et al. (2016) as the seed architectures. A seed architecture has only one sub-module under each output spatial size. For a ResNet using basic residual blocks or bottleneck residual blocks (He et al., 2016), we respectively name it as Basic4ResNet or Bottleneck4ResNet. Plain4Net is also obtained by removing shortcuts in Basic4ResNet.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "SUB-MODULE INITIALIZERS", "text": "Here we explain how to initialize a new sub-module W in initializer(W) mentioned in Algorithm 1. Network Morphism changes DNN architecture meanwhile preserving the loss function via special initialization of new layers, that is, g(X 0 ; W) = g(X 0 ; W \u222a W) \u2200X 0 . A residual sub-module shows a nice property: when stacking a residual block and initializing the last Batch Normalization layer as zeros, the function of the shallower net is preserved but the DNN is morphed to a deeper net. Thus, Network Morphism can be easily implemented by this zero initialization (ZeroInit).\nIn this work, all layers in W are initialized using default randomization, except for a special treatment of the last Batch Normalization layer in a residual sub-module. Besides ZeroInit, we propose a new AdamInit for Network Morphism. In AdamInit, we freeze all parameters except the last Batch Normalization layer in W, and then use Adam optimizer (Kingma & Ba, 2014) to optimize the last Bath Normalization for maximum 10 epochs till the training accuracy of the deeper net is as good as the shallower one. After AdamInit, all parameters are jointly optimized. We view AdamInit as a Network Morphism because the training loss is similar after AdamInit. We empirically find that AdamInit can usually find a solution in less than 3 epochs. We also study random initialization of the last Batch Normalization layer using uniform (UniInit) or Gaussian (GauInit) noises with a standard deviation 1.0. We will show that GauInit obtains the best result, challenging the idea of Network Morphism (Wei et al., 2016;Chen et al., 2015).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "GROWING AND STOPPING POLICIES", "text": "In Algorithm 1, a growing policy refers to meetGrowingPolicy(), which returns true when the network should grow a sub-module. Two growing policies are studied here:\n1. Convergent Growth: meetGrowingPolicy() returns true when the improvement of validation accuracy is less than \u03c4 in the last K epochs. That is, in Convergent Growth, AutoGrow only grows when current network has converged. This is a similar growing criterion adopted in previous works (Elsken et al., 2017;Cai et al., 2018a;b). 2. Periodic Growth: meetGrowingPolicy() always returns true, that is, the network always grows every K epochs. Therefore, K is also the growing period. In the best practice of AutoGrow, K is small (e.g. K = 3) such that it grows before current network converges.\nOur experiments will show that Periodic Growth outperforms Convergent Growth. We hypothesize that a fully converged shallower net is an inadequate initialization to train a deeper net. We will perform experiments to test this hypothesis and visualize optimization trajectory to illustrate it.\nA stopping policy denotes meetStoppingPolicy() in Algorithm 1. When Convergent Growth is adopted, meetStoppingPolicy() returns true if a recent growth does not improve validation accuracy more than \u03c4 within K epochs. We use a similar stopping policy for Periodic Growth; however, as it can grow rapidly with a small period K (e.g. K = 3) before it converges, we use a larger window size J for stop. Specifically, when Periodic Growth is adopted, meetStoppingPolicy() returns true when the validation accuracy improves less than \u03c4 in the last J epochs, where J K.\nHyper-parameters \u03c4 , J and K control the operation of AutoGrow and can be easily setup and generalize well. \u03c4 denotes the significance of accuracy improvement for classification. We simply set \u03c4 = 0.05% in all experiments. J represents how many epochs to wait for an accuracy improvement before stopping the growth of a sub-network. It is more meaningful to consider stopping when the new net is trained to some extent. As such, we set J to the number of epochs T under the largest learning rate when training a baseline. K means how frequently AutoGrow checks the polices. In Convergent Growth, we simply set K = T , which is long enough to ensure convergence. In Periodic Growth, K is set to a small fraction of T to enable fast growth before convergence; more importantly, K = 3 is very robust to all networks and datasets. Therefore, all those hyper-parameters are very robust and strongly correlated to design considerations.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "EXPERIMENTS", "text": "In this paper, we use Basic3ResNet-2-3-2, for instance, to denote a model architecture which contains 2, 3 and 2 sub-modules in the first, second and third sub-networks, respectively. Sometimes we simplify it as 2-3-2 for convenience. AutoGrow always starts from the shallowest depth of 1-1-1 and uses the maximum validation accuracy as the metric to guide growing and stopping. All DNN baselines are trained by SGD with momentum 0.9 using staircase learning rate. The initial learning rate is 0.1 in ResNets and 0.01 in plain networks. On ImageNet, baselines are trained using batch size 256 for 90 epochs, within which learning rate is decayed by 0.1\u00d7 at epoch 30 and 60. In all other smaller datasets, baselines are trained using batch size 128 for 200 epochs and learning rate is decayed by 0.1\u00d7 at epoch 100 and 150.\nOur early experiments followed prior wisdom by growing layers with Network Morphism (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b), i.e., AutoGrow with ZeroInit (or AdamInit) and Convergent Growth policy; however, it stopped early with very shallow DNNs, failing to find optimal depth. We hypothesize that a converged shallow net with Network Morphism gives a bad initialization to train a deeper neural network. Section 3.1 experimentally test that the hypothesis is valid. To tackle this issue, we intentionally avoid convergence during growing by three simple solutions, which are evaluated in Section 3.2. Finally, Section 3.3 and Section 3.4 include extensive experiments to show the effectiveness of our final AutoGrow.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "SUBOPTIMUM OF NETWORK MORPHISM AND CONVERGENT GROWTH", "text": "In this section, we study Network Morphism itself and its integration into our AutoGrow under Convergent Growth. When studying Network Morphism, we take the following steps: 1) train a shallower ResNet to converge, 2) stack residual blocks on top of each sub-network to morph to a deeper net, 3) use ZeroInit or AdamInit to initialize new layers, and 4) train the deeper net in a standard way. We compare the accuracy difference (\"\u2206\") between Network Morphism and training the deeper net from scratch. Table 2 summaries our results. Network Morphism has a lower accuracy (negative \"\u2206\") in all the cases, which validates our hypothesis that a converged shallow network with Network Morphism gives a bad initialization to train a deeper net. We visualize the optimization trajectories in Appendix A.0.1 to illustrate the hypothesis.\nTo further validate our hypothesis, we integrate Network Morphism as the initializer in AutoGrow with Convergent Growth policy. We refer to this version of AutoGrow as c-AutoGrow with \"c-\" denoting \"Convergent.\" More specific, we take ZeroInit or AdamInit as sub-module initializer and \"Convergent Growth\" policy in Algorithm 1. To recap, in this setting, AutoGrow trains a shallower net till it converges, then grows a sub-module which is initialized by Network Morphism, and repeats the same process till there is no further accuracy improvement. In every interval of K training epochs (train(g(X 0 ), K) in Algorithm 1), \"staircase\" learning rate is used. The learning rate is reset to 0.1 at the first epoch, and decayed by 0.1\u00d7 at epoch K 2 and 3K 4 . The results are shown in Table 3 by \"staircase\" rows, which illustrate that c-AutoGrow can grow a DNN multiple times and finally find a depth. However, there are two problems: 1) the final accuracy is lower than training the found net from scratch, as indicated by \"\u2206\", validating our hypothesis; 2) the depth learning stops too early with a relatively shallower net, while a deeper net beyond the found depth can achieve a higher accuracy as we will show in Table 6. These problems provide a circumstantial evidence of the hypothesis that a converged shallow net with Network Morphism gives a bad initialization. Thus, AutoGrow cannot receive signals to continue growing after a limited number of growths. In Appendix A.0.1, Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row \"2-3-6\" in Table 3.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "ABLATION STUDY FOR AutoGrow DESIGN", "text": "Based on the findings in Section 3.1, we propose three simple but effective solutions to further enhance AutoGrow and refer it as p-AutoGrow, with \"p-\" denoting \"Periodic\": (1) Use a large constant learning rate for growing, i.e., 0.1 for residual networks and 0.01 for plain networks. Stochastic gradient descent with a large learning rate intrinsically introduces noises, which help to avoid a full convergence into a bad initialization from a shallower net. Note that staircase learning rate is still used for fine-tuning after discovering the final DNN; (2) Use random initialization (UniInit or GauInit) as noises to escape from an inadequate initialization; (3) Grow rapidly before a shallower net converges by taking Periodic Growth with a small K. p-AutoGrow is our final AutoGrow. In the rest part of this section, we perform ablation study to prove that the three solutions are effective. We start from c-AutoGrow, and incrementally add above solutions one by one and eventually obtain p-AutoGrow. In Table 3, first, we replace the staircase learning rate with a constant learning rate, the accuracy of AutoGrow improves and therefore \"\u2206\" improves; second, we further replace Network Morphism (ZeroInit or AdamInit) with a random initializer (UniInit or GauInit) and result in a bigger gain. Overall, combining a constant learning rate with GauInit performs the best. Thus, constant learning rate and GauInit are adopted in the remaining experiments, unless we explicitly specify them.  Note that, in this paper, we are more interested in automating depth discovery to find a final DNN (\"found net\") with a high accuracy (\"accu\"). Ideally, the \"found net\" has a minimum depth, a larger depth than which cannot further improve \"accu\". We will show in Figure 3 that AutoGrow discovers a depth approximately satisfying this property. The \"\u2206\" is a metric to indicate how well shallower nets initialize deeper nets; a negative \"\u2206\" indicates that weight initialization from a shallower net hurts training of a deeper net; while a positive \"\u2206\" indicates AutoGrow helps training a deeper net, which is a byproduct of this work.\nFinally, we apply the last solution -Periodic Growth, and obtain our final p-AutoGrow. Our ablation study results for p-AutoGrow are summarized in Table 5 and Table 4. Table 5 analyzes the impact of the growing period K. In general, K is a hyper-parameter to trade off speed and accuracy: a smaller K takes a longer learning time but discovers a deeper net, vice versa. Our results validate the preference of a faster growth (i.e. a smaller K). On CIFAR10/CIFAR100, the accuracy reaches plateau/peak at K = 3; further reducing K produces a deeper net while the accuracy gain is marginal/impossible. In the following, we simply select K = 3 for robustness test. More importantly, our quantitative results in Table 5 show that p-AutoGrow finds much deeper nets, overcoming the very-early stop issue in c-AutoGrow in Table 3. That is, Periodic Growth proposed in this work is much more effective than Convergent Growth utilized in previous work.\nFor sanity check, we perform the ablation study of initializers for p-AutoGrow. The results are in Table 8 in Appendix A.0.3, which further validates our wisdom on selecting GauInit. The motivation of Network Morphism in previous work was to start a deeper net from a loss function that has been well optimized by a shallower net, so as not to restart the deeper net training from scratch (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b). In all our experiments, we find this is sure even with random initialization. Figure 2 plots the convergence curves and learning process for \"42-42-42\" in Table 5. Even with GauInit, the loss and accuracy rapidly recover and no restart is observed. The convergence pattern in the \"Growing\" stage is similar to the \"Fine-tuning\" stage under the same learning rate (the initial learning rate 0.1). Similar results on ImageNet will be shown in Figure 8. Our results challenge the necessity of Network Morphism when growing neural networks.\nAt last, we perform the ablation study on the initial depth of the seed network. Table 4 demonstrates that a shallowest DNN works as well as a deeper seed. This implies that AutoGrow can appropriately stop regardless of the depth of the seed network. As the focus of this work is on depth automation, we prefer starting with the shallowest seed to avoid a manual search of a seed depth.  The seed net is Basic3ResNet-1-1-1.  ", "n_publication_ref": 5, "n_figure_ref": 3}, {"heading": "ADAPTABILITY OF AutoGrow", "text": "To verify the adaptability of AutoGrow, we use an identical configuration (p-AutoGrow with K = 3) and test over 5 datasets and 4 seed architectures. Table 6 includes the results of all 20 combinations. Figure 3 compares AutoGrow with manual search which is obtained by training many DNNs with different depths from scratch. The results lead to the following conclusions and contributions:  2. For ResNets, a discovered depth (\" \" in Figure 3) falls at the location where accuracy saturates. This means AutoGrow discovers a near-optimal depth: a shallower depth will lose accuracy while a deeper one gains little. The final accuracy of AutoGrow is as good as training the discovered net from scratch as indicated by \"\u2206\" in Table 6, indicating that initialization from shallower nets does not hurt training of deeper nets. As a byproduct, in plain networks, there are large positive \"\u2206\"s in Table 6. It implies that baselines fail to train very deep plain networks even using Batch Normalization, but AutoGrow enables the training of these networks; In Appendix A.0.3, Table 9 shows the accuracy improvement of plain networks by tuning K, approaching the accuracy of ResNets with the same depth.\n3. For robustness and generalization study purpose, we stick to K = 3 in our experiments, however, we can tune K to trade off accuracy and model size. As shown in Figure 3, Auto-Grow discovers smaller DNNs when increasing K from 3 (\" \") to 50 (\" \"). Interestingly, the accuracy of plain networks even increases at K = 50. This implies the possibility of discovering a better accuracy-depth trade-off by tuning K, although we stick to K = 3 for generalizability study and it generalizes well.\n4. In Table 6, AutoGrow discovers different depths under different sub-modules. The final accuracy is limited by the sub-module design, not by our AutoGrow. Given a sub-module architecture, our AutoGrow can always find a near-optimal depth. With a better sub-module architecture, such as NASNet cell , AutoGrow can improve accuracy.\nFinally, our supposition is that: when the size of dataset is smaller, the optimal depth should be smaller. Under this supposition, we test the effectiveness of AutoGrow by sampling a subset of dataset and verify if AutoGrow can discover a shallower depth. In Appendix A.0.3, Table 11 summarizes the results. As expected, our experiments show that AutoGrow adapts to shallower networks when the datasets are smaller.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "SCALING TO IMAGENET AND EFFICIENCY", "text": "In ImageNet, K = 3 should generalize well, but we explore AutoGrow with K = 2 and K = 5 to obtain an accuracy-depth trade-off line for comparison with human experts. The larger K = 5 enables AutoGrow to obtain a smaller DNN to trade-off accuracy and model size (computation) and the smaller K = 2 achieves higher accuracy. The results are shown in Table 7, which proves that AutoGrow automatically finds a good depth without any tuning. As a byproduct, the accuracy is even higher than training the found net from scratch, indicating that the Periodic Growth in AutoGrow helps training deeper nets. The comparison of AutoGrow and manual depth design (He et al., 2016) is in Figure 4, which shows that AutoGrow achieves better trade-off between accuracy and computation (measured by floating point operations).\nIn Appendix A.0.3, Table 10 summarizes the breakdown of wall-clock time in AutoGrow. The growing/searching time is as efficient as (often more efficient than) fine-tuning the single discovered DNN. The scalability of AutoGrow comes from its intrinsic features that (1) it grows quickly with a short period K and stops immediately if no improvement is sensed; and (2) the network is small at the beginning of growing. ", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "RELATED WORK", "text": "Neural Architecture Search (NAS) (Zoph & Le, 2016) and neural evolution (Miikkulainen et al., 2019;Angeline et al., 1994;Stanley & Miikkulainen, 2002;Liu et al., 2017a;Real et al., 2017) can search network architectures from a gigantic search space. In NAS, the depth of DNNs in the search space is fixed, while AutoGrow learns the depth. Some NAS methods (Bender et al., 2018;Liu et al., 2018b;Cortes et al., 2017) can find DNNs with different depths, however, the maximum depth is pre-defined and shallower nets are obtained by padding zero operations or selecting shallower branches, while our AutoGrow learns the depth in an open domain to find a minimum depth, beyond which no accuracy improvement can be obtained. Moreover, NAS is very computation and memory intensive. To accelerate NAS, one-shot models (Saxena & Verbeek, 2016;Pham et al., 2018;Bender et al., 2018), DARTS (Liu et al., 2018b) andNAS with Transferable Cell (Zoph et al., 2018;Liu et al., 2018a) were proposed. The search time reduces dramatically but is still long from practical perspective. It is still very challenging to deploy these methods to larger datasets such as ImageNet.\nIn contrast, our AutoGrow can scale up to ImageNet thanks to its short depth learning time, which is as efficient as training a single DNN.\nIn addition to architecture search which requires to train lots of DNNs from scratch, there are also many studies on learning neural structures within a single training. Structure pruning and growing were proposed for different goals, such as efficient inference (Wen et al., 2016;Li et al., 2016;Lebedev & Lempitsky, 2016;He et al., 2017;Luo et al., 2017;Liu et al., 2017b;Dai et al., 2017;Huang et al., 2018;Gordon et al., 2018;Du et al., 2019), lifelong learning (Yoon et al., 2017 and model adaptation (Feng & Darrell, 2015;Philipp & Carbonell, 2017). However, those works fixed the network depth and limited structure learning within the existing layers. Optimization over a DNN with fixed depth is easier as the skeleton architecture is known. AutoGrow performs in a scenario where the DNN depth is unknown hence we need to seek for the optimal depth. We hypothesize that a converged shallower net may not be an adequate initialization. Figure 5 visualizes and compares the optimization trajectories of Network Morphism and the training from scratch. In this figure, the shallower net is Basic3ResNet-3-3-3 (ResNet-20) and the deeper one is Basic3ResNet-5-5-5 (ResNet-32) in Table 2. The initializer is ZeroInit. The visualization method is extended from Li et al. (2018). Points on the trajectory are evenly sampled every a few epochs. To maximize the variance of trajectory, we use PCA to project from a high dimensional space to a 2D space and use the first two Principle Components (PC) to form the axes in Figure 5. The contours of training loss function and the trajectory are visualized around the final minimum of the deeper net. When projecting a shallower net to a deeper net space, zeros are padded for the parameters not existing in the deeper net. We must note that the loss increase along the trajectory does not truly represent the situation in high dimensional space, as the trajectory is just a projection. It is possible that the loss remains decreasing in the high dimension while it appears in an opposite way in the 2D space. The sharp detour at \"Morphing\" in Figure 5(a) may indicate that the shallower net plausibly converges to a point that the deeper net struggles to escape. In contrast, Figure 5(b) shows that the trajectory of the direct optimization in the deeper space smoothly converges to a better minimum.\nFigure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row \"2-3-6\" in Table 3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure 6(b) visualizes the trajectory corresponding to row \"2-4-3\" in Table 3, which is much smoother compared to Figure 6(a).   10.\nTable 11 summarizes the adaptability of AutoGrow to the sizes of dataset. In each set of experiments, dataset is randomly down-sampled to 100%, 75%, 50% and 25%. For a fair comparison, K is divided by the percentage of dataset such that the number of mini-batches between growths remains    The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table 10. ", "n_publication_ref": 29, "n_figure_ref": 7}], "references": [{"title": "An evolutionary algorithm that constructs recurrent neural networks", "journal": "IEEE transactions on Neural Networks", "year": "1994", "authors": "J Peter;  Angeline; M Gregory; Jordan B Saunders;  Pollack"}, {"title": "Understanding and simplifying one-shot architecture search", "journal": "", "year": "2018", "authors": "Gabriel Bender; Pieter-Jan Kindermans; Barret Zoph; Vijay Vasudevan; Quoc Le"}, {"title": "Efficient architecture search by network transformation", "journal": "", "year": "2018", "authors": "Han Cai; Tianyao Chen; Weinan Zhang; Yong Yu; Jun Wang"}, {"title": "Path-level network transformation for efficient architecture search", "journal": "", "year": "2018", "authors": "Han Cai; Jiacheng Yang; Weinan Zhang; Song Han; Yong Yu"}, {"title": "Net2net: Accelerating learning via knowledge transfer", "journal": "", "year": "2015", "authors": "Tianqi Chen; Ian Goodfellow; Jonathon Shlens"}, {"title": "Adanet: Adaptive structural learning of artificial neural networks", "journal": "", "year": "2017", "authors": "Corinna Cortes; Xavier Gonzalvo; Vitaly Kuznetsov; Mehryar Mohri; Scott Yang"}, {"title": "Nest: A neural network synthesis tool based on a grow-and-prune paradigm", "journal": "", "year": "2017", "authors": "Xiaoliang Dai; Hongxu Yin; Niraj K Jha"}, {"title": "Cgap: Continuous growth and pruning for efficient deep learning", "journal": "", "year": "2019", "authors": "Xiaocong Du; Zheng Li; Yu Cao"}, {"title": "Simple and efficient architecture search for cnns", "journal": "", "year": "2017", "authors": "Thomas Elsken; Jan-Hendrik Metzen; Frank Hutter"}, {"title": "Learning the structure of deep convolutional networks", "journal": "", "year": "2015", "authors": "Jiashi Feng; Trevor Darrell"}, {"title": "Morphnet: Fast & simple resource-constrained structure learning of deep networks", "journal": "", "year": "2018", "authors": "Ariel Gordon; Elad Eban; Ofir Nachum; Bo Chen; Hao Wu; Tien-Ju Yang; Edward Choi"}, {"title": "Deep residual learning for image recognition", "journal": "", "year": "2016", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"title": "Channel pruning for accelerating very deep neural networks", "journal": "", "year": "2017", "authors": "Yihui He; Xiangyu Zhang; Jian Sun"}, {"title": "Densely connected convolutional networks", "journal": "", "year": "2017", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"title": "Condensenet: An efficient densenet using learned group convolutions", "journal": "", "year": "2018", "authors": "Gao Huang; Shichen Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"title": "Adam: A method for stochastic optimization", "journal": "", "year": "2014", "authors": "P Diederik; Jimmy Kingma;  Ba"}, {"title": "Imagenet classification with deep convolutional neural networks", "journal": "", "year": "2012", "authors": "Alex Krizhevsky; Ilya Sutskever; Geoffrey E Hinton"}, {"title": "Fast convnets using group-wise brain damage", "journal": "", "year": "2016", "authors": "Vadim Lebedev; Victor Lempitsky"}, {"title": "", "journal": "", "year": "2016", "authors": "Hao Li; Asim Kadav; Igor Durdanovic; Hanan Samet; Hans Peter Graf"}, {"title": "Visualizing the loss landscape of neural nets", "journal": "", "year": "2018", "authors": "Hao Li; Zheng Xu; Gavin Taylor; Christoph Studer; Tom Goldstein"}, {"title": "Progressive neural architecture search", "journal": "", "year": "2018", "authors": "Chenxi Liu; Barret Zoph; Maxim Neumann; Jonathon Shlens; Wei Hua; Li-Jia Li; Li Fei-Fei; Alan Yuille; Jonathan Huang; Kevin Murphy"}, {"title": "Hierarchical representations for efficient architecture search", "journal": "", "year": "2017", "authors": "Hanxiao Liu; Karen Simonyan; Oriol Vinyals; Chrisantha Fernando; Koray Kavukcuoglu"}, {"title": "Darts: Differentiable architecture search", "journal": "", "year": "2018", "authors": "Hanxiao Liu; Karen Simonyan; Yiming Yang"}, {"title": "Learning efficient convolutional networks through network slimming", "journal": "", "year": "2017", "authors": "Zhuang Liu; Jianguo Li; Zhiqiang Shen; Gao Huang; Shoumeng Yan; Changshui Zhang"}, {"title": "Thinet: A filter level pruning method for deep neural network compression", "journal": "", "year": "2017", "authors": "Jian-Hao Luo; Jianxin Wu; Weiyao Lin"}, {"title": "Evolving deep neural networks", "journal": "Elsevier", "year": "2019", "authors": "Jason Risto Miikkulainen; Elliot Liang; Aditya Meyerson; Daniel Rawal; Olivier Fink; Bala Francon; Hormoz Raju; Arshak Shahrzad; Nigel Navruzyan;  Duffy"}, {"title": "Efficient neural architecture search via parameter sharing", "journal": "", "year": "2018", "authors": "Hieu Pham; Y Melody; Barret Guan;  Zoph; V Quoc; Jeff Le;  Dean"}, {"title": "Nonparametric neural networks", "journal": "", "year": "2017", "authors": "George Philipp; G Jaime;  Carbonell"}, {"title": "Large-scale evolution of image classifiers", "journal": "", "year": "2017", "authors": "Esteban Real; Sherry Moore; Andrew Selle; Saurabh Saxena; Yutaka Leon Suematsu; Jie Tan; V Quoc; Alexey Le;  Kurakin"}, {"title": "Convolutional neural fabrics", "journal": "", "year": "2016", "authors": "Shreyas Saxena; Jakob Verbeek"}, {"title": "Very deep convolutional networks for large-scale image recognition", "journal": "", "year": "2014", "authors": "Karen Simonyan; Andrew Zisserman"}, {"title": "Gradual dropin of layers to train very deep neural networks", "journal": "", "year": "2016", "authors": "N Leslie; Emily M Smith; Timothy Hand;  Doster"}, {"title": "Evolving neural networks through augmenting topologies", "journal": "Evolutionary computation", "year": "2002", "authors": "O Kenneth; Risto Stanley;  Miikkulainen"}, {"title": "Going deeper with convolutions", "journal": "", "year": "2015", "authors": "Christian Szegedy; Wei Liu; Yangqing Jia; Pierre Sermanet; Scott Reed; Dragomir Anguelov; Dumitru Erhan; Vincent Vanhoucke; Andrew Rabinovich"}, {"title": "Deep growing learning", "journal": "", "year": "2017", "authors": "Guangcong Wang; Xiaohua Xie; Jianhuang Lai; Jiaxuan Zhuo"}, {"title": "International Conference on Machine Learning", "journal": "", "year": "2016", "authors": "Tao Wei; Changhu Wang; Yong Rui; Chang Wen Chen"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: p-AutoGrow on CIFAR10 (K = 3).The seed net is Basic3ResNet-1-1-1.", "figure_data": ""}, {"figure_label": "34", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :Figure 4 :34Figure 3: AutoGrow vs manual search obtained by training many baselines from scratch. x \u2212 axis is the number of parameters. Dataset is CIFAR10.", "figure_data": ""}, {"figure_label": "56", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 5 :Figure 6 :56Figure6(a) visualizes the trajectory of c-AutoGrow corresponding to row \"2-3-6\" in Table3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure6(b) visualizes the trajectory corresponding to row \"2-4-3\" in Table3, which is much smoother compared to Figure6(a). Figure6(c)(d) visualize the trajectories of p-AutoGrow with K = 50 and 3. The 2D projection gives limited information to reveal the advantages of p-AutoGrow", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 7 :7Figure 7: Loss surfaces around minima found by baselines and AutoGrow. Dataset is CIFAR10.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 8:The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table10.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "A simple example of AutoGrow.", "figure_data": "EpochsSeedDiscoveredGrowingsub-nets Sub-modules Stopped sub-nets Initializer Figure 1: Previous works Goal Ease training Depth automation Ours Times Once or a few Unlimited Locations Human defined Learned Layer # Human defined Learned"}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison with previous works about layer growth.", "figure_data": ""}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Network Morphism tested on CIFAR10.", "figure_data": "net backboneshallowerdeeperinitializeraccu %\u2206  *Basic3ResNet3-3-35-5-5ZeroInit AdamInit92.71 92.82-0.77 -0.66Basic3ResNet5-5-59-9-9ZeroInit AdamInit93.64 93.53-0.27 -0.38Basic4ResNet 1-1-1-1 2-2-2-2ZeroInit AdamInit94.96 95.17-0.37 -0.16"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Ablation study of c-AutoGrow.", "figure_data": "datasetlearning rateinitializerfound net  \u2020 accu %\u2206  *datasetlearning rateinitializerfound net  \u2020 accu %\u2206  *staircase ZeroInit2-3-691.77-1.06staircase ZeroInit4-3-470.04-0.65staircase AdamInit3-4-392.21-0.59staircase AdamInit3-3-369.85-0.65CIFAR10constant ZeroInit2-2-492.230.16CIFAR100constant ZeroInit3-2-470.220.35constant AdamInit3-4-492.60-0.41constant AdamInit3-3-370.00-0.50constantUniInit3-4-492.93-0.08constantUniInit4-4-370.390.36constantGauInit2-4-393.120.55constantGauInit3-4-370.660.91\u2020 Basic3ResNet*  \u2206 = (accuracy of c-AutoGrow) \u2212 (accuracy of training from scratch)"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "p-AutoGrow with different seed architecture.", "figure_data": ""}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "p-AutoGrow with different growing interval K.", "figure_data": "CIFAR10CIFAR100Kfound net  \u2020accu %Kfound net  \u2020accu %506-5-392.95508-5-772.07207-7-793.2620 8-11-1072.9310 19-19-1993.4610 18-18-1873.645 23-22-2293.985 23-23-2373.703 42-42-4294.273 54-53-5374.721 77-76-7694.301 68-68-6874.51\u2020 Basic3ResNet\u2020 Basic3ResNet"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "The adaptability of AutoGrow to datasets", "figure_data": "netdatasetfound netaccu %\u2206  *netdatasetfound netaccu %\u2206  *CIFAR1042-42-4294.27-0.03CIFAR1023-22-2290.826.49CIFAR10054-53-5374.72-0.95CIFAR10028-28-2766.3431.53Basic3ResNetSVHN34-34-3497.220.04Plain3NetSVHN36-35-3596.7977.20FashionMNIST 30-29-2994.57-0.06FashionMNIST 17-17-1794.490.56MNIST33-33-3399.64-0.03MNIST20-20-2099.660.12CIFAR1022-22-22-22 95.49-0.10CIFAR1017-17-17-17 94.205.72CIFAR10017-51-16-16 79.471.22CIFAR10016-15-15-15 73.9129.34Basic4ResNetSVHN20-20-19-19 97.32-0.08Plain4NetSVHN12-12-12-11 97.080.32FashionMNIST 27-27-27-26 94.62-0.17FashionMNIST 13-13-13-13 94.470.72MNIST11-10-10-10 99.660.01MNIST13-12-12-12 99.570.03*  \u2206 = (accuracy of AutoGrow) \u2212 (accuracy of training from scratch)"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Scaling up to ImageNet", "figure_data": "netK found netTop-1 Top-5  \u2020 \u2206 Top-1Basic4ResNet2 512-12-11-11 76.28 92.79 9-3-6-4 74.75 91.970.43 0.72Bottleneck4ResNet2 56-6-6-17 6-7-3-977.99 93.91 77.33 93.650.83 0.83Plain4Net2 56-6-6-6 5-5-5-471.22 90.08 70.54 89.760.70 0.93\u2020 \u2206 = (Top-1 of AutoGrow) \u2212 (Top-1 of training from scratch)"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "TaoWei, Changhu Wang, and Chang Wen Chen.  Modularized morphing of neural networks. arXiv preprint arXiv:1701.03281, 2017. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pp. 2074-2082, 2016. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.", "figure_data": "Barret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. arXiv preprintarXiv:1611.01578, 2016.Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architecturesfor scalable image recognition. In Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 8697-8710, 2018.A APPENDIXA.0.1 OPTIMIZATION TRAJECTORIES OF NETWORK MORPHISM"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "p-AutoGrow under initializers with K = 3.", "figure_data": "CIFAR10initializerfound net  \u2020"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_13", "figure_caption": "AutoGrow improves accuracy of plain nets.", "figure_data": "datasetnet layer #methodaccu %Plain4Net-6-6-6-626baseline93.90CIFAR10Plain4Net-6-6-6-626AutoGrow K = 3095.17Basic4ResNet-3-3-3-326baseline95.33Plain3Net-11-11-1034baseline90.45CIFAR10Plain3Net-11-11-1034AutoGrow K = 5093.13Basic3ResNet-6-6-536baseline93.60"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_14", "figure_caption": "The efficiency of AutoGrow", "figure_data": "net GPUsgrowingfine-tuning"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_15", "figure_caption": "The adaptability of AutoGrow to dataset sizes", "figure_data": "Basic3ResNet on CIFAR10Plain3Net on MNISTdataset sizefound netaccu %dataset sizefound netaccu %100%42-42-4294.27100%20-20-2099.6675%32-31-3193.5475%12-12-1299.5450%17-17-1791.3450%12-11-1199.4625%21-12-788.1825%10-9-999.33Basic4ResNet on CIFAR100Plain4Net on SVHNdataset sizefound netaccu %dataset sizefound netaccu %100%17-51-16-1679.47100%12-12-12-1197.0875%17-17-16-1677.2675%9-9-9-996.7150%12-12-12-1172.9150%8-8-8-896.3725%6-6-6-662.5325%5-5-5-595.68"}], "formulas": [], "doi": ""}