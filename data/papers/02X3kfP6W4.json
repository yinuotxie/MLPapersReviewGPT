{"title": "Assessing the validity of saliency maps for abnormality localization in medical imaging", "authors": "Nishanth Thumbavanam Arun; Nathan Gaw; Praveer Singh; Ken Chang; Katharina Viktoria Hoebel; Jay Patel; Mishka Gidwani; Jayashree Kalpathy-Cramer; T Arun", "pub_date": "", "abstract": "Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture.", "sections": [{"heading": "Introduction", "text": "Saliency maps have become a popular approach for post-hoc interpretability of Convolutional Neural Networks (CNNs). (Adebayo et al., 2018) These maps are designed to highlight the salient components of the input images that are important to the model prediction. As a result, many deep learning medical imaging studies have used saliency maps to rationalize model prediction and provide localization. (Rajpurkar et al., 2017;Bien et al., 2018;Mitani et al., 2019) However, the validity of saliency maps has been called into question in a recent study showing that many popular saliency map approaches are not sensitive to model weight or label randomization for models evaluated on several datasets. (Adebayo et al., 2018) In this study, we extend this work by evaluating popular saliency map methods both quantitatively and qualitatively for classification models trained on the RSNA Pneumonia dataset. (Shih et al., 2019) Specifically, we assess the performance of these methods in localizing abnormalities in medical imaging by quantifying overlap with ground truth bounding boxes. Furthermore, we assess the effect of model weight and label randomization on localization performance. Lastly, we empirically study repeatability of the saliency maps, both within the same model architecture and across different model architectures.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "Methods and Results", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Model and Data Randomization", "text": "The saliency methods examined in our experiments are Gradient Explanation (Simonyan et al., 2013), Smoothgrad Integrated Gradients (IG) (Sundararajan et al., 2017), GradCAM (Selvaraju et al., 2016), XRAI (Kapishnikov et al., 2019), and Smoothgrad (Smilkov et al., 2017). Along with using Spearman rank correlation to compare maps before and after model weight and label randomization, we leverage the ground-truth bounding box coordinates Figure 1: a) Visualization of saliency maps under cascading randomization on InceptionV3 (performance before randomization: AUC=0.98, precision=0.92) (b) Dice scores under cascading randomization (c) Spearman rank correlation under cascading randomization provided in the RSNA Pneumonia dataset to establish a quantitative baseline using the dice metric. To investigate the sensitivity of saliency methods under changes to model parameters, we employ cascading randomization. (Adebayo et al., 2018) We observed that among these saliency techniques, GradCAM degraded with model randomization to a large degree whereas the other methods did not (Fig 1). This is also verified in a label randomization experiment shown in Fig 2(c) wherein we randomly flipped the labels and retrained the model to observe the difference in the dice scores of the saliency maps. In both the tests, it can be observed that gradient explanation, Smoothgrad IG, and XRAI do not degrade significantly under randomization, suggesting an undesirable invariance to model parameters and labels.", "n_publication_ref": 6, "n_figure_ref": 3}, {"heading": "Repeatability and Reproducibility", "text": "We also conducted repeatability tests on these saliency methods by comparing maps from a) models with the same architecture trained independently (intra-architecture repeatability) ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Discussion and Conclusion", "text": "In this study, we evaluated the performance of several popular saliency methods on the RSNA Pneumonia Detection dataset in regards to their localization capabilities, robustness to model parameter and label randomization, as well as repeatability and reproducibility with model architectures. It was found that GradCAM showed superior sensitivity to model parameter and label randomization, and was highly agnostic to model architecture. In future studies, we will further examine the effect of different model architectures on saliency maps and validate our findings on a separate medical imaging dataset.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "We would like to thank Julius Adebayo for providing us with the cascading randomization code used in his work. (Adebayo et al., 2018) Research ", "n_publication_ref": 1, "n_figure_ref": 0}], "references": [{"title": "Sanity checks for saliency maps", "journal": "", "year": "2018", "authors": "Julius Adebayo; Justin Gilmer; Michael Muelly; Ian Goodfellow; Moritz Hardt; Been Kim"}, {"title": "Deeplearning-assisted diagnosis for knee magnetic resonance imaging: development and retrospective validation of mrnet", "journal": "PLoS medicine", "year": "2018", "authors": "Nicholas Bien; Pranav Rajpurkar; Robyn L Ball; Jeremy Irvin; Allison Park; Erik Jones; Michael Bereket; N Bhavik; Kristen W Patel; Katie Yeom;  Shpanskaya"}, {"title": "Xrai: Better attributions through regions", "journal": "", "year": "2019", "authors": "Andrei Kapishnikov; Tolga Bolukbasi; Fernanda Vi\u00e9gas; Michael Terry"}, {"title": "Detection of anaemia from retinal fundus images via deep learning", "journal": "Nature Biomedical Engineering", "year": "2019", "authors": "Akinori Mitani; Abigail Huang; Subhashini Venugopalan; Greg S Corrado; Lily Peng; R Dale; Naama Webster; Yun Hammel; Avinash V Liu;  Varadarajan"}, {"title": "Radiologist-level pneumonia detection on chest x-rays with deep learning", "journal": "", "year": "2017", "authors": "Pranav Rajpurkar; Jeremy Irvin; Kaylie Zhu; Brandon Yang; Hershel Mehta; Tony Duan; Daisy Ding; Aarti Bagul; Curtis Langlotz; Katie Shpanskaya"}, {"title": "Why did you say that? arXiv preprint", "journal": "", "year": "2016", "authors": "Abhishek Ramprasaath R Selvaraju; Ramakrishna Das; Michael Vedantam; Devi Cogswell; Dhruv Parikh;  Batra"}, {"title": "Augmenting the national institutes of health chest radiograph dataset with expert annotations of possible pneumonia", "journal": "Radiology: Artificial Intelligence", "year": "2019", "authors": "George Shih; C Carol;  Wu; S Safwan;  Halabi; D Marc;  Kohli; M Luciano; Tessa S Prevedello; Arjun Cook; Judith K Sharma; Veronica Amorosa; Maya Arteaga;  Galperin-Aizenberg"}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "journal": "", "year": "2013", "authors": "Karen Simonyan; Andrea Vedaldi; Andrew Zisserman"}, {"title": "Smoothgrad: removing noise by adding noise", "journal": "", "year": "2017", "authors": "Daniel Smilkov; Nikhil Thorat; Been Kim; Fernanda Vi\u00e9gas; Martin Wattenberg"}, {"title": "Axiomatic attribution for deep networks", "journal": "", "year": "2017", "authors": "Mukund Sundararajan; Ankur Taly; Qiqi Yan"}], "figures": [{"figure_label": "2", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 2 :2Figure 2: (a) Visualizations from two independently trained InceptionV3 models (b) Visualizations from an InceptionV3 model (top row) and a DenseNet121 model (bottom row) (c) Comparison of dice score differences across saliency methods and architectures (d) Comparison of intra-vs. inter-architecture repeatability using Spearman rank correlation", "figure_data": ""}], "formulas": [], "doi": ""}