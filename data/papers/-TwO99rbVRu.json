{"title": "PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION", "authors": "Yuliang Zou; Zizhao Zhang; Han Zhang; Chun-Liang Li; Xiao Bian; Jia-Bin Huang; Tomas Pfister; Virginia Tech; Google Cloud; Google Brain", "pub_date": "", "abstract": "Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss.", "sections": [{"heading": "INTRODUCTION", "text": "Image semantic segmentation is a core computer vision task that has been studied for decades. Compared with other vision tasks, such as image classification and object detection, human annotation of pixel-accurate segmentation is dramatically more expensive. Given sufficient pixellevel labeled training data (i.e., high-data regime), the current state-of-the-art segmentation models (e.g., DeepLabv3+ (Chen et al., 2018)) produce satisfactory segmentation prediction for common practical usage. Recent exploration demonstrates improvement over high-data regime settings with large-scale data, including self-training (Chen et al., 2020a;Zoph et al., 2020) and backbone pretraining (Zhang et al., 2020a).\nIn contrast to the high-data regime, the performance of segmentation models drop significantly, given very limited pixel-labeled data (i.e., low-data regime). Such ineffectiveness at the low-data regime hinders the applicability of segmentation models. Therefore, instead of improving high-data regime segmentation, our work focuses on data-efficient segmentation training that only relies on few pixellabeled data and leverages the availability of extra unlabeled or weakly annotated (e.g., image-level) data to improve performance, with the aim of narrowing the gap to the supervised models trained with fully pixel-labeled data.\nOur work is inspired by the recent success in semi-supervised learning (SSL) for image classification, demonstrating promising performance given very limited labeled data and a sufficient amount of unlabeled data. Successful examples include MeanTeacher (Tarvainen & Valpola, 2017), UDA (Xie et al., 2019), MixMatch (Berthelot et al., 2019b), FeatMatch (Kuo et al., 2020), and FixMatch (Sohn et al., 2020a). One outstanding idea in this type of SSL is consistency training: making predictions consistent among multiple augmented images. FixMatch (Sohn et al., 2020a) shows that using high-confidence one-hot pseudo labels obtained from weakly-augmented unlabeled data to train strongly-augmented counterpart is the key to the success of SSL in image classification.\nHowever, effective pseudo labels and well-designed data augmentation are non-trivial to satisfy for semantic segmentation. Although we observe that many related works explore the second condition (i.e., augmentation) for image segmentation to enable consistency training framework (French et al., 2020;Ouali et al., 2020), we show that a wise design of pseudo labels for segmentation has great veiled potentials.\nIn this paper, we propose PseudoSeg, a one-stage training framework to improve image semantic segmentation by leveraging additional data either with image-level labels (weakly-labeled data) or without any labels. PseudoSeg presents a novel design of pseudo-labeling to infer effective structured pseudo labels of additional data. It then optimizes the prediction of strongly-augmented data to match its corresponding pseudo labels. In summary, we make the following contributions:\n\u2022 We propose a simple one-stage framework to improve semantic segmentation by using a limited amount of pixel-labeled data and sufficient unlabeled data or image-level labeled data. Our framework is simple to apply and therefore network architecture agnostic. ", "n_publication_ref": 12, "n_figure_ref": 0}, {"heading": "RELATED WORK", "text": "Semi-supervised classification. Semi-supervised learning (SSL) aims to improve model performance by incorporating a large amount of unlabeled data during training. Consistency regularization and entropy minimization are two common strategies for SSL. The intuition behind consistencybased approaches (Laine & Aila, 2016;Sajjadi et al., 2016;Miyato et al., 2018;Tarvainen & Valpola, 2017) is that, the model output should remain unchanged when the input is perturbed. On the other hand, the entropy minimization strategy (Grandvalet & Bengio, 2005) argues that the unlabeled data can be used to ensured classes are well-separated, which can be achieved by encouraging the model to output low-entropy predictions. Pseudo-labeling (Lee, 2013) is one of the methods for implicit entropy minimization. Recently, holistic approaches (Berthelot et al., 2019b;a;Sohn et al., 2020a) combining both strategies have been proposed and achieved significant improvement. By redesigning the pseudo label, we propose an efficient one-stage semi-supervised learning framework of semantic segmentation for consistency training. Semi-supervised semantic segmentation. Collecting pixel-level annotations for semantic segmentation is costly and prone to error. Hence, leveraging unlabeled data in semantic segmentation is a natural fit. Early methods utilize a GAN-based model either to generate additional training data (Souly et al., 2017) or to learn a discriminator between the prediction and the ground truth mask (Hung et al., 2018;Mittal et al., 2019). Consistency regularization based approaches have also been proposed recently, by enforcing the predictions to be consistent, either from augmented input images (French et al., 2020;Kim et al., 2020), perturbed feature embeddings (Ouali et al., 2020), or different networks (Ke et al., 2020). Recently, Luo & Yang (2020) proposes a dual-branch training network to jointly learn from pixel-accurate and coarse labeled data, achieving good segmentation performance. To push the performance of state of the arts, iterative self-training approaches (Chen et al., 2020a;Zoph et al., 2020;Zhu et al., 2020) have been proposed. These methods usually assume the available labeled data is enough to train a good teacher model, which will be used to generate pseudo labels for the student model. However, this condition might not satisfy in the low-data regime. Our proposed method, on the other hand, realizing the ideas of both consistency regularization and pseudo-labeling in segmentation, consistently improves the supervised baseline in both low-data and high-data regimes.\nWeakly-supervised semantic segmentation. Instead of supervising network training with accurate pixel-level labels, many prior works exploit weaker forms of annotations (e.g., bounding boxes (Dai et al., 2015), scribbles (Lin et al., 2016), image-level labels). Most recent approaches use imagelevel labels as the supervisory signal, which exploits the idea of class activation map (CAM) (Zhou et al., 2016). Since the vanilla CAM only focus on the most discriminative region of objects, dif- Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss. ferent ways to refine CAM have been proposed, including partial image/feature erasing (Hou et al., 2018;Wei et al., 2017;Li et al., 2018), using an additional saliency estimation model (Oh et al., 2017;Wei et al., 2018), utilizing pixel similarity to propagate the initial score map (Ahn & Kwak, 2018;, or mining and co-segment the same category of objects across images (Sun et al., 2020;. While achieving promising results using the approaches mentioned above, most of them require a multi-stage training strategy. The refined score maps are optimized again using a dense-CRF model (Kr\u00e4henb\u00fchl & Koltun, 2011), and then used as the target to train a separate segmentation network. On the other hand, we assume there exists a small number of fully-annotated data, which allows us to learn stronger segmentation models than general methods without needing pixel-labeled data.", "n_publication_ref": 31, "n_figure_ref": 0}, {"heading": "THE PROPOSED METHOD", "text": "In analogous to SSL for classification, our training objective in PseudoSeg consists of a supervised loss L s applied to pixel-level labeled data D l , and a consistency constraint L u applied to unlabeled data D u 1 . Specifically, the supervised loss L s is the standard pixel-wise cross-entropy loss on the weakly augmented pixel-level labeled examples:\nL s = 1 N \u00d7 |D l | x\u2208D l N \u22121 i=0 CrossEntropy (y i , f \u03b8 (\u03c9(x i ))) ,(1)\nwhere \u03b8 represents the learnable parameters of the network function f and N denotes the number of valid labeled pixels in an image x \u2208 R H\u00d7W \u00d73 . y i \u2208 R C is the ground truth label of a pixel i in H\u00d7W dimensions, and f \u03b8 (\u03c9(x i )) \u2208 R C is the predicted probability of pixel i, where C is the number of classes to predict and \u03c9(\u2022) denotes the weak (common) data augmentation operations used by Chen et al. (2018).\nDuring training, the proposed PseudoSeg estimates a pseudo label y \u2208 R H\u00d7W \u00d7C for each stronglyaugmented unlabeled data x in D u , which is then used for computing the cross-entropy loss. The unsupervised objective can then be written as:\nL u = 1 N \u00d7 |D u | x\u2208Du N \u22121 i=0 CrossEntropy ( y i , f \u03b8 (\u03b2 \u2022 \u03c9(x i ))) ,(2)\nwhere \u03b2(\u2022) denotes a stronger data augmentation operation, which will be described in Section 3.2. We illustrate the unlabeled data training branch in Figure 1.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "THE DESIGN OF STRUCTURED PSEUDO LABELS", "text": "The next important question is how to generate the desirable pseudo label y. A straightforward solution is directly using the decoder output of a trained segmentation model after confidence threshold- However, as we demonstrate later in the experiments, the generated pseudo hard/soft labels as well as other post-processing of outputs are barely satisfactory in the low-data regime, and thus yield inferior final results. To address this issue, our design of pseudo-labeling has two key insights. First, we seek for a distinct yet efficient decision mechanisms to compensate for the potential errors of decoder outputs. Second, wisely fusing multiple sources of predictions to generate an ensemble and better-calibrated version of pseudo labels.\nStarting with localization. Compared with precise segmentation, learning localization is a simpler task as it only needs to provide coarser-grained outputs than pixel level of objects in images. Based on this motivation, we improve decoder predictions from the localization perspective. Class activation map (CAM) (Zhou et al., 2016) is a popular approach to provide localization for class-specific regions. CAM-based methods (Hou et al., 2018;Wei et al., 2017;Ahn & Kwak, 2018) have been successfully adopted to tackle a different weakly supervised semantic segmentation task from us, where they assume only image-level labels are available. In practice, we adopt a variant of class activation map, Grad-CAM (Selvaraju et al., 2017) in PseudoSeg.\nFrom localization to segmentation. CAM estimates the strength of classifier responses on local feature maps. Thus, an inherent limitation of CAM-based approaches is that it is prone to attending only to the most discriminative regions. Although many weakly-supervised segmentation approaches (Ahn & Kwak, 2018;Ahn et al., 2019;Sun et al., 2020) aim at refining CAM localization maps to segmentation masks, most of them have complicated post-processing steps, such as dense CRF (Kr\u00e4henb\u00fchl & Koltun, 2011), which increases the model complexity when used for consistency training. Here we present a computationally efficient yet effective refinement alternative, which is learnable using available pixel-labeled data.\nAlthough CAM only localizes partial regions of interests, if we know the pairwise similarities between regions, we can propagate the CAM scores from the discriminative regions to the rest unattended regions. Actually, it has been shown in many works that the learned high-level deep features are usually good at similarity measurements of visual objects. In this paper, we find hypercolumn (Hariharan et al., 2015) with a learnable similarity measure function works fairly effective.\nGiven the vanilla Grad-CAM output for all C classes, which can be viewed as a spatially-flatten 2-D vector of weight m \u2208 R L\u00d7C , where each row m i is the response weight per class for one region i. Using a kernel function K(\u2022, \u2022) : R H \u00d7R H \u2192 R that measures element-wise similarity given feature h \u2208 R H of two regions, the propagated scorem i \u2208 R C can be computed as follow\u015d\nm i = \uf8eb \uf8ed m i + L\u22121 j=0 e K(W k hi,Wvhj ) L\u22121 k=0 e K(W k hi,Wvh k ) m j \uf8f6 \uf8f8 \u2022 W c .(3)\nThe goal of this function is to train \u0398 = {W k , W v \u2208 R H\u00d7H , W c \u2208 R C\u00d7C } in order to propagate the high value in m to all adjacent elements in the feature space R H (i.e., hypercolumn features) to region i. Adding m i in equation 3 indicates the skip-connection. To compute propagated score for all regions, the operations in equation 3 can be efficiently implemented with self-attention dotproduct (Vaswani et al., 2017). For brevity, we denote this efficient refinement process output as selfattention Grad-CAM (SGC) maps in R H\u00d7H\u00d7C . Figure 6 in Appendix A specifies the architecture. Calibrated prediction fusion. SGC maps are obtained from low-resolution feature maps. It is then resized to the desired output resolution, and thus not sufficient at delineating crisp boundaries. However, compared to the segmentation decoder, SGC is capable of generating more locally-consistent masks. Thus, we propose a novel calibrated fusion strategy to take advantage of both decoder and SCG predictions for better pseudo labels.\nSpecifically, given a batch of decoder outputs (pre-softmax logits)p = f \u03b8 (\u03c9(x)) and SGC mapsm computed from weakly-augmented data \u03c9(x), we generate the pseudo labels y by\nF(p,m) = Sharpen \u03b3 Softmax p Norm(p,m) + (1 \u2212 \u03b3) Softmax m Norm(p,m) , T . (4)\nTwo critical procedures are proposed to use here to make the fusion process successful. First, p andm are from different decision mechanisms and they could have very different degrees of overconfidence. Therefore, we introduce the operation Norm(a, b)\n= |a| i (a 2 i + b 2 i ) as a nor- Input Grad-CAM SGC map Decoder Decoder (strong) Pseudo label\nFigure 2: Visualization of pseudo labels and other predictions. The generated pseudo label by fusing the predictions from the decoder and SGC map is used to supervise the decoder (strong) predictions of the strongly-augmented counterpart. malization factor. It alleviates the over-confident probability after softmax, which could unfavorably dominate the resulted \u03b3-averaged probability. Second, the distribution sharpening operation Sharpen(a, T\n) i = a 1/T i / C j a 1/T j\nadjusts the temperature scalar T of categorical distribution (Berthelot et al., 2019b;Chen et al., 2020b). Figure 2 illustrates the predictions from different sources. More importantly, we investigate the pseudo-labeling from a calibration perspective (Section 4.3), demonstrating that the proposed soft pseudo label y leads to a better calibration metric comparing to other possible fusion alternatives, and justifying why it benefits the final segmentation performance. Training. Our final training objective contains two extra losses: a classification loss L x , and a segmentation loss L sa . First, to compute Grad-CAM, we add a one-layer classification head after the segmentation backbone and a multi-label classification loss L x . Second, as specified in Appendix A (Figure 6), SGC maps are scaled as pixel-wise probabilities using one-layer convolution followed by softmax in equation 3. Learning \u0398 to predict SGC maps needs pixel-labeled data D l . It is achieved by an extra segmentation loss L sa between SGC maps of pixel-labeled data and corresponding ground truth. All the loss terms are jointly optimized (i.e., L u + L s + L x + L sa ), while L sa only optimizes \u0398 (achieved by stopping gradient). See Figure 7 in the appendix for further details.", "n_publication_ref": 13, "n_figure_ref": 5}, {"heading": "INCORPORATING IMAGE-LEVEL LABELS AND AUGMENTATION", "text": "The proposed PseudoSeg can easily incorporate image-level label information (if available) into our one-stage training framework, which also leads to consistent improvement as we demonstrate in experiments. We utilize the image-level data with two following steps. First, we directly use ground truth image-level labels to generate Grad-CAMs instead of using classifier outputs. Second, they are used to increase classification supervision beyond pixel-level labels for the classifier head.\nFor strong data augmentation, we simply follow color jittering operations from SimCLR (Chen et al., 2020b) and remove all geometric transformations. The overall strength of augmentation can be controlled by a scalar (studied in experiments). We also apply once random CutOut (DeVries & Taylor, 2017) with a region of 50 \u00d7 50 pixels since we find it gives consistent though minor improvement (pixels inside CutOut regions are ignored in computing losses).", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "EXPERIMENTAL RESULTS", "text": "We start by specifying the experimental details. Then, we evaluate the method in the settings of using pixel-level labeled data and unlabeled data, as well as using pixel-level labeled data and image-level labeled data, respectively. Next, we conduct various ablation studies to justify our design choices. Lastly, we conduct more comparative experiments in specific settings.\nTo evaluate the proposed method, we conduct the main experiments and ablation studies on the PAS-CAL VOC 2012 dataset (VOC12) (Everingham et al., 2015), which contains 21 classes including background. The standard VOC12 dataset has 1,449 images as the training set and 1,456 images as the validation set. We randomly subsample 1/2, 1/4, 1/8, and 1/16 of images in the standard training set to construct the pixel-level labeled data. The remaining images in the standard training set, together with the images in the augmented set (Hariharan et al., 2011) (around 9k images), are used as unlabeled or image-level labeled data. To further verify the effectiveness of the proposed method, we also conduct experiments on the COCO dataset (Lin et al., 2014). The COCO dataset has 118,287 images as the training set, and 5,000 images as the validation set. We evaluate on the 80 foreground classes and the background, as in the object detection task. As the COCO dataset is larger than VOC12, we randomly subsample smaller ratios, 1/32, 1/64, 1/128, 1/256, 1/512, of images from the training set to construct the pixel-level labeled data. The remaining images in the training set are used as unlabeled data or image-level labeled data. We evaluate the performance using the standard mean intersection-over-union (mIoU) metric. Implementation details can be found in Appendix B.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "EXPERIMENTS USING PIXEL-LEVEL LABELED DATA AND UNLABELED DATA", "text": "Improvement over a strong baseline. We first demonstrate the effectiveness of the proposed method by comparing it with the DeepLabv3+ model trained with only the pixel-level labeled data. As shown in Figure 3 (a), the proposed method consistently outperforms the supervised training baseline on VOC12, by utilizing the pixel-level labeled data and the unlabeled data. The proposed method not only achieves a large performance boost in the low-data regime (when only 6.25% pixellevel labels available), but also improves the performance when the entire training set (1.4k images) is available. In Figure 3 (b), we again observe consistent improvement on the COCO dataset.\nComparisons with the others. Next, we compare the proposed method with recent state of the arts on both the public 1.4k/9k split (in Table 1) and the created low-data splits (in Table 2), on VOC12.\nOur method compares favorably with the others.      Similar to semi-supervised learning using pixel-level labeled data and unlabeled data, we first demonstrate the efficacy of our method by comparing it with a strong supervised baseline. As shown in Figure 4, the proposed method consistently improves the strong baseline on both datasets.\nIn Table 3, we evaluate on the public 1.4k/9k split. The proposed method compares favorably with the other methods. Moreover, we further compare to best compared CCT on the created low-data splits (in Table 4). Both experiments show that the proposed PseudoSeg is more robust than the compared method given less data. On all splits on both datasets, using pixel-level labeled data and image-labeled data shows higher mIoU than the setting using pixel-level labeled data and unlabeled data.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "ABLATION STUDY", "text": "In this section, we conduct extensive ablation experiments on VOC12 to validate our design choices. How to construct pseudo label? We investigate the effectiveness of the proposed pseudo labeling. Table 5 demonstrates quantitative results, indicating that using either decoder output or SGC alone gives an inferior performance. Naively using decoder output as pseudo labels can hardly work well. The proposed fusion consistently performs better, either with or without additional image-level labels. To further answer why our pseudo labels are effective, we study from the model calibration perspective. We measure the expected calibration error (ECE) (Guo et al., 2017) scores of all the intermediate steps and other fusion variants. As shown in Figure 5 (a), the proposed fusion strategy (denoted as G in the figure) achieves the lowest ECE scores, indicating that the significance of jointly using normalization with sharpening (see equation 4) compared with other fusion alternatives. We hypothesize using well-calibrated soft labels makes model training less affected by label noises. The comprehensive calibration study is left as a future exploration direction. Using hypercolumn feature or not? In Figure 5 (b), we study the effectiveness of using hypercolumn features instead of the last feature maps in equation 3. We conduct the experiments on the 1/16 split of VOC12. As we can see, hypercolumn features substantially improve performance. Soft or hard pseudo label? How to utilize predictions as pseudo labels remains an active question in SSL. Next, we study whether we should use soft or hard one-hot pseudo labels. We conduct  the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (c), using all predictions as soft pseudo label yields better performance than selecting confident predictions. This suggests that well-calibrated soft pseudo labels might be important in segmentation than over-simplified confidence thresholding. Temperature sharpening or not? We study the effect of temperature sharpening in equation 4. We conduct the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (d), temperature sharpening shows consistent and clear improvements. Strong augmentation strength. In Figure 5 (e), we study the effects of color jittering in the strong augmentation. The magnitude of jittering strength is controlled by a scalar (Chen et al., 2020b). We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. If the magnitude is too small, performance drops significantly, suggesting the importance of strong augmentation. Impact of different feature backbones. In Figure 5 (f), we compare the performance of using ResNet-50, ResNet-101, and Xception-65 as backbone architectures, respectively. We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. As we can see, the proposed method consistently improves the baseline by a substantial margin across different backbone architectures.", "n_publication_ref": 2, "n_figure_ref": 6}, {"heading": "COMPARISON WITH SELF-TRAINING", "text": "Several recent approaches (Chen et al., 2020a;Zoph et al., 2020) exploit the Student-Teacher selftraining idea to improve the performance with additional unlabeled data. However, these methods only apply self-training in the high-data regime (i.e., sufficient pixel-labeled data to train teachers). Here we compare these methods in the low-data regimes, where we focus on. To generate offline pseudo labels, we closely follow segmentation experiments in Zoph et al. (2020): pixels with a confidence score higher than 0.5 will be used as one-hot pseudo labels, while the remaining are treated as ignored regions. This step is considered important to suppress noisy labels. A student model is then trained using the combination of unlabeled data in VOC12 train and augmented sets with generated one-hot pseudo labels and all the available pixel-level labeled data. As shown in Table 6, although the self-training pretty well improves over the supervised baseline, it is inferior to the proposed method 2 . We conjecture that the teacher model usually produces low confidence scores to pixels around boundaries, so pseudo labels of these pixels are filtered in student training. However, boundary pixels are important for improving the performance of segmentation (Kirillov et al., 2020). On the other hand, the design of our method (online soft pseudo labeling process) bypass this challenge. We will conduct more verification of this hypothesis in future work.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "IMPROVING THE FULLY-SUPERVISED METHOD WITH ADDITIONAL DATA", "text": "We have validated the effectiveness of the proposed method in the low-data regime. In this section, we want to explore whether the proposed method can further improve supervised training in the full training set using additional data. We use the training set (1.4k) in VOC12 as the pixel-level labeled data. The additional data contains additional VOC 9k (V 9k ), COCO training set (C tr ), and COCO unlabeled data (C u ). More training details can be found in Appendix D. As shown in Table 7, the proposed PseudoSeg is able to improve upon the supervised baseline even in the high-data regime, using additional unlabeled or image-level labeled data. ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DISCUSSION AND CONCLUSION", "text": "The key to the good performance of our method in the low-data regime is the novel re-design of pseudo-labeling strategy, which pursues a different decision mechanism from weakly-supervised localization to \"remedy\" weak predictions from segmentation head. Then augmentation consistency training progressively improves segmentation head quality. For the first time, we demonstrate that, with well-calibrated soft pseudo labels, utilizing unlabeled or image-labeled data significantly improves segmentation at low-data regimes. Further exploration of fusing stronger and better-calibrated pseudo labels worth more study as future directions (e.g., multi-scaling). Although color jittering works within our method as strong data augmentation, we have extensively explored geometric augmentations (leveraging STN (Jaderberg et al., 2015) to align pixels in pseudo labels and strongly-augmented predictions) for segmentation but find it not helpful. We believe data augmentation needs re-thinking beyond current success in classification for segmentation usage.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "ACKNOWLEDGEMENT", "text": "We thank Liang-Chieh Chen and Barret Zoph for their valuable comments.    5 . We also adopt a slightly different fusion strategy in this setting by using T = 0.7 and \u03b3 = 0.3.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "E COMPARISON WITH WEAKLY-SUPERVISED APPROACHES", "text": "In Table 9, we benchmark recent weakly supervised semantic segmentation performance on PAS-CAL VOC 2012 val set. Instead of enforcing the consistency between different augmented images as we do, these approaches tackle the semantic segmentation task from a different perspective, by exploiting the weaker annotations (image-level labels). As we can see, by exploiting the imagelevel labels with careful designs, weakly-supervised semantic segmentation methods could achieve reasonably well performance. We believe that both perspectives are feasible and promising for low-data regime semantic segmentation tasks, and complementary to each other. Therefore, these designs could be potentially integrated into our framework to generate better pseudo labels, which leads to improved performance. We conduct an additional performance analysis for temporal sharpening. We conduct experiments over T on the 1/16 split of VOC using pixel-level labeled data and image-level labeled data. As shown in Table 10, adopting a T < 1 for distribution sharpening generally leads to improved performance.  (Cordts et al., 2016). The Cityscapes dataset contains 50 real-world driving sequences. Among these video sequences, 2,975 frames are selected as the training set, and 500 frames are selected as the validation set. Following previous common practice, we evaluate on 19 semantic classes.\nComparison with state of the art. We compare our method with the current state-of-the-art method (French et al., 2020), in the setting of using pixel-level labeled and unlabeled data. We randomly subsample 1/4, 1/8, and 1/30 of the training set to construct the pixel-level labeled data, using the first random seed provided by French et al. (2020). Both French et al. (2020) and our method use ResNet-101 as the feature backbone and DeepLabv3+ (Chen et al., 2018) as the segmentation model. As shown in Table 11, the proposed method achieves promising results on all the three label ratios. Per-class performance analysis. Next, we provide per-class performance break down analysis. We compare our method with the supervised baseline on the 1/30 split, using pixel-level labeled data and unlabeled data. As shown in Table 12, the distribution of the labeled pixels is severely imbalanced. Although our method does not in particular address the data imbalance issue, our method improves upon the supervised baseline on most of the classes (except for \"Wall\" and \"Pole\"). Discussion. Although the scene layouts are quite similar for all the full images, it is still feasible to generate different image-level labels through a more aggressive geometric data augmentation (e.g., scaling, cropping, translation, etc.). In practice, standard segmentation preprocessing steps only crop a sub-region of the whole training images. It only contains partial images with a certain subset of image labels, making the training batches have diverse image-level labels (converted from pixellevel labels, in the fully-labeled+unlabeled setting). Moreover, in the fully-labeled+weakly-labeled setting, in practice, we can collect diverse Internet images and weakly label them, instead of weakly labeling images from Cityscapes.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "H QUALITATIVE RESULTS", "text": "We visualize several model prediction results for PASCAL VOC 2012 (Figure 8) and COCO (Figure 9). As we can see, the supervised baseline struggles to segment some of the categories and small objects, when trained in the low-data regime. On the other hand, PseudoSeg utilizes unlabeled or weakly-labeled data to generate more satisfying predictions.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "APPENDIX A SELF-ATTENTION GRAD-CAM", "text": "We elaborate the detailed pipeline of generating Self-attention Grad-CAM (SGC) maps (equation 3) in Figure 6. To construct the hypercolumn feature, we extract the feature maps from the last two convolutional stages of the backbone network and concatenate them together. We then project the hypercolumn feature to two separate low-dimension embedding spaces to construct \"key\" and \"query\", using two 1 \u00d7 1 convolutional layers. An attention matrix can then be computed via matrix multiplication of \"key\" and \"query\". To construct \"value\", we compute Grad-CAM for each foreground class and then concatenate them together. This results in a H \u00d7 W \u00d7 (C \u2212 1) score map, where the maximum score of each category is normalized to one separately. We then use image-level labels (either from classifier prediction or ground truth annotation) to set the score maps of non-existing classes to be zero. For each pixel localization, we use one to subtract the maximum score to construct the background score map, which is then concatenated with the foreground score maps to form \"value\" (H \u00d7 W \u00d7 C). The attention score matrix can then be used to reweight and propagate the scores in \"value\". The propagated score is added back to the \"value\" score map, and the pass through a 1 \u00d7 1 convolution (w/ batch normalization) to output the SGC map. ", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "B IMPLEMENTATION DETAILS", "text": "We implement our method on top of the publicly available official DeepLab codebase. 3 Unless specified, we adopt the DeepLabv3+ model with Xception-65 (Chollet, 2017) as the feature backbone, which is pre-trained on the ImageNet dataset (Russakovsky et al., 2015). We train our model following the default hyper-parameters (e.g., an initial learning rate of 0.007 with a polynomial learning rate decay schedule, a crop size of 513 \u00d7 513, and an encoder output stride of 16), using 16 GPUs 4 . We use a batch size of 4 for each GPU for pixel-level labeled data, and 4 for unlabeled/image-level labeled data. For VOC12, we train the model for 30,000 iterations. For COCO, we train the model for 200,000 iterations. We set \u03b3 = 0.5 and T = 0.5 unless specified. We do not apply any test time augmentations.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "C LOW-DATA SAMPLING IN PASCAL VOC 2012", "text": "Unlike random sampling in image classification, it is difficult to sample uniformly in a low-data case for semantic segmentation due to the imbalance of rare classes. To avoid the missing classes at extremely low data regimes, we repeat the random sampling process for 1/16 three times (while ensuring each class has a certain amount) and report the results. We use Split 1 in the main manuscript. All splits will be released to encourage reproducibility. The results of all the three splits are shown as in Table 8.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Input", "text": "Ground truth Supervised Ours (unlabeled) Ours (img. label) Figure 8: Qualitative results of PASCAL VOC 2012. Models are trained with 1/16 pixel-level labeled data in the training set.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Input", "text": "Ground truth Supervised Ours (unlabeled) Ours (img. label) Figure 9: Qualitative results of COCO. Models are trained with 1/512 pixel-level labeled data in the training set. Note that white pixel in the ground truth indicates this pixel is not annotated for evaluation.", "n_publication_ref": 0, "n_figure_ref": 1}], "references": [{"title": "Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation", "journal": "", "year": "2018", "authors": "Jiwoon Ahn; Suha Kwak"}, {"title": "Weakly supervised learning of instance segmentation with inter-pixel relations", "journal": "", "year": "2019", "authors": "Jiwoon Ahn; Sunghyun Cho; Suha Kwak"}, {"title": "Remixmatch: Semi-supervised learning with distribution matching and augmentation anchoring", "journal": "", "year": "2019", "authors": "David Berthelot; Nicholas Carlini; D Ekin; Alex Cubuk; Kihyuk Kurakin; Han Sohn; Colin Zhang;  Raffel"}, {"title": "Mixmatch: A holistic approach to semi-supervised learning", "journal": "", "year": "2005", "authors": "David Berthelot; Nicholas Carlini; Ian Goodfellow; Nicolas Papernot; Avital Oliver; Colin A Raffel"}, {"title": "Encoder-decoder with atrous separable convolution for semantic image segmentation", "journal": "", "year": "2018", "authors": "Yukun Liang-Chieh Chen; George Zhu; Florian Papandreou; Hartwig Schroff;  Adam"}, {"title": "Naive-student: Leveraging semi-supervised learning in video sequences for urban scene segmentation", "journal": "", "year": "2008", "authors": "Liang-Chieh Chen; Raphael Gontijo Lopes; Bowen Cheng; D Maxwell;  Collins; D Ekin; Barret Cubuk; Hartwig Zoph; Jonathon Adam;  Shlens"}, {"title": "A simple framework for contrastive learning of visual representations", "journal": "", "year": "", "authors": "Ting Chen; Simon Kornblith; Mohammad Norouzi; Geoffrey Hinton"}, {"title": "Xception: Deep learning with depthwise separable convolutions", "journal": "", "year": "2017", "authors": "Fran\u00e7ois Chollet"}, {"title": "The cityscapes dataset for semantic urban scene understanding", "journal": "", "year": "2016", "authors": "Marius Cordts; Mohamed Omran; Sebastian Ramos; Timo Rehfeld; Markus Enzweiler; Rodrigo Benenson; Uwe Franke; Stefan Roth; Bernt Schiele"}, {"title": "Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation", "journal": "", "year": "2015", "authors": "Jifeng Dai; Kaiming He; Jian Sun"}, {"title": "Improved regularization of convolutional neural networks with cutout", "journal": "", "year": "2017", "authors": "Terrance Devries; W Graham;  Taylor"}, {"title": "The pascal visual object classes challenge: A retrospective", "journal": "IJCV", "year": "2015", "authors": "Mark Everingham; Ali Eslami; Luc Van Gool; K I Christopher; John Williams; Andrew Winn;  Zisserman"}, {"title": "Semi-supervised semantic segmentation needs strong, high-dimensional perturbations", "journal": "", "year": "2006", "authors": "Geoff French; Timo Aila; Samuli Laine; Michal Mackiewicz; Graham Finlayson"}, {"title": "Semi-supervised learning by entropy minimization", "journal": "", "year": "2005", "authors": "Yves Grandvalet; Yoshua Bengio"}, {"title": "On calibration of modern neural networks", "journal": "ICML", "year": "2017", "authors": "Chuan Guo; Geoff Pleiss; Yu Sun; Kilian Q Weinberger"}, {"title": "Semantic contours from inverse detectors", "journal": "", "year": "2011", "authors": "Pablo Bharath Hariharan; Lubomir Arbel\u00e1ez; Subhransu Bourdev; Jitendra Maji;  Malik"}, {"title": "Hypercolumns for object segmentation and fine-grained localization", "journal": "", "year": "2015", "authors": "Pablo Bharath Hariharan; Ross Arbel\u00e1ez; Jitendra Girshick;  Malik"}, {"title": "Self-erasing network for integral object attention", "journal": "", "year": "2018", "authors": "Qibin Hou; Pengtao Jiang; Yunchao Wei; Ming-Ming Cheng"}, {"title": "Weakly-supervised semantic segmentation network with deep seeded region growing", "journal": "", "year": "2018", "authors": "Zilong Huang; Xinggang Wang; Jiasi Wang; Wenyu Liu; Jingdong Wang"}, {"title": "Adversarial learning for semi-supervised semantic segmentation", "journal": "", "year": "2018", "authors": "Wei-Chih Hung; Yi-Hsuan Tsai; Yan-Ting Liou; Yen-Yu Lin; Ming-Hsuan Yang"}, {"title": "Spatial transformer networks", "journal": "", "year": "2015", "authors": "Max Jaderberg; Karen Simonyan; Andrew Zisserman"}, {"title": "Integral object mining via online attention accumulation", "journal": "", "year": "2019", "authors": "Peng-Tao Jiang; Qibin Hou; Yang Cao; Ming-Ming Cheng; Yunchao Wei; Hong-Kai Xiong"}, {"title": "Guided collaborative training for pixel-wise semi-supervised learning", "journal": "", "year": "2020", "authors": "Zhanghan Ke; Di Qiu; Kaican Li; Qiong Yan; Rynson Wh Lau"}, {"title": "Structured consistency loss for semi-supervised semantic segmentation", "journal": "", "year": "", "authors": "Jongmok Kim; Jooyoung Jang; Hyunwoo Park"}, {"title": "Pointrend: Image segmentation as rendering", "journal": "", "year": "", "authors": "Alexander Kirillov; Yuxin Wu; Kaiming He; Ross Girshick"}, {"title": "Efficient inference in fully connected crfs with gaussian edge potentials", "journal": "", "year": "2004", "authors": "Philipp Kr\u00e4henb\u00fchl; Vladlen Koltun"}, {"title": "Featmatch: Feature-based augmentation for semi-supervised learning", "journal": "", "year": "", "authors": "Chia-Wen Kuo; Chih-Yao Ma; Jia-Bin Huang; Zsolt Kira"}, {"title": "Temporal ensembling for semi-supervised learning", "journal": "", "year": "2016", "authors": "Samuli Laine; Timo Aila"}, {"title": "Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks", "journal": "", "year": "2013", "authors": "Dong-Hyun Lee"}, {"title": "Ficklenet: Weakly and semisupervised semantic image segmentation using stochastic inference", "journal": "", "year": "2019", "authors": "Jungbeom Lee; Eunji Kim; Sungmin Lee; Jangho Lee; Sungroh Yoon"}, {"title": "Tell me where to look: Guided attention inference network", "journal": "", "year": "2018", "authors": "Kunpeng Li; Ziyan Wu;  Kuan-Chuan; Jan Peng; Yun Ernst;  Fu"}, {"title": "Scribblesup: Scribble-supervised convolutional networks for semantic segmentation", "journal": "", "year": "2016", "authors": "Di Lin; Jifeng Dai; Jiaya Jia; Kaiming He; Jian Sun"}, {"title": "Microsoft coco: Common objects in context", "journal": "", "year": "2014", "authors": "Tsung-Yi Lin; Michael Maire; Serge Belongie; James Hays; Pietro Perona; Deva Ramanan; Piotr Doll\u00e1r; C Lawrence Zitnick"}, {"title": "Semi-supervised semantic segmentation via strong-weak dual-branch network", "journal": "", "year": "", "authors": "Wenfeng Luo; Meng Yang"}, {"title": "Semi-supervised semantic segmentation with highand low-level consistency", "journal": "TPAMI", "year": "2019", "authors": "Sudhanshu Mittal; Maxim Tatarchenko; Thomas Brox"}, {"title": "Virtual adversarial training: a regularization method for supervised and semi-supervised learning", "journal": "TPAMI", "year": "2018", "authors": "Takeru Miyato; Masanori Shin-Ichi Maeda; Shin Koyama;  Ishii"}, {"title": "Exploiting saliency for object segmentation from image level labels", "journal": "", "year": "2017", "authors": "Rodrigo Seong Joon Oh; Anna Benenson; Zeynep Khoreva; Mario Akata; Bernt Fritz;  Schiele"}, {"title": "Semi-supervised semantic segmentation with crossconsistency training", "journal": "", "year": "2006", "authors": "Yassine Ouali; C\u00e9line Hudelot; Myriam Tami"}, {"title": "Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation", "journal": "", "year": "2015", "authors": "George Papandreou; Liang-Chieh Chen; Kevin P Murphy; Alan L Yuille"}, {"title": "Imagenet large scale visual recognition challenge", "journal": "IJCV", "year": "2015", "authors": "Olga Russakovsky; Jia Deng; Hao Su; Jonathan Krause; Sanjeev Satheesh; Sean Ma; Zhiheng Huang; Andrej Karpathy; Aditya Khosla; Michael Bernstein"}, {"title": "Regularization with stochastic transformations and perturbations for deep semi-supervised learning", "journal": "", "year": "2016", "authors": "Mehdi Sajjadi; Mehran Javanmardi; Tolga Tasdizen"}, {"title": "Grad-cam: Visual explanations from deep networks via gradient-based localization", "journal": "", "year": "2017", "authors": "R Ramprasaath; Michael Selvaraju; Abhishek Cogswell; Ramakrishna Das; Devi Vedantam; Dhruv Parikh;  Batra"}, {"title": "Fixmatch: Simplifying semi-supervised learning with consistency and confidence", "journal": "", "year": "", "authors": "Kihyuk Sohn; David Berthelot; Chun-Liang Li; Zizhao Zhang; Nicholas Carlini; D Ekin; Alex Cubuk; Han Kurakin; Colin Zhang;  Raffel"}, {"title": "A simple semisupervised learning framework for object detection", "journal": "", "year": "2020", "authors": "Kihyuk Sohn; Zizhao Zhang; Chun-Liang Li; Han Zhang; Chen-Yu Lee; Tomas Pfister"}, {"title": "Semi supervised semantic segmentation using generative adversarial network", "journal": "", "year": "2006", "authors": "Nasim Souly; Concetto Spampinato; Mubarak Shah"}, {"title": "Mining cross-image semantics for weakly supervised semantic segmentation", "journal": "", "year": "2004", "authors": "Guolei Sun; Wenguan Wang; Jifeng Dai; Luc Van Gool"}, {"title": "Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results", "journal": "", "year": "2006", "authors": "Antti Tarvainen; Harri Valpola"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; \u0141ukasz Kaiser; Illia Polosukhin"}, {"title": "Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation", "journal": "", "year": "2020", "authors": "Yude Wang; Jie Zhang; Meina Kan; Shiguang Shan; Xilin Chen"}, {"title": "Object region mining with adversarial erasing: A simple classification to semantic segmentation approach", "journal": "", "year": "2017", "authors": "Yunchao Wei; Jiashi Feng; Xiaodan Liang; Ming-Ming Cheng; Yao Zhao; Shuicheng Yan"}, {"title": "Revisiting dilated convolution: A simple approach for weakly-and semi-supervised semantic segmentation", "journal": "", "year": "2018", "authors": "Yunchao Wei; Huaxin Xiao; Honghui Shi; Zequn Jie; Jiashi Feng; Thomas S Huang"}, {"title": "Unsupervised data augmentation for consistency training", "journal": "", "year": "2019", "authors": "Qizhe Xie; Zihang Dai; Eduard Hovy; Minh-Thang Luong; Quoc V Le"}, {"title": "Self-training with noisy student improves imagenet classification", "journal": "", "year": "", "authors": "Qizhe Xie; Minh-Thang Luong; Eduard Hovy; Quoc V Le"}, {"title": "Resnest: Split-attention networks", "journal": "", "year": "2020", "authors": "Hang Zhang; Chongruo Wu; Zhongyue Zhang; Yi Zhu; Zhi Zhang; Haibin Lin; Yue Sun; Tong He; Jonas Muller; R Manmatha; Mu Li; Alexander Smola"}, {"title": "Inter-image communication for weakly supervised localization", "journal": "", "year": "2020", "authors": "Xiaolin Zhang; Yunchao Wei; Yi Yang"}, {"title": "Learning deep features for discriminative localization", "journal": "", "year": "2016", "authors": "Bolei Zhou; Aditya Khosla; Agata Lapedriza; Aude Oliva; Antonio Torralba"}, {"title": "Improving semantic segmentation via self-training", "journal": "", "year": "", "authors": "Yi Zhu; Zhongyue Zhang; Chongruo Wu; Zhi Zhang; Tong He; Hang Zhang; R Manmatha; Mu Li; Alexander Smola"}, {"title": "Rethinking pre-training and self-training", "journal": "", "year": "2004", "authors": "Barret Zoph; Golnaz Ghiasi; Tsung-Yi Lin; Yin Cui; Hanxiao Liu; D Ekin; Quoc V Cubuk;  Le"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Overview of unlabeled data training branch. Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "ing, as suggested by Sohn et al. (2020a); Zoph et al. (2020); Xie et al. (2020); Sohn et al. (2020b).", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ unlabeled data) on VOC12 val (left) and COCO val (right).", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 4 :4Figure 4: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ image-level labeled data) on VOC12 val (left) and COCO val (right).", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Ablation studies on different factors. See Section 4.3 for complete details.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_6", "figure_caption": "+L u +L x ) Decoder (L s +L u ) Classifier (L x ) Self-attention (L sa ) L s : pixel-level labeled data L u : unlabeled/image-level labeled data L x : pixel-level labeled data (converting to image-level) or image-level labeled data L sa : pixel-level labeled data", "figure_data": ""}, {"figure_label": "7", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Figure 7 :7Figure 7: Training. For each network component, we show the loss supervision and the corresponding data.", "figure_data": ""}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Directly applying consistency training approaches validated in image classification renders particular challenges in segmentation. We first demonstrate how well-calibrated soft pseudo labels obtained through wise fusion of predictions from diverse sources can greatly improve consistency training for segmentation. \u2022 We conduct extensive experimental studies on the PASCAL VOC 2012 and COCO datasets.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data). We use the official training set (1.4k) as labeled data, and the augmented set (9k) as unlabeled data.", "figure_data": "MethodNetworkmIoU (%)GANSeg (Souly et al., 2017)VGG1664.10AdvSemSeg (Hung et al., 2018) ResNet-10168.40CCT (Ouali et al., 2020)ResNet-5069.40PseudoSeg (Ours)ResNet-5071.00PseudoSeg (Ours)ResNet-10173.23"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data) using low-data splits. The exact numbers of pixel-labeled images are shown in brackets. All the methods use ResNet-101 as backbone except CCT(Ouali et al.", "figure_data": ", 2020), which uses"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Comparison  ", "figure_data": "with state of the arts on VOC12 valset (w/ pixel-level labeled data and image-level labeleddata). We use the official training set (1.4k) as labeled data,and the augmented set (9k) as image-level labeled data.MethodModelNetworkmIoU (%)WSSN (Papandreou et al., 2015) DeepLab-CRFVGG1664.60GAIN (Li et al., 2018)DeepLab-CRF-LFOV VGG1660.50MDC (Wei et al., 2018)DeepLab-CRF-LFOV VGG1665.70DSRG (Huang et al., 2018)DeepLabv2VGG1664.30GANSeg (Souly et al., 2017)FCNVGG1665.80FickleNet (Lee et al., 2019)DeepLabv2ResNet-10165.80CCT (Ouali et al., 2020)PSP-NetResNet-5073.20PseudoSeg (Ours)DeepLabv3+ResNet-5073.80"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Comparison with state of the", "figure_data": "arts on VOC12 val set with pixel-levellabeled data and image-level labeleddata. Four ratios of pixel-level labeledexamples are tested. Both CCT (Oualiet al., 2020) and our method useResNet-50 as backbone.Split CCT PseudoSeg1/266.8073.511/467.6071.791/862.5069.151/16 51.8065.44"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_5", "figure_caption": "Comparison to alternative pseudo labeling strategies. We conduct experiments using 1/4, 1/8, 1/16 of the pixel-level labeled data, the exact numbers of images are shown in the brackets.", "figure_data": "SourceUsing image-level labels 1/4 (366) 1/8 (183) 1/16 (92)Decoder only-70.2269.3553.20SGC only-67.0762.6153.42Calibrated fusion-73.7973.1367.06Decoder only73.9573.0567.54SGC only71.7367.5764.26Calibrated fusion75.2974.7071.22"}, {"figure_label": "6", "figure_type": "table", "figure_id": "tab_6", "figure_caption": "Comparison with self-training. We use our supervised baseline as the teacher to generate one-hot pseudo labels, followingZoph et al. (2020).", "figure_data": "MethodUsing image-level labels 1/4 (366) 1/8 (183) 1/16 (92)Supervised (Teacher)-70.2064.0056.03Self-training (Student)-72.8569.8864.20PseudoSeg (Ours)-73.7973.1367.06PseudoSeg (Ours)75.2974.7071.22"}, {"figure_label": "7", "figure_type": "table", "figure_id": "tab_7", "figure_caption": "Improving fully supervised model with extra data. No test-time augmentation is used.", "figure_data": "MethodBaseline PseudoSeg (w/o image-level labels) PseudoSeg (w/ image-level labels)Extra data-C tr +C uC tr + C u + V 9kC trC tr + V 9kmIoU (%)76.9677.40 (+0.44)78.20 (+1.24)77.80 (+0.84)79.28 (+2.32)"}, {"figure_label": "8", "figure_type": "table", "figure_id": "tab_8", "figure_caption": "Full results of 1/16 split in VOC12.", "figure_data": "MethodUsing image-level labels Split 1 Split 2 Split 3Supervised-56.0356.8755.92PseudoSeg (Ours)-67.0664.1266.09PseudoSeg (Ours)71.2268.1169.72D HIGH-DATA EXPERIMENTAL SETTINGSHere we provide more details about the experiments in Section 4.5. Since we have a lot moreunlabeled/image-level labeled data, we adopt a longer training schedule (90,000 iterations)"}, {"figure_label": "9", "figure_type": "table", "figure_id": "tab_9", "figure_caption": "Benchmarking state-of-the-art weakly supervised semantic segmentation methods. All the methods use image-level labels from VOC12 training (1.4k) and augmented (9k) sets.", "figure_data": "MethodPixel-level labeled data mIoU (%)FickleNet (Lee et al., 2019)-64.9IRNet (Ahn et al., 2019)-63.5OAA+ (Jiang et al., 2019)-65.2SEAM (Wang et al., 2020)-64.5MCIS (Sun et al., 2020)-66.2PseudoSeg (Ours)1/16 (92)71.22"}, {"figure_label": "10", "figure_type": "table", "figure_id": "tab_10", "figure_caption": "Performance analysis over T.", "figure_data": "Temperature (T)mIoU (%)0.171.110.370.110.5 (default)71.220.772.371.0 (no sharpening)68.15G EXPERIMENTS ON CITYSCAPESIn this section, we conduct additional experiments on the Cityscapes dataset"}, {"figure_label": "11", "figure_type": "table", "figure_id": "tab_11", "figure_caption": "Experiments on Cityscapes (w/ pixel-level labeled data and unlabeled data).", "figure_data": "Method1/4 (744) 1/8 (372) 1/30 (100)CutMix (French et al., 2020)68.3365.8255.71PseudoSeg (Ours)72.3669.8160.96"}, {"figure_label": "12", "figure_type": "table", "figure_id": "tab_12", "figure_caption": "Per-class performance analysis on Cityscapes (w/ pixel-level labeled data and unlabeled data).", "figure_data": "ClassRoad Sidewalk Building Wall Fence Pole Traffic light Traffic sign Vegetation TerrainPixel ratio (%)36.365.6120.990.530.981.190.140.5119.611.29Supervised96.0371.2687.5319.75 29.11 52.1950.1968.0989.9345.79PseudoSeg (Ours) 96.6475.0688.6319.67 34.09 51.7558.1969.9590.4350.48ClassSkyPersonRiderCarTruckBusTrainMotorcycleBicyclePixel ratio (%)3.701.100.166.490.380.130.230.060.54Supervised91.0174.1243.9189.917.6814.1917.7825.8669.88PseudoSeg (Ours) 92.9975.1646.0991.60 20.39 26.3022.1343.9671.30"}], "formulas": [], "doi": ""}