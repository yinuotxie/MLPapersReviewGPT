{"title": "BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION", "authors": "", "pub_date": "", "abstract": "Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.", "sections": [{"heading": "INTRODUCTION", "text": "Time series forecasting has a rich and luminous history, and is essentially important in most of business operations nowadays. The main aim of time series forecasting is to carefully collect and rigorously study the past observations of a time series to develop an appropriate model which could describe the inherent structure of the series, in order to generate future values. For instance, the internet companies are interested in the number of daily active users (DAU), say, what is DAU after certain period of time, or when will reach their target DAU goal. Time series forecasting is a fruitful research area with many existing methodologies. The most popular and frequently used time series model might be the Autoregressive Integrated Moving Average (ARIMA) (Box et al., 2015;Zhang, 2003;Cochrane, 2005;Hipel & McLeod, 1994). Taking seasonality into consideration, Box et al. (2015) proposed the Seasonal ARIMA. The Holt-Winters method (Winters, 1960) is also very popular by using exponential smoothing. State space model (Durbin & Koopman, 2012;Scott & Varian, 2014;Brodersen et al., 2015) also attracts much attention, which is a linear function of an underlying Markov process plus additive noise. Exponential Smoothing State Space Model (ETS)  decomposes times series into error, trend, seasonal that change over time. Recently, deep learning is applied for time-series trend learning using LSTM (Tao Lin, 2017), bidirectional dynamic Boltzmann machine (Osogami et al., 2017) is applied for time-series long-term dependency learning, and coherent probabilistic forecast (Taieb et al., 2017) is proposed for a hierarchy or an aggregation-level comprising a set of time series. Orthogonal to these works, this paper focuses on robust ways of time series forecasting in presence of change points and anomalies.\nIn Internet time series forecasting, Google develops the Bayesian structure time series (BSTS) model (Brodersen et al., 2015;Scott & Varian, 2014) to capture the trend, seasonality, and similar components of the target series. Recently, Facebook proposes the Prophet approach (Taylor & Letham, 2017) based on a decomposable model with interpretable parameters that can be intuitively adjusted by analyst. However, as in the DAU example, some special events like Christmas Holiday or President Election, newly launched apps or features, may cause short period or long-term change of DAU, leading to weird forecasting of those traditional models. The aforementioned special cases are well known as \u2022 Anomaly points. The items, events or observations that don't conform to an expected pattern or other items in the dataset, leading to a sudden spike or decrease in the series.\n\u2022 Change points. A market intervention, such as a new product launch or the onset of an advertising (or ad) campaign, may lead to the level change of the original series.\nTime series forecasting without change/anomaly point detection and adjustment may also lead to bizarre forecasting since these models might learn the abrupt changes in the past. There are literatures on detecting anomaly or change points individually, examples can be found in Twitter (2017); Netflix (2017); Barry & Hartigan (1993); Killick & Eckley (2014); twitter (2017). However, the aforementioned change point detection models could not support detection in the presence of seasonality, while the presence of trend/change point is not handled by the anomaly detection models. Most importantly, there is a discrepancy between anomaly/change points detection and adjustment, and commonly used manually adjustment might be a bit arbitrary. Unfortunately, the forecasting gap caused by abnormal and change points, to the best of our knowledge, has not been given full attention and no good solution has been found so far. This paper is strongly motivated by bridging this gap.\nIn this paper, to overcome the limitations of the most (if not all) current models that the anomaly points and change points are not properly considered, we develop a state space time series forecasting model in the Bayesian framework that can simultaneously detect anomaly and change points and perform forecasting. The learned structure information related to anomaly and change points is automatically incorporated into the forecasting process, which naturally enhances the model prediction based on the feedback of state-space model. To solve the resultant optimization problem, an iterative algorithm based on Bayesian approximate inference with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. The novel model could explicitly capture the structure of change points, anomaly points, trend and seasonality, as also provide the distributions and forecasting intervals due to Bayesian forecasting framework. Both synthetic and real data sets show the better performance of proposed model, in comparison with existing baseline. Moreover, our proposed model outperforms state-of-the-art models in identifying anomaly and change points.\nTo summarize, our work has the following contributions.\n\u2022 We proposed a robust 1 Bayesian state-space time series forecasting model that is able to explicitly capture the structures of change points and anomalies (which are generally ignored in most current models), and therefore automatically adapt for forecasting by incorporating the prior information of trend, seasonality, as well as change points and anomalies using state space modeling. Due to the enhancement of model description capability, the results of model prediction and abnormal and change points detection are mutually improved.\n\u2022 To solve the resultant optimization problem, an effective algorithm based on approximate inference using Markov chain Monte Carlo (MCMC) is proposed with theoretical guaranteed forecasting paths.\n\u2022 Our proposed method outperforms the state-of-the-art methods in time series forecasting in presence of change points and anomalies, and detects change points and anomalies with high accuracy and low false discovery rate on both tasks, outperforming popular change point and anomaly detection methods. Our method is flexible to capture the structure of time series under various scenarios with any component combinations of trend, seasonality, change points and anomalies. Therefore our method can be applied in many settings in practice.", "n_publication_ref": 19, "n_figure_ref": 0}, {"heading": "MODEL OVERVIEW", "text": "State space time series model (Hangos et al., 2014) has been one of the most popular models in time series analysis. It is capable of fitting complicated time series structure including linear trend and seasonality. However, times series observed in real life are almost all prevailed with outliers. Change points, less in frequency but are still widely observed in real time series analysis. Unfortunately, both structures are ignored in the classic state space time series model. In the section, we aim to address this issue by introducing a novel state space time series model. Let y = (y 1 , y 2 , . . . , y n ) be a sequence of time series observations with length n. The ultimate goal is to forecast (y n+1 , y n+2 , . . .). The accuracy in forecasting lies in a successful decomposition of y into existing components. Apart from the residuals, we assume the time series is composed by trend, seasonality, change points and anomaly points. In a nutshell, we have an additive model with time series = trend + seasonality + change point + anomaly point + residual.\nFigure 1 provides a demonstration of desired decomposition of time series. In Figure 1, the left panel shows the observed time series. And it can be decomposed into the remaining five panels. The shift in the change point panel shows where the change point lies. And the spikes in the last panel reveals the anomaly points.\nAs the classical state space model, we have observation equation and transition equations to model y and hidden variables. We use \u00b5 = (\u00b5 1 , \u00b5 2 , . . . , \u00b5 n ) to model trend, and use \u03b3 = (\u03b3 1 , \u03b3 2 , . . . , \u03b3 n ) to model seasonality. We use a binary vector z a = (z a 1 , z a 2 , . . . , z a n ) to indicate anomaly points. Then we have Observation equation:\ny t = \u00b5 t + \u03b3 t + t , if z a t = 0 o t , if z a t = 1\n.\n(1)\nThe deviation between the observation y t and its \"mean\" \u00b5 t + \u03b3 t is modeled by t and o t , depending on the value of z a t . If z a t = 1, then y t is an anomaly point; otherwise it is not. Distinguished from the residues = ( 1 , 2 , . . . , n ), the anomaly is captured by o = (o 1 , o 2 , . . . , o n ) which has relative large magnitude.\nThe hidden state variable \u00b5 and \u03b3 have intrinsic structures. There are two transition equations, for trend and seasonality separately Transition Equations: Trend:\n\u00b5 t = \u00b5 t\u22121 + \u03b4 t\u22121 + u t , if z c t = 0 r t , if z c t = 1 ,(2)\n\u03b4 t = \u03b4 t\u22121 + v t , Seasonality: \u03b3 t = \u2212 S\u22121 s=1 \u03b3 t\u2212s + w t .(3)\nIn Equation (2), \u03b4 = (\u03b4 1 , \u03b4 2 , . . . , \u03b4 n ) can be viewed as the \"slope\" of the trend, measuring how fast the trend changes over time. The change point component is also incorporated in Equation (2) by a binary vector z c = (z c 1 , z c 2 , . . . , z c n ). If z c t = 1, it means the t-th point is a change point, with \u00b5 t differs from \u00b5 t\u22121 + \u03b4 t\u22121 (which can be interpreted as the \"momentum\" from the previous status ) by r t ; otherwise it is not a change point and they differ by u t . We model the change points in a way such that r = (r 1 , r 2 , . . . , r n ) have larger magnitude compared u = (u 1 , u 2 , . . . , u n ). The \"slope\" part \u03b4 also has its own noise v = (v 1 , v 2 , . . . , v n ).\nA first look on Equation (2) may bring up with the question that it is not presented in an exactly the same way as shown in Figure 1. In Figure 1, the change points component is a step function, and it is one of the five additive components along with trend, seasonality, anomaly points and residuals. Here we model the change point directly into the trend component. Though differing in formulation, they are equivalent to each other. We choose to model in as in Equation ( 2) due to simplicity, and its similarity with the definition of anomaly points in Equation (1).\nThe seasonality component is presented in Equation ( 3). Here S is the length of one season and w = (w 1 , w 2 , . . . , w n ) is the noise for seasonality. The seasonality component is assumed to have almost zero average in each season.\nThe observation equation and transition equations (i.e., Equation (1,2,3)) define how y is generated from all the hidden variables including change points and anomaly points. We continue to explore this new model, under a Bayesian framework.", "n_publication_ref": 1, "n_figure_ref": 4}, {"heading": "BAYESIAN FRAMEWORK", "text": "Bayesian methods are widely used in many data analysis fields. It is easy to implement and interpret, and it also has the ability to produce posterior distribution. The Bayesian method on state space time series model has been investigated in Scott & Varian (2014);Brodersen et al. (2015). In this section, we also consider Bayesian framework for our novel state space time series model. We assume all the noises are normally distributed\n{ t } n t=1 iid \u223c N (0, \u03c3 2 ), {o t } n t=1 iid \u223c N (0, \u03c3 2 o ), {u t } n t=1 iid \u223c N (0, \u03c3 2 u ), {r t } n t=1 iid \u223c N (0, \u03c3 2 r ), {v t } n t=1 iid \u223c N (0, \u03c3 2 v ), {w t } n t=1 iid \u223c N (0, \u03c3 2 w )\n, where \u03c3 , \u03c3 o , \u03c3 u , \u03c3 r , \u03c3 v , \u03c3 w are parameters for standard deviation. As binary vectors, a natural choice is to model anomaly point indicator z a and change point indicator z c to the model them as Bernoulli random variables\n{z a t } n t=1 iid \u223c Ber(p a ), {z c t } n t=1 iid \u223c Ber(p c ),\nwhere p a , p c are probabilities for each point to be an anomaly or change point.\nFigure 2: Graphical presentation of our model. Note that y is observed, highlighted by gray background, distinguished from all the remaining ones that are hidden. Among the hidden ones, squares indicate fixed parameters, and circles indicate random variables.\nFor simplicity, we denote \u03b1 t = (\u00b5 t , \u03b4 t , \u03b3 t , \u03b3 t\u22121 , . . . , \u03b3 t\u2212(S\u22122) ) to include the main hidden variables (except z a t and z c t ) in the transition equations. All the \u03b1 t are well defined and can be generated from the previous status, except \u03b1 1 . We denote a 1 to be the parameter for \u03b1 1 , which can be interpreted as the \"mean\" for \u03b1 1 .\nWith Bayesian framework, we are able to represent our model graphically as in Figure 2. As shown in Figure 2, the only observations are y and all the others are hidden. In this paper, we assume there is no additional information on all the hidden states. If we have some prior information, for example, some points are more likely to be change points, then our model can be easily modified to incorporate such information, by using proper prior.\nIn Figure 2, we use squares and circles to classify unknown variables. Despite all being unknown, they actually behave differently according to their own functionality. For those in squares, they behave like turning parameters. Once they are initialized or given, those in circles behaves like latent variables. We call the former \"parameters\" and the latter \"latent variable\", as listed in Table 1. Trend and seasonality z = (z a , z c ) Anomaly and change points Parameter a1\nThe \"mean\" for the initial trend and seasonality p = (pa, pc)\nProbabilities for each point to be anomaly or change point \u03c3 = (\u03c3 , \u03c3o, \u03c3u, \u03c3r, \u03c3v, \u03c3w) Standard deviation\nThe discrepancy between these two categories is clearly captured by the joint likelihood function. From Figure 2, the joint distribution (i.e., the likelihood function) can be written down explicitly as\nLa 1 ,p,\u03c3 (y, \u03b1, z) (4) = {t:z a t =0} g(yt \u2212 \u00b5t \u2212 \u03b3t, \u03c3 ) \u00d7 {t:z a t =1} g(yt \u2212 \u00b5t \u2212 \u03b3t, \u03c3o) \u00d7 {t:z c t =0} g(\u00b5t \u2212 \u00b5 t\u22121 \u2212 \u03b4 t\u22121 , \u03c3u) \u00d7 {t:z c t =1} g(\u00b5t \u2212 \u00b5 t\u22121 \u2212 \u03b4 t\u22121 , \u03c3r) \u00d7 n t=1 g(\u03b4t \u2212 \u03b4 t\u22121 , \u03c3v) \u00d7 n t=1 g(\u2212 S\u22121 s=1 \u03b3 t\u2212s , \u03c3v) \u00d7 n i=1 (pa) z a t (1 \u2212 pa) 1\u2212z a t (pc) z c t (1 \u2212 pc) 1\u2212z c t , where g(x 1 , x 2 ) = 1 \u221a 2\u03c0x2 exp \u2212x 2 1 /(2x2\n2 ) is the density function for normal distribution with mean x 1 and standard deviation x 2 . Here we slightly abuse the notation by using \u00b5 0 , \u03b4 0 , \u03b3 0 , \u03b3 \u22121 , . . . , \u03b3 2\u2212S , which are actually the corresponding coordinates of a 1 .\nAs long with other probabilistic graphical models, our model can also be viewed as a generative model. Given the parameters a 1 , p, \u03c3, we are able to generate time series. We present the generative procedure as follows.\nAlgorithm 1: Generative Procedure Input: Parameters a 1 , \u03c3 = (\u03c3 , \u03c3 o , \u03c3 u , \u03c3 r , \u03c3 v , \u03c3 w ) and p a , p c , length of time series to generate m Output: Time series y = (y 1 , y 2 , . . . , y m ) Generate the indexes where anomalies or change points occur\n{z a t } n t=1 iid \u223c Ber(p a ), {z c t } n t=1 iid \u223c Ber(p c );\nGenerate all the noises , o, u, r, v, w as independent normal random variables with mean zero and standard deviation \u03c3 , \u03c3 o , \u03c3 u , \u03c3 r , \u03c3 v , \u03c3 w respectively; Generate {\u03b1 t } m t=1 sequentially by the transition functions in Equation ( 2) and (3); Generate time series {y t } m t=1 by the observation function in Equation (1).", "n_publication_ref": 2, "n_figure_ref": 5}, {"heading": "INFERENCE", "text": "This section is about inferring unknown variables from y, given the Bayesian setting described in the previous section. The main framework here is to sequentially update each hidden variable by fixing the remaining ones. As stated in the previous section, there are two different categories of unknown variables. Different update schemes need to be used due to the difference in their functionality. For the latent variables, we implement Markov chain Monte Carlo (MCMC) for inference. Particular, we use Gibbs sampler. We will elaborate the details of updates in the following sections.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "UPDATES ON TREND AND SEASONALITY", "text": "In this section, we focus on updating \u03b1 assuming all the other hidden variables are given and fixed. The essence of Gibbs sampler is to obtain posterior distribution p a1,p,\u03c3 (\u03b1|y, z). This can be achieved by a combination of Kalman filter, Kalman smoothing and the so-called \"fake-path\" trick. We provide some intuitive explanation here and refer the readers to Durbin & Koopman (2012) for detailed implementation.\nKalman filter and Kalman smoothing are classic algorithms in signal processing and pattern recolonization for Bayesian inference. It is well related to other algorithms especially message passing algorithm. Kalman filter collects information forwards to obtain E(\u03b1 t |y 1 , y 2 , . . . , y t ); while Kalman smoothing distribute information backwards to achieve E(\u03b1 t |y).\nHowever, the combination of Kalman filter and Kalman smoothing is not enough, as it only gives the the expectations of marginal distributions {E(\u03b1 t |y)} n t=1 , instead of the joint distribution required for Gibbs sampler. To address this issue, we can use the \"fake-path\" trick described in Brodersen et al. (2015); Durbin & Koopman (2012). The main idea underlying this trick lies on the fact that the covariance structure of p(\u03b1 t |y) is not dependent on the means. If we are able to obtain the covariance by some other way, then we can add it up with {E(\u03b1 t |y)} n t=1 to obtain a sample from p(\u03b1|y). This trick involves three steps. Note that all the other hidden variables z, p, \u03c3 are given.\n1. Pick some vector\u00e3 1 , and generate a sequence of time series\u1ef9 from it by Algorithm 1. In this way, we also observe\u03b1. 2. Obtain {E(\u03b1 t |\u1ef9)} n t=1 from\u1ef9 by Kalman filter and Kalman smoothing. 3. We use {\u03b1 t \u2212 E(\u03b1 t |\u1ef9) + E(\u03b1 t |y)} n t=1 as our sampling from the conditional distribution.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "CHANGE POINT AND ANOMALY DETECTION", "text": "In this section, we update z by Gibbs sampler, assuming \u03b1, a 1 , p, \u03c3 are all given and fixed. We need to obtain the conditional distribution p a1,p,\u03c3 (z|y, \u03b1). Note that in the graphical model described in Section 2, {z a t } n t=1 and {z c t } n i=1 are all Bernoulli random variables and independent of each other. Then the conditional distribution p a1,p,\u03c3 (z|y, \u03b1) can also be decomposed into product of Bernoulli density functions. In other words, conditioned on y, \u03b1, {z a t } n t=1 and {z c t } n i=1 are still independent Bernoulli random variables, but possibly with different success probabilities. Thus, we can take the calculation point by point. For example, for the anomaly detection for the t-th point, we have\nz a t = 0 : y t \u2212 \u00b5 t \u2212 \u03b3 t \u223c N (0, \u03c3 2 ) z a t = 1 : y t \u2212 \u00b5 t \u2212 \u03b3 t \u223c N (0, \u03c3 2 o\n). And the prior on z a t is P(z a t = 1) = p a and P(z a t = 0) = p 1 . Let p a t = P(z a t = 1|y, \u03b1). Directly calculation leads to\np a t = pa \u03c3o exp \u2212 (yt\u2212\u00b5t\u2212\u03b3t) 2 2\u03c3 2 o 1\u2212pa \u03c3 exp \u2212 (yt\u2212\u00b5t\u2212\u03b3t) 2 2\u03c3 2 + pa \u03c3o exp \u2212 (yt\u2212\u00b5t\u2212\u03b3t) 2 2\u03c3 2 o .(5)\nThis equality holds for all t = 1, 2, . . . , n. Similarly for change point detection, let p c t = P(z c t = 1|y, \u03b1), and we have\np c t = pc \u03c3r exp \u2212 (\u00b5t\u2212\u00b5t\u22121\u2212\u03b4t\u22121) 2 2\u03c3 2 r 1\u2212pc \u03c3u exp \u2212 (\u00b5t\u2212\u00b5t\u22121\u2212\u03b4t\u22121) 2 2\u03c3 2 u + pc \u03c3r exp \u2212 (\u00b5t\u2212\u00b5t\u22121\u2212\u03b4t\u22121) 2 2\u03c3 2 r .(6)\nAs mentioned above, all the coordinates in z are still independent Bernoulli random variables conditioned on y, \u03b1. Thus, for Gibbs sampler, we can generate z by sampling independently with\n{z a t } n t=1 \u223c Ber(p a t ), {z c t } n t=1 \u223c Ber(p c t ).\nFor change point detection here, we have an additional segment control step. After obtaining {z c t } n t=1 as mentioned above, we need to make sure that the change points detected satisfy some additional requirement on the length of segment among two consecutive change points. This issue arises from the ambiguity between the definitions of change point and anomaly points. For example, consider a time series with value (0, 0, 0, 0, 1, 1, 1, 0, 0, 0). We can view it with two change points, one increases the trend by 1 and the other decreases it by 1. Alternatively, we can also argue the three 1s in this time series are anomalies, though next to each other. One way to address this ambiguity is by defining the minimum length of segment (denoted as ). In this toy example, if we set the minimum length to be 4, then they are anomaly points; if we set it to be 3, then we regard them to be change points. But a more complicated criterion is needed than using minimum length as the time series usually own much more complex structure than this toy example. Consider time series (0, 0, 0, 0, \u22121, \u22121, 1, 1, 1, 1) and the minimum time series parameter = 3. It is reasonable to view it with one change point with increment 1, and the two -1s should be regarded as anomalies. As a combination of all these factors, we propose the following segment control method. A default value for the parameter is the length of seasonality, i.e., = S.\nAlgorithm 2: Segment control on change points Input: change point binary vector z c ,trend \u00b5, standard deviation for outliers \u03c3 r , change point minimum segment Output: change point binary vector z c Denote t 1 < t 2 < . . . to be all the indexes such that z c ti = 1; while there exists i such that\n|t i+1 \u2212 t i | < do Check if |\u00b5 ti\u22121 \u2212 \u00b5 ti+1+1 | \u2264 \u03c3 r /2.\nIf so, exclude both them from change points by setting z c ti = z c ti+1 = 0. Otherwise, randomly exclude one of them by setting the corresponding coordinate in z c to be 0; Update all the indexes of change points in z c . end", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "INITIALIZATION AND UPDATES ON PARAMETERS", "text": "The parameters \u03c3, a 1 and p need both initialization and update. We have different initializations and update schemes for each of them.\nFor all the standard deviations, once we obtain \u03b1 and z, we update them by taking the empirical standard deviation correspondingly. For \u03c3 \u03b4 and \u03c3 \u03b3 , the calculation is straightforward as they only involve \u03b4 and \u03b3 respectively. For \u03c3 , \u03c3 o , \u03c3 u and \u03c3 r , it is a bit more involved due to z. Nevertheless, we can obtain the following update equations for all of them:\n\u03c3 = {t:z a t =0} (yt \u2212 \u00b5t \u2212 \u03b3t) 2 |{t : z a t = 0}| , \u03c3o = {t:z a t =1} (yt \u2212 \u00b5t \u2212 \u03b3t) 2 |{t : z a t = 1}| , \u03c3u = {t:z c t =0} (\u00b5t \u2212 \u00b5t\u22121 \u2212 \u03b4t\u22121) 2 |{t : z c t = 0}| ,(7)\n\u03c3r = {t:z c t =1} (\u00b5t \u2212 \u00b5t\u22121 \u2212 \u03b4t\u22121) 2 |{t : z c t = 0}| , \u03c3 \u03b4 = 1 n n t=1 (\u03b4t \u2212 \u03b4t\u22121) 2 , \u03c3\u03b3 = 1 n n t=1 ( S\u22121 s=0 \u03b3t\u2212s) 2 . (8\n)\nNote that in some iterations, when there is no change point or anomaly detected in z, then the updates above for \u03c3 o , \u03c3 r are not well-defined. In those cases, we simply let them remain the same.\nTo initialize \u03c3, we let them all equal to the standard deviation of y.\nFor a 1 , we initialize it by letting its first coordinate to be equal to the average of y 1 , y 2 , . . . , y S , and all the remaining coordinates to be equal to 0. Since a 1 can be interpreted as the mean vector of \u03b1 1 , in this way the trend is initialized to be matched up with average of the first season, and the slope and seasonality are initialized to be equal to 0. We update a 1 by using information of \u03b1. We let the first two coordinates (trend and slope) of a 1 to be equal to those of \u03b1 1 , and we let the remaining coordinates (seasonality) of a 1 to be equal to those of \u03b1 S+1 . The reason why we do not let a 1 to be equal to \u03b1 1 entirely is due to the consideration on convergence and robustness. Since we initialize the seasonality part in a 1 as 0, it will remain 0 if we let a 1 equals \u03b1 1 entirely (due to the mechanism how we update \u03b1 1 as described in Section 4.1. We can avoid such trouble via using \u03b1 S+1 .\nFor p, we initialize them to be equal to 1/n. If we have additional information on the number of change points or anomaly points, we can initiate them with different values, for example, 0.1/n, or 10/n. We can update p after obtaining z, but we choose not to, also for the sake of robustness. In the early iterations when the algorithm is far from convergence, it is highly possible that z a or z c may turn out to be all 0. If we update p, say, by taking the proportion of change point or anomaly points in z. Then p a or p c might be 0, and it may get stuck in 0 in the remaining iterations.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "FORECASTING", "text": "Once we infer all the latent variables \u03b1, z and tune all the parameters p, a 1 , \u03c3, we are able to forecast the future time series y future . From the graphical model described in Section 3, the future forecasting only involves \u03b1 n instead of the whole \u03b1. Note that we assume that there exists no change point and anomaly point in the future. This is reasonable as in most cases we have no additional information on the future time series. Given \u03b1 n and \u03c3 we can use our predictive procedures (i.e., Algorithm 1) to generate future time series y future . We can further integrate out \u03b1 n to have the posterior predictive distribution as p \u03c3 (y future |y).\nThe forecasting on future time series is not deterministic. There are two sources for the randomness in y future . One comes from the inference of \u03b1 n (and also \u03c3) from y. Under the Bayesian framework in Section 3, we have a posterior distribution over \u03b1 n rather than a single point estimation. The second one comes from the forecasting function itself. The forecasting involves intrinsic noise like t , u t , v t and w t . Thus, the predictive density function p \u03c3 (y future |y, \u03b1 n ) will lead to different path even with fixed \u03c3 and \u03b1 n . In this way we are able to obtain distribution and predictive interval for forecasting. We also suggest to take the average of multiple forecasting paths, as the posterior mean for the forecasting.\nThe average of multiple forecasting paths (denoted as\u0233 future ), if the number of paths is large enough, always takes the form as a combination of linear trend and seasonality. This can be observed in both our synthesis data (Section 7) and real data analysis (Section 8). This seems to be surprising at the first glance, but makes some sense intuitively. Under our assumption, we have no information on the future, and thus a reliable way to forecast the future is to use the information collected at the end of observed time series, i.e., trend \u00b5 n , slope \u03b4 n and seasonality structure. Theorem 1 gives mathematical explanation of the linearity of\u0233 future , in both mean and standard deviation. Theorem 1. Let N be the number of future time series paths we generate from Algorithm 1). Let m be the number of points we are going to forecast. Denote {y\n(1) n+j } m j=1 , {y(2)\nn+j } m j=1 , . . . , {y\nn+j } m j=1 to be the future paths. Define\u0233 future = (\u0233 n+1 ,\u0233 n+2 , . . . ,\u0233 n+m ) to be the average such that\ny n+j = 1 N N i=1 y (i) n+j .\nThen for all j = 1, 2, . . . , N , we have\u0233 n+j as a normal distribution with mean and variance as\nE[\u0233 n+j ] = \u00b5 n + j\u03b4 n + \u03b3 n\u2212S+(j mod S) Var [\u0233 n+j ] = 1 N j(j + 1)\u03c3 2 v /2 + j(\u03c3 2 u + \u03c3 2 w ) + \u03c3 2 .\nConsequently, for all j = 1, 2, . . . , m, E[\u0233 n+j ] is in a linear form with respect to j, and the standard deviation of\u0233 n+j also takes a approximately linear form with respect to j.\nProof. Recall that \u03b1 n , \u03c3 are given and fixed, and we assume there is no change point or anomaly in the future time series. The Equation (2) leads to \u03b4 n+j = \u03b4 n + j l=1 v n+l , which implies that\n\u00b5 n+j = \u00b5 n + j\u03b4 n + j l=1 (j + 1 \u2212 l)v n+l + j l=1 u m+l .\nFor the seasonality part, simple linear algebra together with Equation 3 leads to \u03b3 n+j = \u03b3 n\u2212S+(j mod S) + j l=1 w n+l . Thus,\ny n+j = 1 N N i=1 \u00b5 n + j\u03b4 n + \u03b3 n\u2212S+(j mod S) + j l=1 (j + 1 \u2212 l)v (i) n+l + j l=1 u (i) m+l + j l=1 w (i) n+l + (i) n+j .\nDue to the independence and Gaussian distribution of all the noises,\u0233 n+j is also normally distributed and its means and variance can be calculated accordingly.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "ALGORITHM", "text": "Our proposed method can be divided into three parts: initialization, inference, and forecasting. Section 4 and Section 5 provide detailed explanation and reasoning for each of them. We present a whole picture of our proposed methodology in Algorithm 3. Initialize a 1 such that its first coordinate equals to the average of (y 1 , y 2 , . . . , y S ) and all the remaining S coordinates with 0; Initialize p a and p c by 1/n. Then generate z a and z c as independent Bernoulli random variables with success probability p a and p c respectively;\nPart II: Inference; while the likelihood function L a1,p,\u03c3 (y, \u03b1, z) not converges do Infer \u03b1 by Kalman filter, Kalman smoothing and \"fake-path\" trick described in Section 4.1;\nUpdate z a and z c by sampling from\n{z a t } n t=1 \u223c Ber(p a t ), {z c t } n t=1 \u223c Ber(p c t )\n, where the success probability {p a t } n t=1 and {p c t } n t=1 are defined in Equation ( 5) and ( 6);\nSegment control on z c by Algorithm 2; Update \u03c3 by Equation ( 7) to (8);\nUpdate a 1 such that its first two coordinates equal to the those of \u03b1 1 and the remaining (S \u2212 1) coordinates equals to those of \u03b1 S+1 ;\nCalculate the likelihood function L a1,p,\u03c3 (y, \u03b1, z) given in Equation ( 4); end Part III: Forecasting; With a n and \u03c3, use the generate procedure in Algorithm 1 to generate future time series y future with length m. Repeat the generative procedure to obtain multiple future paths y\n(1) future , y\nfuture , . . . , y\nfuture ; Combine all the predictive paths give the distribution for the future time series forecasting. If needed, calculate the point-wise quantile to obtain predictive intervals. Use the point-wise average as our final forecasting result.\nIt is worth mentioning that our proposed methodology is downward compatible with many simpler state space time series models. By letting p c = 0, we assume there is no change point in the time series. By letting p a = 0, we assume there is no anomaly point in the time series. If both p c and p a are set to be 0, then our model is reduced to the classic state space time series model. Also, the seasonality and slope can be removed from our model, if we know there exists no such structure in the data.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "SIMULATION", "text": "In this section, we study the synthetic data generated from our model. We let S = 7 and provide values for \u03c3 and a 1 . The change points and anomaly points are randomly generated. We use our generative procedure (Algorithm 1) to generate time series with total length 500 by fixed parameters. The first 350 points will be used as training set and the remaining 150 points will be used to evaluate the performance of forecasting.\nWhen generating, we let the time series have weekly seasonality with S = 7. For \u03c3 we have \u03c3 = 0.1, \u03c3 u = 0.1, \u03c3 v = 0.0004, \u03c3 w = 0.01, \u03c3 r = 1, \u03c3 o = 4. For \u03b1 1 we have value for \u00b5 as 20, value for \u03b4 as 0, and value for seasonality as (1, 2, 4, \u22121, \u22123, \u22122)/10. For p we have p c = 4/350 and p a = 10/350. Despite that, to make sure that at least one change point is in existence, we force z c 330 = 1 and r 330 = 2. That is, for each time series we generate, its 330th point is a change point with the mean shifted up by 3. Also to be consistence with our assumption, we force z c i = z a i = 0, \u2200351 \u2264 i \u2264 500 so there exists no change point or anomaly point in the testing part. The top panel of Figure 3 shows one example of synthesis data. The blue line marks the separation between training and testing set. The blue dashed line indicates the locations for the change point, while the yellow dots indicate the positions of anomaly points. Also see Figure 3 for illustration on the results returned by implementing our proposed algorithm on the same dataset. The red line gives the fitting results in the first 350 points and forecasting results in the last 150 points. The change points detected are marked with vertical red dotted line, and the anomaly detected are flagged with purple squares. Figure 3 shows that on this dataset, our proposed algorithm yields perfect detection on both change points and anomaly points. In Figure 3, the gray part indicates the 90% predictive interval for forecasting. We run our generative model 100 times to produce 100 different time series, and implement multiply methods on each of them, and aggregate the results together for comparison. We include the following methodologies.  et al., 2008)), and the Prophet R package by Taylor & Letham (2017). We evaluate the performances by mean absolute percentage error (MAPE), mean square error (MSE) and mean absolute error (MAE) on forecasting set. The mathematical definition of these three criterion is given as follows. Let x 1 , x 2 , . . . , x n be the true value andx 1 ,x 2 , . . . ,x n be the estimation or predictive values.\nThen we have\nMAPE = 1 n n i=1 |x i \u2212x i | x i , MSE = 1 n n i=1 (x i \u2212x i ) 2 , MAE = 1 n n i=1 |x i \u2212x i |.\nThe comparison of our proposed algorithm and the aforementioned algorithms are included below in Table 2. As we mentioned in Section 6, our algorithm is downward compatible with the cases ignoring the existence of change point or anomaly, by setting p c = 0 or p a = 0. We also run proposed algorithm on the synthetic data with p c = 0 (no change point), or p a = 0 (no anomaly point), or p c = p a = 0 (no change and anomaly point), for the purpose of numeric comparison.\nFrom Table 2 it turns out that our proposed algorithm achieves the best performance compared to other existing methods. Our proposed algorithm also performs better compared with the cases ignoring change point or anomaly point. This is a convincing evidence on the importance of incorporating both change point structure and anomaly point structure when modeling, for time series forecasting.\nWe also compare our proposed method with other existing change point detection methods and anomaly detection algorithm with respect to the performance of detections. We evaluate the performance by two criterions: True Positive Rate (TPR) and False Positive (FP). TPR measures the percentage of change points or anomalies to be correctly detected. FP count the number of points wrongly detected as change points or anomaly points. The mathematical definitions of TPR and FP are as follows. Let (z 1 , z 2 , . . . , z n ) be the true binary vector for change points or anomalies, and points in total. We split it such that the first 3000 points are used as training set and last 1000 points are used to evaluate the forecasting performance. From Figure 4, it is obvious that there exists no seasonality or slope structure in the dataset. This motivates us not to include these two components in our model. We implement our proposed algorithm without seasonality and slope, and compare the forecasting performance with other methods in Table 5. Our method outperforms BSTS, ARIMA, ETS and Prophet. However in Table 5 the performance can be slightly improved if we ignore the existence of anomaly points by letting p a = 0. This may be caused by model mis-specification as the data may not generated in a way not entirely captured by our model. Nevertheless, the performances of our method considering anomaly points or not, are comparable to each other. In this dataset there is no ground-truth of change point and anomaly point on their locations or even existence. However, from bottom panel of Figure 4, there are some obvious changes in the sequence and they all successfully captured by our algorithm.", "n_publication_ref": 2, "n_figure_ref": 6}, {"heading": "INTERNET TRAFFIC DATA", "text": "Our second real data is an Internet traffic data acquired from a major Tech company (see Figure 5).\nIt is a daily traffic data, with seasonality S = 7. We use the first 800 observations as training set and evaluate the performance of forecasting on the remaining 265 points. The bottom panel of Figure 5 show the result from implementing our algorithm. We also do the comparison of forecasting performance of our proposed algorithm together with other existing methods, shown in Table 6. We can also see that our algorithm outperforms all the other algorithms with respect to MAPE, MSE and MAE.\nCompared to the aforementioned models, our work differs in Bayesian modeling which samples posterior to estimate hidden components given the independent Bernoulli priors of changing point and anomalies.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "CONCLUSION", "text": "We incorporate the change point structure and anomaly point structure into the classic space state time series model. We provide a Bayesian scheme for inference and time series forecasting. We compare the performance of our methodology and state-of-the-art methods on both synthetic data and real datasets. Our method performs the best with respect to forecasting, change point detection, and anomaly detection as well.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "(\u1e91 1 ,\u1e91 2 , . . . ,\u1e91 n ) are the estimated ones. Then\nFrom the definition, we can see high TPR and low FP means the algorithm has better performance in detection.\nThe comparison on change point detection is shown in Table 3. We compare our results against three popular change point detection methods: Bayesian Change Point (BCP) (Barry & Hartigan, 1993), Change-Point (CP) (Killick & Eckley, 2014) and Breakout (twitter, 2017). From Table 3 our proposed method outperforms the most of the others by both TPR and FP. We have smaller TPR compared to CP, but we are better in FP.  In Table 4, we also compare the performance of our algorithm on anomaly detection with three existing common anomaly detection methods: the AnomalyDetection package by Twitter (2017), RAD by Netflix (2017) and Tsoutlier by Chen & Liu (1993). The comparison is listed in Table 4. We can see our method also outperforms most of the others with respect to anomaly detection, by both TPR and FP. RAD has slightly better TPR but its FP is much worse compared with ours.", "n_publication_ref": 6, "n_figure_ref": 0}, {"heading": "REAL DATA ANALYSIS", "text": "In this section, we implement our proposed method on real-world datasets. We also compare its performance against other existing time series forecasting methodologies. We consider two datasets, one is a public data called Well-log dataset, and the other is an unpublished internet traffic dataset. The bottom panels of Figure 4 and Figure 5 give the result of our proposed algorithms. The blue line separates the training set and testing set. We use red line to show our fitting and forecasting result, vertical red dashed line to indicate change points and purple dots to indicate anomaly points. The gray part shows 90% predication interval.", "n_publication_ref": 0, "n_figure_ref": 2}, {"heading": "WELL-LOG DATA", "text": "This dataset (Fearnhead & Clifford, 2003;JK & WJ, 1996) was collected when drilling a well. It measures the nuclear magnetic response, which provides geophysical information to analyze the structure of rock surrounding the well. This dataset is public and available online 2 . It has 4050 From Figure 5 our proposed algorithm identifies one change point (the 576th point, indicated by the vertical red dashed line), which can be confirmed that this is exactly the only one change point existing in this time series caused by the change of counting methods, by some external information. Thus, we give the perfect change point detection in this Internet traffic data.\nFor this Internet traffic dataset, since we have ground-truth for change point, we can compare the performance of change point detection of different methodologies. BCP returns posterior distribution, which peaks in the the 576th point with posterior probability value 0.5. And it also returns with many other points with posterior probability value around 0.1. CP returns 4 change points, where the 576th point (the only true one) is one of them. Breakout returns 8 change points without including the 576th point. To sum up, our proposed method achieves the best change point detection in this real dataset.", "n_publication_ref": 2, "n_figure_ref": 1}, {"heading": "RELATED WORK", "text": "Parametric models are widely considered in econometric literature for time series forecasting, e.g. Jalles (2009), Commandeur et al. (2011), Gould et al. (2008, Harvey & Peters (1990), Harvey et al. (1998). The general procedure of decomposition method (using trend, seasonal and irregular components) for univariate structural time series modeling is discussed in Harvey & Peters (1990); a unified state space framework is proposed to handle any messy time series in Harvey et al. (1998); and the explicit modeling of both additive and multiplicative seasonalities in Gould et al. (2008); Jalles (2009). Although Kalman filter and MCMC-based approaches are used to sample posterior to estimate hidden components, the changing points and anomalies are not considered and processed in the above works. For example, the irregular component considered in Jalles ( 2009) is simply the noises. Commandeur et al. (2011) discusses the statistical software for state-space modeling which is designed for generic time series analytic and modeling, which cannot directly be used when changing point and anomalies are in existence. Our proposed approach shares similarity with the aforementioned papers as we have similar additive structure of components. However we are able to incorporate the change points and anomalies, two common structure widely observed in real data, into our model by using Bernoulli indicators. This is non-trivial, and cannot be handled by the aforementioned papers or their variants.\nNon-parametric approaches are used for extraction of components from quasi-periodic time-series, e.g., the ensembles of weak detectors using non-parametric measurement are used in Artemov & Burnaev (2016) to detect change-points and anomalies, and the online decomposition algorithm based on per-component is adopted for change-point detection in Alexey Artemov (2015). Different from the above works, to handle the structural brakes and change-points, this paper presents the parametric approach for modeling anomalies and changing points by fitting them in the state-space framework using approximate inference for forecasting path prediction.\nDifferent Bayesian approaches are proposed for change-point detection, e.g., Adams & MacKay (2007) performs Bayesian change point detection from online inference by generating the distribution estimation of the next unseen datum in the sequence given only data already observed, and the Bayesian Online CPD (BOCPD) algorithm proposed by Turner et al. (2009) performs online prediction using hidden variable given the underlying predictive model (UPM) and the hazard function.", "n_publication_ref": 14, "n_figure_ref": 0}], "references": [{"title": "Bayesian online changepoint detection", "journal": "", "year": "2007", "authors": "P Ryan; David J C Adams;  Mackay"}, {"title": "Nonparametric decomposition of quasi-periodic time series for change-point detection", "journal": "", "year": "2015", "authors": "Andrey Lokot Alexey Artemov; Evgeny Burnaev"}, {"title": "Detecting performance degradation of software-intensive systems in the presence of trends and long-range dependence", "journal": "", "year": "2016-12-12", "authors": "Alexey Artemov; Evgeny Burnaev"}, {"title": "A bayesian analysis for change point problems", "journal": "Journal of the American Statistical Association", "year": "1993", "authors": "Daniel Barry;  John A Hartigan"}, {"title": "Time series analysis: forecasting and control", "journal": "John Wiley & Sons", "year": "2015", "authors": "E P George;  Box; M Gwilym;  Jenkins; C Gregory; Greta M Reinsel;  Ljung"}, {"title": "Inferring causal impact using bayesian structural time-series models", "journal": "The Annals of Applied Statistics", "year": "2015", "authors": "H Kay; Fabian Brodersen; Jim Gallusser; Nicolas Koehler;  Remy; L Steven;  Scott"}, {"title": "Joint estimation of model parameters and outlier effects in time series", "journal": "Journal of the American Statistical Association", "year": "1993", "authors": "Chung Chen; Lon-Mu Liu"}, {"title": "Stl: A seasonal-trend decomposition procedure based on loess", "journal": "Journal of Official Statistics", "year": "1990", "authors": "B Robert;  Cleveland; S William; Irma Cleveland;  Terpenning"}, {"title": "Time series for macroeconomics and finance", "journal": "", "year": "2005", "authors": "H John;  Cochrane"}, {"title": "Statistical software for state space methods", "journal": "Journal of Statistical Software", "year": "2011", "authors": "Jacques Commandeur; Siem Koopman; Marius Ooms"}, {"title": "Time series analysis by state space methods", "journal": "OUP Oxford", "year": "2012", "authors": "James Durbin;  Siem Jan Koopman"}, {"title": "On-line inference for hidden markov models via particle filters", "journal": "Journal of the Royal Statistical Society: Series B (Statistical Methodology)", "year": "2003", "authors": "Paul Fearnhead; Peter Clifford"}, {"title": "Forecasting time series with multiple seasonal patterns", "journal": "European Journal of Operational Research", "year": "2008", "authors": "Phillip G Gould; Anne B Koehler; J Keith Ord; Ralph D Snyder; Rob J Hyndman; Farshid Vahid-Araghi"}, {"title": "Analysis and Control of Nonlinear Process Systems", "journal": "Springer-Verlag London", "year": "2014", "authors": "Katalin Hangos; Jzsef Bokor; G Szederknyi"}, {"title": "Estimation procedures for structural time series models", "journal": "Journal of Forecasting", "year": "1990", "authors": "A C Harvey; S Peters"}, {"title": "Messy time series: A unified approach", "journal": "", "year": "1998", "authors": "Andrew Harvey; J Siem Jan Koopman;  Penzer"}, {"title": "Time series modelling of water resources and environmental systems", "journal": "Elsevier", "year": "1994", "authors": "W Keith; A Ian Hipel;  Mcleod"}, {"title": "Forecasting seasonals and trends by exponentially weighted moving averages", "journal": "International journal of forecasting", "year": "2004", "authors": "C Charles;  Holt"}, {"title": "Forecasting with exponential smoothing: the state space approach", "journal": "Springer Science & Business Media", "year": "2008", "authors": "Rob Hyndman; Anne B Koehler; Keith Ord; Ralph D Snyder"}, {"title": "Structural time series models and the kalman filter: a concise review", "journal": "", "year": "2009", "authors": "Joao Jalles"}, {"title": "Numerical bayesian methods applied to signal processing", "journal": "", "year": "1996", "authors": "O R Jk; F Wj"}, {"title": "changepoint: An r package for changepoint analysis", "journal": "Journal of Statistical Software", "year": "2014", "authors": "Rebecca Killick; Idris Eckley"}, {"title": "Time series anomaly detection", "journal": "", "year": "2017", "authors": " Netflix;  Rad"}, {"title": "Bidirectional learning for time-series models with hidden units", "journal": "", "year": "2017-06-11", "authors": "Takayuki Osogami; Hiroshi Kajino; Taro Sekiyama"}, {"title": "Predicting the present with bayesian structural time series", "journal": "International Journal of Mathematical Modelling and Numerical Optimisation", "year": "2014", "authors": "L Steven; Hal R Scott;  Varian"}, {"title": "Coherent probabilistic forecasts for hierarchical time series", "journal": "", "year": "2017-06-11", "authors": "James W Souhaib Ben Taieb; Rob J Taylor;  Hyndman"}, {"title": "Hybrid neural networks for learning the trend in time series", "journal": "", "year": "2017", "authors": "Karl Aberer Tao Lin; Tian Guo"}, {"title": "Prophet: forecasting at scale", "journal": "", "year": "2017", "authors": "S J Taylor; Letham "}, {"title": "Adaptive sequential Bayesian change point detection", "journal": "", "year": "2009", "authors": "Ryan Turner; Yunus Saatci; Carl Edward Rasmussen"}, {"title": "Anomalydetection: Anomaly detection with r. 2017. twitter. Breakout detection via robust e-statistics", "journal": "", "year": "2017", "authors": " Twitter"}, {"title": "Forecasting sales by exponentially weighted moving averages", "journal": "Management science", "year": "1960", "authors": " Peter R Winters"}, {"title": "Time series forecasting using a hybrid arima and neural network model", "journal": "Neurocomputing", "year": "2003", "authors": "Peter Zhang"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Demostration of Decompositions.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Algorithm 3 :3Proposed Algorithm Input: Observed time series y = (y 1 , y 2 , . . . , y n ), seasonality length S, length of time series for forecasting m, number of predictive paths N , change point minimum segment l Output: Change point detection z c , anomaly points z a , forecasting result y future = (y n+1 , y n+1 , . . . , y n+m ) and its distribution or predictive intervals Part I: Initialization; Initialize \u03c3 , \u03c3 o , \u03c3 u , \u03c3 r , \u03c3 v , \u03c3 w all with the empirical standard deviation of y;", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: An example of synthesis data (left), and the result after applying our algorithm (right).", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure 4 :4Figure 4: Well-log Data (left). The result of implementing our proposed algorithm (right).", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 5 :5Figure 5: Internet Traffic Data (top); The result of implementing our proposed algorithm (bottom).", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Two Categories for Hidden Variables \u03b12, . . . , \u03b1n)", "figure_data": "CategoryHidden VariableDefinitionLatent\u03b1 = (\u03b11,Variable"}, {"figure_label": "5", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Comparison of Forecasting in Well-log Data", "figure_data": "MethodsMAPEMSEMAEProposed0.03152963120Proposed (pa = 0)0.02952522957Proposed (pc = 0)0.03354343409Proposed (pa = 0, pc = 0)0.03857033908BSTS0.250 32030 27210ARIMA0.084 104808738ETS0.03760713860Prophet0.159 19530 17480"}], "formulas": [], "doi": "10.24963/ijcai.2017/316"}