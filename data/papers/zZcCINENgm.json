{"title": "WHY DEEP SURGICAL MODELS FAIL?: REVISITING SURGICAL ACTION TRIPLET RECOGNITION THROUGH THE LENS OF ROBUSTNESS", "authors": "Yanqi Cheng; Lihao Liu; Shujun Wang; Yueming Jin; Carola-Bibiane Sch\u00f6nlieb; Angelica I Aviles-Rivero", "pub_date": "", "abstract": "Surgical action triplet recognition provides a better understanding of the surgical scene. This task is of high relevance as it provides the surgeon with contextaware support and safety. The current go-to strategy for improving performance is the development of new network mechanisms. However, the performance of current state-of-the-art techniques is substantially lower than other surgical tasks. Why is this happening? This is the question that we address in this work. We present the first study to understand the failure of existing deep learning models through the lens of robustness and explainability. Firstly, we study current existing models under weak and strong \u03b4 \u2212perturbations via an adversarial optimisation scheme. We then analyse the failure modes via feature based explanations. Our study reveals that the key to improving performance and increasing reliability is in the core and spurious attributes. Our work opens the door to more trustworthy and reliable deep learning models in surgical data science. https://yc443.github.io/robustIVT/Minimally Invasive Surgery (MIS) has become the gold standard for several procedures (i.e., cholecystectomy & appendectomy), as it provides better clinical outcomes including reducing blood loss, minimising trauma to the body, causing less post-operative pain and faster recovery (Velanovich, 2000;Wilson et al., 2014). Despite the benefits of MIS, surgeons lose direct vision and touch on the target, which decreases surgeon-patient transparency imposing technical challenges to the surgeon. These challenges have motivated the development of automatic techniques for the analysis of the surgical workflow (", "sections": [{"heading": "", "text": "In that work, authors proposed a 3D interaction space for learning the triplets. In more recent work, the authors of Nwoye et al. (2022) introduced two new models. The first one is a direct extension of Tripnet called Attention Tripnet, where the novelty relies on a spatial attention mechanism. In the same work, the authors introduced another model called Rendezvous (RDV) that highlights a transformer-inspired neural network. We consider the tasks where the instrument (I), verb (V , action), and target (T , anatomical part) seek to be predicted.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Instrument", "text": "A commonality of existing surgical action triplet recognition techniques is the development of new mechanisms for improving the network architecture. However and despite the potential improvements, the performance of existing techniques is substantially lower than other tasks in surgical sciences-for example, force estimation and navigation assisted surgery. In this work, we go contrariwise existing techniques, and tackle the surgical action triplet recognition problem from the lens of robustness and explainability.\nIn the machine learning community there is a substantial increase of interest in understanding the lack of reliability of deep learning models (e.g., Ribeiro et al. (2016); Koh & Liang (2017); Sundararajan et al. (2017); Liu et al. (2019); Yeh et al. (2019); Hsieh et al. (2020)). To understand the lack of reliability of existing deep networks, a popular family of techniques is the so-called feature based explanations via robustness analysis (Simonyan et al., 2013;Zeiler & Fergus, 2014;Plumb et al., 2018;Wong et al., 2021;Singla & Feizi, 2021). Whilst existing techniques have extensively been evaluated for natural images tasks, there are no existing works addressing the complex problems as in action triplet recognition.\nContributions. In this work, we introduce, to the best of our knowledge, the first study to understand the failure of existing deep learning models for surgical action triplet recognition. To do this, we analyse the failures of existing state-of-the-art solutions through the lens of robustness. Specifically, we push to the limit the existing SOTA techniques for surgical action triplet recognition under weak and strong \u03b4 \u2212perturbations. We then extensively analyse the failure modes via the evaluation criteria Robustness-S, which analyses the behaviour of the models through feature based explanations. Our study reveals the impact of core and spurious features for more robust models. Our study opens the door to more trustworthy and reliable deep learning models in surgical data science, which is imperative for MIS.", "n_publication_ref": 11, "n_figure_ref": 0}, {"heading": "METHODOLOGY", "text": "We describe two key parts for Surgical action triplet recognition task: i) our experimental settings along with assumptions and ii) how we evaluate robustness via adversarial optimisation. The workflow of our work is displayed in Figure 2.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "SURGICAL ACTION TRIPLET RECOGNITION", "text": "In the surgical action triplet recognition problem, the main task is to recognise the triplet IV T , which is the composition of three components during surgery: instrument (I), verb (V ), and target (T ) in a given RGB image x \u2208 R H\u00d7W \u00d73 . Formally, we consider a given set of samples {(x n , y n )} N n=1 with provided labels Y = {0, 1, ..,C IV T \u2212 1} for C IV T = 100 classes. We seek then to predict a function f : X \u2192 Y such that f gets a good estimate for the unseen data. That is, a given parameterised deep learning model takes the image x as input, and outputs a set of class-wise presence probabilities, in our case 100 classes, under the IV T composition, Y IV T \u2208 R 100 , which we call it the logits of IV T . Since there are three individual components under the triplet composition, within the training network, we also considered the individual component d * \u2208 {I,V, T }, each with class number C d * (i.e. C I = 6, C V = 10, C T = 15). The logits of each component, Y d * \u2208 R C d * , are computed and used within the network.\nIn current state-of-the-art (SOTA) deep models (Nwoye et al., 2020;, there is a communal structure divided into three parts: i) the feature extraction backbone; ii) the individual component encoder; and iii) the triplet aggregation decoder that associate the components and output the logits of the IV T triplet. More precisely, the individual component encoder firstly concentrates on the instrument component to output Class Activation Maps (CAMs \u2208 R H\u00d7W \u00d7C d ) and the logits Y I of the instrument classes; the CAMs are then associated with the verb and target components separately for their logits (Y V and Y T ) to address the instrument-centric nature of the triplet.\nThe current SOTA techniques for surgical action triplet recognition focus on improving the components ii) & iii). However, the performance is still substantially lower than other surgical tasks. Our intuition behind such behaviour is due to the inherently complex and ambiguous conditions in MIS, which reflects the inability of the models to learn meaningful features. Our work is then based on the following modelling hypothesis.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Hypothesis 2.1: Deep Features are key for Robustness", "text": "Deep surgical techniques for triplet recognition lacks reliability due to the ineffective features. Therefore, the key to boosting performance, improving trustworthiness and reliability, and understanding failure of deep models is in the deep features. Following previous hypothesis, we address the questions of-why deep triplet recognition models fail? We do that by analysing the feature based explanations via robustness. To do this, we consider the current three SOTA techniques for our study: Tripnet (Nwoye et al., 2020), Attention Tripnet, andRendezvous (Nwoye et al., 2022). Moreover, we extensively investigate the repercussion of deep features using four widely used backbones ResNet-18, ResNet-50 (He et al., 2015), DenseNet-121 (Huang et al., 2016), and Swin Transformer (Liu et al., 2021). In the next section, we detail our strategy for analysing robustness.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "FEATURE BASED EXPLANATIONS VIA ROBUSTNESS", "text": "Our models of the triplet recognition output the logits of triplets composition, we then use it to select our predicted label for the classification result. We define the model from image x to the predicted label\u0177 as f : X \u2192 Y , where\nX \u2282 R H\u00d7W \u00d73 , Y = {0, 1, 2, ...,C IV T \u2212 1}.\nFor each class m \u2208 Y and within each given sample, we seek to recognise core and spurious attributions (Singla & Feizi, 2021;, which definition is as follows.\nCore Attributes: they refer to the features that form a part in the object we are detecting. Spurious Attributes: these are the ones that not a part of the object but co-occurs with it.\nHow We Evaluate Robustness? The body of literature has reported several alternatives for addressing the robustness of deep networks. Our work is motivated by recent findings on perturbation based methods, where even a small perturbation can significantly affect the performance of neural nets. In particular, we consider the setting of adversarial training (Allen-Zhu & Li, 2022;Olah et al., 2018;Engstrom et al., 2019) for robustify a given deep model.\nThe idea behind adversarial training for robustness is to enforce a given model to maintain its performance under a given perturbation \u03b4 . This problem can be seen cast as an optimisation problem over the network parameters \u03b8 as:\n\u03b8 * = arg min \u03b8 E (x,y)\u223cD [L \u03b8 (x, y)].(1)\nwhere E[L \u03b8 (\u2022)] denotes the expected loss to the parameter \u03b8 .\nOne seeks to the model be resistant to any \u03b4 \u2212perturbation. In this work, we follow a generalised adversarial training model, which reads:\nDefinition 2.1: Adversarial training under \u03b4 \u03b8 * = arg min \u03b8 E (x,y)\u223cD [max \u03b4\u2208\u2206 L \u03b8 (x + \u03b4, y)].\nThe goal is to the models do not change their performance even under the worse (strong) \u03b4 .\nThe machine learning literature has explored different forms of the generalised model in definition equation 2.1. For example, a better sparsity regulariser for the adversarial training as in (Xu et al., 2018). In this work, we adopt the evaluation criteria of that (Hsieh et al., 2020), where one seeks to measure the susceptibility of features to adversarial perturbations. More precisely, we can have an insight of the deep features extracted by our prediction through visualising compact set of relevant features selected by some defined explanation methods on trained models, and measuring the robustness of the models by performing adversarial attacks on the relevant or the irrelevant features.\nWe denote the set of all features as U, and consider a general set of feature S \u2286 U. Since the feature we are interested are those in the image x, we further denote the subset of S that related to the image as x S . To measure the robustness of the model, we rewrote the generalised model equation 2.1 following the evaluation criteria of that (Hsieh et al., 2020). A model on input x with adversarial perturbation on feature set S then reads:\nDefinition 2.2: Adversarial \u03b4 & Robustness-S \u03b5 * x S := {min \u03b4 \u2225\u03b4\u2225 p s.t. f (x + \u03b4) \u0338 = y, \u03b4 S = 0},\nwhere y is the ground truth label of image x; \u2225 \u2022 \u2225 p denotes the adversarial perturbation norm; S = U \\ S denotes the complementary set of feature S with \u03b4 S = 0 constraining the perturbation only happens on x S . We refer to \u03b5 * x S as Robustness-S (Hsieh et al., 2020), or the minimum adversarial perturbation norm on x S . We then denote the relevant features selected by the explanation methods as S r \u2286 U, with the irrelevant features as its complementary set S r = U \\ S r . Thus, the robustness on chosen feature sets-S r and S r tested on image x are: \nRobustness-S r = \u03b5 * x Sr ; Robustness-S r = \u03b5 * x Sr .", "n_publication_ref": 8, "n_figure_ref": 0}, {"heading": "EXPERIMENTAL RESULTS", "text": "In this section, we describe in detail the range of experiments that we conducted to validate our methodology.    In our robustness analysis, the main evaluation criteria is the robustness subject to the selected feature set (S r and S r ) on each backbone using the formula in equation 2.2.\nY d k = max m {Y IV T m }, \u2200 m \u2208 {0, 1..,C IV T \u2212 1} s.t. h d (m) = k,", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "IMPLEMENTATION DETAILS", "text": "We evaluate the model performance based on five-fold cross-validation, where we split 45 full videos into 5 equal folds. The testing set is selected from these 5 folds, and we treat the remaining 4 folds as the training set. Moreover, 5 videos from the 36 training set videos are selected as validation set during training.\nThe models are trained using the Stochastic Gradient Descent (SGD) optimiser. The feature extraction backbones are initialised with ImageNet pre-trained weights. Both linear and exponential decay of learning rate are used during training, with initial learning rates as {1e \u22122 , 1e \u22122 , 1e \u22122 } for backbone, encoder and decoder parts respectively. We set the batch size as 32, and epoch which performs the best among all recorded epochs up to AP score saturation on validation set in the specified k-fold. To reduce computational load, the input images and corresponding segmentation masks are resized from 256 \u00d7 448 to 8 \u00d7 14. For fair comparison, we ran all SOTA models (following all suggested protocols from the official repository) under the same conditions and using the official cross-validation split of the CholecT45 dataset (Nwoye & Padoy, 2022).", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "EVALUATION ON DOWNSTREAM TASKS", "text": "In this section, we carefully analyse the current SOTA techniques for triplet recognition from the feature based explainability lens.\nResults on Triplet Recognition with Cross-Validation. As first part of our analysis, we investigate the performance limitation on current SOTA techniques, and emphasise how such limitation is linked to the lack of reliable features. The results are reported in Table 1. In a closer look at the results, we observe that ResNet-18, in general, performs the worst among the compared backbones. However, we can observe that for one case, component analysis, it performs better than ResNet-50 under Tripnet Attention baseline. The intuition being such behaviour is that the MIS setting relies on ambiguous condition and, in some cases, some frames might contain higher spurious features that are better captured by it. We remark that the mean and standard-deviation in Table 1 are calculated from the 5 folds in each combination of backbone and baseline.\nWe also observe that ResNet-50 performs better than ResNet-18 due to the deeper feature extraction. The best performance, for both the tasks-component detection and triplet association, is reported by DenseNet-121. The intuition behind the performance gain is that DenseNet-121 somehow mitigates the issue of the limitation of the capability representation. This is because ResNet type networks are limited by the identity shortcut that stabilises training. These results support our modelling hypothesis that the key of performance is the robustness of the deep features.\nA key finding in our results is that whilst existing SOTA techniques (Nwoye & Padoy, 2022;Nwoye et al., 2022) are devoted to developing new network mechanisms, one can observe that a substantial performance improvement when improving the feature extraction. Moreover and unlike other surgical tasks, current techniques for triplet recognition are limited in performance. Why is this happening? Our results showed that the key is in the reliable features (linked to robustness); as enforcing more meaningful features, through several backbones, a significant performance improvement over all SOTA techniques is observed.\nTo further support our previous findings, we also ran a set of experiments using the trending principle of Transformers. More precisely, an non CNN backbone-the tiny Swin Transformer (Swin-T) (Liu et al., 2021) has also been tested on the Rendezvous, which has rather low AP scores on all of the 6 components in oppose to the 3 CNN backbones. This could be led by the shifted windows in the Swin-T, it is true that the shifted windows largely reduced the computational cost, but this could lead to bias feature attribute within bounding boxes, the incoherent spreading can be seen clearly in the visualisation of detected relevant features in Swin-T in Figure 3 (a).\nIn Table 1 we displayed the average results over all classes but-what behaviour can be observed from the per-class performance? It can be seen from Table 3 that though the best 5 predicted classes are different in each model, the predicted compositions seem clinically sensible supporting our previous discussion. In addition, the top 1 per-class AP score is significantly higher in DenseNet-121 with Rendezvous.\nVisualisation Results. To interpret features is far from being trivial. To address this issue, we provide a human-like comparison via heatmaps in Table 2. The implementation of the heatmaps is adapted from (Zhou et al., 2016). The displayed outputs reflect what the model is focusing based on the extracted features. These results support our hypothesis that deep features are the key in making correct predictions over any new network mechanism.\nWe observed that in the worst performed backbone-Swin-T, the feature been extracted are mostly spread across the images, however, the ones that concentrate on core attributes are not though performed the best. In the best performed DenseNet-121, a reasonable amount of attention are also been paid to spurious attributes; this can be seen more directly in our later discussion on robustness visualisation Figure 3.\nThe reported probability on the predicted label emphasises again the outstanding performance of DenseNet-121 backbone; in the sense that, the higher the probability for the correct label the better, the lower it is for incorrect prediction the better.\nWhy Surgical Triplet Recognition Models Fail? Robustness and Interpretability. We further support our findings through the lens of robustness. We use as evaluation criteria Robustness-S r and Robustness-S r with different explanation methods: vanilla gradient (Grad) (Shrikumar et al., 2017) and integrated gradient (IG) (Sundararajan et al., 2017). The results are in Table 4 & Figure 3. ", "n_publication_ref": 6, "n_figure_ref": 3}, {"heading": "COMPARISON BETWEEN DIFFERENT BACKBONES", "text": "In Table 4, we show the robustness results with top 25% attacked features on the average over 400 frames randomly chosen with exactly 1 labeled triplet. On one hand, we observe that the DenseNet-121 backbone consistently outperforms other network architectures on both evaluation criteria Robustness-S r and Robustness-S r . This suggests that DenseNet-121 backbone does capture different explanation characteristics which ignored by other network backbones. On the other hand, our results are supported by the finding in (Hsieh et al., 2020), as IG performs better than Grad; and the attack on relevant features yields lower robustness than perturbing the same percentage of irrelevant features.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "ROBUSTNESS EXPLANATION FOR SPECIFIC IMAGES", "text": "To more objectively evaluate the robustness explanation for specific images, we show: (a) Visualisation of important features, (b) Robustness-S r , (c) Robustness against the percentage of Top features, and (d) Robustness-S r in Figure 3. In Figure 3 (a), we visualise the Top 15% features (with yellow dots) by Grad and IG, respectively, and overlay it on manually labelled region containing instrument (in red) and target (in green). We observe that the best performed backbone (can be seen from the robustness comparison curves in Figure 3 (c)) on the specific image is the one that not only pays attention to core attributes, but also the spurious attribute. In the image VID08-000188, the best performed model is ResNet-18, which shows the ambiguous condition on individual images. In a closer look at Figure 3 (a), a small portion of the most relevant feature extracted by ResNet-18 is  spread not on the close surrounding of the object area. This importance of spurious attribute is further highlighted in image VID18-001156. We observe that DenseNet-121 provides the most robust result highlighting relevant features within the tissue region and across tool tip. The worst performed model-ResNet-18 merely treated the core attributes as relevant.\nThe relevant role of spurious attributes can be explained by the nature of the triplet, which consists a verb component that is not the physical object. Overall, we observe that reliable deep features are the key for robust models in triplet recognition. Moreover, we observe, unlike existing works of robustness against spurious features, that both core and spurious attributes are key for the prediction.", "n_publication_ref": 0, "n_figure_ref": 4}, {"heading": "CONCLUSION", "text": "We present the first work to understand the failure of existing deep learning models for the task of triplet recognition. We provided an extensive analysis through the lens of robustness. The significance of our work lies on understanding and addressing the key issues associated with the substantially limited in performance of existing techniques. Our work offers a step forward to more trustworthy and reliable models.", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Feature purification: How adversarial training performs robust deep learning", "journal": "IEEE", "year": "2022", "authors": "Zeyuan Allen-Zhu; Yuanzhi Li"}, {"title": "Towards retrieving force feedback in robotic-assisted surgery: A supervised neuro-recurrent-vision approach", "journal": "IEEE transactions on haptics", "year": "2016", "authors": " Angelica I Aviles; M Samar; James K Alsaleh; Alicia Hahn;  Casals"}, {"title": "Modeling and segmentation of surgical workflow from laparoscopic video", "journal": "Springer", "year": "2010", "authors": "Tobias Blum; Hubertus Feu\u00dfner; Nassir Navab"}, {"title": "Automatic data-driven real-time segmentation and recognition of surgical workflow", "journal": "International journal of computer assisted radiology and surgery", "year": "2016", "authors": "Olga Dergachyova; David Bouget; Arnaud Huaulm\u00e9; Xavier Morandi; Pierre Jannin"}, {"title": "Adversarial robustness as a prior for learned representations", "journal": "", "year": "2019", "authors": "Logan Engstrom; Andrew Ilyas; Shibani Santurkar; Dimitris Tsipras; Brandon Tran; Aleksander Madry"}, {"title": "Deep residual learning for image recognition", "journal": "", "year": "2015", "authors": "Kaiming He; Xiangyu Zhang; Shaoqing Ren; Jian Sun"}, {"title": "Evaluations and methods for explanation through robustness analysis", "journal": "", "year": "2020", "authors": "Cheng-Yu Hsieh; Chih-Kuan Yeh; Xuanqing Liu; Pradeep Ravikumar; Seungyeon Kim; Sanjiv Kumar; Cho-Jui Hsieh"}, {"title": "Densely connected convolutional networks", "journal": "", "year": "2016", "authors": "Gao Huang; Zhuang Liu; Laurens Van Der Maaten; Kilian Q Weinberger"}, {"title": "Knowledge-driven formalization of laparoscopic surgeries for rule-based intraoperative context-aware assistance", "journal": "Springer", "year": "2014", "authors": "Darko Kati\u0107; Anna-Laura Wekerle; Fabian G\u00e4rtner; Hannes Kenngott"}, {"title": "Understanding black-box predictions via influence functions", "journal": "PMLR", "year": "2017", "authors": "Wei Pang; Percy Koh;  Liang"}, {"title": "Multi-task deep model with margin ranking loss for lung nodule analysis", "journal": "IEEE transactions on medical imaging", "year": "2019", "authors": "Lihao Liu; Qi Dou; Hao Chen; Jing Qin; Pheng-Ann Heng"}, {"title": "Swin transformer: Hierarchical vision transformer using shifted windows", "journal": "", "year": "2021", "authors": "Ze Liu; Yutong Lin; Yue Cao; Han Hu; Yixuan Wei; Zheng Zhang; Stephen Lin; Baining Guo"}, {"title": "Episode classification for the analysis of tissue/instrument interaction with multiple visual cues", "journal": "Springer", "year": "2003", "authors": "P L Benny; Ara Lo; Guang-Zhong Darzi;  Yang"}, {"title": "Surgical data science: enabling next-generation surgery", "journal": "", "year": "2017", "authors": "Lena Maier-Hein; Swaroop Vedula; Stefanie Speidel; Nassir Navab; Ron Kikinis; Adrian Park; Matthias Eisenmann; Hubertus Feussner; Germain Forestier; Stamatia Giannarou"}, {"title": "Acquisition of process descriptions from surgical interventions", "journal": "Springer", "year": "2006", "authors": "Thomas Neumuth; Gero Strau\u00df; J\u00fcrgen Meixensberger; U Heinz; Oliver Lemke;  Burgert"}, {"title": "Chinedu Innocent Nwoye and Nicolas Padoy. Data splits and metrics for method benchmarking on surgical action triplet datasets", "journal": "", "year": "2022", "authors": ""}, {"title": "Recognition of instrument-tissue interactions in endoscopic videos via action triplets", "journal": "Springer", "year": "2020", "authors": "Cristians Chinedu Innocent Nwoye; Tong Gonzalez; Pietro Yu; Didier Mascagni; Jacques Mutter; Nicolas Marescaux;  Padoy"}, {"title": "Rendezvous: Attention mechanisms for the recognition of surgical action triplets in endoscopic videos", "journal": "Medical Image Analysis", "year": "2022", "authors": "Tong Chinedu Innocent Nwoye; Cristians Yu; Barbara Gonzalez; Pietro Seeliger; Didier Mascagni; Jacques Mutter; Nicolas Marescaux;  Padoy"}, {"title": "The building blocks of interpretability", "journal": "Distill", "year": "2018", "authors": "Chris Olah; Arvind Satyanarayan; Ian Johnson; Shan Carter; Ludwig Schubert; Katherine Ye; Alexander Mordvintsev"}, {"title": "Model agnostic supervised local explanations", "journal": "", "year": "2018", "authors": "Gregory Plumb; Denali Molitor; Ameet S Talwalkar"}, {"title": "Why should i trust you?\" explaining the predictions of any classifier", "journal": "", "year": "2016", "authors": "Sameer Marco Tulio Ribeiro; Carlos Singh;  Guestrin"}, {"title": "Learning important features through propagating activation differences", "journal": "PMLR", "year": "2017", "authors": "Avanti Shrikumar; Peyton Greenside; Anshul Kundaje"}, {"title": "Deep inside convolutional networks: Visualising image classification models and saliency maps", "journal": "", "year": "2013", "authors": "Karen Simonyan; Andrea Vedaldi; Andrew Zisserman"}, {"title": "Salient imagenet: How to discover spurious features in deep learning", "journal": "", "year": "2021", "authors": "Sahil Singla; Soheil Feizi"}, {"title": "Understanding failures of deep networks via robust feature extraction", "journal": "", "year": "2021", "authors": "Sahil Singla; Besmira Nushi; Shital Shah; Ece Kamar; Eric Horvitz"}, {"title": "Axiomatic attribution for deep networks", "journal": "PMLR", "year": "2017", "authors": "Mukund Sundararajan; Ankur Taly; Qiqi Yan"}, {"title": "Endonet: a deep architecture for recognition tasks on laparoscopic videos", "journal": "IEEE transactions on medical imaging", "year": "2016", "authors": "P Andru; Sherif Twinanda; Didier Shehata; Jacques Mutter; Michel Marescaux; Nicolas De Mathelin;  Padoy"}, {"title": "Laparoscopic vs open surgery", "journal": "Surgical endoscopy", "year": "2000", "authors": "Vic Velanovich"}, {"title": "Cai4cai: the rise of contextual artificial intelligence in computer-assisted interventions", "journal": "Proceedings of the IEEE", "year": "2019", "authors": "Tom Vercauteren; Mathias Unberath; Nicolas Padoy; Nassir Navab"}, {"title": "Overview of general advantages, limitations, and strategies", "journal": "Springer", "year": "2014", "authors": "Hossein Erik B Wilson; Vicky D Bagshahi;  Woodruff"}, {"title": "Leveraging sparse linear layers for debuggable deep networks", "journal": "PMLR", "year": "2021", "authors": "Eric Wong; Shibani Santurkar; Aleksander Madry"}, {"title": "Structured adversarial attack: Towards general implementation and better interpretability", "journal": "", "year": "2018", "authors": "Kaidi Xu; Sijia Liu; Pu Zhao; Pin-Yu Chen; Huan Zhang; Quanfu Fan; Deniz Erdogmus; Yanzhi Wang; Xue Lin"}, {"title": "On the (in) fidelity and sensitivity of explanations", "journal": "Advances in Neural Information Processing Systems", "year": "2019", "authors": "Chih-Kuan Yeh; Cheng-Yu Hsieh; Arun Suggala; Pradeep K David I Inouye;  Ravikumar"}, {"title": "Visualizing and understanding convolutional networks", "journal": "Springer", "year": "2014", "authors": "D Matthew; Rob Zeiler;  Fergus"}, {"title": "Learning deep features for discriminative localization", "journal": "", "year": "2016", "authors": "Bolei Zhou; Aditya Khosla; Agata Lapedriza; Aude Oliva; Antonio Torralba"}, {"title": "Deepphase: surgical phase recognition in cataracts videos", "journal": "Springer", "year": "2018", "authors": "Odysseas Zisimopoulos; Evangello Flouty; Imanol Luengo; Petros Giataganas; Jean Nehme"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure1: Visualisation of the surgical action triplet recognition task. We consider the tasks where the instrument (I), verb (V , action), and target (T , anatomical part) seek to be predicted.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure2: Illustration of the main network structure, and how the adversarial perturbation is added to measure robustness.", "figure_data": ""}, {"figure_label": "1", "figure_type": "", "figure_id": "fig_2", "figure_caption": "3. 11DATASET DESCRIPTION AND EVALUATION PROTOCOLDataset Description. We use CholecT45 dataset(Nwoye & Padoy, 2022) to evaluate the robustness of the three SOTA models for the Surgical Action Triplet Recognition task. Specifically, CholecT45 dataset contains 45 videos with annotations including 6 classes of instrument, 10 classes of verb, and 15 classes of target (i.e. C I = 6, C V = 10, C T = 15) generating 900 (6 \u00d7 10 \u00d7 25) potential combinations for triplet labels. To maximise the clinical utility, we utilise the top-100 combinations of relevant labels, which are selected by removing a large portion of spurious combinations according to class grouping and surgical relevance rating(Nwoye et al., 2022). Each video contains around 2, 000 annotated frames extracted at 1 fps in RGB channels, leading to a total of 90, 489 recorded frames. To remove the redundant information, the frames captured after the laparoscope been taken out of the body are blacked out with value [0, 0, 0]. Evaluation Protocol. The triplet action recognition is evaluated by the average precision (AP) metric. Our models can directly output the predictions of triplet class AP IV T . Instead, AP d where d \u2208 {I,V, T, IV, IT } cannot be predicted explicitly. Then we obtain the final predictions of d \u2208 {I,V, T, IV, IT } components according to(Nwoye & Padoy, 2022;Nwoye et al., 2022): ", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "where we calculate the probability of class k \u2208 {0, 1, ..,C d \u2212 1} under component d; and h d (\u2022) maps the class m from IV T triplet compositions to the class under component d.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure 3 :3Figure3: The set of figures shows robustness analysis on randomly selected images with a. the visualisation of the Top 15 percent of important features selected by the 2 explanation methods-Grad and IG; b. (/d.) the trends showing the robustness measured on the relevant S r (/irrelevant S r ) features been selected by the 2 explanation methods against the percentage of Top features been defined as relevant; c. the comparison of the robustness across the 4 backbones embedded in Rendezvous baseline.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "Performance comparison for the task of Triplet recognition. The results are reported in terms of Average Precision (AP%) on the CholecT45 dataset using the official cross-validation split.", "figure_data": "METHODCOMPONENT DETECTIONTRIPLET ASSOCIATIONBASELINEBACKBONEAP IAP VAP TAP IVAP ITAP IV TResNet-1882.4 \u00b1 2.554.1 \u00b1 2.033.0 \u00b1 2.330.6 \u00b1 2.625.9 \u00b1 1.521.2 \u00b1 1.2TripnetResNet-5085.3 \u00b1 1.357.8 \u00b1 1.634.7 \u00b1 1.931.3 \u00b1 2.327.1 \u00b1 2.421.9 \u00b1 1.5DenseNet-12186.9 \u00b1 1.458.7 \u00b1 1.535.6 \u00b1 2.833.4 \u00b1 3.427.8 \u00b1 1.822.5 \u00b1 2.3ResNet-1882.2 \u00b1 2.656.7 \u00b1 3.834.6 \u00b1 2.230.8 \u00b1 1.827.4 \u00b1 1.321.7 \u00b1 1.3Attention TripnetResNet-5081.9 \u00b1 3.056.8 \u00b1 1.134.1 \u00b1 1.431.5 \u00b1 2.227.5 \u00b1 1.021.9 \u00b1 1.2DenseNet-12183.7 \u00b1 3.557.5 \u00b1 3.234.3 \u00b1 1.333.1 \u00b1 2.428.5 \u00b1 1.622.8 \u00b1 1.3ResNet-1885.3 \u00b1 1.458.9 \u00b1 2.635.2 \u00b1 3.433.6 \u00b1 2.630.1 \u00b1 2.824.3 \u00b1 2.3RendezvousResNet-5085.4 \u00b1 1.658.4 \u00b1 1.434.7 \u00b1 2.435.3 \u00b1 3.530.8 \u00b1 2.625.3 \u00b1 2.7DenseNet-12188.5 \u00b1 2.761.7 \u00b1 1.736.7 \u00b1 2.136.5 \u00b1 4.732.1 \u00b1 2.726.3 \u00b1 2.9Swin-T73.6 \u00b1 1.948.3 \u00b1 2.629.2 \u00b1 1.428.1 \u00b1 3.124.7 \u00b1 2.020.4 \u00b1 2.1"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Heatmaps Comparison under different feature extraction backbones. We displayed four randomly selected images in fold 3 when using the best performed weights trained and validated on folds 1,2,4 and 5.", "figure_data": "Ground TruthResNet-18ResNet-50DenseNet-121Swin-TIVTIVTIVTIVTIVTImage ID: VID08-00029099.4%100.0%99.9%81.2%grasperretractgallbladdergrasperretractgallbladdergrasperretractgallbladdergrasperretractgallbladdergrasperretractgallbladderImage ID: VID31-00008096.2%99.5%75.4%92.3%grasperretractgallbladdergrasperretractomentumgrasperretractomentumgrasperretractomentumgrasperretractomentumImage ID: VID36-00006490.6%98.4%99.7%32.9%grasperretractgallbladdergrasperretractgallbladdergrasperretractgallbladdergrasperretractgallbladdergraspergraspgallbladderImage ID: VID57-00079884.2%84.8%35.2%54.1%hookdissectcystic_ducthookdissectgallbladderirrigatoraspiratefluidhookdissectcystic_ducthookdissectgallbladderProbability of the\u2190 INCORRECTCORRECT \u2192IVT class100.0%0.0%0.0%100.0%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "Top 5 predicted Triplet classes in each of the 10 models. The top 5 is assessed by the AP IV T score.", "figure_data": "ResNet-18ResNet-50DenseNet-121Swin-TTripletAPTripletAPTripletAP12:grasper graspspecimen_bag82.60%17:grasper retractgallbladder86.95%17:grasperretractgallbladder86.93%Tripnet17:grasper retract 29:bipolar coagulategallbladder liver81.04% 77.11%12:grasper grasp 60:hook dissectspecimen_bag gallbladder80.50% 77.15%12:grasper 29:bipolargrasp coagulatespecimen_bag liver81.45% 80.19%60:hookdissectgallbladder74.13%29:bipolar coagulateliver75.69%60:hookdissectgallbladder76.35%79:clipper clipcystic_duct61.28%6:grasper graspcystic_plate69.24%79:clipperclipcystic_duct67.75%TripletAPTripletAPTripletAP12:grasper graspspecimen_bag81.38%17:grasper retractgallbladder82.75%17:grasperretractgallbladder83.63%Attention Tripnet17:grasper retract 29:bipolar coagulategallbladder liver78.70% 78.52%12:grasper grasp 29:bipolar coagulatespecimen_bag liver78.53% 76.44%12:grasper 29:bipolargrasp coagulatespecimen_bag liver80.01% 75.68%28:bipolar coagulategallbladder77.44%60:hookdissectgallbladder71.79%60:hookdissectgallbladder75.36%30:bipolar coagulateomentum77.39%28:bipolar coagulategallbladder70.68%30:bipolarcoagulateomentum69.49%TripletAPTripletAPTripletAPTripletAP17:grasper retractgallbladder85.57%30:bipolar coagulateomentum91.36%84:irrigatordissectcystic_pedicle96.84%17:grasper retractgallbladder78.36%Rendezvous29:bipolar coagulate 12:grasper graspliver specimen_bag83.90% 82.77%17:grasper retract 29:bipolar coagulategallbladder liver86.11% 84.94%30:bipolar 17:graspercoagulate retractomentum gallbladder89.60% 89.46%60:hook 12:grasper grasp dissectgallbladder specimen_bag72.57% 69.96%30:bipolar coagulateomentum76.88%12:grasper graspspecimen_bag81.50%12:graspergraspspecimen_bag85.88%30:bipolar coagulateomentum67.03%60:hookdissectgallbladder76.49%28:bipolar coagulategallbladder79.60%29:bipolarcoagulateliver84.43%29:bipolar coagulateliver66.08%"}, {"figure_label": "4", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Robustness measured on 400 examples (i.e. images) randomly selected from the images in the fold 3 videos with exactly 1 labeled triplet. Top 25 percent of relevant S r or irrelevant S r features are selected from 2 explanation methods Grad and IG. We perform attacks on the selected 25 percent.", "figure_data": "ATTACKED FEATURESEXPLANATION METHODSResNet-18BACKBONES (ON RENDEZVOUS) ResNet-50 DenseNet-121Swin-TRobustness-S rGrad2.5996872.6514353.2877981.778592IG2.6219012.6860643.3193111.777737Robustness-S rGrad2.5174042.6080133.1882701.750599IG2.5153432.6031183.1878481.749097"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Image ID: VID08-000188Ground Truth Label: 17:grasper,retract,gallbladder", "figure_data": "a. Original (greyscale)GradIGb. Robustness-c. Backbone Comparisond. Robustness-GradSwin-TResNet-18Robustness-Robustness-ResNet-50IGDenseNet-121Image ID: VID18-001156Ground Truth Label: 60:hook,dissect,gallbladdera. Original (greyscale)GradIGb. Robustness-c. Backbone Comparisond.Robustness-GradSwin-TResNet-18Robustness-Robustness-ResNet-50IGDenseNet-121"}], "formulas": [], "doi": ""}