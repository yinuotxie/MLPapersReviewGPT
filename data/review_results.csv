title,json_file,full_input,abstract_input,human_reviews_summaries,human_reviews_summaries_lengths,gpt_3.5_full_reviews_summaries,gpt_3.5_full_reviews_summaries_lengths,gpt_3.5_full_reviews_summaries_match,gpt_3.5_full_reviews_summaries_match_lengths,gpt_3.5_abstract_reviews_summaries,gpt_3.5_abstract_reviews_summaries_lengths,gpt_3.5_abstract_reviews_summaries_match,gpt_3.5_abstract_reviews_summaries_match_lengths,gpt_3.5_abstract_reviews_one_shot_summaries,gpt_3.5_abstract_reviews_one_shot_summaries_lengths,gpt_3.5_abstract_reviews_one_shot_summaries_match,gpt_3.5_abstract_reviews_one_shot_summaries_match_lengths,gpt_4_full_reviews_summaries,gpt_4_full_reviews_summaries_lengths,gpt_4_full_reviews_summaries_match,gpt_4_full_reviews_summaries_match_lengths,gpt_4_abstract_reviews_summaries,gpt_4_abstract_reviews_summaries_lengths,gpt_4_abstract_reviews_summaries_match,gpt_4_abstract_reviews_summaries_match_lengths,gpt_4_abstract_reviews_one_shot_summaries,gpt_4_abstract_reviews_one_shot_summaries_lengths,gpt_4_abstract_reviews_one_shot_summaries_match,gpt_4_abstract_reviews_one_shot_summaries_match_lengths,mistral_reviews,fine_tuned_mistral_reviews,mistral_reviews_summaries,mistral_reviews_summaries_lengths,fine_tuned_mistral_reviews_summaries,fine_tuned_mistral_reviews_summaries_lengths,mistral_reviews_summaries_match,mistral_reviews_summaries_match_lengths,fine_tuned_mistral_reviews_summaries_match,fine_tuned_mistral_reviews_summaries_match_lengths
A Walk with SGD: How SGD Explores Regions of Deep Network Loss?,B1l6e3RcF7.json,"[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.

[CAPTIONS]
Table 1: Figure 1 :Figure 2 :12Figure1: Plots for VGG-11 architecture trained using SGD on CIFAR-10. Each plot contains the training loss for 40 iterations of training at various epochs. Between the training loss at every consecutive iteration (vertical gray lines), we uniformly sample 10 points between the parameters before and after the training update and calculate the loss at these points. Thus we take a slice of the loss surface between two iterations. These loss values are plotted between every consecutive training loss value from training updates. We find that the loss interpolation between consecutive iterations have a minimum in between in all cases showing barriers are not being crossed. For epochs 25 and 100 this is not clearly visible, but we quantitatively record it and discuss it later. The dashed orange line (only shown in the epoch 1 plot) connects the minimum of the loss interpolation between consecutive iterations and is shown to highlight that the valley floor has ups and downs along the path of SGD (which can be seen for all epochs).
Table 2: Figure 3 :3Figure 3: Plots for MLP architecture trained using SGD on MNIST. All the descriptions are same as described in figure 1.
Table 3: Figure 4 :4Figure 4: Numbers of barriers found during training loss interpolation for every epoch (450 iterations) for VGG-11 on CIFAR-10.We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
Table 4: Figure 5 :5Figure5: Plots for the alignments between mini-batch gradient and hessian with VGG-11 architecture trained using SGD on CIFAR-10 at the end of Epoch 5 and Epoch 10. Alignments are calculated for mini-batch size 100,1000,10000 and 45000 (dataset size).
Table 5: Figure 7 :7Figure 7: Plots of the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100 for different batch sizes.
Table 6: Figure 8 :8Figure 8: Plots for Resnet-56 Epoch 1 trained using full batch Gradient Descent (GD) on CIFAR-10.
Table 7: Figure 9 :9Figure 9: Plots for Resnet-56 Epoch 1 trained using SGD on CIFAR-10.
Table 8: Figure 10 :10Figure 10: Plots for Resnet-56 Epoch 2 trained using SGD on CIFAR-10.
Table 9: Figure 11 :11Figure 11: Plots for Resnet-56 Epoch 25 trained using SGD on CIFAR-10.
Table 10: Figure 12 :12Figure 12: Plots for Resnet-56 Epoch 100 trained using SGD on CIFAR-10.
Table 11: Figure 13 :13Figure 13: Plots for VGG-11 Epoch 2 trained using SGD on CIFAR-10.
Table 12: Figure 14 :14Figure 14: Plots for VGG-11 Epoch 25 trained using SGD on CIFAR-10.
Table 13: Figure 15 :15Figure 15: Plots for VGG-11 Epoch 100 trained using SGD on CIFAR-10.
Table 14: Figure 16 :16Figure 16: Plots for MLP Epoch 1 trained using full batch Gradient Descent (GD) on MNIST.
Table 15: Figure 17 :17Figure 17: Plots for MLP Epoch 1 trained using SGD on MNIST.
Table 16: Figure 18 :18Figure 18: Plots for MLP Epoch 2 trained using SGD on MNIST.
Table 17: Figure 19 :19Figure 19: Plots for VGG-11 Epoch 1 trained using full batch Gradient Descent (GD) on Tiny-ImageNet.
Table 18: Figure 20 :20Figure 20: Plots for VGG-11 Epoch 1 trained using SGD on Tiny-ImageNet.
Table 19: Figure 21 :21Figure 21: Plots for VGG-11 Epoch 1 trained using learning rate 0.3 batch size 100 on CIFAR-10.
Table 20: 

[INTRODUCTION]
The non-convexity of the deep neural network (DNN) loss surface makes the behavior of optimization algorithms less intuitive compared to the convex setting. Moreover, optimization in DNNs is no longer about finding any minimum, but rather about finding ones that generalizes well (Keskar et al., 2016). Since deep networks are initialized randomly, finding such minima will require exploration of different regions of the loss surface. This intuition has been formalized in recent papers that study stochastic gradient descent (SGD) as a diffusion process (Hoffer et al., 2017;Smith & Le, 2017;Jastrzebski et al., 2017;Chaudhari & Soatto, 2017). Briefly, these papers show that SGD simulates a discrete approximation of stochastic differential equation (SDE), and hence performs a random walk on the potential induced by the DNN loss surface.
In this work, we complement the diffusion perspective of SGD with a qualitative view of how SGD explores different regions of the non-convex loss landscape of deep neural networks through empirical evidence. Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process. We show in this work that SGD rarely crosses any barriers along its path during the course of training. By this observation, we do not claim that SGD does not simulate diffusion. Through experimental deductions, we show an alternate mechanism that SGD seems to dominantly use to explore different regions of the non-convex loss landscape.
Further, it is known that larger batch-sizes slow down the diffusion process (Hoffer et al., 2017). We show the qualitative reason behind this slow down to be an oscillation behavior of SGD which prevents it from moving far away from initialization. This behavior is a result of the mini-batch gradients becoming increasingly aligned with the top eigenvectors of the Hessian for larger batch-sizes. This behavior is known to slow down convergence in optimization theory (for instance consider the motivation behind momentum (Polyak, 1964;Sutskever et al., 2013)). We discuss how it also slows down explorations in the non-convex setting of deep network loss surface.
Experiments are conducted on multiple data sets, architectures and hyper-parameter settings. The findings mentioned above hold true on all of them.

[SETUP]
We now describe the details of how we study the existence of barriers along the optimization path of SGD. The main tool we use for studying the DNN loss surface along SGD's path is to interpolate the loss surface between parameters before and after each training update. We note that this strategy of interpolating the loss surface between parameters was introduced by Goodfellow et al. (2014). In their paper, the interpolation is conducted between initial and final (after training) parameter values for analysis purposes. In contrast, we compute interpolations before and after each training update because this interpolation precisely tells us whether or not SGD crosses a barrier during an update step. We say a barrier is crossed when we see a point in the parameter space interpolated between the parameters just before and after an update step, such that the loss at the barrier point is higher than the loss at both the other points.
Consider that the parameters θ of a neural network are initialized to a value θ 0 . When using an optimization method to update these parameters, the t th update step takes the parameter from θ t to θ t+1 using estimated gradient g t as,
θ t+1 = θ t − ηg t (1)
where η is the learning rate. Notice the t th update step implies the t th epoch only in the case when using the full batch gradient descent (GD). In the case of stochastic gradient descent, one iteration is an update from gradient computed from a mini-batch. We then interpolate the DNN loss between the convex combination of θ t and θ t+1 by considering parameter vectors θ α t = (1 − α)θ t + αθ t+1 , where α ∈ [0, 1] is chosen such that we obtain 10 samples uniformly placed between these two parameter points. We note that even though the updates are performed using mini-batches for SGD, the training loss values we compute for the interpolation use the full dataset to visualize the actual loss landscape.

[BARRIERS AND EXPLORATION DURING SGD TRAINING]
For this section, we perform experiments on MNIST (Lecun & Cortes) and CIFAR-10 (Krizhevsky, 2009) datasets, and use multi-layer perceptrons (MLP), VGG-11 (Simonyan & Zisserman, 2014) and Resnet-56 (He et al., 2016) architectures with various batch sizes and learning rates. We discuss our observations for VGG-11 architecture on CIFAR-10 dataset (figure 1) as a reference but the same conclusions hold for experiments on MLP trained on MNIST (figure 3) and Resnet-56 trained on CIFAR-10 (figure 2).
We train VGG-11 on CIFAR-10 with a batch size of 100 and fixed learning rate of 0.1. We report the visualization of loss interpolation between consecutive iterations for 40 iterations from epochs 1, 2, 25 and 100 for visual clarity. The interpolation is shown in figure 1. To be clear, the x-axis is calibrated by the number of iterations, and there are 10 interpolated loss values between each consecutive iteration (vertical gray lines) in the training loss plots. In these plots, we find two interesting behavior of SGD.
First, we find that the interpolated loss between every consecutive update from SGD optimization update appears to be a quadratic-like structure with a minimum in between. Note that while this is not visible for epochs 25 and 100, we later show quantitative measurements that ensures this claim. This plot thus shows that in the iterations plotted, SGD rarely crosses barriers.
Second, we observe how the minimum of each interpolation evolves as training progresses. This is highlighted in figure 1 (a) with a dashed orange line. We find that this minimum has ups and downs along the path of SGD for all our interpolation plots. To draw deductions from this observation, consider a simple example that helps us understand this scenario concretely. Let parameter points θ A , θ B and θ C be a result of three consecutive SGD updates with loss values A , B and C (using full training set). Note that since these are only three points, they exist in a two dimensional subspace and the loss value can be imagined along the third dimension. Then corresponding to the behavior in the plot, there is a parameter point θ AB between θ A and θ B on the line connecting these two points, which has a loss value AB < A , B . Similarly there is a point θ BC between θ B and θ C on the line connecting these two points, which has a loss value BC < B , C . Given this construction, for any configuration of θ A , θ B and θ C on the two dimensional plane, it is easy to see that if AB < BC , any path from θ AB to θ BC will have loss values that must increase at some point. Hence, what this construction essentially represents (as we refer to it), is a situation where SGD has moved over a barrier. Therefore, the ups and downs of the minimum between loss interpolations in figure 1 (a,b,c,d) represents SGD moving over barriers. In this way we find that when running SGD on the loss surface of deep networks, instead of crossing barriers, a more dominant way SGD performs exploration is by moving over them.  on CIFAR-10 and MLP on MNIST. We say a barrier is crossed during an update step if there exists a point interpolated between the parameters before and after an update which has a loss value higher than the loss at either points. For most parts of the training, we find that SGD does not cross any significant number of barriers.
The same qualitative analysis for SGD with different hyper-parameters are also shown in section 1 in appendix. The observations we described here remain consistent for all these experiments.
So far we showed qualitative visualizations to make the claim that SGD rarely crosses barriers. In order to show that the claim extends to the rest of the training instead of only a few iterations we showed above, we now quantitatively measure how many barriers are crossed for the entire epoch in different phase of training. This result is shown in table 1 for VGG-11 and Resnet-56 trained on CIFAR-10 (trained for 100 epochs) and an MLP trained on MNIST (trained for 40 epochs). We note that each case, an epoch consists of more than 450 iterations. As we see, a negligible number of barriers are crossed for most parts of the training compared to the number of iterations performed during each epoch. For concreteness, we further compute the number of barriers crossed for the first 40 epochs for VGG-11 on CIFAR-10 as shown in Figure 4 and reach the same conclusion. We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
L(θt)+L(θt+1)−2L(θ min t ) 2
) are substantially smaller compared with the value of loss at the corresponding iterations (not mentioned here), meaning they are not significant barriers.

[THE EFFECT OF BATCH-SIZE ON EXPLORATION]
4.1 ANALYSIS Hoffer et al. (2017) discuss that SGD training with different batch-sizes leads to different diffusion rates (very large batch sizes being slower). Further, when training for the same number of epochs, a larger batch training performs less number of iterations. Combining these two observations, they reach the conclusion that large batch training makes the diffusion process slow. As empirical evidence, they show that the distance of parameters from initialization evolves logarithmically in the number of iterations.
We now present a complementary optimization perspective to their observation. To continue, we introduce the following notations. Let p i (θ) denote the predicted probability output (of the correct class in the classification setting for instance) of a DNN parameterized by θ for the i th data sample (in total N samples). Then the negative log likelihood loss for the i th sample is given by
L i (θ) = − log(p i (θ)). The gradient g B (θ) from mini-batch SGD at a parameter value θ is ex- pressed as, g B (θ) = 1 B i∈B ∂Li(θ)
∂θ ,ḡ(θ) denotes the expected gradient using all training samples, B is the mini-batch size (and we have also overloaded it to mean the mini-batch set) and C(θ) is the gradient covariance matrix at θ. Then the relation between the Hessian H(θ) and the dataset gradient covariance C(θ) for negative log likelihood loss is described by the Gauss-Newton decomposition as follows,
H(θ) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂w 2 (2)
where H(θ) is the Hessian of the loss. The derivation can be found in section B of the appendix.
To continue with our argument, we note that it has been discussed by Shwartz-Ziv & Tishby (2017) that early on during training, the mean gradient over the training set is larger in magnitude compared to the variance in gradients. The above argument essentially says that the scale of mean gradient g(θ) is larger compared with the scale of C(θ). Ignoring the second order term in the Gauss-Newton decomposition above, we see that the mean gradient must be aligned with the top eigenvectors of the Hessian since the scale of gradient covariance is much smaller early on during training. Finally, we note that using large batch-sizes makes the mini-batch gradient closer to the mean gradient by reducing the scale of mini-batch gradient covariance as shown by Hoffer et al. (2017),
cov(g B (θ), g B (θ)) = 1 B − 1 N C(θ)(3)
The two arguments together imply that gradients from larger batch-sizes are likely to be more aligned with the high curvature directions of the loss surface especially early on during training.
In convex optimization theory, when gradients point along the top eigenvectors of the Hessian (also referred to as the sharp directions of the loss surface), optimization exhibits under-damped convergence, meaning it oscillates along the sharp directions in the case when the learning rate is smaller than a certain threshold. Applying this idea to non-convex loss landscapes, a large alignment between the mini-batch gradient and the sharp directions should also lead to oscillations. At this point, we depart from the conclusions of the convex setting and recall our observation in the previous section that the interpolation between consecutive iterations has a quadratic like shape and SGD moves over barriers for the deep network loss surface. We thus hypothesize that a lower alignment between mini-batch gradients and the sharp directions of the loss surface makes SGD exploration faster by exhibiting less oscillation, and vice-versa.

[EMPIRICAL VERIFICATION]
Based on the theoretical analysis above, we first conduct experiments to empirically verify that the alignment of mini-batch gradient g B (θ) and hessian H(θ) increases when we increase mini-batch size. To do so, we calculate the alignment of mini-batch gradient g B (θ) and hessian H(θ) as Figure 6: Plots for the alignments between mini-batch gradient and hessian with Resnet-56 architecture trained using SGD on CIFAR-100 at the end of Epoch 5 and Epoch 10. All the descriptions are same as described in figure 5.
g T B (θ)H(θ)g B (θ) g B (θ) 2 2 .(4)
Figure 5 and figure 6 show the alignments calculated according to Equation 4 on both VGG-11 with CIFAR-10 and Resnet-56 with CIFAR-100 separately at the end of Epoch 5 and Epoch 10. We calculate the alignment for mini-batch size 100, 1000, 10000 and 45000 (which is the dataset size).
For every mini-batch size, we sample 50 different batches and calculate the alignment of the current mini-batch gradient with the hessian and show both the mean and standard deviation of alignments in the plots. From both figure 5 and figure 6, we can see that the alignment between mini-batch gradient and hessian is larger for larger mini-batch size.
Based on the empirical verification between mini-batch gradient and sharp directions above, we now verify our argument whether it leads SGD to oscillate in the proximity of the parameter initialization, thus slowing down exploration. Note the latter has been shown by Hoffer et al. (2017). Therefore, to substantiate our claim, we show the degree of oscillation in SGD increases with large batchsize. Specifically, while training deep networks, we keep track of the cosine of the angle between mini-batch gradients from every two consecutive SGD iterations,
cos(g t−1 , g t ) := g T t−1 g t ( g t−1 2 g t 2 )
.
(5)
Figure 7 shows the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100. Experiments are run with the same learning rate for batch size 500, 5000 and 45000 (dataset size). We can see from the plot that the cosine of the angle between mini-batch gradients from two consecutive iterations remains smaller for larger batch sizes, which indicates that the gradients from two consecutive iterations point more in opposite directions for larger batch sizes. Together with the parameter distance results from Hoffer et al. (2017) that shows that within the same number of iterations, the parameter norm for larger batch sizes is smaller, our experiment verifies that for larger batch sizes, SGD oscillates more in the proximity of the parameter initialization instead of exploring farther away regions. 

[BACKGROUND AND RELATED WORK]
There have been previous work on visualizing the loss surface although from different motivations.
Perhaps Goodfellow et al. ( 2014) is most similar to our work since we use the loss interpolation tool suggested in their paper to perform our analysis. They perform interpolation between the initial and final parameters and based on their finding, draw the conclusion that the loss along the line connecting these two points does not have any barriers. We note that we use their tool for a different purpose and our conclusions are fundamentally different from theirs because we use the observations to investigate whether SGD crosses barriers during optimization over deep networks' loss landscape. Li et al. (2017b) also visualize the loss landscape of different network architectures.
Our work is closely related to a number of recent papers that study SGD as a diffusion process because we present a complementary qualitative view to an aspect of their theory. Hoffer et al. (2017) hypothesize this view based on the evidence that the parameter distance moved by SGD from initialization as a function of the number of iterations resembles a diffusion process. Li et al. (2017a) hypothesize this behavior of SGD and theoretically show that this diffusion process would allow SGD to escape sharp local minima. The authors use this theoretical result to support the findings of Keskar et al. (2016) who find that SGD with small mini-batch size find wider minima. Kushner & Yin (2003); Mandt et al. (2017); Chaudhari & Soatto (2017); Smith & Le (2017); Jastrzebski et al. (2017); Li et al. (2015) study SGD as a discrete approximation of stochastic differential equation under the assumption of a reasonably small learning rate and batch-size (compared with dataset size). Broadly, these papers show that the stochastic fluctuation in the stochastic differential equation simulated by SGD is governed by the ratio of learning rate to batch size. In this paper we study the qualitative roles of batch size without any assumption on how large it is with respect to dataset size.
We note that Zhu et al. (2018) present an analysis of how the structure of gradient covariance matrix can help SGD escape sharp minima more efficiently. Specifically, they show that when the top eigenvectors of the gradient covariance and Hessian are aligned, the escaping efficiency of SGD out of sharp minima is best. We find our view of exploration of different regions by SGD on the other side of the spectrum. While they compare the alignment between the gradient covariance (noise) and Hessian, we talk about the alignment between the mean gradient and the Hessian, which may be seen as complementary views.
There is a long list of work towards understanding the loss surface geometry of DNNs from a theoretical standpoint which is similar in spirit to our analysis of loss surface. Dotsenko (1995); Amit et al. (1985); Choromanska et al. (2015) show that under certain assumptions, the DNN loss landscape is similar to the spherical spin glass model which is well studied in terms of its critical points. Safran & Shamir (2016) show that under certain mild assumptions, the initialization is likely to be such that there exists a continuous monotonically decreasing path from the initial point to the global minimum. Freeman & Bruna (2016) theoretically show that for DNNs with rectified linear units (ReLU), the level sets of the loss surface become more connected as network over-parametrization increases. This has also been justified by Sagun et al. (2017) who show that the Hessian of deep ReLU networks is degenerate when the network is over-parametrized and hence the loss surface is flat along such degenerate directions. Broadly these studies analyze DNN loss surfaces (either theoretically or empirically) in isolation from the optimization dynamics.
In our work we do not study the loss surface in isolation, but rather analyze it through the lens of SGD. In other words, we study the DNN loss surface along the trajectory of SGD, based on which we make deductions about how SGD explores the different regions of the loss surface and the effect of batch-size on this aspect.

[DISCUSSION AND CONCLUSION]
Through qualitative results that showed how SGD interacts with the DNN loss surface, we showed evidence that SGD rarely crosses barriers during training. We presented an alternate mechanism that SGD uses to explore different regions of the deep network loss landscape.
We draw similarities between the optimization trajectory in DNNs that we have empirically found, with those in quadratic loss optimization (see section 5 of LeCun et al. (1998)). Based on our empirical evidence, we found that the loss interpolation between parameters from consecutive updates is a quadratic-like shape. This is reminiscent of optimization in a quadratic loss setting with a non-isotropic positive semi-definite Hessian, where the optimal learning rate η causes underdamping without divergence along eigenvectors of the Hessian which have eigenvalues λ i such that λ −1 i < η < 2λ −1 i . In the second part of our analysis, we investigated the role of batch-size in exploration, for different regions during SGD optimization of a DNN loss surface. We presented an argument showing mini-batch gradients from larger batch-sizes should align more with the high curvature directions of the loss surface, especially early during training when the scale of mean gradients dominates over gradient covariance. Additionally, we present a complementary view of the exploration aspect of SGD that stems from its diffusion perspective, and show that the alignment of the mini-batch gradient with the sharp directions of the Hessian leads to oscillations preventing SGD from exploring regions far from the initialized parameters.
Finally, much of what we have discussed is based on the loss landscape of specific datasets and architectures along with network parameterization choices like rectified linear activation units (Re-LUs) and batch normalization Ioffe & Szegedy (2015). These conclusions may differ depending on these choices. In these cases analysis similar to ours can be performed to see if similar dynamics hold or not. Studying these dynamics may provide more practical guidelines for setting optimization hyperparameters.

[APPENDIX]
A OPTIMIZATION TRAJECTORY This is a continuation of section 3.1 in the main text. Here we show further experiments on other datasets, architectures and hyper-parameter settings. The analysis of GD training for Resnet-56 on CIFAR-10, MLP on MNIST and VGG-11 on tiny ImageNet are shown in figures 8, 16 and 19 respectively. Similarly, the analysis of SGD training for Resnet-56 on CIFAR-10 dataset with batch size of 100 and learning rate 0.1 for epochs 1, 2, 25 and 100 are shown in figures 9, 10, 11 and 12 respectively. The analysis of SGD training for VGG-11 on CIFAR-10 with the batch size of 100 and learning rate 0.1 on epochs 2, 25,100 are shown in figures 13, 14 and 15. The analysis of SGD training for MLP on MNIST for epochs 1 and 2 are shown in figures 17 and 18. The analysis of SGD training for VGG-11 on tiny ImageNet for epochs 1 is shown in figure 20. We also conducted the same experiment and analysis on various batch sizes and learning rates for every architecture. Results of VGG-11 can be found in figures 21, 22, 23 and 24. Results of Resnet-56 can be found in figures 25, 26, 27 and 28. The observations and rules we discovered and described in section 3 are all consistent for all these experiments. Specifically, for the interpolation of SGD for VGG-11 on tiny ImageNet, the valley-like trajectory is weird-looking but even so, according to our quantitative evaluation there is no barrier between any two consecutive iterations.

[B IMPORTANCE OF SGD NOISE STRUCTURE]
Here we derive in detail the relation between the Hessian and gradient covariance for the negative log likelihood loss L i (θ) = − log(p i (θ)). Note we use the fact that for this particular loss function,
∂Li(θ) ∂pi(θ) = − 1 pi(θ) , and ∂ 2 Li(θ) ∂pi(θ) 2 = 1 p 2 i (θ) , which yields ∂ 2 Li(θ) ∂pi(θ) 2 = ∂Li(θ) ∂pi(θ) 2 . H(θ) = 1 N N i=1 ∂ 2 L i (θ) ∂θ 2 (6) = 1 N N i=1 ∂ ∂θ ∂L i (θ) ∂p i (θ) • ∂p i (θ) ∂θ (7) = 1 N N i=1 ∂ 2 L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (8) = 1 N N i=1 ∂L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (9) = 1 N N i=1 ∂L i (θ) ∂θ ∂L i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (10) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (11
)
whereḡ(θ) = 1 N N i=1 ∂Li(θ)
∂θ .

[C DISCUSSION]
In the main text, we talk about converge in the quadratic setting depending on the value of learning rate relative to the largest eigenvalue of the Hessian. The convergence in this setting has been visualized in ??.              Figure 22: Plots for VGG-11 Epoch 1 trained using learning rate 0.2 batch size 100 on CIFAR-10.
Figure 23: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 500 on CIFAR-10.
Figure 24: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 1000 on CIFAR-10.
Figure 25: Plots for Resnet-56 Epoch 1 trained using learning rate 0.7 batch size 100 on CIFAR-10.
Figure 26: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 100 on CIFAR-10.
Figure 27: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 500 on CIFAR-10.
Figure 28: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 1000 on CIFAR-10.","[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.","{
    ""1"": {
        ""summary"": ""The review expresses concerns about the novelty and substantial contribution of the paper, specifically the limited exploration of important factors affecting SGD behavior, and the unclear motivations behind batch size choices."",
        ""verbatim"": ""The connection between SGD and diffusion has been pointed out in previous papers, as acknowledged by the Authors. The study of the effect of batch size is interesting, but again somewhat derived from previous works. At the moment these important factors are overlooked. It is not clear to me why we would want to avoid larger batch sizes.""
    },
    ""2"": {
        ""summary"": ""The reviewer questions the strength of the submission's individual parts and their connection, suggesting that the findings are neither surprising nor sufficiently connected to justify a publication at ICLR. The reviewer also points out lack of references and theoretical support."",
        ""verbatim"": ""Both parts are, in themselves, not strong enough to warrant publication at ICLR, and the connection between them is rather weak. Especially the second insight should be explored more thoroughly.""
    },
    ""3"": {
        ""summary"": ""This review criticizes the paper for insufficient coverage and convincing evidence regarding how SGD crosses barriers, missing discussions on key concepts, and poorly supported claims, suggesting a lack of depth in understanding of related works."",
        ""verbatim"": ""The paper falls short of providing a convincing evidence (be it theoretical or empirical), and the way it tries to frame itself unique and different in relation to related works only indicate a lack of deep understanding of the existing literature.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The paper lacks quantitative validation and relies heavily on qualitative analysis."",
        ""verbatim"": ""The paper heavily focuses on qualitative analysis and visualizations, which may require more quantitative validation or comparisons with existing methods to strengthen the conclusions.""
    },
    ""2"": {
        ""summary"": ""Concerns about the generalizability of findings across diverse datasets and architectures."",
        ""verbatim"": ""There could be concerns about the generalizability of the findings across various datasets and architectures, as the paper primarily discusses results for specific scenarios.""
    },
    ""3"": {
        ""summary"": ""Lack of detailed discussion on experimental limitations and biases."",
        ""verbatim"": ""The paper lacks a detailed discussion on the limitations and potential biases in the experimental setup, such as the choice of hyperparameters or network architectures, which could impact the interpretation of the results.""
    },
    ""4"": {
        ""summary"": ""Need for further theoretical support and comparative analysis."",
        ""verbatim"": ""The theoretical arguments presented, while insightful, could be further substantiated with additional theoretical derivations or comparative analyses to solidify the claims made in the paper.""
    }
}",4,"{
    ""A2-B4"": {
        ""rationale"": ""Both points criticize the theoretical underpinning of the paper. Review A points out the lack of reference and theoretical support as a major weakness, suggesting the need for strengthening these aspects. Review B similarly stresses the need for additional theoretical support to substantiate the claims made. Both are concerned about theoretical depth and seek more rigorous backing."",
        ""similarity"": ""7""
    },
    ""A3-B1"": {
        ""rationale"": ""Review A criticizes the paper for not providing sufficient convincing evidence, both theoretical or empirical. Review B is concerned with the paper's reliance on qualitative analysis and suggests a need for more quantitative validation. Both points highlight the paper's lack of solid, quantifiable backing for its claims and conclusions."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Lack of comprehensive comparison with existing work"",
        ""verbatim"": ""The paper may lack a comprehensive comparison with existing related work in the field, potentially weakening the novelty and originality of the findings.""
    },
    ""2"": {
        ""summary"": ""Methodology and experimental setup need more elaboration"",
        ""verbatim"": ""The methodology and experimental setup used could benefit from further elaboration to ensure reproducibility and robustness of the results.""
    },
    ""3"": {
        ""summary"": ""Insufficient theoretical depth"",
        ""verbatim"": ""While the findings are insightful, the paper might be lacking in theoretical depth in explaining the observed phenomena which could be critical for academic acceptance.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both points discuss the lack of novelty in the paper by pointing to previous works on similar topics, illustrating an inadequately demonstrated originality, which is a major consideration in both reviews."",
        ""similarity"": ""8""
    },
    ""A3-B3"": {
        ""rationale"": ""Review A criticizes the paper for insufficient evidence and poor understanding of related works, while Review B states it lacks theoretical depth in explaining phenomena. Both point to a foundational weakness in understanding and presenting theory which strongly overlaps."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks discussion on broader implications of the findings."",
        ""verbatim"": ""The paper may benefit from expanding on the broader implications of the findings beyond the specific experimental setup to enhance the significance of the research.""
    },
    ""2"": {
        ""summary"": ""There is a need for further exploration of different hyperparameters or variations in the SGD algorithm."",
        ""verbatim"": ""Further exploration of different hyperparameters or variations in the SGD algorithm could strengthen the analysis and provide a more comprehensive understanding of the exploration dynamics.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper's limited discussion on counterarguments might weaken its persuasiveness and scientific rigour."",
        ""verbatim"": ""'Limited Discussion on Counterarguments': The paper could be criticized for not sufficiently discussing alternative explanations or models that might also account for the observed phenomena.""
    },
    ""2"": {
        ""summary"": ""The paper's high technical density could limit its accessibility and impact on a broader audience."",
        ""verbatim"": ""'Technical Density and Accessibility': The high technical density of the paper might make it less accessible to a wider audience, potentially limiting its impact.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper lacks a comparative analysis with other optimization methods."",
        ""verbatim"": ""'Lack of Comparative Analysis': While the paper offers new insights into the behavior of SGD, it might be viewed as incomplete without a comparative analysis with other optimization methods. Demonstrating how SGD's behavior contrasts with other popular algorithms could enrich the paper's impact and relevance.""
    },
    ""2"": {
        ""summary"": ""The findings may not generalize across different architectures or conditions."",
        ""verbatim"": ""'Potential Generalization Issues': The conclusions drawn are heavily based on empirical observations which might not generalize across different architectures or conditions. A broader range of experiments across various networks could strengthen the findings.""
    }
}",2,"{
    ""A3-B2"": {
        ""rationale"": ""Both Review A3 and Review B2 express concerns about the validity and universality of the paper's findings under different conditions. Review A3 talks about the paper falling short in providing convincing evidence, which implies a lack of generalization or thorough proof across conditions, similar to B2 which directly mentions potential generalization issues with the conclusions being heavily reliant on empirical observations."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of theoretical grounding for empirical findings"",
        ""verbatim"": ""The findings, while empirically supported, lack a strong theoretical framework that explains the observed phenomena, which might be necessary to fully convince the scientific community of the robustness of these results.""
    },
    ""2"": {
        ""summary"": ""Concerns about the generalizability of the results"",
        ""verbatim"": ""There might be concerns about how generalizable the results are across different architectures or deeper networks, which is not extensively discussed in the paper.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both Review A and Review B express significant concern about the theoretical support in the paper. Review A states that the individual parts of the paper do not have strong connections and lack adequate theoretical support, which affects publication viability, while Review B mentions directly that the findings lack a strong theoretical framework."",
        ""similarity"": ""8""
    },
    ""A3-B1"": {
        ""rationale"": ""Review A criticizes the paper for not providing convincing evidence, either theoretical or empirical, and for lacking depth in understanding related works. Review B similarly points out the lack of a strong theoretical framework that explains the observed phenomena. Both reviews emphasize the lack of theoretical grounding and convincing arguments."",
        ""similarity"": ""7""
    }
}",2,"[Significance and novelty]

This paper investigates an important and relevant research question in the field of deep learning: the behavior of stochastic gradient descent (SGD) in the loss landscape of deep neural networks (DNN). The authors provide novel insights into the interaction between the non-convex loss surface geometry and the optimization algorithm, shedding light on the exploration dynamics of SGD in DNN optimization. The study relies on both qualitative and quantitative results, bringing a new perspective to our understanding of deep learning optimization. Additionally, the authors present a potential explanation for why using larger batch sizes can lead to slower exploration and deeper understanding of DNN optimization.

[Potential reasons for acceptance]
1. The paper presents an essential research question that addresses an open problem in deep learning optimization: understanding the relationship between DNN loss landscape geometry and SGD optimization.
2. The authors employ a combination of qualitative and quantitative experiments to explore mini-batch SGD behavior.
3. The study reveals a novel finding on exploration dynamics, stating that mini-batch SGD rarely crosses loss barriers due to the mini-batch noise helping to explore different loss regions.

[Potential reasons for rejection]
1. The paper lacks a comprehensive explanation on the theoretical background of the DNN loss surface and optimization algorithms like SGD.
2. The authors do not directly compare the results from the study with other optimization techniques such as Adam or RMSprop to establish the uniqueness of mini-batch SGD.
3. The authors do not discuss the potential impact of their findings on other optimization algorithms.

[Suggestions for improvement]
1. Clearly explain the theoretical background of the DNN loss landscape and optimization methods like SGD, Adam, and RMSprop in the beginning.
2. Provide a comparison of the novel findings with other state-of-the-art optimization methods.
3. Consider studying the impact of mini-batch size and how it influences the behavior of other optimization algorithms.
4. Investigate the practical implications and potential applications to improve deep learning optimization algorithms.

The suggested improvements aim to enhance the quality and overall impact of the paper by acknowledging the state-of-the-art in theoretical background and comparing the findings with other optimization techniques, including the impact on practical applications.","[Significance and novelty]
<Contribution novelty> The reviewers believe that the contributions are only marginally significant or novel, as they find that the paper's contributions are neither significant nor new.
<Empirical novelty and significance> The reviewers express that the contributions are only marginally significant or novel in both empirical and technical novelty. They argue that the core insight presented in the paper is trivial and can only be observed in small and shallow networks, without providing enough insights into real deep networks. The empirical findings are considered to be mostly heuristic and not providing new insight, but slightly novel findings that provide new corroboration of existing knowledge.

[Potential reasons for acceptance]
<Solid work> The reviewers found the paper to present a solid study that was well-executed and based on correct insight. They acknowledge that the paper was well-written and the findings were supported by experiments.
<Contributions> The core insights and experiments on the study of stochastic gradient descent (SGD) were found to be correct, well-supported, and correct.

[Potential reasons for rejection]
<Lack of novelty> The authors are criticized for providing neither significant nor new insights into the explored problem and lack of novelty in the findings.
<Small and shallow networks> The empirical evaluation mostly relied on small and shallow networks, and the authors failed to present enough insights into the qualitative properties of real deep networks.
<Heuristic findings> The empirical findings were considered to be mostly heuristic and not providing new insight, making only minor contributions in both empirical and technical novelty.

[Suggestions for improvement]
<Deep networks> Authors should focus on demonstrating the qualitative properties of deep networks to make the findings more substantial and relevant.
<Address limitations> Clarity on limitations, qualitative properties of deep networks, and novelty and significance of findings is advised for improvement.

","{
    ""1"": {
        ""summary"": ""Lack of comprehensive theoretical background explanation on DNN loss surface and SGD."",
        ""verbatim"": ""The paper lacks a comprehensive explanation on the theoretical background of the DNN loss surface and optimization algorithms like SGD.""
    },
    ""2"": {
        ""summary"": ""No direct comparison with other optimization techniques like Adam or RMSprop."",
        ""verbatim"": ""The authors do not directly compare the results from the study with other optimization techniques such as Adam or RMSprop to establish the uniqueness of mini-batch SGD.""
    },
    ""3"": {
        ""summary"": ""Lack of discussion on the impact of findings on other optimization algorithms."",
        ""verbatim"": ""The authors do not discuss the potential impact of their findings on other optimization algorithms.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Lack of significant or novel contributions"",
        ""verbatim"": ""The reviewers believe that the contributions are only marginally significant or novel, as they find that the paper's contributions are neither significant nor new.""
    },
    ""2"": {
        ""summary"": ""Trivial core insights in small, shallow networks"",
        ""verbatim"": ""The core insight presented in the paper is trivial and can only be observed in small and shallow networks, without providing enough insights into real deep networks.""
    },
    ""3"": {
        ""summary"": ""Heuristic empirical findings lacking new insights"",
        ""verbatim"": ""The empirical findings are considered to be mostly heuristic and not providing new insight, but slightly novel findings that provide new corroboration of existing knowledge.""
    },
    ""4"": {
        ""summary"": ""Evaluation relying on small and shallow networks"",
        ""verbatim"": ""The empirical evaluation mostly relied on small and shallow networks, and the authors failed to present enough insights into the qualitative properties of real deep networks.""
    }
}",4,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A1 and Review B1 comment on issues relating to the theoretical exploration of SGD. Review A points out the limited exploration of factors affecting SGD behavior including unclear motivations behind batch size choices, which can be interpreted as a gap in presenting a comprehensive theoretical background. Similarly, Review B1 explicitly mentions the lack of comprehensive explanation on the theoretical background of the DNN loss surface and optimization algorithms like SGD. These points reflect a shared concern over the depth and clarity of the theoretical underpinnings of the work presented."",
        ""similarity"": ""7""
    },
    ""A3-B1"": {
        ""rationale"": ""Review A3 criticizes the paper for insufficient coverage and lack of convincing evidence in how SGD functions, implying a weak theoretical foundation especially in relation to existing literature. Review B1, similarly, points out a deficiency in providing a comprehensive theoretical background on related topics. Both reviews substantially focus on the inadequacy of theoretical explanations which underlie the paper's approach and findings, though A is more focused on how this weakness impacts evidence and claims, and B focuses strictly on the description."",
        ""similarity"": ""7""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A1 and Review B1 express concerns regarding the novelty and substantial contribution of the paper. Review A1 criticizes the limited exploration and unclear motivations in the paper's approach to SGD behavior, while Review B1 notes that the contributions are not significant or new."",
        ""similarity"": ""7""
    },
    ""A3-B3"": {
        ""rationale"": ""Review A3 and Review B3 both criticize the paper for its lack of in-depth evidence and novelty in findings. Review A3 points out the paper's shortcomings in providing convincing theoretical or empirical evidence and indicates a superficial treatment of the existing literature. Similarly, Review B3 addresses the heuristic nature of empirical findings lacking new insights."",
        ""similarity"": ""8""
    }
}",2
Neural representation and generation for RNA secondary structures,snOgiCYZgJ7.json,"[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Hierarchical encoding. Panel (A)shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
Table 2: (A), to determine if the decoder should expand to a new tree node or backtrack to its parent node, based on MLP topo (hĜ t,i ); • tree node prediction in Figure 2 (B), on condition that a new tree node is needed due to a possible topological expansion. This procedure determines the label of the new tree node from the set of {S, H, I, M }, based on MLP node (hĜ t,i ); • nucleotide segment decoding in Figure 2 (C), using a single-layered LSTM, whose initial hidden state is MLP dec
Table 3: Figure 3 :3Figure 3: RNAs generated with structural constraints from HierVAE on a random axis in the latent space (step size: 1e-4), for short (top), medium-length (middle), and long (bottom) RNAs. The Free energy (FE) and its deviation (DEV) from the MFE are given for each structure. which means only the canonical base-pairs and Wobble base-pairs are allowed, i.e. [A-U], [G-C] and [G-U];(2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3; (3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
Table 4: Figure S1 :S1FigureS1: A nested RNA secondary structure can be represented by: (A) dotbracket annotation, where base-pairs corresponding to matching parentheses, or (B) a molecular planar graph with two types of edges, corresponding to consecutive nucleotides (backbone) and basepairing interactions, or (C) a junction tree where node are labeled as stems (S), hairpins (H), internal loops (I), or multiloops (M), and edges correspond to the connections between these elements. All three forms are equivalent.
Table 5: Figure S2: This figure contains information of the unlabeled RNA dataset. (A) The number of hypernodes appears to grow linearly with the length of RNA, and (B) the junction tree height also grows as the length increases but on a more moderate scale. (C) and (D) have shown bar-plots of the number of hypernodes and tree height, indicating that the junction tree of RNA can take on significant depth hence contributing to the diversity and complexity of RNA secondary structures represented in this dataset.
Table 6: Figure S4 :S4Figure S4: Neighborhood visualization of tRNA-Cys 6 which is marked by the red bounding box in the center and the walk in the latent space takes place on two random orthogonal axes. Note that actual secondary structure of tRNA-Cys plotted in the figure is different compared to the one deposited online due to the prediction of RNAfold.
Table 7: We evaluate RNAs sampled from the posterior distribution: q(z|x), with a held-out test set of 20,000 RNAs. Each molecule is encoded and decoded 5 times. We also evaluate samples from the prior distribution: N (0, I) subject to the transformation of a latent CNF, where we sample 10,000 encodings and each encoding is decoded 10 times. Normed refers to length normalized FE DEV.
Table 8: Training semi-supervised HierVAE on labeled RNAcompete-S dataset. A test split is used to evaluate the accuracy of embedding classifiers and RNAs decoded from the posterior distribution under two settings: constrained and stochastic (C& S), unconstrained and deterministic (NC&D). RECON ACC refers to reconstruction accuracy which measures the percentage of RNA molecules decoded exactly as the input.
Table 9: 
Table 10: Algorithm 1: DFS decode RNA secondary structure1 Given: z T , z G , M TI, M SI a 2 Initialize: stack ← [ ] 3 function decode(z T , z G ) 4 root ← sample(MLP node (z T )) ;
Table 11: Hyperparameters for training VAE and full classifier models. Note that hidden units refer to the dimensionality of encoders and decoders from LSTMVAE, GraphVAE as well as HierVAE models. Dropout is applied to the embedding MLP classifier in case of training semi-supervised VAEs, which contains one hidden layer. Performance of simple MLP classifiers on top of fixed latent embeddings from VAE models, which have been pretrained on the unlabeled RNA dataset as originally shown in Table1.

[INTRODUCTION]
There is an increasing interest in developing deep generative models for biochemical data, especially in the context of generating drug-like molecules. Learning generative models of biochemical molecules can facilitate the development and discovery of novel treatments for various diseases, reducing the lead time for discovering promising new therapies and potentially translating in reduced costs for drug development (Stokes et al., 2020). Indeed, the study of generative models for molecules has become a rich and active subfield within machine learning, with standard benchmarks (Sterling & Irwin, 2015), a set of well-known baseline approaches (Gómez-Bombarelli et al., 2018;Kusner et al., 2017;Jin et al., 2018), and high-profile cases of real-world impact 1 .
Prior work in this space has focused primarily on the generation of small molecules (with less than 100 atoms), leaving the development of generative models for larger and more complicated biologics and biosimilar drugs (e.g., RNA and protein peptides) an open area for research. Developing generative models for larger biochemicals is critical in order to expand the frontiers of automated treatment design. More generally, developing effective representation learning for such complex biochemicals will allow machine learning systems to integrate knowledge and interactions involving these biologically-rich structures.
In this work, we take a first step towards the development of deep generative models for complex biomolecules, focusing on the representation and generation of RNA structures. RNA plays a crucial role in protein transcription and various regulatory processes within cells which can be influenced by its structure (Crick, 1970;Stefl et al., 2005), and RNA-based therapies are an increasingly active area of research (Pardi et al., 2018;Schlake et al., 2012), making it a natural focus for the development of deep generative models. The key challenge in generating RNA molecules-compared to the generation of small molecules-is that RNA involves a hierarchical, multi-scale structure, including a primary sequential structure based on the sequence of nucleic acids as well as more complex secondary and tertiary structures based on the way that the RNA strand folds onto itself. An effective generative model for RNA must be able to generate sequences that give rise to these more complex emergent structures.
There have been prior works on optimizing or designing RNA sequences-using reinforcement learning or blackbox optimization-to generate particular RNA secondary structures (Runge et al., 2019;Churkin et al., 2017). However, these prior works generally focus on optimizing sequences to conform to a specific secondary structure. In contrast, our goal is to define a generative model, which can facilitate the sampling and generation of diverse RNA molecules with meaningful secondary structures, while also providing a novel avenue for targeted RNA design via search over a tractable latent space.
Key contributions. We propose a series of benchmark tasks and deep generative models for the task of RNA generation, with the goal of facilitating future work on this important and challenging problem. We propose three interrelated benchmark tasks for RNA representation and generation:
1. Unsupervised generation: Generating stable, valid, and diverse RNAs that exhibit complex secondary structures. 2. Semi-supervised learning: Learning latent representations of RNA structure that correlate with known RNA functional properties. 3. Targeted generation: Generating RNAs that exhibit particular functional properties.
These three tasks build upon each other, with the first task only requiring the generation of stable and valid molecules, while the latter two tasks involve representing and generating RNAs that exhibit particular properties. In addition to proposing these novel benchmarks for the field, we introduce and evaluate three generative models for RNA. All three models build upon variational autoencoders (VAEs) (Kingma & Welling, 2014) augmented with normalizing flows (Rezende & Mohamed, 2015;Kingma et al., 2016), and they differ in how they represent the RNA structure. To help readers better understand RNA structures and properties, a self-contained explanation is provided in appendix B.
The simplest model (termed LSTMVAE) learns using a string-based representation of RNA structure. The second model (termed GraphVAE) leverages a graph-based representation and graph neural network (GNN) encoder approach (Gilmer et al., 2017). Finally, the most sophisticated model (termed HierVAE) introduces and leverages a novel hierarchical decomposition of the RNA structure. Extensive experiments on our newly proposed benchmarks highlight how the hierarchical approach allows more effective representation and generation of complex RNA structures, while also highlighting important challenges for future work in the area.

[TASK DESCRIPTION]
Given a dataset of RNA molecules, i.e. sequences of nucleotides and corresponding secondary structures, our goals are to: (a) learn to generate structurally stable, diverse, and valid RNA molecules that reflect the distribution in this training dataset; (b) learn latent representations that reflect the functional properties of RNA. A key factor in both these representation and generation processes is that we seek to jointly represent and generate both the primary sequence structure as well as the secondary structure conformation. Together, these two goals lay the foundations for generating novel RNAs that satisfy certain functional properties. To meet these goals, we create two types of benchmark datasets, each one focusing on one aspect of the above mentioned goals:
Unlabeled and variable-length RNA. The first dataset contains unlabeled RNA with moderate and highly-variable length (32-512 nts), obtained from the human transcriptome (Aken et al., 2016) and through which we focus on the generation aspect of structured RNA and evaluate the validity, stability and diversity of generated RNA molecules. In particular, our goal with this dataset is to jointly generate RNA sequences and secondary structures that are biochemically feasible (i.e., valid), have low free energy (i.e., stable), and are distinct from the training data (i.e., diverse). We will give an extended assessment of the generation aspect under different circumstances, e.g., when constraining the generation procedures with explicit rules.
Labeled RNA. The second dataset is pulled and processed from a previous study on in vitro RNAprotein interaction, which features labeled RNAs with shorter and uniform length (40 nts) (Cook et al., 2017). With this dataset, our objective is slightly expanded (to include obj. a), so that the latent space is adequately organized and reflective of the interaction with proteins. Therefore, key assessment for the latent space includes AUROC for the classification of protein binding, which is crucial for the design of desired novel RNA molecules.
Essentially, this creates slight variations in the task formulation, with the first dataset suited to unsupervised learning of a generative model, while the second datasets involves additional supervision (e.g., for a semi-supervised model or targeted generation). Our specific modeling choices, to be introduced in section 3, are invariant to different task formulations, and flexible enough to handle different representations of RNA secondary structures. We refer readers to appendix C for detailed explanation for the dataset and evaluation metrics on the generated molecules and latent embeddings.

[METHODS]
In this section, we introduce three different generative models for RNA. All three models are based upon the variational autoencoder (VAE) framework, involving three key components:
1. A probabilistic encoder network q φ (z|x), which generates a distribution over latent states given an input representation of an RNA. We experiment with three different types of input encodings for RNA sequence and secondary structures (see Figure S1: a dot-bracket annotated string, a graph with adjacency matrix representing base-pairings, and a graph augmented with a hierarchical junction tree annotation for the secondary structure.
2. A probabilistic decoder network p θ (x|z), which defines a joint distribution over RNA sequences and secondary structures, conditioned on a latent input. As with the encoder network, we design architectures based on a linearized string decoding and a graph-based hierarchical junction-tree decoding approach.
3. A parameterized prior p ψ (z), which defines a prior distribution over latent states and is learned based on a continuous normalizing flow (CNF) (Chen et al., 2018). shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
For all the approaches we propose, the model is optimized via stochastic gradient descent to minimize the evidence lower bound (ELBO):
L = −E q φ (z|x) [p θ (x|z)] + β KL(q φ (z|x)|p ψ (z))
where β is a term to allow KL-annealing over the strength of the prior regularization.
In the following sections, we explain our three different instantiations of the encoder (section 3.1), decoder (section 3.2), as well as our procedures to structurally constrain the decoding process using domain knowledge (section 3.3) and our procedures to avoid posterior collapse (section 3.4).

[ENCODING RNA SECONDARY STRUCTURES]
The input to the encoder is a structured RNA molecule, with its sequence given by an ordered array of nucleotides x 1 . . . x L , with x i ∈ {A, C, G, U }, where L is the length of the sequence, and its secondary structure, either represented as (1) a dot-bracket string S =ẋ 1 . . .ẋ L withẋ i ∈ {., (, )};
(2) or as a graph G with two types of edges -covalent bonds along the RNA backbone, and hydrogen bonds between the base-pairs 2 . We use x uv to denote edge features between nucleotides u and v;
(3) or as a hypergraph T -a depth-first ordered array of subgraphsĜ 1 . . .Ĝ D with L(Ĝ i ) ∈ {S, H, I, M } indicating the subgraph label, and I(Ĝ i ) = {j|j ∈ {1 . . . L}} indicating the assignment of nucleotides to each subgraph.
Encoding RNA secondary structure as sequence. First, we obtain a joint encoding over the nucleotide and the dot-bracket annotation, using the joint sequence-structure vocabulary {A, C, G, U } × {., (, )}. Then, these one-hot encodings are processed by a stacked bidirectional LSTM (Hochreiter & Schmidhuber, 1997), followed by a multi-head self-attention module (Vaswani et al., 2017) to weigh different positions along the RNA backbone. A global max-pooling is used to aggregate the information into h S , and then we obtain mean µ S and log variance log σ S from h S through linear transformations, and draw latent encoding z S from N (µ S , σ S ) using the reparameterization trick (Kingma & Welling, 2014).
Learning graph representation of RNA secondary structure. To encode the graph view G of an RNA secondary structure, we pass rounds of neural messages along the RNA structure, which falls into the framework of Message Passing Neural Network (MPNN) as originally discussed in Gilmer et al. (2017) and similarly motivated by Jin et al. (2018).
For much longer RNAs, it is conceptually beneficial to pass more rounds of messages so that a nucleotide may receive information on its broader structural context. However, this may introduce undesired effects such as training instability and over-smoothing issues. Therefor , we combine our MPNN network with gating mechanism, which is collectively referred as the G-MPNN:
v t−1 uv = σ(W g local [x u || x uv ] + W g msg w∈N (u) v t−1 wu ) (1) v t uv = GRU(v t−1 uv , v t−1 uv )(2)
where [. . . || . . . ] denotes concatenation, σ denotes the activation function and GRU indicates the gated recurrent unit (Cho et al., 2014). Then, after T iterations of message passing, the final nucleotide level embedding is given by:
h u = σ(W g emb [x u || v∈N (u) v T vu ]
). Before pooling the nucleotide level embeddings into the graph level, we pass h 1 . . . h L through a single bidirectional LSTM layer, obtainingĥ 1 . . .ĥ L at each step, and h g = max({ĥ i |i ∈ 1...L}). The latent encoding z G is similarly obtained from h G using the reparameterization trick.
Hierarchical encoding of the RNA hypergraph. To encode the junction tree T of RNA, we employ a type of GRU specifically suited to tree-like structures, which has previously been applied in works such as GGNN (Li et al., 2016) and JTVAE (Jin et al., 2018). We refer to this tree encoding network as T-GRU, and the format of its input is shown in Figure 1.
One major distinction between our RNA junction tree and the one used for chemical compounds (Jin et al., 2018) is that an RNA subgraph assumes more variable nucleotide composition such that it is impossible to enumerate based on the observed data. Therefore, we need to dynamically compute the features for each node in an RNA junction tree based on its contained nucleotides, in a hierarchical manner to leverage the nucleotide level embeddings learnt by G-MPNN.
Considering a subgraphĜ i in the junction tree T , we initialize its node feature with:
xĜ i = [L(Ĝ i ) || max u∈I(Ĝi) h u ].
Notably, max u∈Ĝi h u is a max-pooling over all nucleotides assigned toĜ i , and nucleotide embedding h u comes from G-MPNN. To compute and pass neural messages between adjacent subgraphs in the RNA junction tree T , we use the T-GRU network in Eq.3
v tĜ i,Ĝj = T-GRU(xĜ i , {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}) (3) hĜ i = σ(W t emb [xĜ i || Ĝ ∈N (Ĝi) hĜ]) (4)
with details of T-GRU provided in the appendix D, and compute the embeddings for subgraphs with Eq. 4. Further, we obtain a depth-first traversal of the subgraph embeddings hĜ 1 . . . hĜ D which is also the order for hierarchical decoding to be discussed later. This ordered array of embeddings is processed by another bi-directional LSTM , and the final tree level representation h T is again given by the max-pooling over the bi-LSTM outputs. Likewise, latent encoding z T is obtained from h T .

[RNA MOLECULAR GENERATION]
Decoding linearized sequence and structure. In this setting, the decoder simply autoregressively decodes a token at each step, from the joint sequence-structure vocabulary mentioned before in section 3.1, plus one additional symbol to signal the end of decoding. To simplify the design choice, we use a single-layered forward-directional LSTM, and its hidden state is initialized with the latent encoding z, which can be either z S , z G or z T .
Figure 2: Hierarchical decoding of a structured RNA, involving three types of predictions, that are on the topological level, node level, and nucleotide level. These three types of prediction are interleaved into the procedures of decoding the junction tree structure of RNA and the nucleotide segments.
Hierarchically decoding hypergraph and nucleotide segments. The input to this more sophisticated hierarchical decoder are latent encodings z G which contains order and basic connectivity information of the nucleotides, and z T which contains higher order information about the arrangements of nucleotide branches and their interactions. We give a concise description of the decoding procedures here, along with a detailed algorithm in appendix E. On a high level, we hierarchically decode the tree structure in a depth-first manner, and autoregressively generate a nucleotide segment for each visited tree branch. For these purposes, we interleave three types of prediction (Figure 2).
Denote the current tree node at decode step t and at the i-th visit asĜ t,i , whose features include (1) its node label L(Ĝ t,i ) and, (2) a summary over the already existing i − 1 nucleotide segments max{h l,j u | u ∈Ĝ t,i and l < t and j < i}, with l denoting the nucleotide is decoded at step l, and j indicating the nucleotide belongs to the j-th branch (this feature is simply zeros when i = 1). Then, its local feature xĜ t,i is defined as the concatenation of (1) and (2).
We make use of a notion called node state: hĜ t,i , which is obtained by: hĜ t,i = T-GRU(xĜ t,i , {vĜ ,Ĝ t,i |Ĝ ∈ N (Ĝ t,i )}). Note its similarity to Eq. 3, and hĜ t,i is used to make:
• topological prediction in Figure 2 
([hĜ t,i || z T || z G ])
. The start token is the last nucleotide from the last segment.
Our hierarchical decoder starts off by predicting the label of the root node using z T , followed by topological prediction on the root node and decoding the first nucleotide segment. The algorithm terminates upon revisiting the root node, topologically predicted to backtrack and finishing the last segment of the root node. The decoded junction tree naturally represents an RNA secondary structure that can be easily transformed to the dot-bracket annotation, and the RNA sequence is simply recovered by connecting nucleotide segments along the depth-first traversal of the tree nodes.

[STRUCTURALLY CONSTRAINED DECODING]
To better regulate the decoding process so that generated RNAs have valid secondary structures, a set of constraints can be added to the decoding procedures at the inference stage. Essentially, a valid RNA secondary structure needs to observe the following rules: (1) base-pairing complementarity, (2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3;
(3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
We will translate the above rules into specific and applicable constraints, depending on specific decoders. For the sake of space, we only give a broad remark and leave more details in the appendix.
Linearized decoding constraints. Since the linearized decoder simply proceeds in an autoregressive fashion, constraints can be easily enforced in a way that at each step, a nucleotide with an appropriate structural annotation is sampled by making use of masks and re-normalizing the probabilities. Likewise, a stop token can only sampled when all opening nucleotides have been closed. More details to follow in appendix F.
Hierarchical decoding constraints. The specific set of constraints for hierarchical decoding is discussed in appendix G. Overall, considering the different natures of the three associated types of prediction, each one should require a set of different strategies, which are once again applicable by adding proper masks before sampling. As shown in the algorithm in appendix E, the set of constraints are applied to line 13, 24 and 14 with marked asterisk.

[AVOIDING POSTERIOR COLLAPSE]
As discussed in a line of previous works, VAEs with strong autoregressive decoders are susceptible to posterior collapse, an issue where the decoder simply ignores the latent encoding of the encoder (He et al., 2019). Therefore, to avoid posterior collapsing, we make use of a carefully chosen KL annealing schedule during training to help the encoder adapt its information content in the latent encoding and in coordination with the decoder. This schedule is detailed in section 4. We also learn a parameterized prior as suggested in Chen et al. (2017), but using a CNF instead, following a similar implementation to Yang et al. (2019), with details given in appendix H.
Our KL annealing schedule is chosen based on empirical observations, as to our knowledge, there has yet to exist any principled methods of selecting such schedule. We have used diagnostic metrics such as mutual information (He et al., 2019) and active units (Burda et al., 2016) along with a validation set to select a proper KL annealing schedule which is to be described later in section 4

[RESULTS]
We consider three modes of evaluation: (1) unsupervised RNA generation; (2) generation using semi-supervised VAE models and (3) targeted RNA design from an organized latent space. Results are presented below, and relevant hyperparameters can be found in Table S1.
Unsupervised RNA generation. Here, we evaluate generated RNAs from models trained on the unlabeled RNA dataset for 20 epochs using a KL annealing schedule including 5 epochs of warm-up, followed by gradually increasing the KL annealing term to 3e-3 (for LSTMVAE and GraphVAE), or 2e-3 (for HierVAE). The KL annealing schedule was chosen using a validation set of 1,280 RNAs.
Table 1 compares the generation capability of different models, from the posterior as well as the prior distribution, and in scenarios such as applying structural constraints to the decoding process or not.
It clearly shows that our most advanced model, HierVAE which employs a hierarchical view of the structure in its encoding/decoding aspects, achieves the best performance across different evaluation regimes, generating valid and stable RNAs even when the decoding processed is unconstrained. It is also observed that despite having structural constraints, the validity of our generated RNAs are always slightly below 100%. This can be explained by the threshold hyperparameter which sets the maximum number of steps for topological prediction as well as the maximal length of each nucleotide segment, as shown in Algorithm 1 in appendix E.
To further demonstrate the benefits of model training from structural constraints, we sample RNAs from the prior of an untrained HierVAE model. With structural constraints, the validity amounts to 66.34% with an extremely high free energy deviation of 22.613. Without structural constraints, the validity translates to a mere 9.37% and the model can only decode short single stranded RNAs as it lacks the knowledge of constructing more complex structures. This comparison illustrates that model training is essential for obtaining stable RNA folding.
The junction tree hierarchy of RNAs developed in our work shares certain modelling similarities with the probabilistic context free grammar (Dowell & Eddy, 2004) used by covariance models (CM) (Eddy & Durbin, 1994). Infernal (Nawrocki & Eddy, 2013) is one of the representative works based on CM, which is capable of sampling RNA secondary structures from a CM built around a consensus secondary structure for a conserved RNA family. However, due to the lack of homologous sequences in our dataset, Infernal is seriously limited and can only sample single stranded RNAs.
Figure 3 illustrate RNA structures generated using HierVAE from a randomly chosen short path through the latent space. Notably, latent encoding provided by HierVAE translates smoothly in the RNA structure domain: nearby points in the latent space result in highly similar, yet different, structures. The generated structures are particularly stable for short and medium-size RNAs, and slightly less so for longer RNAs with highly complex structures. A side-by-side comparison between generated RNA secondary structures and MFE structures in Figure S3 shows that generated structures can evolve smoothly in the latent space along with their corresponding MFE structures. We also visualize neighborhoods of a Cysteine-carrying transfer RNA and a 5S ribosomal RNA in figure S4 and S5.
Supervised RNA generation. We then evaluate our generative approaches in a semi-supervised setting using seven RBP binding data sets from RNAcompete-S. First, we compare the efficacy of different representational choices while excluding the generative components, i.e. we jointly train VAE encoders followed by simple MLP classifiers on top of the latent encodings for binary classification on RBP binding.  2. Since our strategy for targeted RNA design makes use of seed molecules in the latent space, we mainly sample RNAs from the posterior distribution of these semi-supervised VAE models. Therefore, we select a KL annealing schedule that tends to retain more information in the latent encodings, i.e. setting maximum β to 5e-4 and training 10 epochs.
Results are promising in that classification AUROC measured by the held-out test set is comparable to the fully supervised classification models in Table S3, and much better compared to models only using fixed and pretrained VAE embeddings as shown in Table S2. Also, RNA structures generated from the posterior distribution, even under the setting of unconstrained and deterministic decoding, have high success rates, very stable conformation and good reconstruction accuracy. Targeted RNA design. We next studied the task of designing RNAs with high RBP binding affinity. Starting from the latent encodings of 10,000 randomly chosen RNA molecules that have negative labels in each RNAcompete-S test set, and use activation maximization to gradually alter the latent encodings so that the predicted binding probability from the embedding classifiers increases. These embedding classifiers have been trained jointly with the VAE models with accuracy reported earlier (Table 2). Then, we use separately trained full classifiers (also earlier shown in Table S3) as proxy of oracles for evaluating the ""ground truth"" probability of RBP binding. Table 3, report the success rate (fraction of RNAs whose ""ground truth"" RBP binding probability was improved), along with the average improvement in binding probabilities. An example of a trajectory of optimized RNAs is shown in Fig. S6.

[RELATED WORK]
Over the years, the field of computational drug discovery has witnessed the emergence of graphcentric approaches. One of the earliest method, proposed in Gómez-Bombarelli et al. (2018), is defined on the linearized format of molecular structures and represents a family of methods that rely on sequential models to represent and generate SMILES strings of chemical compounds. Later methods have sought to construct more chemical priors into the model, via (1) leveraging graph based representation and generation techniques, (2) enforcing direct chemical constraints to the decoding process, (3) considering a multi-scale view of the molecular structures, or (4) using reinforcement learning to integrate more training signal of the molecular structure and function. As a result, greater success has been achieved by models such as Kusner et al. (2017); ; Jin et al. (2018); You et al. (2018) at generating and searching valid and more useful chemical compounds.
Graph representation learning is at the heart of these more recent approaches, to help understand the rules governing the formation of these molecular structures, as well as the correspondence between structures and functions. Duvenaud et al. (2015) were among the first to apply GNN to learn molecular fingerprints, and the general neural message passing framework for molecules is proposed in Gilmer et al. (2017), which demonstrate the power of MPNN across various molecular benchmarking tasks. These prior works on molecular MPNN, together with other GNN architectures developed in other areas, such as considering relational edges (Schlichtkrull et al., 2018) and attention (Velickovic et al., 2018), have laid the foundation for the success of these deep generative models.
Despite the fact that RNA molecules can adopt complex structures, dedicated graph representation learning techniques have been scarce, with some recent works beginning to leverage graph related learning techniques to predict RNA folding (Chen et al., 2020;Singh et al., 2019) and to represent RNA molecular structures (Yan et al., 2020;Oliver et al., 2020). Prior to our work, the design of RNA has mostly focused on the inverse design problem, which is to conditionally generate an RNA sequence whose MFE secondary structure corresponds to an input secondary structure. Therefore, the line of prior works have predominantly relied on sequential techniques, with some representative methods based on reinforcement learning (Runge et al., 2019), or more classically framed as a combinatorial optimization problem and solved with sampling based techniques (Churkin et al., 2017). These prior works are mainly concerned with querying from an energy model with fixed thermodynamic parameters and fixed dynamics of RNA folding, which is in itself limited compared to learning based approaches (Chen et al., 2020;Singh et al., 2019), and are unable to model a joint distribution over RNA sequences and possible folds.

[CONCLUSION AND FUTURE WORKS]
In this work we propose the first graph-based deep generative approach for jointly embedding and generating RNA sequence and structure, along with a series of benchmarking tasks. Our presented work has demonstrated impressive performance at generating diverse, valid and stable RNA secondary structures with useful properties.
For future works, there are several important directions to consider. First, it would be beneficial to obtain non-coding RNA families from the RFAM database (Kalvari et al., 2017) which would help our models learn more biologically-meaningful representation indicative of RNA homology and functions, in addition to the evolutionarily conserved RNA structural motifs that would enable the generation of more stable RNA secondary structures. In that context, a detailed comparison to Infernal and other probabilistic context-free grammar models would be meaningful.
On the methodological aspect, in light of the recent advances in protein sequences pretraining across a large evolutionary-scale (Rives et al., 2019;Elnaggar et al., 2020), our models for RNAs may similarly benefit by such a procedure with the data collected from RFAM. After the pretraining step, reinforcement learning can be used to finetune the generative component of our model with customizable rewards defined jointly on RNA structural validity, folding stability and functions such as binding to certain proteins.
On the evaluation side, it would be of great interest to analyze our models for any potential RNA tertiary structural motifs and to compare them with those deposited in the CaRNAval (Reinharz et al., 2018) or RNA 3D motifs database (Parlea et al., 2016). Our models would also need modifications to allow non-canonical interactions and pseudoknots, which are common in RNA tertiary structures.
All in all, the representation, generation and design of structured RNA molecules represent a rich, promising, and challenging area for future research in computational biology and drug discovery, and an opportunity to develop fundamentally new machine learning approaches.

[A ACKNOWLEDGEMENTS]
We would like to thank all members of the Hamilton lab, Blanchette lab, and the four anonymous reviewers for their insightful suggestions. This work was funded by a Genome Quebec/Canada grant to MB and by the Institut de Valorisation des Données (IAVDO) PhD excellence scholarship to ZY. WLH is supported by a Canada CIFAR AI Chair. We also thank Compute Canada for providing the computational resources.
B BACKGROUND: RNA STRUCTURE AND KEY PROPERTIES , hence stabilizing the molecule 3 . The set of pairs of interacting nucleotides in an RNA forms its so-called RNA secondary structure. In computational analyses of RNA, it is standard to assume that a secondary structure is nested: if [i, j] and [k, l] form base pairs with i < k, then either l < j (nesting) or k > j (non-overlapping). This enables simple string or planar graph representations (Figure S1 a, b).
The nested structure assumption means that secondary structures can be modelled by a probabilistic context free grammar (Dowell & Eddy, 2004), or by the closely related junction tree structure (Figure S1 c) (Sarrazin-Gendron et al., 2020), where each hypernode corresponds to a particular secondary substructure element: (1) stem: consecutive stacked base-pairs locally forming a double-stranded structure;
(2) hairpin loop : unpaired regions closed by a base-pair;
(3) internal loop: unpaired regions located between two stems; (4) multiloop: unpaired regions at the junction of at least three stems. Edges link elements that are adjacent in the structure.
Validity and stability of RNA folding. The notion of free energy of RNA secondary structures can be used to characterize the stability of a particular conformation. Given an RNA sequence, there are combinatorially many valid RNA secondary structures which all need to obey a set of constraints (summarized in section 3.3). However, some structures are more stable than the others by having lower free energy. Therefore, these structures are more likely to exist (hence more useful) in reality due to the stochastic nature of RNA folding. The free energy of an RNA secondary structure can be estimated by an energy-based model with thermodynamic parameters obtained from experiments (Mathews et al., 2004), wherein the minimum free energy (MFE) structure can be predicted, up to a reasonable approximation (Lorenz et al., 2011). 4 

[C DATASET AND METRICS]
The unlabeled dataset is obtained from the complete human transcriptome which is downloaded from the Ensembl database (Aken et al. (2016); version GRCh38). We slice the transcripts into snippets with length randomly drawn between 32 and 512 nts, and use RNAfold to obtain the MFE structures. We randomly split the dataset into a training set that contains 1,149,859 RNAs, and 20,000 held-out RNAs for evaluating decoding from the posterior distribution. More information on the structural diversity and complexity of this dataset is shown in Figure S2, which should present significant challenges for our algorithms.
The labeled dataset is pulled from a previous study on sequence and structural binding preference of RNA binding proteins (RBP), using an in vitro selection protocol called RNAcompete-S (Cook et al., 2017) which generates synthesized RNA sequences bound or unbound to a given RBP. RNAs in this experiment are of uniform length, i.e. 40 nts, and offer a rich abundance of RNA secondary structures compared to its predecessor protocols such as RNAcompete (Ray et al., 2009;2013). Since no benchmark has been ever established since its publication, we randomly sample 500,000 positive sequences bound to an RBP, and the same amount of negative sequences from the pool of unbound sequences, to curate a dataset for each of the seven RBPs investigated in the paper. Then, 80% of all RNAs are randomly selected to the train split, and the rest goes to the test split.
Our evaluation scheme for the generated RNA secondary structures includes the following metrics:
• validity: percentage of generated RNA secondary structures that conform to the structural constraints specified in section 3.3. • free energy deviation (FE DEV): difference of free energy between the generated RNA secondary structure and the MFE structure of the corresponding sequence, which quantifies the gap of both structures from an energy perspective. A lower FE DEV should indicate higher stability of generated RNAs. • free energy deviation normalized by length (Normed FE DEV): FE DEV divided by the length of generated RNA, which distributes the contribution of total FE DEV to each base. • 5-mer sequence diversity: entropy of the normalized counts of 5-mer substrings, which directly measures the diversity of RNA sequences, and indirectly for RNA secondary structures when this metric is combined with FE DEV, since monolithic structures of diverse sequences would lead to high FE DEV.

[D TREE ENCODING GRU]
Following Eq.3, T-GRU computes a new message v tĜ i,Ĝj fromĜ i andĜ j , based on the features in G i denoted by xĜ i , as well as neural messages from neighboring subgraphs toĜ i , i.e. {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}. The internal structure of T-GRU is equivalent to the tree encoder employed in Jin et al. (2018), which is essentially a neural analogue of the belief propagation algorithm on junction trees. Nevertheless, we write down the message passing formulas of T-GRU here:
sĜ i,Ĝj = Ĝ k ∈N (Ĝi) v t−1 G k ,Ĝi(S1)
zĜ i,Ĝj = σ(W z [xĜ i || sĜ i,Ĝj ] + b z ) (S2) rĜ k ,Ĝi = σ(W r [xĜ i || v t−1 G k ,Ĝi ] + b r ) (S3) vĜ i,Ĝj = Tanh(W [xĜ i || Ĝ k ∈N (Ĝi) rĜ k ,Ĝi • v t−1 G k ,Ĝi ]) (S4) v tĜ i,Ĝj = (1 − zĜ i,Ĝj ) sĜ i,Ĝj + zĜ i,Ĝj vĜ i,Ĝj(S5)
E ALGORITHM FOR HIERARCHICALLY DECODING STRUCTURED RNA a M TI refers to the threshold which set the maximum allowed number of topological prediction steps; M SI is another threshold to limit the length of each decoded nucleotide segment.

[H DETAILS FOR PARAMETERIZING PRIOR DISTRIBUTION USING NORMALIZING FLOW]
A normalizing flow involves a series of bijective transformation with tractable Jacobian logdeterminant, to map an observed datapoint x ∼ p θ (x) from a complex distribution to a simpler one, such as the standard normal distribution.
Considering the simplified case where we have a single bijective function f θ : Z → X to map some simple latent variables z to observed datapoint x, then, using the change of variable theorem, the likelihood of the observed datapoint can be evaluated as:
p θ (x) = p z (f −1 θ (x))|det ∂f −1 θ (x) ∂x | (S6)
where p z (.) denotes some simple base distribution, e.g. N (0; I). Then, it becomes clear the efficiency of this scheme heavily relies on the efficiency of inverting the forward mapping f θ as well as computing its Jacobian log-determinant.
In this project, we use a type of continuous normalizing flow (CNF) which simplifies the above mentioned computation (Chen et al., 2018). Consider a time continuous dynamics f ψ (z(t), t) of some intermediate data representation z(t), and again z(t 0 ) ∼ p z (.), the transformation of variable, along with its inverse mapping, can be expressed as:
z z(t 1 ) = z(t 0 ) + t1 t0 f ψ (z(t), t)dt (S7) z(t 0 ) = z(t 1 ) + t0 t1 f ψ (z(t), t)dt (S8)
and the change of probability density can be expressed as:
log p ψ (z) = log p z (z(t 0 )) − t1 t0 tr( ∂f θ ∂z(t) )dt (S9)
Note that the invertibility issue is no longer a concern under some mild constraints (Chen et al., 2018). Also, Eq. S9 only involves a more light-weight trace operation on the Jacobian rather than evaluating its log-determinant.
Therefore, we learn a parameterized prior using a CNF, and observe the decomposition of the KL term in the VAE objective:
KL(q φ (z|x)|p ψ (z)) = −E z∼q φ (z|x) [p ψ (z)] − H[q φ (z|x)](S10)
Therefore, during training our CNF parameterized with ψ works on the transformation of complex latent encodings z ∼ q φ (z|x) to some simple z(t 0 ) ∼ N (0; I), with an exact likelihood described by Eq. S9 and integrated into Eq. S10 for the complete training objective. During inference, we simply sample z t0 ∼ N (0; I), and use our CNF to reversely transform it to z ∼ p ψ (.) which should be closer to the approximate posterior. J HYPERPARAMETERS  The optimization takes place in the latent space of HierVAE, starting from the initial encoding of a random RNA molecule in the test set, and at each step altering the latent encoding by using activation maximization on the embedding classifier. The trajectory of generated RNAs is shown in the order of left to right and top to bottom, and the field PRED indicates that the probability of binding, as predicted by another external full classifier on the decoded molecular structure, is overall increasing as the decoded RNA structures smoothly evolving.

[F DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO LINEARIZED DECODING PROCEDURES]
When decoding from the joint vocabulary of sequence and dot-bracket structure ({A, C, G, U } × {., (, )}), whenever a nucleotide nuc i with a left bracket is sampled at step i, we append them to a stack, i.e. {(nuc i0 , i 0 ) . . . (nuc i , i)}. Then, at decode step j,
• if |i − j| ≤ 3, a proper mask will be added to the categorical logits of the vocabulary, to avoid sampling any nucleotides with right brackets, which means only an unpaired nucleotide or one that comes with a left bracket can be sampled; • if |i − j| > 3, a mask will be applied to make sure that only a nucleotide complementary to nuc i can be sampled with the right bracket. Sampling nucleotides with other forms of structures are allowed.
As soon as a nucleotide with a closing right bracket is sampled, we pop out (nuc i , i) from the stack. The special symbol for stop decoding can only be sampled when the stack has become empty.

[G DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO HIERARCHICAL DECODING PROCEDURES]
Additional constraints to be enforced during the hierarchical decoding process to ensure the validity of the decoded RNA secondary structure. Recall in section 3.2 that three types of predictions are involved with the hierarchical decoding, therefore, each type is associated with its own set of rules.
All set of rules can be observed by adding proper masks to the categorical logits before sampling, which are detailed below.
Constraints for making topological prediction, when the current node is
• stem node, then the algorithm always expands to a new node upon its first visit, or backtracks to its parent node upon re-visit; • hairpin node, then the algorithm always backtracks; • internal loop, then the algorithm acts similarly as for stem node; • multi-loop, then the algorithm always expands upon first visit and the next re-visit. Further re-visits to the same multi-loop node are not regulated.
Constraints for predicting new tree node, when the current node is • stem node, then its child node when exists can be either a hairpin loop, an internal loop, or a multi-loop; • hairpin node, internal loop or multi-loop, then its child node must be a stem node.
Constraints for decoding nucleotide segment. Due to the property of non-empty intersection between adjacent subgraphs, the start token for decoding a segment at the current node, is always the last nucleotide decoded at the last node. Therefore, without explicitly mentioning, the algorithm needs to decode at least one new nucleotide at each segment. When the current node is • stem node, and if it is upon its first visit (i.e. decoding the first segment of a stem), then there is no for constraints. Otherwise, upon its re-visit, the algorithm needs to decode exactly the complementary bases and in the reverse order, according to the first decoded segment; • hairpin node, then the decoder needs to decode at least four nucleotides before seeing the stop symbol, unless the hairpin is also the root node. • internal loop node, and if it is upon its first, then constraint is not necessary. Otherwise, upon its revisit, the algorithm needs to decode at least one unpaired nucleotide on condition that the first decoded internal loop segment does not contain any unpaired nucleotides; • multi-loop node, then there is no need for constraints. HuR 0.880 ± 0.000 0.880 ± 0.000 0.880 ± 0.000 0.888 ± 0.002 PTB 0.900 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 QKI 0.820 ± 0.000 0.830 ± 0.000 0.825 ± 0.002 0.830 ± 0.000 Vts1 0.900 ± 0.000 0.908 ± 0.002 0.637 ± 0.079 0.910 ± 0.000 RBMY 0.905 ± 0.002 0.880 ± 0.003 0.802 ± 0.055 0.870 ± 0.002 SF2 0.890 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 SLBP 0.777 ± 0.002 0.790 ± 0.000 0.797 ± 0.002 0.797 ± 0.002","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.","{
    ""1"": {
        ""summary"": ""The paper lacks comprehensive comparison with established models outside its own framework and clarity in evaluation metrics."",
        ""verbatim"": [
            ""The main issue is that all model comparisons are only with their own VAEs. It would be nice to show how their VAEs stack up against other established methods."",
            ""It’s difficult to assess the quality of the generated secondary structures shown in various plots throughout the paper and appendices."",
            ""Evaluation is very tricky and it’s unclear how to navigate this.""
        ]
    },
    ""2"": {
        ""summary"": ""Concerns about control for homologous sequences, definition validity, model comparisons, and evaluation metrics."",
        ""verbatim"": [
            ""This paper falls into the common pitfall of not controlling for homologous sequences between the training and test set."",
            ""The definition of \""valid structures\"" is poorly defined."",
            ""I think a comparison with an algorithm like INFERNAL is necessary.""
        ]
    },
    ""3"": {
        ""summary"": ""Concerns about the practical application and automation of the model, and a comparison with discriminative learning approaches."",
        ""verbatim"": [
            ""Is it possible to use this generation technique to use in solving problems like RNA folding or targeted RNA design?"",
            ""Is it also feasible to learn the structural constraints from the data?""
        ]
    },
    ""4"": {
        ""summary"": ""Issues related to the validity of sequence-structure combinations, model evaluation, and dataset generation."",
        ""verbatim"": [
            ""As noted by the authors, their decoders may generate invalide sequence-structure combinations."",
            ""From the results in Table 1 it is still difficult to tease apart the relative contributions of model training, inductive bias of decoder architecture, and decoding constraints."",
            ""Since the authors introduce a new dataset it would great to see more information about its composition.""
        ]
    }
}",4,"{
    ""1"": {
        ""summary"": ""Concerns about the robustness of the model due to a reduction in validity of generated RNAs despite structural constraints."",
        ""verbatim"": ""While the models showcased positive results, the slight reduction in validity of generated RNAs even with structural constraints might raise concerns about the overall robustness of the approach.""
    },
    ""2"": {
        ""summary"": ""The complexity and imposed constraints might limit model scalability."",
        ""verbatim"": ""The complexity of the hierarchical decoding process and the constraints imposed may limit the scalability of the models to larger datasets or more diverse RNA structures.""
    },
    ""3"": {
        ""summary"": ""Potential limitation in generalizability to different RNA sequences and structures, as the evaluation used human transcriptome data."",
        ""verbatim"": ""The evaluation on human transcriptome data may limit the generalizability of the models to a broader range of RNA sequences and structures.""
    },
    ""4"": {
        ""summary"": ""Insufficient comparison with existing methods or benchmarks may question the proposed models' superiority."",
        ""verbatim"": ""Limited comparison with existing methods or benchmarks in the same research domain may raise questions about the superiority of the proposed models.""
    }
}",4,"{
    ""A1-B4"": {
        ""rationale"": ""Both points emphasize the lack of comparative analysis with established models. In Review A, the reviewer notes that the paper only compares its models with other VAEs, missing a broader comparison with other established methods. Review B criticizes limited comparison with existing methods, suggesting that it raises questions about the models' superiority."",
        ""similarity"": ""8""
    },
    ""A2-B1"": {
        ""rationale"": ""Review A criticizes the validity of the sequence-structure combinations, similar to Review B, which points out concerns on the reduction in validity of generated RNAs even with structural constraints, questioning the robustness of the model."",
        ""similarity"": ""8""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Lack of comparative analysis with existing methods"",
        ""verbatim"": ""Lack of clarity on how the proposed framework compares to existing methods for RNA structure generation.""
    },
    ""2"": {
        ""summary"": ""Need for more detailed experimental validation"",
        ""verbatim"": ""The paper may benefit from more detailed experimental validation to showcase the effectiveness of the proposed techniques.""
    },
    ""3"": {
        ""summary"": ""Insufficient discussion on computational complexity or scalability"",
        ""verbatim"": ""Insufficient discussion on the computational complexity or scalability of the methods proposed for generating RNA structures.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A and Review B highlight the lack of a comprehensive comparative analysis of the proposed model against established models in the field. Review A specifically mentions the absence of comparisons outside of their own VAEs, while Review B points to a general lack of clarity in how the framework compares to existing methods."",
        ""similarity"": ""9""
    },
    ""A2-B2"": {
        ""rationale"": ""Review A points to concerns about the validity and control mechanisms of the presented work such as the control over homologous sequences and model comparisons, while Review B mentions the need for more detailed experimental validation. Both points essentially urge the necessity for rigorous and thorough validation methods to establish the model's effectiveness."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper may be rejected due to insufficient empirical validation of the proposed RNA structure generation framework."",
        ""verbatim"": ""<Lack of empirical validation> The paper may face rejection if it lacks sufficient empirical validation to demonstrate the effectiveness of the proposed framework in generating diverse and valid RNA structures.""
    },
    ""2"": {
        ""summary"": ""The paper risks rejection if it does not adequately explain the organization of the latent space for RNA molecular embeddings in relation to protein interactions."",
        ""verbatim"": ""<Clarity in latent space organization> There could be potential rejection if the paper does not clearly explain how the latent space organization for RNA molecular embeddings relates to the interaction with proteins and the effectiveness of targeted optimization.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both points express concerns about the adequacy of evaluation and validation of the VAE-based generation framework for RNA structures proposed in the paper. Review A emphasizes the lack of comprehensive comparison with established models and unclear evaluation metrics, while Review B specifically addresses the lack of empirical validation as a key issue that could lead to rejection."",
        ""similarity"": ""7""
    },
    ""A4-B1"": {
        ""rationale"": ""Review A discusses the generation of invalid sequence-structure combinations and difficulty in evaluating the model's contributions, aligning closely with Review B's concern on insufficient empirical validation of the model's effectiveness in generating valid RNA structures. Both points indicate that the validation of the output quality and effectiveness of the model is critical."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Limited diversity of RNA structures in the models"",
        ""verbatim"": ""Limited diversity of RNA structures: While the generated structures are diverse within the context of the models, the paper could explore a broader array of RNA secondary structures beyond those generated through current models. The diversity in real-world applications could be larger than that covered in the current experimental setup.""
    },
    ""2"": {
        ""summary"": ""Specificity of models limiting broader application"",
        ""verbatim"": ""Specificity of models and generalization: The models are tailored primarily for RNA structures; their applicability to other similar biomolecules or potential for adaptation isn't discussed, which might limit the perceived scope and impact.""
    },
    ""3"": {
        ""summary"": ""Complex methods may hinder reproducibility and accessibility"",
        ""verbatim"": ""Complexity and accessibility of methods: The complex nature of the proposed HierVAE and associated graph-based techniques could pose reproducibility challenges or require significant computation power, thereby limiting wider adoption without substantial computational resources or specific expertise.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A1 and Review B1 discuss the lack of comprehensive comparisons and diversity in the models presented in the paper. Review A1 points out the issue by comparing the models only within their own VAE framework, while Review B1 also addresses the limited diversity within the model-generated structures. Both are concerned with the limitations of the models in capturing broader or real-world diversity."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of empirical validation for the framework"",
        ""verbatim"": ""'Lack of Empirical Validation': Although the framework proposes advanced theoretical methodologies, there is a noticeable absence of comprehensive empirical validation or case studies demonstrating the efficacy of the generated RNA molecules.""
    },
    ""2"": {
        ""summary"": ""Complexity and accessibility issues of the techniques"",
        ""verbatim"": ""'Complexity and Accessibility of Techniques': The complexity of the techniques involved may make it difficult for other researchers to reproduce or adapt the findings without extensive background knowledge or resources.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""Complexity and scalability issues of the graph-based modeling approach."",
        ""verbatim"": ""The complexity of the graph-based modeling approach and potential scalability issues related to handling large RNA datasets might pose questions on the practicality of the framework.""
    },
    ""2"": {
        ""summary"": ""Limited proof of the framework's applicability across different RNA structures."",
        ""verbatim"": ""If the paper does not adequately demonstrate the framework's applicability across a diverse set of RNA structures, this could be seen as a limitation affecting its generalizability.""
    }
}",2,{},0,"[Significance and novelty]

Our paper presents an innovative approach to the field of RNA (Ribonucleic Acid) design and generation using advanced graph-based deep generative modeling techniques [1]. This work is significant due to the spurring dedicate methods for designing large-scale and complex biological structures, a key but underappreciated aspect of computational drug discovery [2]. The novelty of our methods lies in our proposed framework to jointly embed and generate RNA structural modalities and their sequences in a meaningful latent space [3]. This framework integrates strong inductive bias about RNA structural regularity and folding mechanism to ensure high structural validity, stability, and diversity of generated RNAs [4]. Moreover, we emphasize adequate organization of the latent space of RNA molecular embeddings with regard to the interaction with proteins [5], and utilize targeted optimization to navigate this latent space for searching desired novel RNA molecules [6].

[Potential reasons for acceptance]
1. Novel and significant research topic contributing to the computational drug discovery, designing and generating RNA structures using advanced modeling techniques;
2. Sound theoretical foundation and clear research objectives and hypothesis;
3. Methodology utilizing advanced deep learning techniques for RNA structural generation;
4. Experimental design with thorough validation procedures for model performance assessment;
5. Novel RNA structure generation results displaying structural validity, stability, and diversity.

[Potential reasons for rejection]
1. Complex theoretical understanding required for fully grasping proposed methods;
2. Lack of a comprehensive comparative analysis with other existing approaches;
3. Unclear justification for model choices and evaluations;
4. Insufficient experimental validation and lack of control experiments.

[Suggestions for improvement]
1. Provide a more detailed and comprehensive introduction to the background and significance of computational RNA design and deep learning methods;
2. Include a comparative analysis with other existing approaches to demonstrate the advantages of the proposed framework;
3. Clearly justify the choices made during model architecture, optimization, and data preparation;
4. Incorporate control experiments into the experimental design for more accurate performance evaluations.","[Significance and novelty]
<Technical contributions> The contributions of the paper are considered significant and somewhat new. The paper's technical contributions are viewed as significant and somewhat new in the field, despite some aspects of the contributions appearing in prior work.
<Methodological contributions> The use of graph generative models to encode both the structural graph and junction tree of RNAs into a Euclidean space, and optimization methods to search for RNA sequences with desired properties in this space are recognized as non-trivial contributions.

[Potential reasons for acceptance]
<Innovative approach> The paper introduces an innovative strategy for encoding and generating RNA structural information into latent space, offering potential improvements in modeling and design of RNAs.
<Relevancy of the addressed problem> The problem addressed in the paper is considered important, which adds to its potential for acceptance.

[Potential reasons for rejection]
<Lack of reproducible results> Reviewers raised concerns about the reproducibility of the results due to poor experimental details and insufficient explanation of methods, which could be a potential reason for rejection.
<Unclear presentation and explanation> The paper's unclear organization and presentation, as well as the lack of detailed elaboration on the model framework, could lead to rejection.

[Suggestions for improvement]
<Enhance clarity and organization> The paper would benefit from clearer and more organized presentation, including explaining the model framework in more detail and better organizing sections for improved readability.
<Improved description of experiments> More detailed explanations of experiments and experimental setup are needed to ensure reproducibility and address concerns about unclear experiment details.

","{
    ""1"": {
        ""summary"": ""Complexity of the theoretical aspects could hinder understanding"",
        ""verbatim"": ""Complex theoretical understanding required for fully grasping proposed methods""
    },
    ""2"": {
        ""summary"": ""Lack of comparison with existing approaches"",
        ""verbatim"": ""Lack of a comprehensive comparative analysis with other existing approaches""
    },
    ""3"": {
        ""summary"": ""Unclear justification for model choices and evaluations"",
        ""verbatim"": ""Unclear justification for model choices and evaluations""
    },
    ""4"": {
        ""summary"": ""Insufficient experimental validation"",
        ""verbatim"": ""Insufficient experimental validation and lack of control experiments""
    }
}",4,"{
    ""1"": {
        ""summary"": ""Concerns about the reproducibility of results due to insufficient experimental details."",
        ""verbatim"": ""Lack of reproducible results> Reviewers raised concerns about the reproducibility of the results due to poor experimental details and insufficient explanation of methods, which could be a potential reason for rejection.""
    },
    ""2"": {
        ""summary"": ""Issues with unclear presentation and lack of detailed explanation of the model framework."",
        ""verbatim"": ""Unclear presentation and explanation> The paper's unclear organization and presentation, as well as the lack of detailed elaboration on the model framework, could lead to rejection.""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both points criticize the lack of comparative analysis of the proposed models with other existing approaches. Review A emphasizes comparisons within their own VAE framework only, suggesting expansion, while Review B generically notes the absence of comparative analysis with existing approaches."",
        ""similarity"": ""7""
    },
    ""A2-B4"": {
        ""rationale"": ""Both points highlight issues with experimental validation and model evaluation. Review A focuses on controlling for homologous sequences and defines valid structures, while Review B emphasizes the insufficiency of control experiments and experimental validation broadly."",
        ""similarity"": ""6""
    },
    ""A4-B3"": {
        ""rationale"": ""Both reviews express concerns regarding the clarity and justification of model evaluation. Review A discusses the difficulty in determining contributions from various model aspects, while Review B directly calls out the unclear justification of model choices and evaluations."",
        ""similarity"": ""7""
    }
}",2,{},0
Deep Neural Networks as Gaussian Processes,B1EA-M-0Z.json,"[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency).

[CAPTIONS]
Table 1: Figure1: The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
Table 2: TO DEEP SIGNAL PROPAGATION Several prior works (Poole et al. (2016); Schoenholz et al. (2017); Daniely et al. (2016); Duvenaud et al. (
Table 3: Figure 2 :Figure 3 :23Figure2: Generalization gap for five hidden layer fully-connected networks with variable widths, using ReLU and Tanh nonlinearities on CIFAR-10. Random optimization and initialization hyperparameters were used and results were filtered for networks with 100% classification training accuracy, resulting in a total of 125 Tanh and 55 ReLU networks. The best generalizing networks are consistently the widest.
Table 4: Figure 4 :4Figure 4: The best performing NNGP hyperparameters agree with those predicted by deep signal propagation. Test set accuracy heatmaps for NNGPs evaluated for a grid of σ 2 w and σ 2 b values. The right plot in each subfigure (a), (b) is a theoretical phase diagram for that nonlinearity following the methodology ofSchoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure9.
Table 5: Figure 5 :5Figure 5: Samples from an NNGP prior for 1D functions. Different lines correspond to different draws (arbitrary colors).
Table 6: Figure 7 :7Figure 7: Graphical model for neural network's computation.
Table 7: Figure 10 :10Figure 10: Best performing NNGPs are distributed near the critical line. Weight and bias variance distribution for the 25 best performing runs for NNGP with the given training set size is shown.
Table 8: The NNGP often outperforms finite width networks. Test accuracy on MNIST and CIFAR-10 datasets. The reported NNGP results correspond to the best performing depth, σ 2 w , and σ 2 b values on the validation set. The traditional NN results correspond to the best performing depth, width and optimization hyperparameters. Best models for a given training set size are specified by (depthwidth-σ 2 w -σ 2 b ) for NNs and (depth-σ 2 w -σ 2 b ) for GPs. More results are in Appendix Table2.

[INTRODUCTION]
Deep neural networks have emerged in recent years as flexible parametric models which can fit complex patterns in data. As a contrasting approach, Gaussian processes have long served as a traditional nonparametric tool for modeling. An equivalence between these two approaches was derived in Neal (1994a), for the case of one layer networks in the limit of infinite width. Neal (1994a) further suggested that a similar correspondence might hold for deeper networks.
Consider a deep fully-connected neural network with i.i.d. random parameters. Each scalar output of the network, an affine transformation of the final hidden layer, will be a sum of i.i.d. terms. As we will discuss in detail below, in the limit of infinite width the Central Limit Theorem 1 implies that the function computed by the neural network (NN) is a function drawn from a Gaussian process (GP). In the case of single hidden-layer networks, the form of the kernel of this GP is well known (Neal (1994a); Williams (1997)). This correspondence implies that if we choose the hypothesis space to be the class of infinitely wide neural networks, an i.i.d. prior over weights and biases can be replaced with a corresponding GP prior over functions. As noted by (Williams, 1997), this substitution enables exact Bayesian inference for regression using neural networks. The computation requires building the necessary covariance matrices over the training and test sets and straightforward linear algebra computations.
In light of the resurgence in popularity of neural networks, it is timely to revisit this line of work. We delineate the correspondence between deep and wide neural networks and GPs and utilize it for Bayesian training of neural networks on regression tasks.

[RELATED WORK]
Our work touches on aspects of GPs, Bayesian learning, and compositional kernels. The correspondence between infinite neural networks and GPs was first noted by Neal (1994a;b). Williams (1997) computes analytic GP kernels for single hidden-layer neural networks with error function or Gaussian nonlinearities and noted the use of the GP prior for exact Bayesian inference in regression. Duvenaud et al. (2014) discusses several routes to building deep GPs and observes the degenerate form of kernels that are composed infinitely many times -a point we will return to Section 3.2but they do not derive the form of GP kernels as we do. Hazan & Jaakkola (2015) also discusses constructing kernels equivalent to infinitely wide deep neural networks, but their construction does not go beyond two hidden layers with nonlinearities.
Related work has also appeared outside of the GP context but in compositional kernel constructions. Cho & Saul (2009) derives compositional kernels for polynomial rectified nonlinearities, which includes the Sign and ReLU nonlinearities, and can be used in GPs; our manner of composing kernels matches theirs, though the context is different. Daniely et al. (2016) extends the construction of compositional kernels to neural networks whose underlying directed acyclic graph is of general form. They also prove, utilizing the formalism of dual activations, that compositional kernels originating from fully-connected topologies with the same nonlinearity become degenerate when composed infinitely many times. In a different context than compositional kernels, Poole et al. (2016); Schoenholz et al. (2017) study the same underlying recurrence relation for the specific case of fully-connected networks and bounded nonlinearities. They distinguish regions in hyperparameter space with different fixed points and convergence behavior in the recurrence relations. The focus in these works was to better understand the expressivity and trainability of deep networks.
Drawing inspiration from the multi-layer nature of deep neural networks, there is a line of work considering various approaches to stacking GPs, such as deep GPs (Lawrence & Moore (2007); Damianou & Lawrence (2013); Hensman & Lawrence (2014); Duvenaud et al. (2014); Bui et al. (2016)), which can give rise to a richer class of probabilistic models beyond GPs. This contrasts with our work, where we study GPs that are in direct correspondence with deep, infinitely wide neural networks. Krauth et al. (2016) has recently explored the performance of GP models with deep kernels given in Cho & Saul (2009), implemented with scalable approximations. However, they do not discuss the equivalence between deep neural networks and GPs with compositional kernels, which constitutes a conceptual contribution of our work. Furthermore, we note that the GP kernels in our work are more general than the compositional kernel construction outlined in Cho & Saul (2009) in two respects: (i) we are not limited to rectified polynomials but can deal with general nonlinearities, and (ii) we consider two additional hyperparameters in the kernels, which would correspond to the weight and bias parameter variances in a neural network. Finally, Gal & Ghahramani (2016) connects dropout in deep neural networks with approximate Bayesian inference in deep GPs.
Another series of recent works (Wilson et al. (2016b;a); Al-Shedivat et al. (2017)), termed deep kernel learning, utilize GPs with base kernels which take in features produced by a deep multilayer neural network, and train the resulting model end-to-end. Our work differs from these in that our GP corresponds to a multilayer neural network. Additionally, our GP kernels have many fewer parameters, and these parameters correspond to the hyperparameters of the equivalent neural network.

[SUMMARY OF CONTRIBUTIONS]
We begin by specifying the form of a GP which corresponds to a deep, infinitely wide neural network -hereafter referred to as the Neural Network GP (NNGP) -in terms of a recursive, deterministic computation of the kernel function. The prescription is valid for generic pointwise nonlinearities in fully-connected feedforward networks. We develop a computationally efficient method (Section 2.5) to compute the covariance function corresponding to deep neural networks with fixed hyperparameters.
In this work, as a first proof of concept of our NNGP construction, we focus on exact Bayesian inference for regression tasks, treating classification as regression on class labels. While less principled, least-squares classification performs well (Rifkin et al., 2003) and allows us to compare exact inference via a GP to prediction by a trained neural network on well-studied tasks (MNIST and CIFAR-10 classification). Note that it is possible to extend GPs to softmax classification with cross entropy loss (Williams & Barber (1998); Rasmussen & Williams (2006)), which we aim to investigate in future work.
We conduct experiments making Bayesian predictions on MNIST and CIFAR-10 (Section 3) and compare against NNs trained with standard gradient-based approaches. The experiments explore different hyperparameter settings of the Bayesian training including network depth, nonlinearity, training set size (up to and including the full dataset consisting of tens of thousands of images), and weight and bias variance. Our experiments reveal that the best NNGP performance is consistently competitive against that of NNs trained with gradient-based techniques, and the best NNGP setting, chosen across hyperparameters, often surpasses that of conventional training (Section 3, Table 1). We further observe that, with increasing network width, the performance of neural networks with gradient-based training approaches that of the NNGP computation, and that the GP uncertainty is strongly correlated with prediction error. Furthermore, the performance of the NNGP depends on the structure of the kernel, which can be connected to recent work on signal propagation in networks with random parameters (Schoenholz et al., 2017).

[DEEP, INFINITELY WIDE NEURAL NETWORKS ARE DRAWN FROM GPS]
We begin by specifying the correspondence between GPs and deep, infinitely wide neural networks, which hinges crucially on application of the Central Limit Theorem. We review the single-hidden layer case (Section 2.2) before moving to the multi-layer case (Section 2.3).

[NOTATION]
Consider an L-hidden-layer fully-connected neural network with hidden layers of width N l (for layer l) and pointwise nonlinearities φ. Let x ∈ R din denote the input to the network, and let z L ∈ R dout denote its output. The ith component of the activations in the lth layer, post-nonlinearity and postaffine transformation, are denoted x l i and z l i respectively. We will refer to these as the post-and pre-activations. (We let x 0 i ≡ x i for the input, dropping the Arabic numeral superscript, and instead use a Greek superscript x α to denote a particular input α). Weight and bias parameters for the lth layer have components W l ij , b l i , which are independent and randomly drawn, and we take them all to have zero mean and variances σ 2 w /N l and σ 2 b , respectively. GP(µ, K) denotes a Gaussian process with mean and covariance functions µ(•), K(•, •), respectively.

[REVIEW OF GAUSSIAN PROCESSES AND SINGLE-LAYER NEURAL NETWORKS]
We briefly review the correspondence between single-hidden layer neural networks and GPs (Neal (1994a;b); Williams (1997)). The ith component of the network output, z 1 i , is computed as,
z 1 i (x) = b 1 i + N1 j=1 W 1 ij x 1 j (x), x 1 j (x) = φ b 0 j + din k=1 W 0 jk x k ,(1)
where we have emphasized the dependence on input x. Because the weight and bias parameters are taken to be i.i.d., the post-activations x 1 j , x 1 j are independent for j = j . Moreover, since z 1 i (x) is a sum of i.i.d terms, it follows from the Central Limit Theorem that in the limit of infinite width N 1 → ∞, z 1 i (x) will be Gaussian distributed. Likewise, from the multidimensional Central Limit Theorem, any finite collection of {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have a joint multivariate Gaussian distribution, which is exactly the definition of a Gaussian process. Therefore we conclude that z 1 i ∼ GP(µ 1 , K 1 ), a GP with mean µ 1 and covariance K 1 , which are themselves independent of i.
Because the parameters have zero mean, we have that µ 1 (x) = E z 1 i (x) = 0 and,
K 1 (x, x ) ≡ E z 1 i (x)z 1 i (x ) = σ 2 b + σ 2 w E x 1 i (x)x 1 i (x ) ≡ σ 2 b + σ 2 w C(x, x ),(2)
where we have introduced C(x, x ) as in Neal (1994a); it is obtained by integrating against the distribution of W 0 , b 0 . Note that, as any two z 1 i , z 1 j for i = j are joint Gaussian and have zero covariance, they are guaranteed to be independent despite utilizing the same features produced by the hidden layer.

[GAUSSIAN PROCESSES AND DEEP NEURAL NETWORKS]
The arguments of the previous section can be extended to deeper layers by induction. We proceed by taking the hidden layer widths to be infinite in succession (N 1 → ∞, N 2 → ∞, etc.) as we continue with the induction, to guarantee that the input to the layer under consideration is already governed by a GP. In Appendix C we provide an alternative derivation in terms of Bayesian marginalization over intermediate layers, which does not depend on the order of limits, in the case of a Gaussian prior on the weights. A concurrent work (de G. Matthews et al., 2018) further derives the convergence rate towards a GP if all layers are taken to infinite width simultaneously, but at different rates.
Suppose that z l−1 j is a GP, identical and independent for every j (and hence x l j (x) are independent and identically distributed). After l − 1 steps, the network computes
z l i (x) = b l i + N l j=1 W l ij x l j (x), x l j (x) = φ(z l−1 j (x)).(3)
As before, z l i (x) is a sum of i.i.d. random terms so that, as N l → ∞, any finite collection {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have joint multivariate Gaussian distribution and
z l i ∼ GP(0, K l ). The covariance is K l (x, x ) ≡ E z l i (x)z l i (x ) = σ 2 b + σ 2 w E z l−1 i ∼GP(0,K l−1 ) φ(z l−1 i (x))φ(z l−1 i (x )) . (4)
By induction, the expectation in Equation 4 is over the GP governing z l−1 i , but this is equivalent to integrating against the joint distribution of only z l−1 i (x) and z l−1 i (x ). The latter is described by a zero mean, two-dimensional Gaussian whose covariance matrix has distinct entries K l−1 (x, x ), K l−1 (x, x), and K l−1 (x , x ). As such, these are the only three quantities that appear in the result. We introduce the shorthand
K l (x, x ) = σ 2 b + σ 2 w F φ K l−1 (x, x ), K l−1 (x, x), K l−1 (x , x )(5)
to emphasize the recursive relationship between K l and K l−1 via a deterministic function F whose form depends only on the nonlinearity φ. This gives an iterative series of computations which can be performed to obtain K L for the GP describing the network's final output.
For the base case
K 0 , suppose W 0 ij ∼ N (0, σ 2 w /d in ), b 0 j ∼ N (0, σ 2 b )
; we can utilize the recursion relating K 1 and K 0 , where
K 0 (x, x ) = E z 0 j (x)z 0 j (x ) = σ 2 b + σ 2 w x•x din .(6)
In fact, these recurrence relations have appeared in other contexts. They are exactly the relations derived in the mean field theory of signal propagation in fully-connected random neural networks (Poole et al. (2016);Schoenholz et al. (2017)) and also appear in the literature on compositional kernels (Cho & Saul (2009); Daniely et al. (2016)). For certain activation functions, Equation 5 can be computed analytically (Cho & Saul (2009); Daniely et al. (2016)). In the case of the ReLU nonlinearity, it yields the well-known arccosine kernel (Cho & Saul (2009)) whose form we reproduce in Appendix B. When no analytic form exists, it can instead be efficiently computed numerically, as described in Section 2.5.

[BAYESIAN TRAINING OF NEURAL NETWORKS USING GAUSSIAN PROCESS PRIORS]
Here we provide a short review of how a GP prior over functions can be used to do Bayesian inference; see e.g. (Rasmussen & Williams, 2006) for a comprehensive review of GPs. Given a dataset D = {(x 1 , t 1 ), ..., (x n , t n )} consisting of input-target pairs (x, t), we wish to make a Bayesian prediction at test point x * using a distribution over functions z(x). This distribution is constrained to take values z ≡ (z 1 , ..., z n ) on the training inputs x ≡ (x 1 , ..., x n ) and,
P (z * |D, x * ) = dz P (z * |z, x, x * ) P (z|D) = 1 P (t) dz P (z * , z|x * , x) P (t|z) ,(7)
where t = (t 1 , ..., t n ) T are the targets on the training set, and P (t|z) corresponds to observation noise. We will assume a noise model consisting of a Gaussian with variance σ 2 centered at z.
If the conditions of Section 2.2 or 2.3 apply, our choice of prior over functions implies that z 1 , ..., z n , z * are n + 1 draws from a GP and z * , z|x * , x ∼ N (0, K) is a multivariate Gaussian whose covariance matrix has the form
K = K D,D K T x * ,D K x * ,D K x * ,x * ,
where the block structure corresponds to the division between the training set and the test point. That is, K D,D is an n × n matrix whose (i, j)th element is K(x i , x j ) with x i , x j ∈ D, while e.g. the ith
element of K x * ,D is K(x * , x i ), x i ∈ D.
As is standard in GPs, the integral in Equation 7 can be done exactly, resulting in z * |D, x * ∼ N (μ,K) with
µ = K x * ,D (K D,D + σ 2 I n ) −1 t (8) K = K x * ,x * − K x * ,D (K D,D + σ 2 I n ) −1 K T x * ,D(9)
where I n is the n × n identity. The predicted distribution for z * |D, x * is hence determined from straightforward matrix computations, yet nonetheless corresponds to fully Bayesian training of the deep neural network. The form of the covariance function used is determined by the choice of GP prior, i.e. the neural network model class, which depends on depth, nonlinearity, and weight and bias variances. We henceforth resume placing a superscript L as in K L to emphasize the choice of depth for the compositional kernel.

[EFFICIENT IMPLEMENTATION OF THE GP KERNEL]
Given an L-layer deep neural network with fixed hyperparameters, constructing the covariance matrix K L for the equivalent GP involves computing the Gaussian integral in Equation 4 for all pairs of training-training and training-test points, recursively for all layers. For some nonlinearities, such as ReLU, this integration can be done analytically. However, to compute the kernel corresponding to arbitrary nonlinearities, the integral must be performed numerically.
The most direct implementation of a numerical algorithm for K L would be to compute integrals independently for each pair of datapoints and each layer. This is prohibitively expensive and costs O n 2 g L(n 2 train + n train n test ) , where n 2 g is the sampling density for the pair of Gaussian random variables in the 2D integral and n train , n test are the training and test set sizes, respectively. However, by careful pipelining, and by preprocessing all inputs to have identical norm, we can improve this cost to O n 2 g n v n c + L(n 2 train + n train n test ) , where n v and n c are sampling densities for a variance and correlation grid, as described below. In order to achieve this, we break the process into several steps: 1. Generate: pre-activations u = [−u max , • • • , u max ] consisting of n g elements linearly spaced between −u max and u max ; variances s = [0, • • • , s max ] with n v linearly spaced elements, where s max < u 2 max ; and correlations c = (−1, • • • , 1) with n c linearly spaced elements. Note that we are using fixed, rather than adaptive, sampling grids to allow operations to be parallelized and reused across datapoints and layers.
2. Populate a matrix F containing a lookup table for the function F φ in Equation 5. This involves numerically approximating a Gaussian integral, in terms of the marginal variances s and correlations c. We guarantee that the marginal variance is identical for each datapoint, by preprocessing all datapoints to have identical norm at the input layer, so the number of entries in the lookup table need only be n v n c . These entries are computed as 2 :
F ij = ab φ(u a )φ(u b ) exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b ab exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b .(10)
3. For every pair of datapoints x and x in layer l, compute K l (x, x ) using Equation 5.
Approximate the function F φ K l−1 (x, x ); K l−1 (x, x); K l−1 (x , x ) by bilinear interpolation into the matrix F from Step 2, where we interpolate into s using the value of K l−1 (x, x), and interpolate into c using K l−1 (x, x )/K l−1 (x, x) . Remember that K l−1 (x, x) = K l−1 (x , x ), due to data preprocessing to guarantee constant norm.
4. Repeat the previous step recursively for all layers. Bilinear interpolation has constant cost, so this has cost O L(n 2 train + n train n test ) .
This computational recipe allows us to compute the covariance matrix for the NNGP corresponding to any well-behaved nonlinearity φ. All computational steps above can be implemented using accelerated tensor operations, and computation of K L is typically faster than solving the system of linear equations in Equation 8-9. Figure 6 illustrates the close agreement between the kernel function computed numerically (using this approach) and analytically, for the ReLU nonlinearity. It also illustrates the angular dependence of the kernel and its evolution with increasing depth.
Finally, note that the full computational pipeline is deterministic and differentiable. The shape and properties of a deep network kernel are purely determined by hyperparameters of the deep neural network. Since GPs give exact marginal likelihood estimates, this kernel construction may allow principled hyperparameter selection, or nonlinearity design, e.g. by gradient ascent on the log likelihood w.r.t. the hyperparameters. Although this is not the focus of current work, we hope to return to this topic in follow-up work.
An open source implementation of the algorithm is available at https://github.com/brainresearch/nngp.

[DESCRIPTION]
We compare NNGPs with SGD 3 trained neural networks on the permutation invariant MNIST and CIFAR-10 datasets. The baseline neural network is a fully-connected network with identical width at each hidden layer. Training is on the mean squared error (MSE) loss, chosen so as to allow direct comparison to GP predictions. Formulating classification as regression often leads to good results (Rifkin & Klautau, 2004). Future work may involve evaluating the NNGP on a cross entropy loss using the approach in (Williams & Barber, 1998;Rasmussen & Williams, 2006). Training used the Adam optimizer (Kingma & Ba (2014)) with learning rate and initial weight/bias variances optimized over validation error using the Google Vizier hyperparameter tuner (Golovin et al., 2017). Dropout was not used. In future work, it would be interesting to incorporate dropout into the NNGP covariance matrix using an approach like that in (Schoenholz et al., 2017). For the study, nonlinearities were chosen to be either rectified linear units (ReLU) or hyperbolic tangent (Tanh). Class labels were encoded as a one-hot, zero-mean, regression target (i.e., entries of -0.1 for the incorrect class and 0.9 for the correct class). We constructed the covariance kernel numerically for ReLU and Tanh nonlinearities following the method described in Section 2.5.
Performance: We find that the NNGP often outperforms trained finite width networks. See Table 1 and Figure 1. The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
We additionally find the performance of the best finite-width NNs, trained with a variant of SGD, approaches that of the NNGP with increasing layer width. This is interesting from at least two, potentially related, standpoints. (1) NNs are commonly believed to be powerful because of their ability to do flexible representation learning, while our NNGP uses fixed basis functions; nonetheless, in our experiments we find no salient performance advantage to the former. (2) It hints at a possible relationship between SGD and Bayesian inference in certain regimes -were the neural networks trained in a fully Bayesian fashion, rather than by SGD, the approach to NNGP in the large width limit would be guaranteed. There is recent work suggesting that SGD can implement approximate Bayesian inference (Mandt et al., 2017) under certain assumptions.
The similarity of the performance of the widest NN in Figure 1 with the NNGP suggests that the limit of infinite network width, which is inherent to the GP, is far from being a disadvantage. Indeed, in practice it is found that the best generalizing NNs are in fact the widest. To support this, in Figure 2 we show generalization gap results from an experiment in which we train 180 fully-connected networks with five hidden layers on CIFAR-10 with a range of layer widths. For this experiment, we trained the networks using a standard cross entropy loss rather than MSE, leading to a slight difference in performance.
Uncertainty: One benefit in using a GP is that, due to its Bayesian nature, all predictions have uncertainty estimates (Equation 9). For conventional neural networks, capturing the uncertainty in a model's predictions is challenging (Gal, 2016). In the NNGP, every test point has an explicit estimate of prediction variance associated with it (Equation 9). In our experiments, we observe that the NNGP uncertainty estimate is highly correlated with prediction error (Figure 3).  2014)) have noted the recurrence relations Equation 5 commonly approach a functionally uninteresting fixed point with depth l → ∞, in that K ∞ (x, x ) becomes a constant or piecewise constant map. We now briefly relate our ability to train NNGPs with the convergence of K l (x, x ) to the fixed-point kernel.

[RELATIONSHIP]
We will be particularly interested in contextualizing our results in relation to Poole et al. (2016);Schoenholz et al. (2017) which analyzed the fixed points and the approach to them in detail for bounded nonlinearities. To briefly recapitulate: there are regions of hyperparameter space (called phases) where K ∞ (x, x ) changes only quantitatively with σ 2 w and σ 2 b . However, there are low For the Tanh nonlinearity, there are two distinct phases respectively called the ""ordered"" phase and the ""chaotic"" phase that can be understood as a competition between the weights and the biases of the network. A diagram showing these phases and the boundary between them is shown in Figure 4a. In the ordered phase, the features obtained by propagating an input through the each layer of the recursion become similar for dissimilar inputs. Fundamentally, this occurs because the different inputs share common bias vectors and so all inputs end up just approaching the random bias. In this case the covariance K l (x, x ) → q * for every pair of inputs x, x , where q * is a constant that depends only on σ 2 w and σ 2 b . All inputs have unit correlation asymptotically with depth. By contrast in the chaotic phase the weight variance σ 2 w dominates and similar inputs become dissimilar with depth as they are randomly projected by the weight matrices. In this case, the covariance K l (x, x ) → q * for x = x but q * c * for x = x . Here c * < 1 is the fixed point correlation. In each of these regimes, there is also a finite depth-scale ξ which describes the characteristic number of layers over which the covariance function decays exponentially towards its fixed point form. Exactly at the boundary between these two regimes is a line in (σ 2 w , σ 2 b )-space where the decay K l (x, x ) towards its fixed  2017) that this approach to the fixed-point covariance fundamentally bounded whether or not neural networks could successfully be trained. It was shown that initializing networks on this line allowed for significantly deeper neural networks to be trained.
For ReLU networks a similar picture emerges, however there are some subtleties due to the unbounded nature of the nonlinearity. In this case for all σ 2 w and σ 2 b , K ∞ (x, x ) = q * for all x, x and every point becomes asymptotically correlated. Despite this, there are again two phases: a ""bounded"" phase in which q * is finite (and nonzero) and an unbounded phase in which q * is either infinite or zero. As in the Tanh case there are depth scales that control the rate of convergence to these fixed points and therefore limit the maximum trainable depth. The phase diagram for the ReLU nonlinearity is also shown in Figure 4b.
In a striking analogy with the trainability of neural networks, we observe that the performance of the NNGP appears to closely track the structure from the phase diagram, clearly illustrated in Figure 4. Indeed, we see that as for hyperparameter settings that are far from criticality, the GP is unable to train and we encounter poor test set performance. By contrast, near criticality we observe that our models display high accuracy. Moreover, we find that the accuracy appears to drop more quickly away from the phase boundary with increase in depth L of the GP kernel, K L . To understand this effect we note that information about data will be available to our model only through the difference K L (x, x ) − K ∞ (x, x ). However, as the depth gets larger, this difference becomes increasingly small and at some point can no longer be represented due to numerical precision. At this point our test accuracy begins to quickly degrade to random chance.

[CONCLUSION AND FUTURE DIRECTIONS]
By harnessing the limit of infinite width, we have specified a correspondence between priors on deep neural networks and Gaussian processes whose kernel function is constructed in a compositional, but fully deterministic and differentiable, manner. Use of a GP prior on functions enables exact Bayesian inference for regression from matrix computations, and hence we are able to obtain predictions and uncertainty estimates from deep neural networks without stochastic gradient-based training. The performance is competitive with the best neural networks (within specified class of fully-connected models) trained on the same regression task under similar hyperparameter settings. While we were able to run experiments for somewhat large datasets (sizes of 50k), we intend to look into scalability for larger learning tasks, possibly harnessing recent progress in scalable GPs (Quiñonero-Candela & Rasmussen (2005); Hensman et al. (2013)).  b) is a theoretical phase diagram for that nonlinearity following the methodology of Schoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure 9.
In our experiments, we observed the performance of the optimized neural network appears to approach that of the GP computation with increasing width. Whether gradient-based stochastic optimization implements an approximate Bayesian computation is an interesting question (Mandt et al., 2017). Further investigation is needed to determine if SGD does approximately implement Bayesian inference under the conditions typically employed in practice.
Additionally, the NNGP provides explicit estimates of uncertainty. This may be useful in predicting model failure in critical applications of deep learning, or for active learning tasks where it can be used to identify the best datapoints to hand label.
A DRAWS FROM AN NNGP PRIOR Figure 5 illustrates the nature of the GP prior for the ReLU nonlinearity by depicting samples of 1D functions z(x) drawn from a ReLU GP, GP(0, K L ), with fixed depth L = 10 and (σ 2 w , σ 2 b ) = (1.8, 0.01). Figure 6: The angular structure of the kernel and its evolution with depth. Also illustrated is the good agreement between the kernel computed using the methods of Section 2.5 (blue, starred) and the analytic form of the kernel (red). The depth l in K l runs from l = 0, ..., 9 (flattened curves for increasing l), and (σ 2 w , σ 2 b ) = (1.6, 0.1).
In the main text, we noted that the recurrence relation Equation 5 can be computed analytically for certain nonlinearities. In particular, this was computed in Cho & Saul (2009) for polynomial rectified nonlinearities. For ReLU, the result including the weight and bias variance is
K l (x, x ) = σ 2 b + σ 2 w 2π K l−1 (x, x)K l−1 (x , x ) sin θ l−1 x,x + (π − θ l−1 x,x ) cos θ l−1 x,x , θ l x,x = cos −1 K l (x, x ) K l (x, x)K l (x , x ) . (11
)
To illustrate the angular form of K l (x, x ) and its evolution with l, in Figure 6 we plot K l (θ) for the ReLU nonlinearity, where θ is the angle between x and x with norms such that ||x|| 2 = ||x || 2 = d in . We observe a flattening of the angular structure with increase in depth l, as predicted from the understanding in Section 3.2. Simultaneously, the figure also illustrates the good agreement between the kernel computed using the numerical implementation of Section 2.5 (blue, starred) and the analytic arccosine kernel, Equation 11 (red), for a particular choice of hyperparameters (σ 2 w , σ 2 b ).

[C BAYESIAN MARGINALIZATION OVER INTERMEDIATE LAYERS]
In this section, we present an alternate derivation of the equivalence between infinitely wide deep neural networks and Gaussian process by marginalization over intermediate layers. For this derivation, we take the weight and bias parameters to be drawn from independent Gaussians, with zero mean and appropriately scaled variance.
We are interested in finding the distribution p(z L |x) over network outputs z L ∈ R dout×B , conditioned on network inputs x ∈ R din×B , for input dimensionality d in , output dimensionality d out , and dataset size B. Intervening layers will have width N l , z l ∈ R N l+1 ×B for L > l > 0. We define the second moment matrix (here post-nonlinearity) for each layer l to be
K l ab = 1 din n x na x nb l = 0 1 N l n φ(z l−1 na )φ(z l−1 nb ) l > 0 . (12
)
Our approach is to think of intermediate random variables corresponding to these second moments defined above. By definition, K l only depends on z l−1 . In turn, the pre-activations z l are described by a Gaussian process conditioned on the second moment matrix K l ,
p(z l |K l ) = N vec z l ; 0, G K l ⊗ I =: GP z l ; 0, G K l ,(13)
where
G K l := σ 2 w K l + σ 2 b 11 T .(14)
This correspondence of each layer to a GP, conditioned on the layer's second moment matrix, is exact even for finite width N l because the parameters are drawn from a Gaussian. Altogether, this justifies the graphical model depicted in Figure 7.
We will write p(z L |x) as an integral over all the intervening second moment matrices K l ,
p(z L |x) = p z L , K 0 , K 1 , • • • , K L |x dK 0•••L . (15
)
This joint distribution can be decomposed as
p(z L |x) = p(z L |K L ) L l=1 p(K l |K l−1 ) p(K 0 |x)dK 0•••L .(16)
The directed decomposition in Equation 16 holds because K L is a function only of z L−1 (see Equation 12), z L−1 depends only on K L−1 (see Equation 13), K L−1 is a function only of z L−2 , etc (Figure 7). The sum in Equation 12for l > 0 is a sum over i.i.d. terms. As N l grows large, the Central Limit Theorem applies, and p K l |K l−1 converges to a Gaussian with variance that shrinks as 1 N l . Further, in the infinite width limit it will go to a delta function,
z L K L ... K 1 K 0 z L−1 K L−1 z L−2 x z 0
lim N l →∞ p K l |K l−1 = δ K l − (F • G) K l−1 ,(17)
with F (•) defined as in Equation 5. Similarly, the dependence of K 0 on x can be expressed as a delta function,
p K 0 |x = δ K 0 − 1 d in x T x .(18)
Substituting p(z L |K L ), p K l |K l−1 and p K 0 |x into Equation 16, we get
lim N L →∞,...,N1→∞ p(z L |x) = GP z L ; 0, G K L L l=1 δ K l − (F • G) K l−1 δ K 0 − 1 d in x T x dK 0•••L = GP z L ; 0, G • (F • G) L 1 d in x T x = GP z L ; 0, G • (F • G) L K 0 . (19
)
So, in the limit of infinite width, z L |x is described by a Gaussian process with kernel G • (F • G)
L K 0 .

[D DETAILS OF THE EXPERIMENTS]
We outline details of the experiments for Section 3. For MNIST we use a 50k/10k/10k split of the training/validation/test dataset. For CIFAR-10, we used a 45k/5k/10k split. The validation set was used for choosing the best hyperparameters and evaluation on the test set is reported.
For training neural networks hyperparameters were optimized via random search on average 250 trials for each choice of (n train , depth, width, nonlinearity).
Random search range: Learning rate was sampled within (10 −4 , 0.2) in log-scale, weight decay constant was sampled from (10 −8 , 1.0) in log-scale, σ w ∈ [0.01, 2.5], σ b ∈ [0, 1.5] was uniformly sampled and mini-batch size was chosen equally among [16,32,64,128,256].
For the GP with given depth and nonlinearity, a grid of 30 points evenly spaced from 0.1 to 5.0 (for σ 2 w ) and 30 points evenly spaced from 0 to 2.0 (for σ 2 b ) was evaluated to generate the heatmap. The best GP run was chosen among the 900 evaluations in the σ 2 w -σ 2 b grid. Computation time: We report computation times for NNGP experiments. The grid generation with took 440-460s with 6 CPUs for n g = 501, n v = 501, n c = 500, which was amortized over all the experiments. For full (50k) MNIST, constructing K DD for each layer took 90-140s (depending on CPU generation) running on 64 CPUs. Solving linear equations via Cholesky decomposition took 180-220s for 1000 test points.

[DETAILS OF NNGP IMPLEMENTAION:]
For all the experiments we used pre-computed lookup tables F with n g = 501, n v = 501, n c = 500, and s max = 100. Default value for the target noise σ 2 was set to 10 −10 and was increased by factor of 10 when Cholesky decomposition failed while solving Equation 8 and 9. We refer to Rasmussen & Williams (2006) for standard numerically stable implementation of GP regression.

[E FURTHER RESULTS]
Here we include more results from experiments described in Section 3.
Uncertainty: Relationship between the target MSE and the GP's uncertainty estimate for smaller training set size is shown in Figure 8.
Performance: Performance of grid points of σ 2 w -σ 2 b for varying depth is shown in Figure 9. The best performing NNGP's hyperparameters are distributed near the critical line (Figure 10) where the phase changes as described in Section 3.2. 

[ACKNOWLEDGMENTS]
We thank Ryan Adams, Samy Bengio, and Matt Hoffman for useful discussions and feedback, and Gamaleldin Elsayed and Daniel Levy for helpful comments on the manuscript.","[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency).","{
    ""1"": {
        ""summary"": ""Lack of computational performance details and future work unaddressed."",
        ""verbatim"": ""Although the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article.""
    },
    ""2"": {
        ""summary"": ""Questionable practical utility of the proposed model regime and absence of competitive results."",
        ""verbatim"": ""Although this work introduces a new GP covariance function inspired by deep wide NNs, I am unconvinced of the usefulness of this regime for the cases in which deep learning is useful.""
    },
    ""3"": {
        ""summary"": ""Insufficient exploration of related work and potential misrepresentation of findings' relevance."",
        ""verbatim"": ""This paper misses some obvious connections and references... The very title 'Deep neural networks as Gaussian processes' is misleading, since it’s not really the deep neural networks that we know and love. In fact, you lose valuable structure when you take these limits, and what you get is very different than a standard deep neural network.""
    },
    ""4"": {
        ""summary"": ""Inadequate empirical evaluation and failure to address vital practical applicability to deep learning."",
        ""verbatim"": ""Unfortunately, the empirical evaluation is very preliminary, and provides no reassurance that this approach will have any practical relevance.""
    }
}",4,"{
    ""1"": {
        ""summary"": ""Computational complexity might limit scalability for large-scale datasets or complex architectures."",
        ""verbatim"": ""The computational complexity of computing the covariance matrix for NNGPs, especially for arbitrary nonlinearities, may limit the scalability of the approach for large-scale datasets or more complex architectures.""
    },
    ""2"": {
        ""summary"": ""Paper mainly focuses on regression tasks with insufficient exploration of classification tasks."",
        ""verbatim"": ""The paper focuses mainly on regression tasks, and while classification is briefly mentioned as a future investigation, a more in-depth exploration of classification tasks with NNGPs could strengthen the paper.""
    },
    ""3"": {
        ""summary"": ""Lack of detailed discussion on the limitations and assumptions of the NNGP approach."",
        ""verbatim"": ""The paper lacks detailed discussion on the limitations of the proposed approach, such as the assumptions made or scenarios where NNGPs might not be the best choice.""
    },
    ""4"": {
        ""summary"": ""Insufficient analysis of trade-offs between NNGPs and traditional neural networks in terms of computational efficiency and generalization performance."",
        ""verbatim"": ""While the experiments demonstrate promising results, a deeper analysis of the trade-offs between NNGPs and traditional neural networks in terms of computational efficiency and generalization performance could further augment the findings.""
    }
}",4,"{
    ""A1-B4"": {
        ""rationale"": ""Both reviews express concerns about computational aspects. Review A highlights a lack of reported computational times for computing the covariance matrix, while Review B is concerned with not enough analysis of computational efficiency in comparisons between NNGPs and traditional neural networks."",
        ""similarity"": ""6""
    },
    ""A2-B3"": {
        ""rationale"": ""Both points question the practical utility and discuss the limitations of the proposed model or approach. Review A is unconvinced about the usefulness of the new GP covariance function regime, while Review B notes a lack of detailed discussion on limitations of the NNGP approach."",
        ""similarity"": ""6""
    },
    ""A4-B4"": {
        ""rationale"": ""Both points critique the empirical evaluation and practical applicability of the approaches discussed. Review A mentions that the empirical evaluation is preliminary and offers no reassurance about practical relevance, similar to Review B's call for a deeper analysis of trade-offs in computational efficiency and generalization performance."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper needs more detailed experimental results and comparisons."",
        ""verbatim"": ""The paper could be enhanced by providing more detailed experimental results and comparisons with existing methods or approaches to further validate the efficacy of the proposed framework.""
    },
    ""2"": {
        ""summary"": ""Additional empirical evidence is needed to strengthen practical significance."",
        ""verbatim"": ""While the theoretical equivalence between deep networks and GPs is established, additional empirical evidence or real-world applications could strengthen the practical significance and generalizability of the findings.""
    },
    ""3"": {
        ""summary"": ""A more thorough discussion on limitations and challenges is necessary."",
        ""verbatim"": ""The paper could benefit from a more thorough discussion on the limitations or potential challenges associated with implementing the proposed approach in practical deep learning scenarios.""
    }
}",3,"{
    ""A2-B2"": {
        ""rationale"": ""Both points raise concerns about the practical utility of the model and the lack of empirical evidence to support its significance in real-world applications."",
        ""similarity"": ""8""
    },
    ""A4-B1"": {
        ""rationale"": ""Both points criticize the insufficient empirical evaluation and suggest a need for more detailed experimental results to support the validity of the model."",
        ""similarity"": ""8""
    },
    ""A4-B2"": {
        ""rationale"": ""Both comments highlight the issues with insufficient empirical evidence and the need to demonstrate real-world relevance and practical applicability of the theoretical findings."",
        ""similarity"": ""8""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The paper's limited use of datasets"",
        ""verbatim"": ""The paper's reliance on MNIST and CIFAR-10 datasets for experimentation may limit the generalizability of the findings and could be a potential reason for rejection.""
    },
    ""2"": {
        ""summary"": ""Lack of thorough comparative analysis"",
        ""verbatim"": ""The paper could benefit from a more in-depth comparative analysis between GP predictions and finite-width network predictions to strengthen the conclusions drawn from the experiments.""
    }
}",2,"{
    ""A4-B1"": {
        ""rationale"": ""Both Review A4 and Review B1 express concerns about the limited practical relevance of the approach under discussion. Review A4 criticizes the preliminary nature of the empirical evaluation and its failure to assure practical relevance, while Review B1 notes the limited use of datasets which may impact the generalizability and practical applicability of the findings."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Concern about the impracticality of infinite width assumption in neural networks."",
        ""verbatim"": ""Assumption of Infinite Width: The practical implications of the theoretical findings may be limited, as the equivalence holds under the assumption of infinite width, which is not achievable in practical scenarios.""
    },
    ""2"": {
        ""summary"": ""Complexity of the methods could make them inaccessible to researchers unfamiliar with the theoretical background."",
        ""verbatim"": ""Complexity and Accessibility of Methods: The methods described are complex and might be inaccessible to those not already familiar with Gaussian processes and deep neural network theoretical underpinnings.""
    },
    ""3"": {
        ""summary"": ""Limited validation tests on only two benchmark datasets may not sufficiently demonstrate generalizability."",
        ""verbatim"": ""Lack of Broader Validation: While tested on two benchmark datasets, broader evaluations across more varied tasks could strengthen the case for generalizability.""
    }
}",3,"{
    ""A2-B1"": {
        ""rationale"": ""Both points discuss the limitations of the models proposed in the research due to unrealistic assumptions. Review A criticizes the practical utility of the model inspired by deep wide NNs for cases where deep learning is essential, questioning its usefulness. Review B addresses the impracticality of the infinite width assumption in neural networks, highlighting that such theoretical findings are limited in practical scenarios."",
        ""similarity"": ""7""
    },
    ""A4-B3"": {
        ""rationale"": ""Both points raise concerns about the sufficiency of empirical validations to ensure the practical applicability and generalizability of the methods. Review A notes the empirical evaluation is preliminary and does not reassure its practical relevance. Review B comments on the limited validation tests on only two benchmark datasets, suggesting that broader evaluations are necessary to affirm the generalizability."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper may not generalize to architectures other than fully-connected networks."",
        ""verbatim"": ""'Generalization to Different Architectures': The study focuses on fully-connected networks, and it is unclear whether the results generalize to other popular architectures like convolutional neural networks or recurrent neural networks.""
    },
    ""2"": {
        ""summary"": ""Scalability of the computational methods for very large datasets or deep networks is questionable."",
        ""verbatim"": ""'Scalability Concerns': While the computational tools developed are a step forward, the scalability of these methods to extremely large datasets or very deep networks may still be in question, which could limit the practical utility of the findings.""
    },
    ""3"": {
        ""summary"": ""Lacks detailed comparison with current state-of-the-art methods."",
        ""verbatim"": ""'Comparison with State-of-the-art': The paper lacks a detailed comparison with state-of-the-art Bayesian methods or other neural network approaches, which might make it difficult for readers to gauge the relative improvement offered by the proposed method.""
    }
}",3,"{
    ""A2-B2"": {
        ""rationale"": ""Both points discuss the practical utility and applicability of the computational methods introduced in their respective papers. Review A is critical of the model regime's usefulness in practical situations where deep learning is applicable, while Review B questions the scalability of these methods for large datasets or deep networks, which directly impacts their practical utility."",
        ""similarity"": ""7""
    },
    ""A4-B2"": {
        ""rationale"": ""Both points express concerns about the practical implications and relevance of the research methods being presented. Review A is unsatisfied with the empirical evaluation and its implications for practical relevance, while Review B similarly doubts the scalability and practical utility of the computational methods for large-scale applications."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Computational demands may limit practical applicability"",
        ""verbatim"": ""The computational pipeline for calculating the covariance functions, while efficient within the scope of the paper, may still be prohibitive for broader adoption without further optimization.""
    },
    ""2"": {
        ""summary"": ""Focus on specific model configurations might limit generalizability"",
        ""verbatim"": ""The focus on infinitely wide networks might limit the generalizability of the findings.""
    },
    ""3"": {
        ""summary"": ""Limited exploration of network structures"",
        ""verbatim"": ""The deep network structures explored may not cover the variety and configurations employed in current deep learning applications, which may affect the acceptance.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A1 and Review B1 discuss the computational aspects of the approach being studied; however, A1 focuses more on the absence of reported computational times, whereas B1 emphasizes the potential practical limitations due to the computational demands. Both suggest a need for optimizing computational procedures."",
        ""similarity"": ""7""
    },
    ""A4-B1"": {
        ""rationale"": ""Review A4 and Review B1 both discuss the practical relevance and applicability of the approach, with A4 questioning the empirical evaluation's adequacy and its practical relevance, and B1 discussing computational demands as a barrier to practical application. Both express concerns over the scalability or broad application of the methods in practical scenarios."",
        ""similarity"": ""7""
    },
    ""A3-B3"": {
        ""rationale"": ""Both Review A3 and Review B3 address issues related to the scope and relevance of the network structures explored in the study. A3 criticizes the paper for misrepresenting how applicable the model is to conventional deep neural networks, mentioning the loss of valuable structure. B3 comments on the deep network structures not covering the variety used in current applications, which could affect acceptance. Both points highlight concerns regarding the model's representation and its divergence from traditional deep learning setups."",
        ""similarity"": ""7""
    }
}",3,"[Significance and novelty]

1. Extension of Gaussian Processes (GPs) to Deep Neural Networks (DNNs): The paper establishes the equivalence between infinitely wide DNNs and GPs for the first time, enabling Bayesian prediction with a deep neural network.
2. Using Kernel Functions for Deep GPs: The novelty lies in exploiting existing kernel functions to represent deep GPs and enabling fully Bayesian prediction with DNNs.
3. Connection to Signal Propagation in Random Neural Networks: The paper discusses the link between deep GP performance and signal propagation in random neural networks which is yet to be explored in the literature.

[Potential reasons for acceptance]
1. Novel theoretical result: The paper presents a new, theoretically grounded approach to deep learning, which builds upon the understanding of GPs and their equivalence to wide neural networks.
2. Applicability to Regression Tasks: By enabling exact Bayesian inference, the method can be useful for regression tasks, which form a significant portion of machine learning problems.
3. Computational Efficiency: The proposed pipeline to compute the covariance function for deep GPs is computationally efficient and can contribute to a faster implementation and training process.

[Potential reasons for rejection]
1. Limited Datasets: The paper restricts its application to MNIST and CIFAR-10 datasets. Extending the study to larger-scale and more complex datasets would demonstrate the generalizability of the method and strengthen the validity of the results.
2. Limited Exploration of Kernel Functions: The paper focuses on kernel functions that mimic multi-layer random neural networks. A more thorough exploration of other kernel functions and their impact on deep GP performance would add to the depth of the study.
3. Limited Comparisons: The study does not compare the deep GP method with other state-of-the-art methods directly. Adding such comparisons would help assess the competitiveness and strengths of this approach.

[Suggestions for improvement]
1. Extended Comparisons: Add comparisons with other state-of-the-art methods to evaluate the performance and advantages of the proposed method.
2. Analysis of Different Kernel Functions: Explore the impact of various kernel functions on deep GP performance and their suitability for different types of data.
3. Application to other Machine Learning Tasks: Apply the proposed method to other machine learning tasks, such as classification, and evaluate its utility for those tasks.
4. Large-scale Datasets: Test the deep GP method on large-scale real-world datasets to assess its scalability and demonstrate its applicability in real-world scenarios.","[Significance and novelty]
<Equivalence between deep networks and Gaussian processes> The paper establishes the equivalence between infinitely wide deep networks and Gaussian processes, contributing to the understanding of deep learning models.
<Connection to deep kernel learning> The work explores the connection between deep kernel learning and mean field Gaussian processes, which is a novel and significant contribution to the field.

[Potential reasons for acceptance]
<Technically solid and well-written paper> Reviewers find the paper to be technically solid, with well-written content that demonstrates potential for high impact in multiple areas.
<Experimental analysis and comparison> The paper provides an extensive experimental analysis and compares the methods with previous works, demonstrating the significance of its contributions.
<Novelty and impact in the field> The reviewers recognize the potential impact of the paper's findings on the understanding of deep neural networks and Gaussian processes, which may contribute to future research and applications in the field.

[Potential reasons for rejection]
<No related terms> -

[Suggestions for improvement]
<Clarification on the motivation and significance of results> Address the need for better explanation regarding the motivation for mean field Gaussian processes and further elucidation on the significance of the results, especially in comparison to finite width networks.
<Improvement in experimental evaluations> Enhance experimental evaluations and comparisons to address the limitations noted by the reviewers, aiming to strengthen the empirical evidence to support the theoretical claims.
<Enhanced interpretation of results> Provide better interpretation and analysis of the experimental results to enhance understanding and relevance of the findings.

[Title of supporting points]
<Motivation and significance of mean field Gaussian processes>
<Experimental evaluations and limitations>
<Enhanced interpretation of results>

[Detailed supporting points]
<Clarify motivation for mean field Gaussian processes> Explain the motivation for considering mean field Gaussian processes and how it impacts the understanding of deep learning models.
<Address limitations in experimental evaluations> Enhance experimental evaluations and comparisons to provide a more comprehensive assessment of the methods and results.
<Improve interpretation of experimental results> Provide clearer interpretation and analysis of the experimental results to increase the relevance and significance of the findings.

","{
    ""1"": {
        ""summary"": ""Restricted use of datasets potentially limits the generalizability and validity of the study."",
        ""verbatim"": ""Limited Datasets: The paper restricts its application to MNIST and CIFAR-10 datasets. Extending the study to larger-scale and more complex datasets would demonstrate the generalizability of the method and strengthen the validity of the results.""
    },
    ""2"": {
        ""summary"": ""Focus on specific kernel functions may undermine the depth and applicability of the study to different scenarios."",
        ""verbatim"": ""Limited Exploration of Kernel Functions: The paper focuses on kernel functions that mimic multi-layer random neural networks. A more thorough exploration of other kernel functions and their impact on deep GP performance would add to the depth of the study.""
    },
    ""3"": {
        ""summary"": ""Lack of comparative analysis with other state-of-the-art methods hampers assessment of the proposed method's competitiveness and strengths."",
        ""verbatim"": ""Limited Comparisons: The study does not compare the deep GP method with other state-of-the-art methods directly. Adding such comparisons would help assess the competitiveness and strengths of this approach.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Need for better explanation of motivation for mean field Gaussian processes and significance of results."",
        ""verbatim"": ""Clarification on the motivation and significance of results> Address the need for better explanation regarding the motivation for mean field Gaussian processes and further elucidation on the significance of the results, especially in comparison to finite width networks.""
    },
    ""2"": {
        ""summary"": ""Necessity to enhance experimental evaluations and comparisons to strengthen empirical support for theoretical claims."",
        ""verbatim"": ""Improvement in experimental evaluations> Enhance experimental evaluations and comparisons to address the limitations noted by the reviewers, aiming to strengthen the empirical evidence to support the theoretical claims.""
    },
    ""3"": {
        ""summary"": ""Requirement for better interpretation and analysis of experimental results."",
        ""verbatim"": ""Enhanced interpretation of results> Provide better interpretation and analysis of the experimental results to enhance understanding and relevance of the findings.""
    }
}",3,"{
    ""A2-B3"": {
        ""rationale"": ""Both points focus on the inability of the study to validate or compete with existing methods due to a lack of comparative analysis. Review A expresses doubts about the practical utility of the proposed model because it does not convincingly address how it competes with deep learning in useful scenarios. Review B similarly criticizes the paper for not comparing the deep GP method with other state-of-the-art approaches, which is essential for assessing its competitiveness and strengths."",
        ""similarity"": ""7""
    },
    ""A4-B3"": {
        ""rationale"": ""Both points express concerns regarding the inadequate validation of the proposed model's relevance and competitiveness. In Review A, it is stated that the empirical evaluation is preliminary and fails to provide reassurance of practical relevance. Review B echoes this concern, noting the absence of comparative analysis with state-of-the-art methods, which limits understanding of the proposed method's strengths and competitiveness."",
        ""similarity"": ""8""
    }
}",2,"{
    ""A1-B3"": {
        ""rationale"": ""Both Review A point 1 and Review B point 3 highlight the absence or inadequacy of specifics related to computational aspects in two different contexts; Review A focuses on the lack of reported computational times for calculating the covariance matrix, while Review B points out the need for better interpretation of experimental results which include computational evaluations."",
        ""similarity"": ""6""
    },
    ""A2-B2"": {
        ""rationale"": ""Both reviews express concerns over the lack of thorough empirical evaluation. Review A discusses it in the context of a new proposed model, while Review B focuses on enhancing experimental evaluations to back theoretical claims. Both stress the need for stronger empirical support."",
        ""similarity"": ""7""
    },
    ""A4-B2"": {
        ""rationale"": ""Review A point 4 and Review B point 2 both criticize the preliminary nature of empirical evaluations and stress the need for improved experimental and empirical assessments to validate the practical applications of the theoretical concepts discussed."",
        ""similarity"": ""8""
    }
}",2
PseudoSeg: Designing Pseudo Labels for Semantic Segmentation,-TwO99rbVRu.json,"[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Overview of unlabeled data training branch. Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss.
Table 2: ing, as suggested by Sohn et al. (2020a); Zoph et al. (2020); Xie et al. (2020); Sohn et al. (2020b).
Table 3: Figure 3 :3Figure 3: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ unlabeled data) on VOC12 val (left) and COCO val (right).
Table 4: Figure 4 :4Figure 4: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ image-level labeled data) on VOC12 val (left) and COCO val (right).
Table 5: Ablation studies on different factors. See Section 4.3 for complete details.
Table 6: +L u +L x ) Decoder (L s +L u ) Classifier (L x ) Self-attention (L sa ) L s : pixel-level labeled data L u : unlabeled/image-level labeled data L x : pixel-level labeled data (converting to image-level) or image-level labeled data L sa : pixel-level labeled data
Table 7: Figure 7 :7Figure 7: Training. For each network component, we show the loss supervision and the corresponding data.
Table 8: Directly applying consistency training approaches validated in image classification renders particular challenges in segmentation. We first demonstrate how well-calibrated soft pseudo labels obtained through wise fusion of predictions from diverse sources can greatly improve consistency training for segmentation. • We conduct extensive experimental studies on the PASCAL VOC 2012 and COCO datasets.
Table 9: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data). We use the official training set (1.4k) as labeled data, and the augmented set (9k) as unlabeled data.
Table 10: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data) using low-data splits. The exact numbers of pixel-labeled images are shown in brackets. All the methods use ResNet-101 as backbone except CCT(Ouali et al.
Table 11: Comparison  
Table 12: Comparison with state of the
Table 13: Comparison to alternative pseudo labeling strategies. We conduct experiments using 1/4, 1/8, 1/16 of the pixel-level labeled data, the exact numbers of images are shown in the brackets.
Table 14: Comparison with self-training. We use our supervised baseline as the teacher to generate one-hot pseudo labels, followingZoph et al. (2020).
Table 15: Improving fully supervised model with extra data. No test-time augmentation is used.
Table 16: Full results of 1/16 split in VOC12.
Table 17: Benchmarking state-of-the-art weakly supervised semantic segmentation methods. All the methods use image-level labels from VOC12 training (1.4k) and augmented (9k) sets.
Table 18: Performance analysis over T.
Table 19: Experiments on Cityscapes (w/ pixel-level labeled data and unlabeled data).
Table 20: Per-class performance analysis on Cityscapes (w/ pixel-level labeled data and unlabeled data).

[INTRODUCTION]
Image semantic segmentation is a core computer vision task that has been studied for decades. Compared with other vision tasks, such as image classification and object detection, human annotation of pixel-accurate segmentation is dramatically more expensive. Given sufficient pixellevel labeled training data (i.e., high-data regime), the current state-of-the-art segmentation models (e.g., DeepLabv3+ (Chen et al., 2018)) produce satisfactory segmentation prediction for common practical usage. Recent exploration demonstrates improvement over high-data regime settings with large-scale data, including self-training (Chen et al., 2020a;Zoph et al., 2020) and backbone pretraining (Zhang et al., 2020a).
In contrast to the high-data regime, the performance of segmentation models drop significantly, given very limited pixel-labeled data (i.e., low-data regime). Such ineffectiveness at the low-data regime hinders the applicability of segmentation models. Therefore, instead of improving high-data regime segmentation, our work focuses on data-efficient segmentation training that only relies on few pixellabeled data and leverages the availability of extra unlabeled or weakly annotated (e.g., image-level) data to improve performance, with the aim of narrowing the gap to the supervised models trained with fully pixel-labeled data.
Our work is inspired by the recent success in semi-supervised learning (SSL) for image classification, demonstrating promising performance given very limited labeled data and a sufficient amount of unlabeled data. Successful examples include MeanTeacher (Tarvainen & Valpola, 2017), UDA (Xie et al., 2019), MixMatch (Berthelot et al., 2019b), FeatMatch (Kuo et al., 2020), and FixMatch (Sohn et al., 2020a). One outstanding idea in this type of SSL is consistency training: making predictions consistent among multiple augmented images. FixMatch (Sohn et al., 2020a) shows that using high-confidence one-hot pseudo labels obtained from weakly-augmented unlabeled data to train strongly-augmented counterpart is the key to the success of SSL in image classification.
However, effective pseudo labels and well-designed data augmentation are non-trivial to satisfy for semantic segmentation. Although we observe that many related works explore the second condition (i.e., augmentation) for image segmentation to enable consistency training framework (French et al., 2020;Ouali et al., 2020), we show that a wise design of pseudo labels for segmentation has great veiled potentials.
In this paper, we propose PseudoSeg, a one-stage training framework to improve image semantic segmentation by leveraging additional data either with image-level labels (weakly-labeled data) or without any labels. PseudoSeg presents a novel design of pseudo-labeling to infer effective structured pseudo labels of additional data. It then optimizes the prediction of strongly-augmented data to match its corresponding pseudo labels. In summary, we make the following contributions:
• We propose a simple one-stage framework to improve semantic segmentation by using a limited amount of pixel-labeled data and sufficient unlabeled data or image-level labeled data. Our framework is simple to apply and therefore network architecture agnostic. 

[RELATED WORK]
Semi-supervised classification. Semi-supervised learning (SSL) aims to improve model performance by incorporating a large amount of unlabeled data during training. Consistency regularization and entropy minimization are two common strategies for SSL. The intuition behind consistencybased approaches (Laine & Aila, 2016;Sajjadi et al., 2016;Miyato et al., 2018;Tarvainen & Valpola, 2017) is that, the model output should remain unchanged when the input is perturbed. On the other hand, the entropy minimization strategy (Grandvalet & Bengio, 2005) argues that the unlabeled data can be used to ensured classes are well-separated, which can be achieved by encouraging the model to output low-entropy predictions. Pseudo-labeling (Lee, 2013) is one of the methods for implicit entropy minimization. Recently, holistic approaches (Berthelot et al., 2019b;a;Sohn et al., 2020a) combining both strategies have been proposed and achieved significant improvement. By redesigning the pseudo label, we propose an efficient one-stage semi-supervised learning framework of semantic segmentation for consistency training. Semi-supervised semantic segmentation. Collecting pixel-level annotations for semantic segmentation is costly and prone to error. Hence, leveraging unlabeled data in semantic segmentation is a natural fit. Early methods utilize a GAN-based model either to generate additional training data (Souly et al., 2017) or to learn a discriminator between the prediction and the ground truth mask (Hung et al., 2018;Mittal et al., 2019). Consistency regularization based approaches have also been proposed recently, by enforcing the predictions to be consistent, either from augmented input images (French et al., 2020;Kim et al., 2020), perturbed feature embeddings (Ouali et al., 2020), or different networks (Ke et al., 2020). Recently, Luo & Yang (2020) proposes a dual-branch training network to jointly learn from pixel-accurate and coarse labeled data, achieving good segmentation performance. To push the performance of state of the arts, iterative self-training approaches (Chen et al., 2020a;Zoph et al., 2020;Zhu et al., 2020) have been proposed. These methods usually assume the available labeled data is enough to train a good teacher model, which will be used to generate pseudo labels for the student model. However, this condition might not satisfy in the low-data regime. Our proposed method, on the other hand, realizing the ideas of both consistency regularization and pseudo-labeling in segmentation, consistently improves the supervised baseline in both low-data and high-data regimes.
Weakly-supervised semantic segmentation. Instead of supervising network training with accurate pixel-level labels, many prior works exploit weaker forms of annotations (e.g., bounding boxes (Dai et al., 2015), scribbles (Lin et al., 2016), image-level labels). Most recent approaches use imagelevel labels as the supervisory signal, which exploits the idea of class activation map (CAM) (Zhou et al., 2016). Since the vanilla CAM only focus on the most discriminative region of objects, dif- Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss. ferent ways to refine CAM have been proposed, including partial image/feature erasing (Hou et al., 2018;Wei et al., 2017;Li et al., 2018), using an additional saliency estimation model (Oh et al., 2017;Wei et al., 2018), utilizing pixel similarity to propagate the initial score map (Ahn & Kwak, 2018;, or mining and co-segment the same category of objects across images (Sun et al., 2020;. While achieving promising results using the approaches mentioned above, most of them require a multi-stage training strategy. The refined score maps are optimized again using a dense-CRF model (Krähenbühl & Koltun, 2011), and then used as the target to train a separate segmentation network. On the other hand, we assume there exists a small number of fully-annotated data, which allows us to learn stronger segmentation models than general methods without needing pixel-labeled data.

[THE PROPOSED METHOD]
In analogous to SSL for classification, our training objective in PseudoSeg consists of a supervised loss L s applied to pixel-level labeled data D l , and a consistency constraint L u applied to unlabeled data D u 1 . Specifically, the supervised loss L s is the standard pixel-wise cross-entropy loss on the weakly augmented pixel-level labeled examples:
L s = 1 N × |D l | x∈D l N −1 i=0 CrossEntropy (y i , f θ (ω(x i ))) ,(1)
where θ represents the learnable parameters of the network function f and N denotes the number of valid labeled pixels in an image x ∈ R H×W ×3 . y i ∈ R C is the ground truth label of a pixel i in H×W dimensions, and f θ (ω(x i )) ∈ R C is the predicted probability of pixel i, where C is the number of classes to predict and ω(•) denotes the weak (common) data augmentation operations used by Chen et al. (2018).
During training, the proposed PseudoSeg estimates a pseudo label y ∈ R H×W ×C for each stronglyaugmented unlabeled data x in D u , which is then used for computing the cross-entropy loss. The unsupervised objective can then be written as:
L u = 1 N × |D u | x∈Du N −1 i=0 CrossEntropy ( y i , f θ (β • ω(x i ))) ,(2)
where β(•) denotes a stronger data augmentation operation, which will be described in Section 3.2. We illustrate the unlabeled data training branch in Figure 1.

[THE DESIGN OF STRUCTURED PSEUDO LABELS]
The next important question is how to generate the desirable pseudo label y. A straightforward solution is directly using the decoder output of a trained segmentation model after confidence threshold- However, as we demonstrate later in the experiments, the generated pseudo hard/soft labels as well as other post-processing of outputs are barely satisfactory in the low-data regime, and thus yield inferior final results. To address this issue, our design of pseudo-labeling has two key insights. First, we seek for a distinct yet efficient decision mechanisms to compensate for the potential errors of decoder outputs. Second, wisely fusing multiple sources of predictions to generate an ensemble and better-calibrated version of pseudo labels.
Starting with localization. Compared with precise segmentation, learning localization is a simpler task as it only needs to provide coarser-grained outputs than pixel level of objects in images. Based on this motivation, we improve decoder predictions from the localization perspective. Class activation map (CAM) (Zhou et al., 2016) is a popular approach to provide localization for class-specific regions. CAM-based methods (Hou et al., 2018;Wei et al., 2017;Ahn & Kwak, 2018) have been successfully adopted to tackle a different weakly supervised semantic segmentation task from us, where they assume only image-level labels are available. In practice, we adopt a variant of class activation map, Grad-CAM (Selvaraju et al., 2017) in PseudoSeg.
From localization to segmentation. CAM estimates the strength of classifier responses on local feature maps. Thus, an inherent limitation of CAM-based approaches is that it is prone to attending only to the most discriminative regions. Although many weakly-supervised segmentation approaches (Ahn & Kwak, 2018;Ahn et al., 2019;Sun et al., 2020) aim at refining CAM localization maps to segmentation masks, most of them have complicated post-processing steps, such as dense CRF (Krähenbühl & Koltun, 2011), which increases the model complexity when used for consistency training. Here we present a computationally efficient yet effective refinement alternative, which is learnable using available pixel-labeled data.
Although CAM only localizes partial regions of interests, if we know the pairwise similarities between regions, we can propagate the CAM scores from the discriminative regions to the rest unattended regions. Actually, it has been shown in many works that the learned high-level deep features are usually good at similarity measurements of visual objects. In this paper, we find hypercolumn (Hariharan et al., 2015) with a learnable similarity measure function works fairly effective.
Given the vanilla Grad-CAM output for all C classes, which can be viewed as a spatially-flatten 2-D vector of weight m ∈ R L×C , where each row m i is the response weight per class for one region i. Using a kernel function K(•, •) : R H ×R H → R that measures element-wise similarity given feature h ∈ R H of two regions, the propagated scorem i ∈ R C can be computed as followŝ
m i =   m i + L−1 j=0 e K(W k hi,Wvhj ) L−1 k=0 e K(W k hi,Wvh k ) m j   • W c .(3)
The goal of this function is to train Θ = {W k , W v ∈ R H×H , W c ∈ R C×C } in order to propagate the high value in m to all adjacent elements in the feature space R H (i.e., hypercolumn features) to region i. Adding m i in equation 3 indicates the skip-connection. To compute propagated score for all regions, the operations in equation 3 can be efficiently implemented with self-attention dotproduct (Vaswani et al., 2017). For brevity, we denote this efficient refinement process output as selfattention Grad-CAM (SGC) maps in R H×H×C . Figure 6 in Appendix A specifies the architecture. Calibrated prediction fusion. SGC maps are obtained from low-resolution feature maps. It is then resized to the desired output resolution, and thus not sufficient at delineating crisp boundaries. However, compared to the segmentation decoder, SGC is capable of generating more locally-consistent masks. Thus, we propose a novel calibrated fusion strategy to take advantage of both decoder and SCG predictions for better pseudo labels.
Specifically, given a batch of decoder outputs (pre-softmax logits)p = f θ (ω(x)) and SGC mapsm computed from weakly-augmented data ω(x), we generate the pseudo labels y by
F(p,m) = Sharpen γ Softmax p Norm(p,m) + (1 − γ) Softmax m Norm(p,m) , T . (4)
Two critical procedures are proposed to use here to make the fusion process successful. First, p andm are from different decision mechanisms and they could have very different degrees of overconfidence. Therefore, we introduce the operation Norm(a, b)
= |a| i (a 2 i + b 2 i ) as a nor- Input Grad-CAM SGC map Decoder Decoder (strong) Pseudo label
Figure 2: Visualization of pseudo labels and other predictions. The generated pseudo label by fusing the predictions from the decoder and SGC map is used to supervise the decoder (strong) predictions of the strongly-augmented counterpart. malization factor. It alleviates the over-confident probability after softmax, which could unfavorably dominate the resulted γ-averaged probability. Second, the distribution sharpening operation Sharpen(a, T
) i = a 1/T i / C j a 1/T j
adjusts the temperature scalar T of categorical distribution (Berthelot et al., 2019b;Chen et al., 2020b). Figure 2 illustrates the predictions from different sources. More importantly, we investigate the pseudo-labeling from a calibration perspective (Section 4.3), demonstrating that the proposed soft pseudo label y leads to a better calibration metric comparing to other possible fusion alternatives, and justifying why it benefits the final segmentation performance. Training. Our final training objective contains two extra losses: a classification loss L x , and a segmentation loss L sa . First, to compute Grad-CAM, we add a one-layer classification head after the segmentation backbone and a multi-label classification loss L x . Second, as specified in Appendix A (Figure 6), SGC maps are scaled as pixel-wise probabilities using one-layer convolution followed by softmax in equation 3. Learning Θ to predict SGC maps needs pixel-labeled data D l . It is achieved by an extra segmentation loss L sa between SGC maps of pixel-labeled data and corresponding ground truth. All the loss terms are jointly optimized (i.e., L u + L s + L x + L sa ), while L sa only optimizes Θ (achieved by stopping gradient). See Figure 7 in the appendix for further details.

[INCORPORATING IMAGE-LEVEL LABELS AND AUGMENTATION]
The proposed PseudoSeg can easily incorporate image-level label information (if available) into our one-stage training framework, which also leads to consistent improvement as we demonstrate in experiments. We utilize the image-level data with two following steps. First, we directly use ground truth image-level labels to generate Grad-CAMs instead of using classifier outputs. Second, they are used to increase classification supervision beyond pixel-level labels for the classifier head.
For strong data augmentation, we simply follow color jittering operations from SimCLR (Chen et al., 2020b) and remove all geometric transformations. The overall strength of augmentation can be controlled by a scalar (studied in experiments). We also apply once random CutOut (DeVries & Taylor, 2017) with a region of 50 × 50 pixels since we find it gives consistent though minor improvement (pixels inside CutOut regions are ignored in computing losses).

[EXPERIMENTAL RESULTS]
We start by specifying the experimental details. Then, we evaluate the method in the settings of using pixel-level labeled data and unlabeled data, as well as using pixel-level labeled data and image-level labeled data, respectively. Next, we conduct various ablation studies to justify our design choices. Lastly, we conduct more comparative experiments in specific settings.
To evaluate the proposed method, we conduct the main experiments and ablation studies on the PAS-CAL VOC 2012 dataset (VOC12) (Everingham et al., 2015), which contains 21 classes including background. The standard VOC12 dataset has 1,449 images as the training set and 1,456 images as the validation set. We randomly subsample 1/2, 1/4, 1/8, and 1/16 of images in the standard training set to construct the pixel-level labeled data. The remaining images in the standard training set, together with the images in the augmented set (Hariharan et al., 2011) (around 9k images), are used as unlabeled or image-level labeled data. To further verify the effectiveness of the proposed method, we also conduct experiments on the COCO dataset (Lin et al., 2014). The COCO dataset has 118,287 images as the training set, and 5,000 images as the validation set. We evaluate on the 80 foreground classes and the background, as in the object detection task. As the COCO dataset is larger than VOC12, we randomly subsample smaller ratios, 1/32, 1/64, 1/128, 1/256, 1/512, of images from the training set to construct the pixel-level labeled data. The remaining images in the training set are used as unlabeled data or image-level labeled data. We evaluate the performance using the standard mean intersection-over-union (mIoU) metric. Implementation details can be found in Appendix B.

[EXPERIMENTS USING PIXEL-LEVEL LABELED DATA AND UNLABELED DATA]
Improvement over a strong baseline. We first demonstrate the effectiveness of the proposed method by comparing it with the DeepLabv3+ model trained with only the pixel-level labeled data. As shown in Figure 3 (a), the proposed method consistently outperforms the supervised training baseline on VOC12, by utilizing the pixel-level labeled data and the unlabeled data. The proposed method not only achieves a large performance boost in the low-data regime (when only 6.25% pixellevel labels available), but also improves the performance when the entire training set (1.4k images) is available. In Figure 3 (b), we again observe consistent improvement on the COCO dataset.
Comparisons with the others. Next, we compare the proposed method with recent state of the arts on both the public 1.4k/9k split (in Table 1) and the created low-data splits (in Table 2), on VOC12.
Our method compares favorably with the others.      Similar to semi-supervised learning using pixel-level labeled data and unlabeled data, we first demonstrate the efficacy of our method by comparing it with a strong supervised baseline. As shown in Figure 4, the proposed method consistently improves the strong baseline on both datasets.
In Table 3, we evaluate on the public 1.4k/9k split. The proposed method compares favorably with the other methods. Moreover, we further compare to best compared CCT on the created low-data splits (in Table 4). Both experiments show that the proposed PseudoSeg is more robust than the compared method given less data. On all splits on both datasets, using pixel-level labeled data and image-labeled data shows higher mIoU than the setting using pixel-level labeled data and unlabeled data.

[ABLATION STUDY]
In this section, we conduct extensive ablation experiments on VOC12 to validate our design choices. How to construct pseudo label? We investigate the effectiveness of the proposed pseudo labeling. Table 5 demonstrates quantitative results, indicating that using either decoder output or SGC alone gives an inferior performance. Naively using decoder output as pseudo labels can hardly work well. The proposed fusion consistently performs better, either with or without additional image-level labels. To further answer why our pseudo labels are effective, we study from the model calibration perspective. We measure the expected calibration error (ECE) (Guo et al., 2017) scores of all the intermediate steps and other fusion variants. As shown in Figure 5 (a), the proposed fusion strategy (denoted as G in the figure) achieves the lowest ECE scores, indicating that the significance of jointly using normalization with sharpening (see equation 4) compared with other fusion alternatives. We hypothesize using well-calibrated soft labels makes model training less affected by label noises. The comprehensive calibration study is left as a future exploration direction. Using hypercolumn feature or not? In Figure 5 (b), we study the effectiveness of using hypercolumn features instead of the last feature maps in equation 3. We conduct the experiments on the 1/16 split of VOC12. As we can see, hypercolumn features substantially improve performance. Soft or hard pseudo label? How to utilize predictions as pseudo labels remains an active question in SSL. Next, we study whether we should use soft or hard one-hot pseudo labels. We conduct  the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (c), using all predictions as soft pseudo label yields better performance than selecting confident predictions. This suggests that well-calibrated soft pseudo labels might be important in segmentation than over-simplified confidence thresholding. Temperature sharpening or not? We study the effect of temperature sharpening in equation 4. We conduct the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (d), temperature sharpening shows consistent and clear improvements. Strong augmentation strength. In Figure 5 (e), we study the effects of color jittering in the strong augmentation. The magnitude of jittering strength is controlled by a scalar (Chen et al., 2020b). We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. If the magnitude is too small, performance drops significantly, suggesting the importance of strong augmentation. Impact of different feature backbones. In Figure 5 (f), we compare the performance of using ResNet-50, ResNet-101, and Xception-65 as backbone architectures, respectively. We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. As we can see, the proposed method consistently improves the baseline by a substantial margin across different backbone architectures.

[COMPARISON WITH SELF-TRAINING]
Several recent approaches (Chen et al., 2020a;Zoph et al., 2020) exploit the Student-Teacher selftraining idea to improve the performance with additional unlabeled data. However, these methods only apply self-training in the high-data regime (i.e., sufficient pixel-labeled data to train teachers). Here we compare these methods in the low-data regimes, where we focus on. To generate offline pseudo labels, we closely follow segmentation experiments in Zoph et al. (2020): pixels with a confidence score higher than 0.5 will be used as one-hot pseudo labels, while the remaining are treated as ignored regions. This step is considered important to suppress noisy labels. A student model is then trained using the combination of unlabeled data in VOC12 train and augmented sets with generated one-hot pseudo labels and all the available pixel-level labeled data. As shown in Table 6, although the self-training pretty well improves over the supervised baseline, it is inferior to the proposed method 2 . We conjecture that the teacher model usually produces low confidence scores to pixels around boundaries, so pseudo labels of these pixels are filtered in student training. However, boundary pixels are important for improving the performance of segmentation (Kirillov et al., 2020). On the other hand, the design of our method (online soft pseudo labeling process) bypass this challenge. We will conduct more verification of this hypothesis in future work.

[IMPROVING THE FULLY-SUPERVISED METHOD WITH ADDITIONAL DATA]
We have validated the effectiveness of the proposed method in the low-data regime. In this section, we want to explore whether the proposed method can further improve supervised training in the full training set using additional data. We use the training set (1.4k) in VOC12 as the pixel-level labeled data. The additional data contains additional VOC 9k (V 9k ), COCO training set (C tr ), and COCO unlabeled data (C u ). More training details can be found in Appendix D. As shown in Table 7, the proposed PseudoSeg is able to improve upon the supervised baseline even in the high-data regime, using additional unlabeled or image-level labeled data. 

[DISCUSSION AND CONCLUSION]
The key to the good performance of our method in the low-data regime is the novel re-design of pseudo-labeling strategy, which pursues a different decision mechanism from weakly-supervised localization to ""remedy"" weak predictions from segmentation head. Then augmentation consistency training progressively improves segmentation head quality. For the first time, we demonstrate that, with well-calibrated soft pseudo labels, utilizing unlabeled or image-labeled data significantly improves segmentation at low-data regimes. Further exploration of fusing stronger and better-calibrated pseudo labels worth more study as future directions (e.g., multi-scaling). Although color jittering works within our method as strong data augmentation, we have extensively explored geometric augmentations (leveraging STN (Jaderberg et al., 2015) to align pixels in pseudo labels and strongly-augmented predictions) for segmentation but find it not helpful. We believe data augmentation needs re-thinking beyond current success in classification for segmentation usage.

[ACKNOWLEDGEMENT]
We thank Liang-Chieh Chen and Barret Zoph for their valuable comments.    5 . We also adopt a slightly different fusion strategy in this setting by using T = 0.7 and γ = 0.3.

[E COMPARISON WITH WEAKLY-SUPERVISED APPROACHES]
In Table 9, we benchmark recent weakly supervised semantic segmentation performance on PAS-CAL VOC 2012 val set. Instead of enforcing the consistency between different augmented images as we do, these approaches tackle the semantic segmentation task from a different perspective, by exploiting the weaker annotations (image-level labels). As we can see, by exploiting the imagelevel labels with careful designs, weakly-supervised semantic segmentation methods could achieve reasonably well performance. We believe that both perspectives are feasible and promising for low-data regime semantic segmentation tasks, and complementary to each other. Therefore, these designs could be potentially integrated into our framework to generate better pseudo labels, which leads to improved performance. We conduct an additional performance analysis for temporal sharpening. We conduct experiments over T on the 1/16 split of VOC using pixel-level labeled data and image-level labeled data. As shown in Table 10, adopting a T < 1 for distribution sharpening generally leads to improved performance.  (Cordts et al., 2016). The Cityscapes dataset contains 50 real-world driving sequences. Among these video sequences, 2,975 frames are selected as the training set, and 500 frames are selected as the validation set. Following previous common practice, we evaluate on 19 semantic classes.
Comparison with state of the art. We compare our method with the current state-of-the-art method (French et al., 2020), in the setting of using pixel-level labeled and unlabeled data. We randomly subsample 1/4, 1/8, and 1/30 of the training set to construct the pixel-level labeled data, using the first random seed provided by French et al. (2020). Both French et al. (2020) and our method use ResNet-101 as the feature backbone and DeepLabv3+ (Chen et al., 2018) as the segmentation model. As shown in Table 11, the proposed method achieves promising results on all the three label ratios. Per-class performance analysis. Next, we provide per-class performance break down analysis. We compare our method with the supervised baseline on the 1/30 split, using pixel-level labeled data and unlabeled data. As shown in Table 12, the distribution of the labeled pixels is severely imbalanced. Although our method does not in particular address the data imbalance issue, our method improves upon the supervised baseline on most of the classes (except for ""Wall"" and ""Pole""). Discussion. Although the scene layouts are quite similar for all the full images, it is still feasible to generate different image-level labels through a more aggressive geometric data augmentation (e.g., scaling, cropping, translation, etc.). In practice, standard segmentation preprocessing steps only crop a sub-region of the whole training images. It only contains partial images with a certain subset of image labels, making the training batches have diverse image-level labels (converted from pixellevel labels, in the fully-labeled+unlabeled setting). Moreover, in the fully-labeled+weakly-labeled setting, in practice, we can collect diverse Internet images and weakly label them, instead of weakly labeling images from Cityscapes.

[H QUALITATIVE RESULTS]
We visualize several model prediction results for PASCAL VOC 2012 (Figure 8) and COCO (Figure 9). As we can see, the supervised baseline struggles to segment some of the categories and small objects, when trained in the low-data regime. On the other hand, PseudoSeg utilizes unlabeled or weakly-labeled data to generate more satisfying predictions.

[APPENDIX A SELF-ATTENTION GRAD-CAM]
We elaborate the detailed pipeline of generating Self-attention Grad-CAM (SGC) maps (equation 3) in Figure 6. To construct the hypercolumn feature, we extract the feature maps from the last two convolutional stages of the backbone network and concatenate them together. We then project the hypercolumn feature to two separate low-dimension embedding spaces to construct ""key"" and ""query"", using two 1 × 1 convolutional layers. An attention matrix can then be computed via matrix multiplication of ""key"" and ""query"". To construct ""value"", we compute Grad-CAM for each foreground class and then concatenate them together. This results in a H × W × (C − 1) score map, where the maximum score of each category is normalized to one separately. We then use image-level labels (either from classifier prediction or ground truth annotation) to set the score maps of non-existing classes to be zero. For each pixel localization, we use one to subtract the maximum score to construct the background score map, which is then concatenated with the foreground score maps to form ""value"" (H × W × C). The attention score matrix can then be used to reweight and propagate the scores in ""value"". The propagated score is added back to the ""value"" score map, and the pass through a 1 × 1 convolution (w/ batch normalization) to output the SGC map. 

[B IMPLEMENTATION DETAILS]
We implement our method on top of the publicly available official DeepLab codebase. 3 Unless specified, we adopt the DeepLabv3+ model with Xception-65 (Chollet, 2017) as the feature backbone, which is pre-trained on the ImageNet dataset (Russakovsky et al., 2015). We train our model following the default hyper-parameters (e.g., an initial learning rate of 0.007 with a polynomial learning rate decay schedule, a crop size of 513 × 513, and an encoder output stride of 16), using 16 GPUs 4 . We use a batch size of 4 for each GPU for pixel-level labeled data, and 4 for unlabeled/image-level labeled data. For VOC12, we train the model for 30,000 iterations. For COCO, we train the model for 200,000 iterations. We set γ = 0.5 and T = 0.5 unless specified. We do not apply any test time augmentations.

[C LOW-DATA SAMPLING IN PASCAL VOC 2012]
Unlike random sampling in image classification, it is difficult to sample uniformly in a low-data case for semantic segmentation due to the imbalance of rare classes. To avoid the missing classes at extremely low data regimes, we repeat the random sampling process for 1/16 three times (while ensuring each class has a certain amount) and report the results. We use Split 1 in the main manuscript. All splits will be released to encourage reproducibility. The results of all the three splits are shown as in Table 8.

[INPUT]
Ground truth Supervised Ours (unlabeled) Ours (img. label) Figure 9: Qualitative results of COCO. Models are trained with 1/512 pixel-level labeled data in the training set. Note that white pixel in the ground truth indicates this pixel is not annotated for evaluation.","[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss.","{
    ""1"": {
        ""summary"": ""Concerns about the method's effectiveness on boundaries, temperature selection for distribution sharpen operation, performance analysis over the parameter T, and the absence of evaluation on urban datasets influencing results on minor classes."",
        ""verbatim"": ""Does \""distribution sharpen operation\"" always use temperature < 1? If yes, what is the reason? How is the temperature $T$ is chosen? May the authors produce a performance analysis over T? In Sec 3.4, it's not clear to me the advantage of proposed method on boundaries. CAM-based activations mostly focus on most discriminative areas (usually inner areas). So hardly SGC can find pseudo-labels on boundaries. Why does the proposed method have an advantage there? More and more segmentation works report results in urban datasets like cityscapes or camvid. It would be interesting to see results on those datasets. One interesting aspect in urban datasets is the natural long-tail class distributions, which severely damages performance on minor classes, especially in low-data regime.""
    },
    ""2"": {
        ""summary"": ""Questions about the novelty over existing approaches with similar design features, absence of segmentation framework information in comparative tables, and the method's applicability to weakly-supervised settings."",
        ""verbatim"": ""CCT (Ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features. I'm wondering if authors can provide some insights about why the proposed method can achieve better performance than CCT when they both include the consistency training and data augmentation in the designs. In table 3, I suggest to include the segmentation framework used by each method in the table. In early works, old version of deeplab is usually treated as the standard. I understand using deeplab v3 is a fair comparison with CCT. It would be good to make this information clear in the table. It is also suggested to report the performance on PASCAL VOC test set as it is a common practice in this area (although CCT does not do so). Sine the unlabeled data training branch does not rely on any pixel-level annotations, I'm wondering if the proposed method can also work under weakly-supervised setting, where no pixel-level annotations are available during the training.""
    },
    ""3"": {
        ""summary"": ""Concerns about the technical novelty and clarity issues in some sections of the paper."",
        ""verbatim"": ""The overall approach seems incremental because it is a combination of a lot of existing approaches and there is not a strong technical contribution. For instance, the model uses several loss functions and all the losses are jointly optimized. I think the related work section should be in the main paper instead of the supplementary. I feel some parts are a bit difficult to read because of some misleading information. For example, the title of section 3.1 is “Experiments using unlabeled data” but the model still uses some labeled data.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Insufficient exploration and validation of calibration metrics"",
        ""verbatim"": ""While the paper introduces a calibration perspective for pseudo-labeling, further exploration and validation of the calibration metrics could strengthen the arguments for the proposed fusion strategy.""
    },
    ""2"": {
        ""summary"": ""Limited comparison with weakly supervised approaches"",
        ""verbatim"": ""Although the paper briefly compares with weakly supervised approaches, a more in-depth analysis and comparison in different scenarios could enhance the paper's completeness.""
    },
    ""3"": {
        ""summary"": ""Complexity and practicality concerns of the fusion strategy"",
        ""verbatim"": ""The complexity of the proposed calibrated fusion strategy may raise concerns about its practicality and computational cost, which could be a potential drawback for adoption in real-world applications.""
    },
    ""4"": {
        ""summary"": ""Lack of insights into real-world implementation and challenges"",
        ""verbatim"": ""While the method shows promise in experimental results, insights into real-world implementation challenges or considerations are somewhat lacking, which might reduce the practical applicability of the proposed approach.""
    }
}",4,"{
    ""A2-B2"": {
        ""rationale"": ""Both points from Review A and Review B discuss the interaction of the proposed method with weakly supervised approaches. Review A questions whether the method can operate under weakly-supervised settings, given its design, while Review B suggests that a deeper comparative analysis with weakly supervised approaches is needed to enhance the paper's completeness."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks comparisons with current state-of-the-art pseudo-labeling methods, which may weaken its contribution."",
        ""verbatim"": ""Lack of comparison with existing state-of-the-art methods for pseudo-labeling in semantic segmentation tasks could weaken the paper's contribution to the field.""
    },
    ""2"": {
        ""summary"": ""There's a need for more detailed quantitative results and comparisons to strengthen the findings."",
        ""verbatim"": ""While the paper mentions the effectiveness of the proposed strategy, more detailed quantitative results and comparisons with baseline methods could further solidify the findings.""
    },
    ""3"": {
        ""summary"": ""The paper should discuss the limitations or potential challenges of the proposed strategy for better understanding of its applicability."",
        ""verbatim"": ""The paper should provide more insights into the limitations or potential challenges of the proposed pseudo-labeling strategy to ensure a comprehensive understanding of its applicability in different scenarios.""
    }
}",3,"{
    ""A2-B2"": {
        ""rationale"": ""Both Review A2 and Review B2 critique the presentation and completeness of comparative data in the paper. Review A2 points out the absence of segmentation framework information in comparative tables and asks for further clarity, while Review B2 desires more detailed quantitative results and comparisons with baseline methods."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks a detailed comparison with existing methods."",
        ""verbatim"": ""The paper could face rejection if it lacks a detailed comparison with existing methods or benchmarks to clearly demonstrate the superiority of the proposed pseudo-labeling strategy.""
    },
    ""2"": {
        ""summary"": ""The methodology and implementation details of the pseudo-labeling approach are not clearly explained."",
        ""verbatim"": ""Unclear explanations or lack of details regarding the methodology and implementation of the pseudo-labeling approach may lead to rejection due to difficulties in replicability.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both Review A point 2 and Review B point 1 raise concerns regarding the novelty and comparative analysis of the proposed method against existing methods. Review A specifically highlights the similarity and possible lack of distinct improvement over CCT, while Review B generalizes this concern toward all existing methods, stressing the importance of demonstrating the proposed method's superiority."",
        ""similarity"": ""7""
    },
    ""A2-B2"": {
        ""rationale"": ""Review A point 2 questions how the proposed method compares against existing ones like CCT, specifically in terms of implementation details (using data augmentation and consistency training) which relate closely to Review B point 2's concern about unclear explanations or lack of details in the approach's methodology and implementation."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The model and process are too complex to understand easily."",
        ""verbatim"": ""'Complexity in model and process understanding'""
    },
    ""2"": {
        ""summary"": ""The paper lacks discussion on failure cases and limitations of the model."",
        ""verbatim"": ""'Limited discussion on failure cases or limitations'""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper lacks a detailed comparative analysis with other state-of-the-art SSL methods."",
        ""verbatim"": ""The paper might be criticized for not including a detailed comparative analysis with other state-of-the-art SSL methods for semantic segmentation.""
    },
    ""2"": {
        ""summary"": ""There is a potential risk of overfitting to the data augmentation strategies used in the pseudo-labeling process."",
        ""verbatim"": ""While strong data augmentation is part of the pseudo-labeling strategy, there is a potential risk that the models might overfit to these augmentation strategies, thus possibly limiting generalization in real-world scenarios.""
    }
}",2,"{
    ""A2-B1"": {""rationale"": ""Review A2 questions about the novelty over existing approaches and includes a desire to see the method compared with segmentations frameworks in tables, while Review B1 points out the absence of a detailed comparative analysis with other state-of-the-art SSL methods for segmentation. Both points criticize the lack of comparative depth in the research."", ""similarity"": ""7""}
}",1,"{
    ""1"": {
        ""summary"": ""Comparison with state-of-the-art methods may be insufficient."",
        ""verbatim"": ""If the experiments do not compare the new method against current state-of-the-art methods comprehensively, it could raise questions about its real-world superiority or value addition.""
    },
    ""2"": {
        ""summary"": ""Lack of detailed explanation in methodological novelty."",
        ""verbatim"": ""The paper may face criticism if the specifics of the pseudo-labeling redesign are not thoroughly explained or fail to clearly highlight the innovative elements compared to existing methods.""
    }
}",2,"{
    ""A2-B2"": {
        ""rationale"": ""Both Review A2 and Review B2 question the novelty and distinctiveness of the proposed method over existing approaches. Review A2 seeks clarification on how the proposed method distinguishes itself from similar consistency training and augmentation techniques, specifically in relation to CCT, while Review B2 emphasizes the need for a detailed explanation of the methodological novelty, particularly concerning how the pseudo-labeling redesign differs from existing methods."",
        ""similarity"": ""7""
    },
    ""A3-B2"": {
        ""rationale"": ""Review A3 and Review B2 both express concerns about the technical novelty and clarity of explanation. Review A3 mentions that the overall approach seems incremental with a combination of existing methods, questioning the strength of technical contribution. Similarly, Review B2 points to a potential criticism if the method's novel elements aren't thoroughly explained or clearly highlighted compared to existing solutions."",
        ""similarity"": ""7""
    }
}",2,"[Significance and novelty]
1. novel pseudo-labeling strategy for semantic segmentation: The paper proposes a new method for pseudo-labeling that effectively generates well-calibrated structured pseudo labels for training segmentation models with unlabeled or weakly-labeled data.
2. Data-efficient training for semantic segmentation: The innovative method aims to improve the training process for semantic segmentation tasks, which require more intensive labeling compared to image classification, through data-efficient methods.

[Potential reasons for acceptance]
1. Demonstrated effectiveness: The study shows significant improvements in both low-data and high-data segmentation regimes using the proposed consistency training framework and pseudo-labeling strategy.
2. Network agnostic design: The pseudo-labeling strategy is presented as network structure agnostic, enabling it to be easily adapted in various segmentation architectures.
3. Availability of code: The open-source release of the code further increases the acceptability of the study.

[Potential reasons for rejection]
1. Insufficient detail in the implementation: Lack of thorough description of the model architecture, optimization method, and precise details of the data used could hinder the reproducibility of the results.
2. Comparison with other works: Limited comparison with existing segmentation and pseudo-labeling strategies and lack of experimental verification of the superiority of the proposed method could cast doubts on its uniqueness and effectiveness.

[Suggestions for improvement]
1. Clear description of methods: The paper would significantly benefit from greater detail and clarity in describing the model architecture, optimization method (learning rate schedule, batch size, etc.), and the exact composition of the training dataset.
2. Comparison with related works: The paper could further strengthen its contribution by providing a comprehensive comparison with other semantic segmentation methods, including ablation studies and analyses of their performance differences.","[Significance and novelty]
<Novel pseudo-labeling strategy> The paper introduces a novel pseudo-labeling strategy specifically designed for semantic segmentation tasks, addressing the challenges of leveraging weakly labeled or unlabeled data for semantic segmentation.
<Comparison with existing methods> The paper shows clear improvements over the state-of-the-art method Pseudo-Label with additional data augmentation and pseudo-label fusion. However, it would be beneficial to provide a more detailed comparison to existing methods, especially those that also address the challenges of semantic segmentation with weakly labeled training data.

[Potential reasons for acceptance]
<Empirical results> The paper presents detailed experiments to support claims, showing consistent and significant performance improvements achieved by the proposed pseudo-labeling scheme. The method shows improvement both in high and low labeled data settings, which is a significant contribution to the field.
<Innovative pseudo labeling> The paper presents a simple and effective pseudo-labeling strategy for semantic segmentation tasks, indicating a potential for high impact within the subfield of AI or across more than one sub-area of AI.

[Potential reasons for rejection]
<Limited comparison to state-of-the-art methods> The paper could benefit from a more direct and thorough comparison to existing state-of-the-art methods that address the same problem of weakly labeled training data for semantic segmentation, demonstrating the technical advancements made by the proposed approach.
<Technical novelty and contribution> While the proposed approach is effective, additional clarity around how it advances the state-of-the-art in the field and its technical novelty would strengthen the paper.

[Suggestions for improvement]
<Comparison with existing methods> A more detailed comparison with existing SSL based approaches in semantic segmentation addressing training with low-data would strengthen the paper.
<Technical contributions> To enhance the paper, the authors should provide more clarity around the technical contributions, especially in comparison to the state-of-the-art methods, while considering the challenges of semantic segmentation with weakly labeled training data.

","{
    ""1"": {
        ""summary"": ""Insufficient detail in implementation affecting reproducibility"",
        ""verbatim"": ""Lack of thorough description of the model architecture, optimization method, and precise details of the data used could hinder the reproducibility of the results.""
    },
    ""2"": {
        ""summary"": ""Limited comparison with existing methods affecting validation of uniqueness and effectiveness"",
        ""verbatim"": ""Limited comparison with existing segmentation and pseudo-labeling strategies and lack of experimental verification of the superiority of the proposed method could cast doubts on its uniqueness and effectiveness.""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks a detailed comparison to state-of-the-art methods."",
        ""verbatim"": ""The paper could benefit from a more direct and thorough comparison to existing state-of-the-art methods that address the same problem of weakly labeled training data for semantic segmentation, demonstrating the technical advancements made by the proposed approach.""
    },
    ""2"": {
        ""summary"": ""The paper needs more clarity on its technical novelty and contributions."",
        ""verbatim"": ""While the proposed approach is effective, additional clarity around how it advances the state-of-the-art in the field and its technical novelty would strengthen the paper.""
    }
}",2,"{
    ""A2-B2"": {
        ""rationale"": ""Both Review A2 and Review B2 address the comparative aspect and uniqueness of the proposed methods against existing techniques. Review A2 queries how the proposed method varies from CCT, which uses similar features, and suggests providing comparisons in specific contexts (segmentation framework, and test set performance). Review B2 criticizes the approach for its limited comparison with existing methods, questioning the method's uniqueness and effectiveness. Although they discuss different specifics, the root concern about adequacy and thoroughness of comparative evaluation between the proposed and existing methods aligns them closely."",
        ""similarity"": ""7""
    }
}",1,"{
    ""A3-B2"": {
        ""rationale"": ""Both points discuss concerns regarding the technical novelty and contributions of the method presented in the paper. Although A3 focuses also on issues related to clarity keeping technical novelty as a secondary point, while B2 explicitly targets the need for more clarity around technical novelty, both express a desire for better explanation and justification of the new technical advancements."",
        ""similarity"": ""7""
    }
}",1
Model-Free Model Reconciliation,Byex4T2XcV.json,"[TITLE]
Model-Free Model Reconciliation

[ABSTRACT]
Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as modelreconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs.

[CAPTIONS]
Table 1: Figure 3 :3Figure 3: The test accuracy for increasing sizes of training set.

[INTRODUCTION]
A significant barrier to integrating AI systems into our daily lives has been their inability to interact and work with us humans in an intuitive and explicable manner. Orchestrating such interactions would require the agents to have the ability to help users in the loop better understand the rationale behind their various actions. Thankfully there has been a lot of effort within the AI research community to develop systems capable of holding explanatory dialogues with users and thus help them understand the decisions under question [David W. Aha and Magazzeni, 2018;Daniele Magazzeni, 2018]. Such explanatory systems could help users resolve confusions regarding agent decisions that may stem from either a (1) lack of understanding (or even misunderstanding) of the task or (2) from their inferential limitations. While many earlier works Figure 1: A general overview of the explanation as model reconciliation.
in explanation have generally focused on the latter (cf. [Khan et al., 2009;Hayes and Shah, 2017;Seegebarth et al., 2012;Topin and Veloso, 2019]), there is a growing consensus on the importance of explanatory mechanisms that can help bridge the knowledge asymmetry between the system and the user.
In particular, in explanation as model-reconciliation [Chakraborti et al., 2017] we had studied the problem of reconciling knowledge asymmetries between the user and the agent within the context of planning. Works in this direction have generally looked at cases where the user's model of the task (i.e their belief about the initial state, the transition dynamics, and the goal) is known beforehand (in a representation scheme comparable to the one used by the agent) and do not match the agent's model. This mismatch means that the user would not be able to correctly evaluate the validity or the optimality of a given plan. Thus in this paradigm the explanations consist of information about the agent's model that the user could incorporate into his/her own model to correctly evaluate the plan in question.
Unfortunately, it is not always possible to have access to such models. In the most general case, we are dealing with the user's model of the agent and hence the user may not be capable of presenting traces or decisions that could be generated from this model. Even if the system tries to learn such a representation based on interactions with the user, there is no guarantee that the specific representational assumptions of the learned model and the vocabulary used would be satisfied by the user's mental model.
The definition of explanation as model reconciliation may leave one with the idea that there is no way around it. How could one ever truly perform effective reconciliation when there exists no user model guiding us to choose the parts of the model, which when revealed to the user will help them correctly evaluate the current decision? Are we left with revealing the entire agent model to the user as the only option? In this paper, we propose a simple and intuitive way we could still generate minimal explanations in the absence of declarative models. We argue that we could drive such explanations by using learned models that can predict how human expectations could be affected by possible explanations (derived completely from information about the agent model) and in fact show how this method could be viewed as a variation of previous approaches that have been put forth to identify explicable behavior.
We will start by extending model reconciliation to the more general setting of planning with Markov Decision Processes (Section 4). The rest of the paper will investigate how these ideas could be used when the human mental model of the task is unavailable, and will formulate a learning problem that allows us to learn simple models that could be used to identify minimal explanations (Section 5). Finally, we will evaluate our method on a set of standard MDP benchmarks and perform user studies to validate its viability (Section 6).

[BACKGROUND]
Figure 1 presents a general schematic representation for explanation as model reconciliation. The automated agent (henceforth referred to as robot) starts with a model M R that can be used to generate a decision π (where depending on the context, π may be a single action, plan, policy or a program). In this setting, M R h corresponds to the human's preconceived notions about the robot model. The explainer's job then becomes providing information about the model M R , such that the updated human model can correctly evaluate the validity of the robot decisions.
In this case, the robot could have chosen to provide the entire model, but for most realistic tasks, such models could be quite large, so dumping the entire model could be both unnecessary and impractical. It's also well known that people generally prefer explanations that are selective [Miller, 2018;Lombrozo, 2006]. Thus the users would be happier with explanations that asks them to update a subset of beliefs as opposed to a complete update. Note that M R need not be the original agent model, but rather some abstraction/approximation of the underlying robot model (that conserves some desired property of the decision like optimality or validity). In [Chakraborti et al., 2017] where model reconciliation was first introduced, M R was a classical planning model hence inherently interpretable and thus the reconciliation could easily be achieved, but the idea could be applied beyond just planning models. For example, one could understand the explanation methodology used by LIME [Ribeiro et al., 2016] as being a special case of model reconciliation. In their case, they assume the human model is empty and M R is an approximation of the underlying decision model is automatically generated for each decision using a set of prede-fined features.
In this work, we will be looking at the agents that use discounted infinite horizon Markov Decision Processes (or MDPs) [Russell and Norvig, 2003] as the decision making framework. Each MDP M is defined by a tuple S, A, T, R, γ, µ , where the S provides the set of possible atomic states, A defines the set of actions, T is the transition function, R the reward, γ the discounting factor (where 0 ≤ γ < 1) and µ corresponds to the distribution of possible initial states. T : S × A × S → [0, 1] provides the probability that for given state s ∈ S, the execution of an action a would induce a transition to a new state s , and R : S × A × S → R defines the reward corresponding to this transition. The solution concept in MDP takes the form of a policy π that maps each state to a potential action. A policy is said to be optimal for M (denoted as π * M ) if there exists no other policy that dominates the given policy in terms of the expected value of states (where the value of a state s under a policy π for a model M is denoted as V π M (s)). Executing the policy in a state results in a sequence of state action state tuples called execution trajectory or simply a trajectory, denoted as τ = (s 1 , a 1 , s 2 ), ..., (s n−1 , a n−1 , s n ) and we will use P M (τ |π) to denote the probability of sampling the given trajectory τ for a policy π in model M.
In the explanatory setting we are interested in, the robot uses a model M R = S, A, T R , R R , γ R of the task to come up with the policy to act on. For now we will assume this MDP already defines an interpretable model and the human uses a model M R h = S, A, T R h , R R h , γ R h to evaluate it (we will relax this assumption in later sections). Now the task ahead of us will be to formulate how we could still identify minimal information that could resolve user confusion when M R h , but before we can do that we need to reinterpret the ideas of inexplicability and the idea of model reconciliation that was defined in [Chakraborti et al., 2017] in the context of MDPs and we will start by considering a simple scenario.

[ILLUSTRATIVE EXAMPLE]
Consider a warehouse scenario, where a robot is tasked with moving packages from racks and dropping them off at the dispatch chute. The robot is powered by a battery pack that can be recharged by visiting its docking station. The docking station also doubles as a quality assurance station that the robot needs to visit whenever it picks up a box labeled #013 (which means the box is fragile). The robot's operations are mostly deterministic, apart from a small probability of slipping (0.25) in some cells, that could leave the robot in the same position. Now suppose the warehouse has just hired a new part-time employee to oversee the operations. The employee is just getting used to this new setting and is puzzled by the robot's decision to once in a while take a detour from the drop-off activity and visit a specific position of the factory floor (which is, in fact, the docking location). If we wished the robot to be explainable, then it would need to be capable of helping the employee better understand the underlying model used by the robot (i.e achieve some form of model reconciliation). Given the fact that the robot may not have an exact model of the user, one way to achieve this could be by providing robot's entire model to the user. Unfortunately, this could easily overwhelm the user.
Another possibility could be to allow the user to specify which robot actions appear inexplicable, and focus on providing facts relevant to those actions. This explanation may still prove to be quite verbose and may in fact not help resolve their confusion. For example, imagine a case where the robot is visiting the station to recharge its batteries and the human says that the visit action is inexplicable. Now even if the robot mentions that visiting the station recharges it, the employee may still be confused if they are under the incorrect assumption that the robot is operating on full battery. Similarly, if the human had expected the robot to go to the docking station due to some confusion regarding the box codes, the human may mark the robot decision to not go to the dropoff as being inexplicable and the explanations that could resolve the confusion may have little to do with that specific action marked as inexplicable.

[EXPLANATION AS MODEL RECONCILIATION FOR MDPS]
In this setting the human and robot models are captured as MDPs defined over the same set of states and thus we can characterize both models by the tuple θ = θ T , θ R , θ γ , θ µ , where the θ T provides the set of parameters that defines the transition probabilities P (.|s, a), while θ R the parameters corresponding to the reward function, θ γ the parameters corresponding to the discount factor and θ µ the parameters for the initial state distribution. For simple MDP models with atomic states, θ T contains parameters of the categorical distribution for each transition (θ µ will contain similar parameters for the initial state distribution), θ R contains the reward associated with each transition (an s, a, s tuple) and θ γ just contains the value of the discount factor. The specific instantiations of the parameters for each model M is captured as θ(M). For simplicity, we will denote each of the unique parameters in the tuple θ using indexes. For example, θ s,a T (M R ), will correspond to the parameters for the distribution P (.|s, a) for the model M R .
If we use M to capture the set of all possible models and Θ = θ T × θ R × θ γ × θ µ , then model reconciliation operation can be captured as a function E M R h ,M R : 2 Θ → M that takes in a set of model parameters and generates a new version of the model M R h where the set of specified parameters will be set to values from M R . For example,
M = E M R h ,M R (θ s1,a T )
will be a new model such that θ(M) will be identical to θ(M R h ), except that θ s1,a TM , will be equal to θ s1,a T R .
Practically, the model reconciliation operation corresponds to the robot informing the human about some part of its model. This communication could incur cost and we can capture this by using the cost function C : 2 Θ → R that maps a given set of a threshold to a cost. Now the question we need to ask is whether the agent is trying to explain its policy or if it is trying to explain some behavior (i.e an execution trace). Most of the earlier work that looks at model reconciliation explanation (cf. [Chakraborti et al., 2017;Sreedharan et al., 2018a;2018b]) has looked at sequential plans and has generally ignored this differentiation and treated the problem of explaining plans to be same as that of explaining behavior. In general, a given plan or policy compactly represents a set of possible behaviors and the choice of explaining behavior vs explaining the plans/policies could affect the content of the explanation being given. For example, when explaining policies there is the additional challenge of presenting the entire policy to the user and the explainer may need to justify action choices for extremely unlikely states or contingencies. On the other hand, when explaining a given set of behaviors the explainer needs to only justify their action choices for cases they actually witnessed. For example, when explaining traces from the warehouse scenario, given the small probability of slipping, the robot may never have to mention what to do when it slips, but on the other hand if we are dealing with full policies, the agent may need to talk about the states where the robot is in the slipped positions and they need to get up from that position and move on.
Explaining policies or plans becomes more relevant when we consider explanatory dialogues where the agent and the user are trying to jointly come to agreement on what policy/plans to follow (eg: decision support systems), while the latter may be more useful when the user is observing some agent operating in an environment.
With respect to policies, we assume that the user is presented with the entire policy. A given policy is said to be explicable to the human, if the policy is optimal for the human model. Therefore the goal of the explainer becomes that of ensuring the optimality of the given policy Definition 1. A set of parameters θ E corresponds to a complete policy explanation for the given robot policy π * M R , if the policy is also optimal for E M R h ,M R (θ E ) and is said to be the minimally complete policy explanation if there exists no other complete explanation θ E , such that, C(θ E ) < C(θ E ) Finding a complete policy explanation is relatively straightforward (the set of all parameters automatically meets this requirement). The more challenging case becomes that of finding the minimal or the cheapest explanations i.e. the minimally complete explanations. Such minimally complete explanations can be calculated by adopting a search strategy similar to [Chakraborti et al., 2017]. The search can start at the human model and try to find the minimal number of parameters that needs to be updated in the human model for the current policy to become optimal. Similar to generating minimally complete explanations, i.e, we can also generate monotonic explanations (i.e explanations where no further information about parameters in the robot model can affect the optimality of the current plan).
In the case of policies, we can also describe explicable planning and balancing cost of explanations with that of choosing policies that are inherently explicable, where inexplicablity score (I E ) of a policy π is defined as
I E (π, M R h ) = |E[V M R h π * (s)|s ∼ µ R h ] − E[V M R h π (s)|s ∼ µ R h ]
| Where π * is the optimal policy in the human model. Explicable planning thus becomes the problem of choosing Figure 2: Subfigure (a) shows a visualization of a trajectory expected by the user described in the illustrative example, and (b) shows the visualization of a trajectory the user may observe. Subfigure (c), shows the various explanatory messages that could be used in this scenario, note that the messages span information from multiple abstractions of the given task policies that minimize inexplicability score [Kulkarni et al., 2016], while minimizing the potential loss in optimality due to the policy choice (since the most explicable plan may not be an optimal policy). Balanced planning, as studied in [Chakraborti et al., 2019;, proposes going one step further and also takes into account possible savings in inexplicability score that can be achieved by providing explanation (while incurring additional cost of communicating the required explanations).
For explaining behavior, we will look at the simplest case, namely the agent needs to explain a set of behaviors that the user has just observed. We will assume that the observer has full observability of the state and is seeing the robot behavior for the first time. In such a setting, a given trace τ would appear explicable to the user if it could be sampled from their expected MDP policy (i.e a policy optimal in their model) or more generally, i.e P M R h (τ |π) > δ, where δ is some small threshold. 1 Definition 2. A set of parameters θ E corresponds to a complete behavior explanation for a set of traces
T = {τ 1 , ...τ n }, if ∀τ ∈ T, ∃π such that P E M R h ,M R (θ E ) (τ |π) > δ and π is an optimal policy for the model P E M R h ,M R (θ E ) .
The explanation is said to be the minimally complete behavior explanation if there exists no other complete explanation θ E , such that,
C(θ E ) < C(θ E )
Note that given the above definition, if δ is set very high it may not be possible to find a complete explanation, as the trace may genuinely contain low probability transitions. In this work we will assume δ to be zero.
While model reconciliation could be an important component of either policy or behavior explanation, the applicabil-ity of the model reconciliation explanations on their own for policies is limited by the fact that in all but problems with the smallest state spaces, the user would have trouble going over the entire policy. Thus in these settings, explanatory systems would need to also utilize policy approximation or summarization methods, then allow users the ability to drill down on policy details as required. Since our main goals was to focus on developing approaches that allow us to generate model reconciliation explanations without explicitly defined user models, the rest of the paper will mostly focus on behavior explanation. In Section 8, we will have a brief discussion on how these methods could potentially be extended to policy explanation scenarios.

[EXPLAINING WITHOUT EXPLICIT HUMAN MENTAL MODELS]
Now we will look at how we can identify cheap complete behavior explanations when the human model is unknown. We will go one step further from identifying not only the parameters that need to be explained, but also capturing the right modality/abstractions to present the information about the parameters. That is, we will no longer assume that the human is using a full MDP model to come up with their decisions. Instead, the robot starts with a set of explanatory messages Ψ = {m 1 , m 2 , ..., m n } that can be presented to the user. Where the messages correspond to a set of parameter values (the parameters corresponding to a set of messages {m 1 , ..m k } is denoted as E({m 1 , ..m k })) of the model as captured in some abstraction of this model and has a corresponding cost ( C) associated with it. The abstractions to consider may depend on the specific scenario and the previous information about the intended users (laypeople vs. experts). Some simple possibilities may be to consider qualitative models (say non-deterministic ones instead of stochastic)
and considering state abstractions the given task. Note that, technically E(Ψ), need not span the set of all possible model parameters, but could rather be limited to a subset of parameters identified to be relevant to the given problem. One possible way may be to consider variations of explanation techniques like MSE [Khan et al., 2009] to identify set of possible factors that affect the optimality of each action. In Figure 2, the subfigure (c) shows a set of possible explanatory messages for the warehouse domain, that consists of each parameter mapped to some english statement. For models captured using factored representations that use relational or propositional fluents, such statements could be easily generated using templates (cf. [Hayes and Shah, 2017]). Given this setting, we will now make some simplifying assumptions, namely, (1) the order in which the explanatory messages are presented does not matter (2) we have access to a set of observers with similar models and they share this model with the target user (3) the robot is viewing the task at the same level or at a more detailed level of granularity than the user and (4) the user and robot have some shared vocabulary in regards to the task. While assumption ( 1) is easily met since we are mostly dealing with model information and ( 4) is a prerequisite for most explanatory approaches, in section 8 we will discuss how we can possibly relax requirements (2) and (3). Now our goal is to learn a predictive model that is able to predict whether a given user would find a given s, a, s tuple explicable and how the user's perception changes with the given explanatory messages.
For example, at the beginning of an episode the user may be presented with the following explanatory messages,Ψ = {m 1 = ""Robot slips with probability 0.25 at grey cells""}, which corresponds to the fact that P (s i |a, s i ) = 0.25, for all states s i where the feature grey cell is true and for all actions a. Now the user will be presented with a sequence of transitions, say (1, 2), right, (2, 2) and asked whether the transition was explicable or not. Then the tuple (1, 2), right, (2, 2) , {m 1 }, l 1 , where l 1 is the label assigned by the user to the transition, becomes input to our learning method.
The exact function that we would want to learn would be
L( s, a, s , {m 1 , ..., m k }) =      1 if s, a, s ∼ π * E M R h ,M R (θ({m1,...,m k }) (s) 0 otherwise
Note that this is a modified version of the sequential model we introduced in [Zhang et al., 2017] for identifying whether a given plan is explicable or not. Though our methods vary in some significant aspects, namely, (1) we allow for the possibility that the explicability of the actions/traces could be affected by explanations provided by the system; (2) we no longer use labels of high level tasks as a proxy for the explicability of the trace. Instead, we just use a simple binary label on whether the transition is explicable or not; (3) we no longer consider sequence models but rather a much simpler labeling model that maps a single transition to the explicability label. We argue that in cases where the human is markovian on the same set of features as the agent, this rather simpler model suffices.
It is also important that our learning approach is more tractable than the ones studied in [Zhang et al., 2017], since in their case to build a balanced dataset (of explicable and inexplicable plans), they would need to uniformly sample through the entire plan space (an extremely hard endeavour with no obvious known approaches), while we stick to traces generated from the optimal policy and only need to randomly generate possible sets of explanatory messages, which is clearly a smaller set.
Once we have learned an approximation of the above labeling functionL, the problem of explanation generation for a trace τ = s 0 , a 0 , s 1 , ..., s n , a n , s n+1 becomes that of finding the subset of Ψ that balances the cost of communication with the reduction in the inexplicability of the given trace, i.e arg min
Ψ (C M (Ψ) + α * Σ n i=0 (1 −L( s i , a i , s i+1 ,Ψ)))
WhereΨ is a subset of Ψ and α is some scaling factor that balances the cost of explanation with the number of inexplicable transitions for a given trace.

[EVALUATION]
The success of the approach described above would be directly dependent on whether we can learn high accuracy labeling models. Once we have access to such a model, we could be quite confident in our ability to generate useful explanation (provided the user's model is the same as the one the labeler was trained on) and identifying the best explanation becomes a matter of just searching for the required subset of messages that minimizes the objective defined in section 5. So to evaluate the method our focus was on identifying if it was possible to learn high accuracy models. We validated our approach on both simulations and on data collected from users.

[EVALUATION ON SIMULATED DATA]
For simulations, we used a slightly modified versions of the Taxi domain [Dietterich, 1998] (of size 6*6), the Four rooms domain [Sutton et al., 1999] (of size 9*9) and the warehouse scenario (of size 9*9) described before (implemented using the SimpleRL framework [Abel, 2019]). For each domain, we start with an MDP instance (henceforth referred to as the robot model) and then create a space of possible user models by identifying a set of possible values for each MDP parameter. For example, in the taxi domain the parameters include position of the passengers, their destination, the step cost, discounting etc., for the Four rooms this included the goal locations, locations with negative rewards, discounting, step cost, slip probability, etc., and finally for the warehouse, the position of the box, the position of station #1, the step cost, slipping probabilities and the discounting factors were selected as potential parameters that can be updated. In this setting, we assume that there exists a single explanatory message for each possible parameter.
For each individual test, we select a random subset of three parameters and then randomly choose a value for each of these. We then treat this new MDP model as a stand-in for the user model and use it to label traces generated from the original MDP. The traces were generated by choosing a random initial state and then following the optimal policy of the robot until either the terminal state is reached or the trace length reaches a predefined limit. For each trace, a random subset of the explanations was selected and presented to the human. This means updating the MDP parameters to their corresponding values in the robot model only for the parameters specified by the current subset of explanation. Each individual transition was then labeled using this updated MDP. A transition was labeled as inexplicable if the action is not the optimal one in the human model (i.e. Q value is lower) or the next state had a probability of occurring of δ = 0.
We then used this set of labeled transitions to create a training set and test set for a decision tree learner. The input features to the decision tree consist of current state features, (just x and y for Four rooms and the position of the the taxi and passengers for the Taxi domain and for Warehouse it included the position of the robot and the fact whether the agent picked up the box or visited station #1), the index of the action and features capturing the current subset of explanations being considered. In each Warehouse and Four rooms test instance, we collected 900 unique data points as training set and 100 data points as the test set. Due to the complexity of the taxi domain, we generated less data points (since for each different explanation subset we need to solve a new planning problem) and used close to 220 unique points as training data and on average 28 data points as the test set.
We then tested on 20 such instances for each domain. Figure 3 plots the average test accuracy for models trained with training sets of varying sizes. As evident from the graph, a simple decision tree seems to be able to easily model the effect of explanations on labeling for these simulated scenarios. We chose a simple learning model to establish the viability of this method, but one could easily see that the use of more sophisticated learning methods and/or more informed features should lead to better results.

[USER STUDIES]
Next, we wanted to establish if we can still learn such simple models when the labels are collected from naive users. Our goal here is not to consider scenarios with possible differences in the user's knowledge, but rather cases where, even in the presence of a set of users with similar backgrounds, their responses to explanations would be too varied to learn useful models. To test this, we used the Warehouse domain as a test bed and collected feedback on how users would view the explicability of traces generated from this domain when presented with explanatory messages detailed in Figure 2.
For the study, we recruited 45 master turkers from the Amazon Mechanical Turk. Each participant was provided with the URL to a website (https://goo.gl/Hun3ce) where they could view and label various robot behaviors. We considered a setting where the robot had a full battery, but was picking up a fragile box and thus still needs to visit the station #1. The robot could slip on some cells marked in dark grey with probability 0.25 (slipping here meant the robot picture is tilted to give an impression that it slipped on the cell and didn't prevent the robot from moving to the next cell). To make sure that all the users had similar mental models at the start, they were provided with the following facts, (a) that robot couldn't pass through racks, (b) whenever the robot runs low on battery it needs to get to Station 1, (c) whenever the robot has a green battery sign next to the robot, that means their battery is full and (d) the robot needs to take the shortest route to the goal. Also, they were presented with an example trace in this instructions section and were made to take a small pre-test that allowed them to revise the above facts in various scenarios. After the pre-test, they were shown eight traces from the robot policy sampled according to their probabilities. After the first trace, the user was given an explanation message before each trace, where the message was taken from the seven possible messages (the order of the messages was always randomized).
From the data collected from 45 turkers, we removed data from seven users, based on the fact they didn't find any of the transition in the first trace (i.e the case where no explanation was provided) inexplicable. We imagine this number would go down when we move to expert users or users who are invested in the success of the robot. The data generated for the remaining 38 users were then used to train a decision tree. Since the placement of other objects in the environment were fixed, we were able to use rather simple features for the model like the current position of the robot (x and y), previous position (again x and y), the action, whether they have slipped and finally the explanations given. We found the model to have an average 10-fold cross validation score of 0.935. For a randomly generated train and test split (where the test split was 10% and contained around 7% inexpicable labels) the precision score was 0.9637 and the recall score was 0.9568.
Furthermore, we could see that the model was able to correctly predict the usefulness of intuitive minimal explanations for the given scenario. For example, it predicted that while the robots decision to visit station #1 would be considered inexeplicable by the user in the absence of any explanation, the user would mark it as explicable when they are explained about the box being fragile and that fragile boxes need to be inspected at station #1. In fact the model predicted that only the message that ""fragile boxes need to be inspected at station #1"" is enough to convince the user about the need for that action (i.e the user could deduce that the box must have been fragile). This shows that such learned models may help us generate cheaper explanations (the above set of explanations is smaller than the corresponding minimal complete behavior explanation for the domain), by taking into account the users ability to correctly predict missing information in simple cases. Another point of interest was that the model predicted all slipping events as explainable even in the absence of any explanations. The cases where the user saw a slip before being told about the possibility of slipping was rare (since there are two explanatory messages related to slipping and the probability of slipping was 0.25). Furthermore when we went over the data, we found that in most such cases, the users did mark it as explainable. This may be because the effect of slipping may not have been that detrimental to the overall plan (it doesn't take you off the current path). It would be interesting to see if this result would be the same in cases where slipping was a more likely event and if it had a more apparent effect on the robot's plan.

[RELATED WORK]
To the best of our knowledge, this work represents the first attempt at learning proxies for user mental models that allows an agent to predict the potential impact of providing explanations as model reconciliation to observers. With that said, there have been works that have looked at the problem of generating explanations in the presence of model uncertainty for human models. In particular, our previous works like [ Sreedharan et al., 2018a;2018b] have looked at cases where the agent has access to a set of potential human models. One drawback of considering a set of possible models is either they would need to have explicit sensing to identify the user model (which could mean asking a large number of questions to the user) or providing a large amount of information to cover the space of all possible models. In our work, the problem of identifying the specifics of the user model is resolved through an offline training process.
Another work quite related to the discussion covered in this paper is [Reddy et al., 2018], wherein the authors tried to identify cases where they can learn a potential model for the human's expectation of the task transition dynamics when they do not align with the real world dynamics. Unlike their work, we do not assume that the user can provide traces for the given task, rather they may be able to provide some highlevel feedback on the action (i.e. they may not be able to do or even know the right action but may be able to point out actions or transitions that surprise them). Moreover, their work requires that the user and the robot must have the same reward function, which is again an assumption we do not make. Even if we had followed their technique to learn a potential approximation of the human's transition model for the task, there is no guarantee that the learned representation would be one that makes sense to the human.

[DISCUSSION AND CONCLUSION]
This paper proposes a possible way in which model reconciliation explanation could be applied to cases where the user model is unknown. The method described here is a rather simple and general method to identify information that could potentially affect the user's mental model and produce effects that align with the agent's requirements. There is no requirement here that the messages have to align with actual facts about the world. This again points to the rather troubling similarities between the mechanisms needed to generate useful explanations and lies .
Two important assumptions we made throughout the work is that the user only considers the current state (as defined by the robot) to make their decisions and we have access to a model that was learned from interactions to previous users who had similar knowledge level to the current user. Relaxing the first assumption would require us to go beyond learning models that map each transitions to labels. Instead we have to consider sequential labeling models (for example models based on LSTM or CRF) of the type considered in [Zhang et al., 2017] to capture the human's expectations. For example, we considered a simple extension of the warehouse domain where the human believes the robot should visit two locations (i.e the human state contains variables that record whether the user has visited the locations). Even though here the user is considering a more detailed model, we were able to learn labeling models of 80% accuracy by using simple CRFs. As for the second, instead of assuming that all users are of the same type, a more reasonable assumption may be that the users could be clustered into N groups and we could learn a different labeling model for each user type. Now we still have a challenge of identifying the user type of a new user and one way to overcome this would be by adopting a decision-theoretic approach to this problem and modeling it as a POMDP (where user labels become observations and previously learned user models the observation models).
The work discussed in this paper only covers explanations that allow the user and the system to reconcile any model difference. This only covers a part of the entire explanatory dialogue. Even if there is no difference in models, the user may still have questions about parts of the policy or may raise alternative policies they think should be followed. This may arise from a difference in inferential abilities and may require providing information that is already part of their deductive closure eg: help them understand the long term consequences of taking some actions. Once you have access to a set of such messages one could use a method similar to the one described in the paper to find the set of helpful ones. Unlike the model reconciliation setting where the messages stand for information about the model, it is not quite clear how one could automatically generate such messages.

[ACKNOWLEDGMENTS]
This research is supported in part by the ONR grants N00014-16-1-2892, N00014-18-1-2442, N00014-18-1-2840, the AFOSR grant FA9550-18-1-0067, NASA grant NNX17AD06G and a JP Morgan AI Faculty Research grant .","[TITLE]
Model-Free Model Reconciliation

[ABSTRACT]
Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as modelreconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs.","{
    ""1"": {
        ""summary"": ""Lack of clarity on the question the explanations are answering, ambiguity in the explanation of the theta function/model, skepticism on the 'model-free' moniker, and inadequate experimental design description."",
        ""verbatim"": ""- I think the paper could be improved by defining what question the explanations are answering here.\n- It is not quite clear (to me at least) what the purpose of the theta function/model is at the start of Section 4.\n- While I understand where the authors are going with the 'model-free' idea, the model-free moniker and the section 5 title could be considered somewhat cheeky.\n- Section 6 could be improved by clearly outlining the goals of the experiment.""
    },
    ""2"": {
        ""summary"": ""Assumptions scattered throughout the paper, query about scalability and generalization towards more complex scenarios, and unclear which class of questions the approach aims to answer."",
        ""verbatim"": ""- I noticed that the authors make a number of assumptions, but they are scattered throughout the paper.\n- I wonder how this approach can actually scale or be generalised to other (more complex) scenarios. Can the authors comment on potential/limitation to scale to larger scenarios?\n- What's the class of questions this approach aims to answer/address?""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Assumption of similar mental models among all users could impact the effectiveness in diverse scenarios"",
        ""verbatim"": ""One potential shortcoming could be the assumption that all users have similar mental models, as relaxing this assumption would require more complex sequential labeling models to capture diverse user expectations.""
    },
    ""2"": {
        ""summary"": ""Focus on behavior explanation over policy explanation may limit applicability in crucial scenarios"",
        ""verbatim"": ""The paper focuses more on behavior explanation rather than policy explanation, which could limit its applicability in scenarios where understanding policies is crucial.""
    },
    ""3"": {
        ""summary"": ""Lack of extensive comparison with existing methods may omit a full analysis of the proposed method's benefits"",
        ""verbatim"": ""The paper does not extensively compare the proposed method with existing explanation generation approaches, potentially missing a comprehensive analysis of its advantages over other techniques.""
    },
    ""4"": {
        ""summary"": ""Reliance on pre-learned models may raise concerns about generalizability and adaptability"",
        ""verbatim"": ""The method's reliance on pre-learned models from previous users could raise concerns about the generalizability and adaptability of the approach to new or diverse user populations.""
    }
}",4,"{
    ""A2-B4"": {
        ""rationale"": ""Both Review A2 and Review B4 are concerned with the generalization and scalability of the approach. A2 questions how the approach can scale or generalize to more complex scenarios, while B4 raises concerns about the reliance on pre-learned models affecting the generalizability and adaptability. Both points address the limitations regarding adaptability of the proposed method to new or more complex conditions."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of Experimental Validation"",
        ""verbatim"": ""Lack of Experimental Validation: The paper may face rejection if there is a lack of experimental validation or empirical results to support the effectiveness of the proposed model reconciliation approach in real-world scenarios.""
    },
    ""2"": {
        ""summary"": ""Theoretical Focus"",
        ""verbatim"": ""Theoretical Focus: If the paper remains too theoretical without practical implementation or application examples, it could be deemed lacking in practical relevance.""
    },
    ""3"": {
        ""summary"": ""Inadequate Comparison"",
        ""verbatim"": ""Inadequate Comparison: If the paper fails to compare the proposed approach with existing explanation methods or lacks a comprehensive analysis of its advantages over current techniques, it may not offer sufficient grounds for acceptance.""
    },
    ""4"": {
        ""summary"": ""Limited Scope"",
        ""verbatim"": ""Limited Scope: If the adaptation of the model reconciliation approach to a more general planning paradigm is not sufficiently justified in terms of its significance or impact, the paper may be considered too narrow in scope for broader acceptance.""
    }
}",4,"{
    ""A1-B3"": {
        ""rationale"": ""Both points highlight insufficient explanation and description in the paper. Review A criticizes the ambiguous explanation and purpose of the theta function/model, and Review B points out the lack of comparison with existing methods, which might include a lack of detailed analysis and clear exposition. Both are concerned with the clarity and comprehensiveness of content presented in the paper."",
        ""similarity"": ""7""
    },
    ""A2-B4"": {
        ""rationale"": ""Review A queries the scalability and generalization to more complex scenarios and asks what class of questions the approach is meant to address, showing concern for its broader applicability and relevance. Review B expresses concerns about the limited scope if the adaptation of the model is not well justified, focusing on the significance and impact in broader contexts. Both points coalesce around the need for greater clarification on the model's applicability in a broader, more complex context."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper may be rejected for lack of empirical validation."",
        ""verbatim"": ""<Lack of empirical validation> The paper may face rejection if it lacks empirical validation of the proposed model reconciliation approach, which is crucial to demonstrating the effectiveness of the methods in real-world scenarios.""
    },
    ""2"": {
        ""summary"": ""The paper may be rejected for not sufficiently comparing with existing approaches."",
        ""verbatim"": ""<Incomplete comparison with existing approaches> If the paper fails to thoroughly compare its approach with existing methods in the field of model reconciliation and explainable AI, it may be deemed insufficiently innovative or impactful for acceptance.""
    }
}",2,"{
    ""A1-1-B1"": {
        ""rationale"": ""Both points express concerns about the clarity and effectiveness of the model or approach being presented, focusing on the need for better definition and validation."",
        ""similarity"": ""7""
    },
    ""A2-3-B2"": {
        ""rationale"": ""Both points address the lack of comparison with existing approaches and general applicability of the model presented in broader or more complex scenarios."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Concerns about the generalizability of the findings to other domains or complex environments."",
        ""verbatim"": ""Generalizability Concerns: While the paper presents results from simulations and user studies, the diversity of user models and AI settings tested might still be limited. Concerns may arise about how generalizable the findings are to other domains or more complex decision environments.""
    },
    ""2"": {
        ""summary"": ""The proposed method's scalability to more complex or real-world scenarios is not thoroughly addressed."",
        ""verbatim"": ""Complexity and Scalability: The proposed method's scalability to more complex or real-world scenarios where the dynamics of human models are more unpredictable and varied isn’t deeply addressed.""
    },
    ""3"": {
        ""summary"": ""Risk of overfitting due to reliance on learned approximations from data that might not represent all types of users or decision contexts."",
        ""verbatim"": ""Potential for Overfitting: The model relies significantly on learned approximations from data that might not adequately represent all types of users or decision contexts. This could lead to overfitting, where models perform well on known data but poorly on unseen or new user interactions.""
    },
    ""4"": {
        ""summary"": ""Assumptions about initial conditions for human models and interaction settings may limit the applicability of the findings."",
        ""verbatim"": ""Assumptions Made: The assumptions made about the initial conditions for human models and interaction settings might limit the applicability of the findings. These assumptions need to be critically evaluated and justified further.""
    }
}",4,"{
    ""A2-B2"": {
        ""rationale"": ""Both points express concern about the scalability of the approach and its generalization to more complex scenarios. Review A asks specifically about the potential to scale to larger scenarios, while Review B explicitly states the lack of deep addressal in scaling to complex or real-world scenarios."",
        ""similarity"": ""8""
    },
    ""A2-B1"": {
        ""rationale"": ""Although Review A does not directly mention generalizability, the question on scalability and applicability to more complex scenarios implicitly includes generalization concerns, which aligns with Review B's worries about the generalizability of findings to diverse domains and environments."",
        ""similarity"": ""7""
    },
    ""A2-B4"": {
        ""rationale"": ""Review A mentions assumptions scattered throughout the paper without clear consolidation or critical evaluation, which somewhat matches Review B's concern about assumptions regarding human models and interaction settings that may limit applicability."",
        ""similarity"": ""6""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The paper lacks empirical evaluation to support the effectiveness of the proposed approach."",
        ""verbatim"": ""Lack of empirical evaluation: The paper proposes a theoretical framework and methodology but does not provide empirical evidence or experimental results to support the effectiveness of the proposed approach.""
    },
    ""2"": {
        ""summary"": ""The paper provides limited detail on the implementation of its methodology."",
        ""verbatim"": ""Limited detail on implementation: There is a vague description of the methodology used for the labeling model, which might leave the reader questioning the practical implementation and feasibility of the approach.""
    },
    ""3"": {
        ""summary"": ""The paper does not discuss potential scalability issues related to adapting the model reconciliation framework to MDPs."",
        ""verbatim"": ""Scalability concerns: The adaptation of the model reconciliation framework to MDPs might face scalability issues, which are not discussed in the paper.""
    }
}",3,"{
    ""A1-2-B2"": {
        ""rationale"": ""Both Review A point 2 and Review B point 2 express concerns regarding the lack of clear and adequate information about the methodology and its implementation."",
        ""similarity"": ""7""
    },
    ""A2-3-B3"": {
        ""rationale"": ""Review A point 2 questions the scalability and generalization of the approach, which aligns closely with Review B point 3, discussing potential scalability issues in adapting the model to more complex scenarios."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Concern about the lack of sufficient empirical evidence for the proposed model"",
        ""verbatim"": ""The paper might face criticism for not providing sufficient empirical evidence to support the efficacy of the proposed labeling model in achieving effective model reconciliation.""
    },
    ""2"": {
        ""summary"": ""Concern about the depth of theoretical analysis and robustness of the proposed model"",
        ""verbatim"": ""There could be concerns regarding the depth of theoretical analysis, particularly how the labeling model works in diverse scenarios and its limitations.""
    }
}",2,"{
    ""A2-2-B2"": {
        ""rationale"": ""Both Review A(2) and Review B(2) raise concerns about the general applicability and robustness of the model. Review A questions the scalability and generalization of the approach to more complex scenarios, while Review B addresses how the model works in diverse scenarios and its limitations."",
        ""similarity"": ""7""
    }
}",1,"[Significance and novelty]
This paper proposes a new approach to model reconciliation for agent explanation in more general planning paradigms, particularly when user models are no longer explicitly available. The authors build upon the model reconciliation framework, which hypothesizes that user confusion arises from the mismatch between the user's model of the agent's task model and the agent's actual model. Previous works in this area have primarily focused on classical planning settings with available user models in a declarative form.

[Potential reasons for acceptance]
1. The paper introduces a new, simple, and easy-to-learn labeling model that assists the explainer in achieving model reconciliation between the user and the agent in the context of planning with Markov Decision Processes (MDPs).
2. Model reconciliation is a growing area of research in human-AI interaction, and this paper advances the field by expanding its application to more general planning paradigms.
3. The authors present a clear and concise methodology and provide extensive detail in their proposed approach.

[Potential reasons for rejection]
1. The simplicity of the proposed labeling model may not be generalizable to more complex planning paradigms beyond MDPs.
2. The authors do not provide experimental evidence or validation for their presented labeling model.
3. The practical application and scalability of the proposed model are not addressed in this paper.

[Suggestions for improvement]
1. Conduct experiments to assess the performance and scalability of the proposed labeling model in more complex planning paradigms beyond MDPs.
2. Perform a thorough comparison with existing model reconciliation approaches and benchmark the effectiveness of the proposed labeling model.
3. Explore ways to integrate the labeling model into more practical applications to assess its applicability and real-world utility.
4. Extend the labeling model to accommodate more complex user models that cannot be represented in a declarative form.","[Significance and novelty]
<Application of model reconciliation to MDPs> The paper is the first to apply the model reconciliation approach to more general planning paradigms, particularly in the context of Markov Decision Processes (MDPs), showcasing a new application of this approach in an interesting and novel domain.
<Implications and generalization of reconciliation approach> The paper introduces the idea of using MDPs to model reconciliation and discusses the implications of generalizing the approach to non-planning domains, providing theoretical and empirical evidence for model reconciliation in various settings.

[Potential reasons for acceptance]
<Contribution to the literature> The paper's extensive experimental analysis demonstrates the benefits of the presented algorithm for the reconciliation-style explanation approaches, presenting a substantial contribution to the literature.
<Well-supported claims and excellent presentation> All claims are well-supported and correct, and the technical quality of the paper is convincing, presenting both theoretical guarantees and empirical evidence to support the proposed approach.

[Potential reasons for rejection]
<No related terms> -

[Suggestions for improvement]
<Generalization to non-planning domains> The paper can further strengthen its novelty and significance by addressing the implications of generalizing the reconciliation approach to other non-planning domains as mentioned in the rebuttal to showcase a broader applicability of the proposed method.
<Providing clearer justifications> Further explanation on why the approach can generalize to non-planning domains would strengthen the robustness of the paper, addressing a potential weakness mentioned in the evaluation.

","{
    ""1"": {
        ""summary"": ""The simplicity of the proposed model might limit its applicability to more complex scenarios beyond MDPs."",
        ""verbatim"": ""The simplicity of the proposed labeling model may not be generalizable to more complex planning paradigms beyond MDPs.""
    },
    ""2"": {
        ""summary"": ""The paper lacks experimental validation for the proposed labeling model."",
        ""verbatim"": ""The authors do not provide experimental evidence or validation for their presented labeling model.""
    },
    ""3"": {
        ""summary"": ""The practical application and scalability of the model are not discussed, which raises concerns about its utility in real-world situations."",
        ""verbatim"": ""The practical application and scalability of the proposed model are not addressed in this paper.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""No explicit reasons for rejection provided in the review."",
        ""verbatim"": ""No related terms -""
    }
}",1,"{
    ""A1-2-B2"": {
        ""rationale"": ""Both Review A1 and Review B2 critique the paucity of experimental validation or outline for the authors' proposed models, albeit in different contexts (explanation clarity and experimental goals in A vs. labeling model validation in B)."",
        ""similarity"": ""7""
    },
    ""A2-3-B3"": {
        ""rationale"": ""Review A2 raises concerns about scalability and generalization of the model to more complex scenarios, similarly to how Review B3 doubts the practical application and scalability of the model, questioning its utility in broader or real-world contexts."",
        ""similarity"": ""7""
    }
}",2,{},0
The Early Phase of Neural Network Training,Hkl1iRNFwS.json,"[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Accuracy of IMP when rewinding to various iterations of the early phase for ResNet-20 sub-networks as a function of sparsity level.
Table 2: Figure 2 :2Figure 2: Rough timeline of the early phase of training for ResNet-20 on CIFAR-10.
Table 3: Figure 3 :Figure 4 :34Figure 3: Basic telemetry about the state of ResNet-20 during the first 4000 iterations (10 epochs). Top row: evaluation accuracy/loss; average weight magnitude; percentage of weights that change sign from initialization; the values of ten randomly-selected weights. Bottom row: gradient magnitude; L2 distance of weights from their initial values and final values at the end of training; cosine similarity of weights from their initial values and final values at the end of training.
Table 4: Figure 5 :5Figure 5: Performance of an IMP-derived ResNet-20 sub-network on CIFAR-10 initialized with the weights at iteration k permuted within various structural elements. Left: k = 500. Right: k = 2000.
Table 5: Figure 6 :6Figure 6: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Left: k = 500. Right: k = 2000.
Table 6: Figure 7 :7Figure 7: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k and Gaussian noise of nσ, where σ is the standard deviation of the initialization distribution for each layer. Left: k = 500. Right: k = 2000.
Table 7: Figure 8 :8Figure 8: The effective standard deviation of various perturbations as a function of mean evaluation accuracy (across 5 seeds) at sparsity 26.2%. The mean of each perturbation was approximately 0. Left: k = 500, r = −0.672, p = 0.008; Right: k = 2000, r = −0.726, p = 0.003.
Table 8: Figure 9 :9Figure 9: The effect of pre-training ResNet-20 on CIFAR-10 with random labels, self-supervised rotation, 4x blurring, and 4x blurring and self-supervised rotation.
Table 9: Figure 10 :10Figure 10: The effect of pretraining sparse sub-networks of Resnet-20 (rewound to iteration 500) with 40 epochs of self-supervised rotation before training on CIFAR-10.
Table 10: Figure A2 :A2Figure A2: The effect of IMP rewinding iteration on the accuracy of sub-networks at various levels of sparsity. Accompanies Figure 1.
Table 11: Figure A3 :Figure A4 :Figure A5 :A3A4A5Figure A3: Basic telemetry about the state of all networks in Table A1 during the first 4000 iterations of training. Accompanies Figure 3.
Table 12: Figure A6 :Figure A7 :A6A7Figure A6: The effect of training an IMP-derived sub-network initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Accompanies Figure 6.
Table 13: Figure A8 :A8Figure A8: The effective standard deviation of each of the perturbations studied in Section 5 as a function of mean evaluation accuracy (across five seeds). Accompanies Figure 8.
Table 14: Figure A9 :Figure A10 :Figure A11 :Figure A12 :A9A10A11A12Figure A9: The effect of pre-training CIFAR-10 with random labels. Accompanies Figure 9.

[INTRODUCTION]
Over the past decade, methods for successfully training big, deep neural networks have revolutionized machine learning. Yet surprisingly, the underlying reasons for the success of these approaches remain poorly understood, despite remarkable empirical performance (Santurkar et al., 2018;Zhang et al., 2017). A large body of work has focused on understanding what happens during the later stages of training (Neyshabur et al., 2019;Yaida, 2019;Chaudhuri & Soatto, 2017;Wei & Schwab, 2019), while the initial phase has been less explored. However, a number of distinct observations indicate that significant and consequential changes are occurring during the most early stage of training. These include the presence of critical periods during training , the dramatic reshaping of the local loss landscape (Sagun et al., 2017;Gur-Ari et al., 2018), and the necessity of rewinding in the context of the lottery ticket hypothesis . Here we perform a thorough investigation of the state of the network in this early stage.
To provide a unified framework for understanding the changes the network undergoes during the early phase, we employ the methodology of iterative magnitude pruning with rewinding (IMP), as detailed below, throughout the bulk of this work . The initial lottery ticket hypothesis, which was validated on comparatively small networks, proposed that small, sparse sub-networks found via pruning of converged larger models could be trained to high performance provided they were initialized with the same values used in the training of the unpruned model . However, follow-up work found that rewinding the weights to their values at some iteration early in the training of the unpruned model, rather than to their initial values, was necessary to achieve good performance on deeper networks such as ResNets . This observation suggests that the changes in the network during this initial phase are vital for the success of the training of small, sparse sub-networks. As a result, this paradigm provides a simple and quantitative scheme for measuring the importance of the weights at various points early in training within an actionable and causal framework.
We make the following contributions, all evaluated across three different network architectures:
1. We provide an in-depth overview of various statistics summarizing learning over the early part of training.
2. We evaluate the impact of perturbing the state of the network in various ways during the early phase of training, finding that:
(i) counter to observations in smaller networks , deeper networks are not robust to reinitializion with random weights, but maintained signs (ii) the distribution of weights after the early phase of training is already highly non-i.i.d., as permuting them dramatically harms performance, even when signs are maintained (iii) both of the above perturbations can roughly be approximated by simply adding noise to the network weights, though this effect is stronger for (ii) than (i)
3. We measure the data-dependence of the early phase of training, finding that pre-training using only p(x) can approximate the changes that occur in the early phase of training, though pre-training must last for far longer (∼32× longer) and not be fed misleading labels.  Lottery ticket rewinding: The original lottery ticket paper  rewound weights to initialization, i.e., k = 0, during IMP. Follow up work on larger models demonstrated that it is necessary to rewind to a later point during training for IMP to succeed, i.e., k << T , where T is total training iterations . Notably, the benefit of rewinding to a later point in training saturates quickly, roughly between 500 and 2000 iterations for ResNet-20 on CIFAR-10 (Figure 1). This timescale is strikingly similar to the changes in the Hessian described below.
Hessian eigenspectrum: The shape of the loss landscape around the network state also appears to change rapidly during the early phase of training (Sagun et al., 2017;Gur-Ari et al., 2018). At initialization, the Hessian of the loss contains a number of large positive and negative eigenvalues. However, very rapidly the curvature is reshaped in a few marked ways: a few large eigenvalues emerge, the bulk eigenvalues are close to zero, and the negative eigenvalues become very small. Moreover, once the Hessian spectrum has reshaped, gradient descent appears to occur largely within the top subspace of the Hessian (Gur-Ari et al., 2018). These results have been largely confirmed in large scale studies (Ghorbani et al., 2019), but note they depend to some extent on architecture and (absence of) batch normalization (Ioffe & Szegedy, 2015). A notable exception to this consistency is the presence of substantial L 1 energy of negative eigenvalues for models trained on ImageNet.
Critical periods in deep learning:  found that perturbing the training process by providing corrupted data early on in training can result in irrevocable damage to the final performance of the network. Note that the timescales over which the authors find a critical period extend well beyond those we study here. However, architecture, learning rate schedule, and regularization all modify the timing of the critical period, and follow-up work found that critical periods were also present for regularization, in particular weight decay and data augmentation (Golatkar et al., 2019).

[PRELIMINARIES AND METHODOLOGY]
Networks: Throughout this paper, we study five standard convolutional neural networks for CIFAR-10. These include the ResNet-20 and ResNet-56 architectures designed for CIFAR-10 (He et al., 2015), the ResNet-18 architecture designed for ImageNet but commonly used on CIFAR-10 ( He et al., 2015), the WRN-16-8 wide residual network (Zagoruyko & Komodakis, 2016)  VGG-13 network (Simonyan & Zisserman (2015) as adapted by ). Throughout the main body of the paper, we show ResNet-20; in Appendix B, we present the same experiments for the other networks. Unless otherwise stated, results were qualitatively similar across all three networks. All experiments in this paper display the mean and standard deviation across five replicates with different random seeds. See Appendix A for further model details.
Iterative magnitude pruning with rewinding: In order to test the effect of various hypotheses about the state of sparse networks early in training, we use the Iterative Magnitude Pruning with rewinding (IMP) procedure of  to extract sub-networks from various points in training that could have learned on their own. The procedure involves training a network to completion, pruning the 20% of weights with the lowest magnitudes globally throughout the network, and rewinding the remaining weights to their values from an earlier iteration k during the initial, pre-pruning training run. This process is iterated to produce networks with high sparsity levels. As demonstrated in , IMP with rewinding leads to sparse sub-networks which can train to high performance even at high sparsity levels > 90%.
Figure 1 shows the results of the IMP with rewinding procedure, showing the accuracy of ResNet-20 at increasing sparsity when performing this procedure for several rewinding values of k. For k ≥ 500, sub-networks can match the performance of the original network with 16.8% of weights remaining. For k > 2000, essentially no further improvement is observed (not shown).

[THE STATE OF THE NETWORK EARLY IN TRAINING]
Many of the aforementioned papers refer to various points in the ""early"" part of training. In this section, we descriptively chart the state of ResNet-20 during the earliest phase of training to provide context for this related work and our subsequent experiments. We specifically focus on the first 4,000 iterations (10 epochs). See Figure A3 for the characterization of additional networks. We include a summary of these results for ResNet-20 as a timeline in Figure 2, and include a broader timeline including results from several previous papers for ResNet-18 in Figure A1.
As shown in Figure 3, during the earliest ten iterations, the network undergoes substantial change. It experiences large gradients that correspond to a rapid increase in distance from the initialization and a large number of sign changes of the weights. After these initial iterations, gradient magnitudes drop and the rate of change in each of the aforementioned quantities gradually slows through the remainder of the period we observe. Interestingly, gradient magnitudes reach a minimum after the first 200 iterations and subsequently increase to a stable level by iteration 500. Evaluation accuracy, improves rapidly, reaching 55% by the end of the first epoch (400 iterations), more than halfway to the final 91.5%. By 2000 iterations, accuracy approaches 80%.
During the first 4000 iterations of training, we observe three sub-phases. In the first phase, lasting only the initial few iterations, gradient magnitudes are very large and, consequently, the network changes rapidly. In the second phase, lasting about 500 iterations, performance quickly improves, weight magnitudes quickly increase, sign differences from initialization quickly increase, and gradient magnitudes reach a minimum before settling at a stable level. Finally, in the third phase, all of these quantities continue to change in the same direction, but begin to decelerate.  

[PERTURBING NEURAL NETWORKS EARLY IN TRAINING]
Figure 1 shows that the changes in the network weights over the first 500 iterations of training are essential to enable high performance at high sparsity levels. What features of this weight transformation are necessary to recover increased performance? Can they be summarized by maintaining the weight signs, but discarding their magnitudes as implied by ? Can they be represented distributionally? In this section, we evaluate these questions by perturbing the early state of the network in various ways. Concretely, we either add noise or shuffle the weights of IMP sub-networks of ResNet-20 across different network sub-compenents and examine the effect on the network's ability to learn thereafter. The sub-networks derived by IMP with rewinding make it possible to understand the causal impact of perturbations on sub-networks that are as capable as the full networks but more visibly decline in performance when improperly configured. To enable comparisons between the experiments in Section 5 and provide a common frame of reference, we measure the effective standard deviation of each perturbation, i.e. stddev(w perturb − w orig ).  show that, for a set of small convolutional networks, signs alone are sufficient to capture the state of lottery ticket sub-networks. However, it is unclear whether signs are still sufficient for larger networks early in training. In Figure 4, we investigate the impact of combining the magnitudes of the weights from one time-point with the signs from another. We found that the signs at iteration 500 paired with the magnitudes from initialization (red line) or from a separate random initialization (green line) were insufficient to maintain the performance reached by using both signs and magnitudes from iteration 500 (orange line), and performance drops to that of using  both magnitudes and signs from initialization (blue line). However, while using the magnitudes from iteration 500 and the signs from initialization, performance is still substantially better than initialization signs and magnitudes. In addition, the overall perturbation to the network by using the magnitudes at iteration 500 and signs from initialization (mean: 0.0, stddev: 0.033) is smaller than by using the signs at iteration 500 and the magnitudes from initialization (0.0 ± 0.042, mean ± std). These results suggest that the change in weight magnitudes over the first 500 iterations of training are substantially more important than the change in the signs for enabling subsequent training.

[ARE SIGNS ALL YOU NEED?]
By iteration 2000, however, pairing the iteration 2000 signs with magnitudes from initialization (red line) reaches similar performance to using the signs from initialization and the magnitudes from iteration 2000 (purple line) though not as high performance as using both from iteration 2000. This result suggests that network signs undergo important changes between iterations 500 and 2000, as only 9% of signs change during this period. Our results also suggest that counter to the observations of  in shallow networks, signs are not sufficient in deeper networks.

[ARE WEIGHT DISTRIBUTIONS I.I.D.?]
Can the changes in weights over the first k iterations be approximated distributionally? To measure this, we permuted the weights at iteration k within various structural sub-components of the network (globally, within layers, and within convolutional filters). If networks are robust to these permutations, it would suggest that the weights in such sub-compenents might be approximated and sampled from. As Figure 5 shows, however, we found that performance was not robust to shuffling weights globally (green line) or within layers (red line), and drops substantially to no better than that of the original initialization (blue line) at both 500 and 2000 iterations. 1 Shuffling within filters (purple line) performs slightly better, but results in a smaller overall perturbation (0.0 ± 0.092 for k = 500) than shuffling layerwise (0.0±0.143) or globally (0.0±0.144), suggesting that this change in perturbation strength may simply account for the difference. Are the signs from the rewinding iteration, k, sufficient to recover the damage caused by permutation? In Figure 6, we also consider shuffling only amongst weights that have the same sign. Doing so substantially improves the performance of the filter-wise shuffle; however, it also reduces the extent of the overall perturbation (0.0 ± 0.049 for k = 500). It also improves the performance of shuffling within layers slightly for k = 500 and substantially for k = 2000. We attribute the behavior for k = 2000 to the signs just as in Figure 4: when the magnitudes are similar in value (Figure 4 red line) or distribution (Figure 6 red and green lines), using the signs improves performance. Reverting back to the initial signs while shuffling magnitudes within layers (brown line), however, damages the network too severely (0.0 ± 0.087 for k = 500) to yield any performance improvement over random noise. These results suggest that, while the signs from initialization are not sufficient for high performance at high sparsity as shown in Section 5.1, the signs from the rewinding iteration are sufficient to recover the damage caused by permutation, at least to some extent.

[IS IT ALL JUST NOISE?]
Some of our previous results suggested that the impact of signs and permutations may simply reduce to adding noise to the weights. To evaluate this hypothesis, we next study the effect of simply adding Gaussian noise to the network weights at iteration k. To add noise appropriately for layers with different scales, the standard deviation of the noise added for each layer was normalized to a multiple of the standard deviation σ of the initialization distribution for that layer. In Figure 7, we see that for iteration k = 500, sub-networks can tolerate 0.5σ to 1σ of noise before performance degrades back to that of the original initialization at higher levels of noise. For iteration k = 2000, networks are surprisingly robust to noise up to 1σ, and even 2σ exhibits nontrivial performance.
In Figure 8, we plot the performance of each network at a fixed sparsity level as a function of the effective standard deviation of the noise imposed by each of the aforementioned perturbations. We find that the standard deviation of the effective noise explained fairly well the resultant performance (k = 500: r = −0.672, p = 0.008; k = 2000: r = −0.726, p = 0.003). As expected, perturbations that preserved the performance of the network generally resulted in smaller changes to the state of the network at iteration k. Interestingly, experiments that mixed signs and magnitudes from different points in training (green points) aligned least well with this pattern: the standard deviation of the perturbation is roughly similar among all of these experiments, but the accuracy of the resulting networks changes substantially. This result suggests that although the standard deviation of the noise is certainly indicative of lower accuracy, there are still specific perturbations that, while small in overall magnitude, can have a large effect on the network's ability to learn, suggesting that the observed perturbation effects are not, in fact, just a consequence of noise.

[THE DATA-DEPENDENCE OF NEURAL NETWORKS EARLY IN TRAINING]
Section 5 suggests that the change in network behavior by iteration k is not due to easilyascertainable, distributional properties of the network weights and signs. Rather, it appears that training is required to reach these network states. It is unclear, however, the extent to which various aspects of the data distribution are necessary. Mainly, is the change in weights during the early phase of training dependent on p(x) or p(y|x)? Here, we attempt to answer this question by mea-  suring the extent to which we can re-create a favorable network state for sub-network training using restricted information from the training data and labels. In particular, we consider pre-training the network with techniques that ignore labels entirely (self-supervised rotation prediction, Section 6.2), provide misleading labels (training with random labels, Section 6.1), or eliminate information from examples (blurring training examples Section 6.3).
We first train a randomly-initialized, unpruned network on CIFAR-10 on the pre-training task for a set number of epochs. After pre-training, we train the network normally as if the pre-trained state were the original initialization. We then use the state of the network at the end of the pre-training phase as the ""initialization"" to find masks for IMP. Finally, we examine the performance of the IMPpruned sub-networks as initialized using the state after pre-training. This experiment determines the extent to which pre-training places the network in a state suitable for sub-network training as compared to using the state of the network at iteration k of training on the original task.

[RANDOM LABELS]
To evaluate whether this phase of training is dependent on underlying structure in the data, we drew inspiration from Zhang et al. (2017) and pre-trained networks on data with randomized labels. This experiment tests whether the input distribution of the training data is sufficient to put the network in a position from which IMP with rewinding can find a sparse, trainable sub-network despite the presence of incorrect (not just missing) labels. Figure 9 (upper left) shows that pre-training on random labels for up to 10 epochs provides no improvement above rewinding to iteration 0 and that pre-training for longer begins to hurt accuracy. This result suggests that, though it is still possible that labels may not be required for learning, the presence incorrect labels is sufficient to prevent learning which approximates the early phase of training.

[SELF-SUPERVISED ROTATION PREDICTION]
What if we remove labels entirely? Is p(x) sufficient to approximate the early phase of training? Historically, neural network training often involved two steps: a self-supervised pre-training phase followed by a supervised phase on the target task (Erhan et al., 2010). Here, we consider one such self-supervised technique: rotation prediction (Gidaris et al., 2018). During the pre-training phase, the network is presented with a training image that has randomly been rotated 90n degrees (where n ∈ {0, 1, 2, 3}). The network must classify examples by the value of n. If self-supervised pretraining can approximate the early phase of training, it would suggest that p(x) is sufficient on its own. Indeed, as shown in Figure 9 (upper right), this pre-training regime leads to well-trainable subnetworks, though networks must be trained for many more epochs compared to supervised training (40 compared to 1.25, or a factor of 32×). This result suggests that the labels for the ultimate task themselves are not necessary to put the network in such a state (although explicitly misleading labels are detrimental). We emphasize that the duration of the pre-training phase required is an order of magnitude larger than the original rewinding iteration, however, suggesting that labels add important information which accelerates the learning process.

[BLURRING TRAINING EXAMPLES]
To probe the importance of p(x) for the early phase of training, we study the extent to which the training input distribution is necessary. Namely, we pretrain using blurred training inputs with the correct labels. Following , we blur training inputs by downsampling by 4x and then upsampling back to the full size. Figure 9 (bottom left) shows that this pre-training method succeeds: after 40 epochs of pre-training, IMP with rewinding can find sub-networks that are similar in performance to those found after training on the original task for 500 iterations (1.25 epochs).
Due to the success of the the rotation and blurring pre-training tasks, we explored the effect of combining these pre-training techniques. Doing so tests the extent to which we can discard both the training labels and some information from the training inputs. Figure 9 (bottom right) shows that doing so provides the network too little information: no amount of pre-training we considered makes it possible for IMP with rewinding to find sub-networks that perform tangibly better than rewinding to iteration 0. Interestingly however, as shown in Appendix B, trainable sub-networks are found for VGG-13 with this pre-training regime, suggesting that different network architectures have different sensitivities to the deprivation of labels and input content.

[SPARSE PRETRAINING]
Since sparse sub-networks are often challenging to train from scratch without the proper initialization (Han et al., 2015;, does pre-training make it easier for sparse neural networks to learn? Doing so would serve as a rough form of curriculum learning (Bengio et al., 2009) for sparse neural networks. We experimented with training sparse sub-networks of ResNet-20 (IMP sub-networks, randomly reinitialized sub-networks, and randomly pruned subnetworks) first on self-supervised rotation and then on the main task, but found no benefit beyond rewinding to iteration 0 (Figure 10). Moreover, doing so when starting from a sub-network rewound to iteration 500 actually hurts final accuracy. This result suggests that while pre-training is sufficient to approximate the early phase of supervised training with an appropriately structured mask, it is not sufficient to do so with an inappropriate mask. 

[DISCUSSION]
In this paper, we first performed extensive measurements of various statistics summarizing learning over the early part of training. Notably, we uncovered 3 sub-phases: in the very first iterations, gradient magnitudes are anomalously large and motion is rapid. Subsequently, gradients overshoot to smaller magnitudes before leveling off while performance increases rapidly. Then, learning slowly begins to decelerate. We then studied a suite of perturbations to the network state in the early phase finding that, counter to observations in smaller networks , deeper networks are not robust to reinitializing with random weights with maintained signs. We also found that the weight distribution after the early phase of training is highly non-independent. Finally, we measured the data-dependence of the early phase with the surprising result that pre-training on a self-supervised task yields equivalent performance to late rewinding with IMP.
These results have significant implications for the lottery ticket hypothesis. The seeming necessity of late rewinding calls into question certain interpretations of lottery tickets as well as the ability to identify sub-networks at initialization. Our observation that weights are highly non-independent at the rewinding point suggests that the weights at this point cannot be easily approximated, making approaches which attempt to ""jump"" directly to the rewinding point unlikely to succeed. However, our result that labels are not necessary to approximate the rewinding point suggests that the learning during this phase does not require task-specific information, suggesting that rewinding may not be necessary if networks are pre-trained appropriately.","[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.","{
    ""1"": {
        ""summary"": ""The paper lacks practical application and efficient guidance for the lottery tickets hypothesis."",
        ""verbatim"": ""Although the paper gives extensive measurements and observations about the early phase about network training, it doesn't provide useful and efficient guidance for the lottery tickets hypothesis. In other words, the observation is interesting but the novelty is arguable. How can we use these 'implications' to find better tickets? I mean it should be efficient. It is not reasonable cost more the find the best weight magnitude point or sign point for a better initialization.""
    },
    ""2"": {
        ""summary"": ""The paper has inconsistencies in results and lacks some explanations."",
        ""verbatim"": ""The results of Fig. 4 and Fig.6 can be inconsistent. In Figure 4, it says that signs are less important than magnitudes. In Figure 6, it says that signs are more important than magnitudes if shuffling filters and keep signs. Any explanation on the inconsistency?""
    },
    ""3"": {
        ""summary"": ""The paper lacks originality in proposing new network properties and the conclusions are very empirical and limited in generalization."",
        ""verbatim"": ""The work itself did not propose any new network properties or any new metric to measure. Most experiments are designed for previous observations and mostly for verification purpose. I am concern about the core motivation of this work, like to identify or solve any new problems, in addition to experimentally verify the observations during network’s training.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Study's limited focus on specific neural network architectures"",
        ""verbatim"": ""The study focuses on a specific set of convolutional neural network architectures, which may limit the generalizability of the findings.""
    },
    ""2"": {
        ""summary"": ""Lack of practical implications articulated"",
        ""verbatim"": ""While the paper delves into the theoretical aspects of early network training, the practical implications of the research may not be clearly articulated, potentially reducing the broader impact of the study.""
    },
    ""3"": {
        ""summary"": ""Absence of comparative analysis"",
        ""verbatim"": ""The absence of comparative analysis with other existing methods or approaches in studying the early phase of training could weaken the novelty and impact of the research.""
    },
    ""4"": {
        ""summary"": ""Limited scope of experimental validation"",
        ""verbatim"": ""The experiments conducted mainly focus on ResNet-20 architecture on CIFAR-10, and the paper could benefit from broader experimentation on diverse datasets and network architectures to strengthen the validity of the conclusions.""
    }
}",4,"{
    ""A1-B2"": {
        ""rationale"": ""Both points criticize the lack of practical applications or guidance emerging from the study's theoretical findings. Review A focuses on the insufficiency of providing efficient guidance for utilizing the lottery tickets hypothesis practically, while Review B highlights that the paper does not clearly articulate the practical implications, affecting the broader impact. Both reviews express concerns about the research's direct applicability."",
        ""similarity"": ""8""
    },
    ""A3-B3"": {
        ""rationale"": ""Both reviews raise concerns regarding the originality and novelty of the work presented. Review A mentions the lack of new network properties or metrics, implying that the study lacks innovative elements and is mostly verifying previous observations. Review B criticizes the absence of a comparative analysis with other methods, suggesting that this omission might weaken the novelty and impact. Both points express the issue from the perspective of needing comparison or uniqueness to enhance the study's contribution."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks sufficient clarification on practical implications of early training changes."",
        ""verbatim"": ""The paper could benefit from further clarification on the practical implications of the observed changes in deep neural networks during the early training phase.""
    },
    ""2"": {
        ""summary"": ""The paper needs more extensive experimental validation for the conclusions."",
        ""verbatim"": ""While the findings are intriguing, there may be a need for more extensive experimental validation to strengthen the conclusions drawn about the network behavior.""
    },
    ""3"": {
        ""summary"": ""The significance of early training changes needs clearer articulation in relation to existing literature."",
        ""verbatim"": ""The paper could face rejection if the significance of the observed changes during early training is not clearly articulated in relation to existing literature on neural network training dynamics.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A1 and Review B1 express concerns regarding the practical usability and implications of the observations made during the early training phase of neural networks. Each review criticizes the paper for not providing actionable or useful guidelines on how to implement or benefit from these observations, particularly in relation to the lottery tickets hypothesis in Review A1."",
        ""similarity"": ""7""
    },
    ""A2-B2"": {
        ""rationale"": ""Review A2 and Review B2 both spotlight the reliability of the experimental results presented in the paper. Review A2 discusses specific inconsistencies among the results shown in different figures and highlights the lack of explanations for these inconsistencies. Review B2 suggests that the experiments might need more extensive validation to solidify the conclusions, addressing a similar concern about the strength and consistency of the paper's experimental evidence."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The study might lack general utility across the wider range of neural network applications."",
        ""verbatim"": ""The focus on the early phase of training may limit the generalizability of the findings to the overall performance and behavior of neural networks.""
    },
    ""2"": {
        ""summary"": ""The paper lacks a thorough comparison with existing methods, which could provide more comprehensive insights."",
        ""verbatim"": ""The paper could benefit from a more comprehensive comparison with existing techniques and methodologies related to early-phase training dynamics for a better context and insights.""
    }
}",2,"{
    ""A3-B1"": {
        ""rationale"": ""Review A3 discusses the limitation of the paper in providing original network properties or broader, generalizable solutions, reflecting on the empirical and limited nature of the conclusions. Review B1 touches on similar concerns regarding the general utility and generalizability of the results due to the narrow focus on the early phase of neural network training. Both reviews critique the limited scope and the applicative breadth of the research, hinting at the paper's lack in offering broader, innovative understandings applicable across diverse scenarios."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The study's focus is largely limited to ResNet architectures, potentially reducing the generalizability of its findings to other network types."",
        ""verbatim"": ""'Limited focus on architectural dependency': The study predominantly uses ResNet architectures and might not account sufficiently for behaviors in other network types, which could limit the generalizability of the findings.""
    },
    ""2"": {
        ""summary"": ""The complexity of experimental setups involving iterative magnitude pruning with rewinding could make the research difficult to understand and reproduce."",
        ""verbatim"": ""'Complexity of experimental setups': The potentially complex nature of the experiments involving iterative magnitude pruning with rewinding could obscure understanding and reproducibility for some readers.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper lacks detailed methodology, which could lead to questions regarding reproducibility and scientific rigor."",
        ""verbatim"": ""'Lack of Detailed Methodology': The paper does not adequately detail the methodologies used for the experimental setup, which might lead to questions regarding reproducibility and scientific rigor.""
    },
    ""2"": {
        ""summary"": ""The absence of detailed statistical analysis might leave the conclusions less convincing."",
        ""verbatim"": ""'Insignificant Statistical Analysis': The absence of detailed statistical analysis to support the findings could be seen as a significant gap, as it leaves the conclusions less convincing.""
    },
    ""3"": {
        ""summary"": ""The paper's narrow focus on the initial training phase might limit its appeal to a broader audience."",
        ""verbatim"": ""'Narrow Focus on Initial Training Phase': The exclusive focus on the early phase of training, while novel, might limit the paper's appeal to a broader audience interested in the entire lifecycle of neural network training.""
    }
}",3,"{
    ""A1-B3"": {
        ""rationale"": ""Both points criticize the paper for not providing actionable or broad-reaching insights. Review A states a lack of practical application of the findings for advancing the lottery tickets hypothesis, while Review B mentions that the paper's focus on the initial training phase limits its broader applicability and appeal. Both reviews highlight a similar gap in the paper's utility across a wider context."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Concerns about methodological gaps"",
        ""verbatim"": ""The paper could face criticism for potential gaps in methodology, particularly if the methods used for analyzing the network changes are not adequately validated against established benchmarks or if the experimental setup lacks sufficient detail.""
    },
    ""2"": {
        ""summary"": ""Concerns about the generalization of results"",
        ""verbatim"": ""There might be concerns regarding the generalizability of the findings across different architectures or datasets, as deep learning behaviors can significantly vary under different conditions.""
    }
}",2,"{
    ""A3-B2"": {
        ""rationale"": ""Both Review A and B highlight concerns about the generalization of the results and implications of the experiments. Review A discusses the lack of originality and empirical nature of the results that are limited in their generalization, while Review B raises concerns about the generalizability of the findings across various architectures or datasets."",
        ""similarity"": ""7""
    }
}",1,"[Significance and novelty]
This paper contributes to the field of deep learning research by shedding new insights into the early phase of neural network training. Previous studies have shown that significant changes occur during the initial stages of training, yet the nature of these changes remains poorly understood. The paper presents novel findings on the network state during this crucial period, using extensive measurements and quantitative analyses. The results demonstrate that deep networks are not robust to random weight reinitialization and that weight distributions become highly dependent after only a few hundred iterations. The paper also suggests that these changes are not inherently label-dependent but accelerated by labeled data.

[Potential reasons for acceptance]
1. Novel findings on neural network training: The paper contributes original insights into the early learning phase of deep neural networks, expanding our understanding of this pivotal stage of the training process.
2. Quantitative analysis: The study uses rigorous measurements and precise frameworks to probe deep networks, producing quantitative evidence for the stated results.
3. Relevance to deep learning community: The findings in this study hold significant implications for the broader deep learning research community, as understanding the early stages of neural network training can lead to improved optimization techniques and accelerated learning.

[Potential reasons for rejection]
1. Limited generalizability: The results in this study are specific to deep neural networks within the stated framework, and it remains unclear whether the findings can be extended to other architectures or training methodologies.
2. Missing theoretical explanation: While the paper presents empirical evidence for observed network changes, it lacks a clear theoretical explanation for why these phenomena occur.

[Suggestions for improvement]
1. Generalize the findings: Expand the analysis beyond the specific network architecture and exploration framework to assess the applicability of the results to other neural network designs and optimization techniques.
2. Provide theoretical explanation: Offer a theoretical understanding of the observed phenomena, establishing a cohesive, interpretable framework for understanding the changes in deep learning networks during training.
3. Validate the findings: Conduct additional experiments and simulations to validate the results, ensuring the robustness of the stated observations.","[Significance and novelty]
<Early training behavior analysis> The paper explores the behavior of neural networks during the early training phase, focusing on the stability and sparsity of the network over a limited number of iterations. It discusses the effects of pretraining methods and presents a new metric, weight dispersion, to study network stability.
<Comparison with related work> The paper compares its findings with previous studies and offers a potential improvement in the form of the weight dispersion metric, contributing to the existing body of knowledge on neural network training.

[Potential reasons for acceptance]
<Exploratory study> The paper is acknowledged for its exploration of an interesting and understudied topic of neural network behavior during the early training phase.
<Insightful findings> The paper is recognized for presenting interesting insights into the behavior of network parameters in the early training phase and for proposing a new metric, weight dispersion, to study network stability.

[Potential reasons for rejection]
<Limited practicality of proposed metric> Reviewers express concerns about the practical utility of the weight dispersion metric, noting that its practical use is unclear and its value is not proven in a controlled manner.
<Lack of clarity on implications> The practical implications of the study are uncertain, and the link to real-world applications, such as model selection, is questioned by the reviewers.

[Suggestions for improvement]
<Enhanced experimentation> Reviewers recommend additional experiments to demonstrate the usefulness of the weight dispersion metric to understand and predict training and testing errors.
<Clarification of practical significance> Clear and extensive discussions are suggested to demonstrate how the findings will be useful in real-world applications, such as model selection in the supervised setting.

","{
    ""1"": {
        ""summary"": ""The study's results may not apply to diverse neural network architectures or training methodologies, limiting its broader applicability."",
        ""verbatim"": ""Limited generalizability: The results in this study are specific to deep neural networks within the stated framework, and it remains unclear whether the findings can be extended to other architectures or training methodologies.""
    },
    ""2"": {
        ""summary"": ""The paper lacks a detailed theoretical explanation for the observed changes in neural networks during the early stages of training."",
        ""verbatim"": ""Missing theoretical explanation: While the paper presents empirical evidence for observed network changes, it lacks a clear theoretical explanation for why these phenomena occur.""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Limited practical utility of the weight dispersion metric"",
        ""verbatim"": ""Limited practicality of proposed metric: Reviewers express concerns about the practical utility of the weight dispersion metric, noting that its practical use is unclear and its value is not proven in a controlled manner.""
    },
    ""2"": {
        ""summary"": ""Uncertainty in practical implications of the study"",
        ""verbatim"": ""Lack of clarity on implications: The practical implications of the study are uncertain, and the link to real-world applications, such as model selection, is questioned by the reviewers.""
    }
}",2,"{
    ""A3-B1"": {
        ""rationale"": ""Both review points express concerns about the generalization of the study. Review A emphasizes the lack of originality in proposing new network properties and suggests that the conclusions drawn are very empirical and limited in generalization. Review B highlights limited generalizability to different architectures or training methodologies. Both reviews share concerns about the extrapolation of the results beyond the specific scenarios tested in the study."",
        ""similarity"": ""7""
    }
}",1,"{
    ""A1-B2"": {
        ""rationale"": ""Both points raise concerns about the practical applications or utility of the study's findings or metrics. Review A criticizes the lack of efficient guidance for applying the lottery tickets hypothesis, emphasizing the need for cost-effective methods in finding better network initialization points. Review B questions the clarity and practical implications of the study, particularly how it links to real-world applications like model selection. Both reviews suggest a gap between the theoretical or experimental findings and their practical, actionable applications."",
        ""similarity"": ""7""
    }
}",1
Cyclic orthogonal convolutions for long-range integration of features,868DWd46dv2.json,"[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual.

[CAPTIONS]
Table 1: Figure 2 :2Figure2: Illustration of CycleNet. (a) One cycle is defined as three convolutions performed in sequence: first on (x, y) (green), second on (x, z) (red), and third on (y, z) (blue), each convolution is followed by BatchNorm, ReLU and Dropout (not shown). (b) The full model consists of a stack of cycles, plus a first convolutional layer and a final dense layer.
Table 2: Figure 3 :3Figure 3: Test accuracy on image classification. (a) Performance of CycleNet and the CNN baseline on CIFAR-10, as a function of the number of parameters. Each point corresponds to a different network depth, from one to six cycles, and is an average of five experiments with identical optimized hyperparameters. ResNet-18 is taken from (He et al. (2016)). (b). Left: performance of CycleNet, the CNN baseline and 2:3 hybrid model on ImageNet (128×128) as a function of the number of parameters. MobileNet is taken from (Howard et al. (2017)). Right: we progressively substitute CycleNet with standard convolutional cycles. Non-cubic: experiments with non-cubic tensor shape. Details about different CycleNet architectures an be found in appendix B.1.
Table 3: Figure 4 :4Figure 4: Receptive field sizes and shapes. (a) Normalized receptive field size as a function of activation depth (number of layers) of CycleNet and the CNN baseline, for ImageNet and CIFAR-10. Each point is the mean and standard deviation computed on 100 receptive fields chosen at random. CycleNet achieves large size after one cycle (3 layers), while the size increases with depth in the baseline, and achieves similar size after 18 layers. (b) Sample saliency maps in CycleNet (top) and the baseline (bottom) on ImageNet, after 3 (left) and 15 layers (right). Yellow ellipses show the receptive field shape, covering 3 standard deviations.
Table 4: Figure 5 :5Figure 5: Pathfinder challenge. Each panel shows the accuracy of CycleNet and the CNN baseline as a function of kernel size. Task difficulty increases from left to right (path length n =6, 9, 14). Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[INTRODUCTION]
Several computer vision tasks require capturing long-range dependencies between features. For example, in order to recognize a teapot, it is necessary to identify and integrate several of its parts in their correct spatial relationship, e.g. a spout, a handle, a lid, while each part by itself is usually not enough to determine the object class. Convolutional layers have proven very useful at several computer vision tasks such as classification, and they are at the core of most state-of-the-art neural networks (LeCun et al. (2015)). A convolution transforms each pixel depending on a few neighboring pixels, and the transformation is shared across all pixels; however, the small size of the neighbourhood implies that in a single layer features at distant locations cannot be integrated. In deep convolutional networks, the receptive field size increases sub-linearly with depth (Luo et al. (2017)), therefore a large number of layers is necessary for integrating features across long distances. We propose a novel neural network architecture that retains the efficient parameterization of convolutions, while promoting long-range interactions of distant features.
Similar to the human visual system, individual 'neurons' within each CNN have receptive fields with size increasing with the number of layers. However, while the human visual system achieves large receptive fields within a handful of layers, CNNs need many more (Luo et al. (2017)). In fact, the number of layers keeps expanding to 100s or 1000s of layers for state of the art CNN models (Tan & Le (2019); He et al. (2016)). A neuron of the visual system connects not only to other neurons responding to similar spatial locations, but also to neurons at distant locations, provided that they share similar visual features, such as edge orientation (see Fig. 1a). These connections not only enlarge the receptive fields, but they also facilitate solving tasks that require long-range integration of features. For example, they allow tracking curved contours (Fig. 1b). The neurons responsible for the long-range connections are excitatory pyramidal cells, which are known to have a long range connections in the cortex. Instead, inhibitory GABA-ergic cells do not have this property.
Our model architecture agrees with these biological principles of information processing taking place in the brain. Given a tensor of horizontal (x) and vertical (y) coordinates and features (z), we propose to apply convolutions not only on (x, y) coordinates, but also on (x, z) and (y, z) in sequence. These orthogonal convolutions allow the interaction of 'neurons' at distant locations, provided that they share similar visual features, similarly to the human visual system. We show that, after a cycle of three convolutions over the three axes, each element of the tensor depends on all elements of the input, and features at all locations have a chance to interact. Our model, named CycleNet, is obtained by adding a permutation of the axes to a convolution, and therefore is easy to compare with a standard CNN. In order to evaluate our model, we compare it with a CNN baseline network with identical architecture, i.e. same number of parameters and tensor shape in all layers.
In this paper we show that Cyclenet: outperforms the CNN baseline at image classification on CIFAR-10 and ImageNet (Krizhevsky (2009), Deng et al. (2009)), and approaches the performance of ResNet and MobileNet (He et al. (2016), Howard et al. (2017)) with a similar number of parameters; achieves the maximum receptive field size after a single cycle and outperforms the baseline CNN in transfer learning to stylised data, suggesting the use of more features dependent on shape rather than texture; outperforms the baseline by a large margin at the Pathfinder challenge, a task inspired by the study of human visual perception where CNNs are known to fail dramatically (Linsley et al. (2019)).   (Linsley et al. (2019)). Each image has two circles attached to a path. The goal of this task is to classify images into connected and disconnected, two examples shown for each class.

[RELATED WORK]
A few recent papers showed that deep convolutional networks struggle to learn features integrating large spatial domains. For instance Luo et al. (2017) showed that the receptive field size of a deep CNN is smaller than the sum of its kernel sizes. Geirhos et al. (2019) showed that ImageNet-trained ResNet relies on small image patches and textures, rather than object shapes, and does not transfer well to stylised images. Linsley et al. (2019) showed that ResNet struggles on the Pathfinder challenge, a simple task that requires integrating features over long distances.
Other methods have been proposed to learn large scale features. Pooling layers increase the receptive field size in CNNs, but they lose a significant amount of information, including the spatial relationship between features (Boureau et al. (2010)). Multiscale pyramids use kernels of different sizes arranged in parallel, but larger kernels have lower resolution in order to keep a reasonable number of parameters (Farabet et al. (2013)). Deformable convolutions adaptively learn the shape of the kernel, at the cost of an increased complexity of the model (Dai et al. (2017)). Dilated convolutions increase the kernel size along network depth (Yu & Koltun (2015)) and they are equivalent to a tensor decomposition (Huszar (2016)). Tensor decompositions have been widely used to reduce the complexity of convolutional and dense layers (Kuzmin et al. (2019); Novikov et al. (2015)). We show below that one cycle of CycleNet is equivalent to a decomposition of a dense layer, in the simple case of 1 × 1 convolutions and linear activations, but not in the general case. CycleNet is also different from a 3D convolution (Ji et al. (2013)), since spatial coordinates are fully connected in the second and third layers of a cycle.
Among non-convolutional architectures, self-attention networks naturally capture long-range dependencies. Initially designed for natural language processing (Vaswani et al. (2017)), transformers were recently shown to exhibit good performance on vision tasks ). Similar to self-attention networks, CycleNet breaks translational symmetry and has the best performance on ImageNet when standard convolutions are included in the first layers (Ramachandran et al. (2019)).

[MODEL: A CYCLE OF THREE ORTHOGONAL CONVOLUTIONS]
In this section we describe a single cycle, the basic building block of CycleNet, which is composed of three layers arranged in sequence (Fig. 2a). We start by describing a single convolutional layer. We denote the input tensor as I(x, y, z), at coordinates x (horizontal), y (vertical) and feature z. The output tensor S is equal to:
S(x, y, z) = dx,dy,z K(dx, dy, z, z )I(x + dx, y + dy, z ).(1)
where K denotes the convolutional kernel. We assume zero padding and kernel stride equal to one, while other parameters vary in different experiments (kernel size, input and output tensor shapes). The convolution operation is local: the output at a given location (x, y) depends only on the input displaced by dx, dy, spanning neighboring pixels up to the kernel size, which is typically much smaller than the size of the input. On the other hand, features z are all-to-all fully connected. This is a standard convolutional layer and is depicted as the first layer of Fig. 2a (green). Convolutional layers are usually stacked and interleaved with other types of layers, such as nonlinearities, down(up)sampling, skip connections, normalization, regularization, etc. In this study, we use BatchNorm (Ioffe & Szegedy (2015)), ReLU (Glorot et al. (2011)), Dropout (Srivastava et al. (2014)), and no skip connections. In order to promote long-distance integration, we propose to apply convolutions not only in the (x, y) plane, but also in the (x, z) and (y, z) planes. With some abuse of notation, we denote I as the output of the previous layer, and S as the output of the current layer. The second layer in a cycle is given by the following convolution,
S(x, y, z) = dx,y ,dz K(dx, dz, y, y )I(x + dx, y , z + dz),(2)
also illustrated in Fig. 2a (red). Here, vertical coordinates y are fully connected. Subsequently, after another BatchNorm, ReLU and Dropout, we apply a convolution in the (y, z) plane:
S(x, y, z) =
x ,dy,dz K(dy, dz, x, x )I(x , y + dy, z + dz).
(3) This is the third and final layer in a cycle, illustrated in Fig. 2a (blue), and is also followed by BatchNorm, ReLU and Dropout. Here, horizontal coordinates x are fully connected. The sequence of three convolutions along the three different axes constitutes a cycle. Note that a cycle could be defined with a different ordering of the three convolutions, but we did not explore other configurations.
Further architecture details about the long range integration, the number of parameters, the comparison with a convolutional baseline and the scaling to deeper networks can be found in Appendix A.
3 Experiments

[RECEPTIVE FIELDS]
We train CycleNet and the baseline CNN on CIFAR-10 and ImageNet, benchmarks for image classification over which CNNs have been extensively optimized. Experimental results can be seen in Figure 3 and detailed description and discussion of these experiments can be found in appendix B.1. Figure 3 shows that CycleNet has a higher accuracy than the baseline and is comparable to state of the art CNN models of similar size. Is this because CycleNet learns substantially different representations of the images compared to standard CNN? We investigate this using receptive fields. At the end of a cycle, each element of the output tensor may depend on all elements of the input tensor (see appendix A.1), thus CycleNet is expected to have large receptive fields. In contrast, the receptive fields of CNNs are usually small (Luo et al. (2017)). Here, we compute the receptive fields at the output of each cycle of our models with 18 layers for CIFAR and 15 layers for ImageNet: first, we compute saliency maps following Simonyan et al. (2014), then we use those maps to compute receptive fields following Luo et al. (2017). Details can be found in section 3.3.
Figure 4a shows receptive field size as a function of depth, where the receptive field is normalized by the resolution (32 for CIFAR, 128 for ImageNet). Each bar is computed across 100 receptive fields sampled at random, 10 activations times 10 images. As expected, CycleNet achieves a large receptive field size after one cycle (3 layers), while the CNN baseline increases the receptive field size with depth, and approaches the size of CycleNet after 18 layers. Fig. 4b shows a few visual examples of receptive field shapes for both CycleNet and the CNN baseline at different layer positions.
Larger receptive fields are fundamental in tasks requiring long-range integration of features and the features developed implicitly rely on more global patters, favouring classification of images by shape rather than texture. These are investigated in Section 3.2 and section 3.3. 

[PATHFINDER CHALLENGE]
In order to test whether large receptive fields help integrating long-range features, we evaluated CycleNet on the Pathfinder challenge (Fig. 1d). This task is inspired by the study of human visual perception, and requires tracking paths over long distances to distinguish whether two white circles are connected. Deep convolutional networks struggle on this task (e.g. ResNet50), because of their limited ability of integrating features over long distances (Linsley et al. (2019)). Since CycleNet integrates all pixels after one cycle, we predict that it should perform well on this task. Linsley et al. (2019) proposes a biologically-inspired horizontal gated recurrent neural network that performs 100% on the task, but their model is hard to compare with standard CNNs. Details about the Pathfinder dataset as well as the architectures used can be found in appendix B.3.
Figure 5 shows the performance of CycleNet and the CNN baseline in 3 challenges of increasing difficulty, i.e. increasing path lengths: n = 6, 9, and 14. For kernel sizes 4, 8, and 12, the CNN baseline does not learn the task. It performs at chance level (50%), except in one experiment of path length 6. On the other hand, CycleNet shows good accuracy, up to nearly 100%, suggesting that it is able to take advantage of its larger receptive fields. Furthermore, its performance decreases with path length and increases with kernel size. Note that large receptive fields may not be enough to solve this task, it also requires an increasing level of expressivity at larger path lengths. This is provided by a larger kernel size in CycleNet. For kernel size 20 the CNN baseline is able to perform the task, sometimes better than CycleNet; note that however for this kernel size the training has not converged as we were limited by computational resources and the models were very large due to the size of the kernel. We can only observe that for this case the CNN converged faster. At this size, a single kernel covers a substantial part of the image and is effectively long-range. However, the number of parameters is large in this case and the efficiency of the parameterization is lost. . Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[STYLISED IMAGES]
Given that CycleNet has large receptive fields, we hypothesise that it should classify images by the shape rather than texture of objects. Geirhos et al. (2019) designed new benchmarks to explore the shape vs texture bias of deep neural networks trained on ImageNet (IN). In the Stylised ImageNet dataset (SIN), the texture of each image is replaced with a randomly selected painting style (Fig. 6, top). In the Cue-Conflict dataset (CC), a few images are generated by iterative style transfer from selected textures consisting of patches of other classes (Fig. 6, bottom). We predict that our IN pre-trained CycleNet should transfer better than the baseline to both SIN and CC. We generate SIN following the scripts at https://github.com/rgeirhos/Stylized-ImageNet, and data for CC is sourced from https://github.com/rgeirhos/texture-vs-shape. Both experiments are evaluated at resolution 128x128.
Table 1 shows evaluation top-5 accuracy on Stylised ImageNet and top-1 accuracy on the Cue-Conflict dataset. The results confirm that CycleNet transfers better to both datasets. Although the overall accuracy on ImageNet is higher for a hybrid network where the first two cycles have standard convolutions (2:3), the best transfer to stylised images is obtained by CycleNet, where all cycles use orthogonal convolutions (0:5). These results confirm that large receptive fields are instrumental for biasing a neural network towards shape, and that the base performance on ImageNet by itself is not a good predictor of such bias. 

[DISCUSSION]
We proposed a biologically inspired neural network architecture, CycleNet, that achieves better performance than a CNN baseline in classification tasks, and it develops a significantly different representation of the input image, where node activations have large receptive fields and are thus able to represent large portions of an image. This is in agreement with the human visual system that achieves large receptive fields within few layers and in sharp contrast with CNNs, which have smaller receptive fields. CycleNet transfers better than the CNN baseline to stylised images, suggesting that the large receptive fields bias the model towards shape rather than texture of objects, and performs much better at the Pathfinder task, which requires long-range integration of features.
We emphasize that CycleNet loses translational symmetry, which is considered a strong feature of CNNs. However, recent evidence suggests that this property may not be crucial. Symmetries do not have to be hard coded in the architecture: they can be learned by stochastic gradient descent (Achille & Soatto (2018)) and data augmentation (Taylor & Nitschke (2017)). Furthermore, several nontranslational symmetric architectures recently achieved near state-of-the-art performance in image classification (Ramachandran et al. (2019)). The good performance of CycleNet on classification adds to this line of research, suggesting that built-in translational symmetry may not be necessary.
Performance in other tasks is likely to benefit from long-range integration, such as image segmentation, generation, reconstruction, etc. Future work may focus on testing CycleNet on those tasks.
A Appendix: Architecture details

[A.1 LONG-RANGE INTEGRATION]
We combine Eqs.1, 2 and 3 to express the effect of one cycle. We consider the simple case of 1 × 1 convolutions (thus dx = dy = dz = 0), and we ignore all nonlinearities in between layers, obtaining S(x, y, z) =
x ,y ,z
K 3 (x, x )K 2 (y, y )K 1 (z, z )I(x , y , z ).(4)
where K 1 , K 2 and K 3 are the three successive kernels. Therefore, after a cycle of three convolutions, each element of the output tensor depends on all elements of the input tensor. It is straightforward to check that, even for k × k convolutions (k > 1) and nonlinearities, each element of the output tensor may still depend on all elements of the input tensor. However, it is not guaranteed that CycleNet makes use of the entire receptive range after training on a given task, and therefore we test this hypothesis in Section 3.1.
Note that if we substitute K(x, y, z, x , y , z
) = K 3 (x, x )K 2 (y, y )K 1 (z, z )(5)
into Eq.4, then the expression becomes equivalent to a dense layer, where the kernel is decomposed into three factors. Thus, Eq.4 is equivalent to a tensor decomposition. However, CycleNet is different from a tensor decomposition when nonlinearities are included between layers, and in the more general case for convolutions of kernel size k > 1.

[A.2 NUMBER OF PARAMETERS AND OPERATIONS]
The expression to calculate the number of parameters in CycleNet is the same as in a standard convolutional layer, simply the convolutions are performed along a different axis. We denote by k the size of the kernel, by X in , Y in , Z in the shape of the input tensor and by X out , Y out , Z out the shape of the output tensor. The number of parameters of the first layer is k 2 Z in Z out , each one operated on X out Y out times. Similarly, the number of parameters of the second layer is k 2 Y in Y out , times X out Z out operations, and for the third k 2 X in X out parameters, times Y out Z out operations. Note that the number of parameters of CycleNet may differ from a standard convolution if the number of horizontal pixels X or vertical pixels Y is different from the number of features Z.

[A.3 CUBIC BASELINE]
In order to compare the performance of CycleNet with standard CNNs, we define a baseline to study the relative improvement introduced by orthogonal convolutions. The baseline is exactly identical to CycleNet, except that we do not permute the axes, instead we always perform convolutions along the same axis. This corresponds to a standard stack of convolutional layers, interleaved with BatchNorm, ReLU and Dropout. However, in order to keep the number of parameters equal between CycleNet and the CNN baseline, we consider cubic tensors in most of our experiments, i.e. in which the number of horizontal pixels X, vertical pixels Y and the number of features Z are equal within each cycle. This is a strong constraint that makes such baseline quite specific, but we still refer to it simply as ""baseline"". It is likely that a better performance could be obtained, both by CycleNet and the CNN, without this cubic constraint. Therefore, we also perform experiments on CycleNet with non-cubic tensors, and compare its performance with other convolutional architectures, e.g. ResNet and MobileNet, which are also non-cubic.

[A.4 DEEP NETWORK]
As illustrated in Fig. 2b, we stack cycles in sequence to construct a deep network model. We add a standard convolution as a first layer of the network, to obtain the appropriate number of features to be used in the first cycle, and a dense layer as the last layer, thus reducing the (flattened) tensor to the size of the final output. In cubic models, tensor width is changed from one cycle to the next using tri-linear interpolation. In non-cubic models, tensor shape is changed by controlling the output size of the fully connected coordinate at each layer. In the case of the Pathfinder challenge, we use global pooling before the dense layer, in order to match the baseline architectures of Linsley et al. (2019). Code will be made available to the reviewers in the supplementary material, and to the public upon acceptance of the paper.

[B APPENDIX: EXPERIMENTAL DETAILS B.1 IMAGE CLASSIFICATION]
We start by training CycleNet on CIFAR-10 and ImageNet (at 128 × 128 resolution), standard benchmarks for image classification on which CNNs have been extensively optimized (Fig. 1a,b). We do not aim at beating the current state-of-the-art. Our goal is to compare CycleNet with standard CNNs of equal or similar size, and use them to test our working hypotheses in the next sections. We use relatively small models for ease of implementation, in order to explore a variety of hyperparameters. The CIFAR dataset consists of 60,000 32x32 colour images in 10 classes, 50,000 training images and 10,000 test images. We use the standard cross-entropy loss with L2 regularization. We train on a single GPU (GeForce RTX 2080 Ti) using the Keras API in TensorFlow. The training cycle is 300 epochs and the network hyperparameters are optimized by running twelve experiments varying dropout rates and L2 lambda. RMSprop was used with batch size 64 and initial learning rate 0.001, which is divided by 10 when the error plateaus. Data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. The first convolution outputs 80 features, and the deepest architecture has the following tensor widths in successive cycles: 75, 60, 45, 30, 15, 5. For the shallower networks, we use the smaller end of the sequence, e.g. width 5 for one cycle, widths 15, 5 for two cycles, widths 30, 15, 5 for three cycles and so on and so forth. The non-cubic CycleNet starts with a convolution with 100 features (32 × 32 × 100) followed by four cycles (45 × 45 × 100, 30 × 30 × 66, 15 × 15 × 33, 5 × 5 × 8). Fig. 3a shows the performance of models of up to six cycles (18 layers, k = 3) on CIFAR-10. CycleNet performs better than the CNN baseline, and the non-cubic CycleNet approaches the performance of a ResNet-18 (He et al. (2016)) with a similar number of parameters. These results suggest that long-range integration may be instrumental for image classification, but their significance is limited by the small resolution of CIFAR-10 (32 × 32).
To confirm our results on higher resolution, we next tested CycleNet on ImageNet (128 × 128). Fig. 3b (left) shows the performance of five-cycle models (15 layers, k = 3) on ImageNet. The ImageNet dataset consists of 1.28 million training and 50,000 test images in 1000 classes. We use 128 × 128 resultion, and the standard cross-entropy loss. We train on a 24 GPUs (GeForce RTX 2080 Ti) distributed training system (Sergeev & Del Balso (2018)) for 120 epochs. Adam optimizer was used with effective batch size 384 and learning rate 0.024 (using warm-up as in (Goyal et al. (2017)), which is divided by 10 when the error plateaus. Similar to CIFAR, data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. In the cubic models, the first convolution outputs 128 features, followed by cycles of the following tensor widths: 106, 106, 106, 106, 12. The non-cubic CycleNet starts with a convolution with 260 features (128 × 128 × 260) followed by five cycles (106 × 106 × 260, 106 × 106 × 190, 106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11). The non-cubic 2:3 CNN:CycleNet model has a convolution of 200 filters, followed by two CNN cycles (equivalent to 6 convolutions of output features 200,200,200,190,170,159) and three CycleNet cycles (106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11).
CycleNet performs better than the baseline, but its accuracy is significantly lower when compared to MobileNet (Howard et al. (2017)), which has a slightly larger size. We experiment with larger models, and we followed two approaches to increase performance up to a comparable level. First, similar to our CIFAR-10 experiment, we try a non-cubic architecture and find a 10% gain in relative performance. Second, following previous work (Ramachandran et al. (2019)), we hypothesise that image classification benefits from standard convolutions in the first layers, therefore we vary the number of initial cycles with standard convolutions. Fig.3b (right) shows that a model with two cycles of standard convolutions (2:3) has the best performance, and combining this with a noncubic architecture, the model approaches the performance of MobileNet with a similar number of parameters. This motivates using CycleNet and the CNN baseline for testing our working hypotheses in the next sections.
We used relatively small CycleNet models, in order to have a baseline that can be easily compared with, and to be able to explore CycleNet with a variety of choices of hyperparameters. It would be interesting to look at whether state of the art performance can be achieved on datasets of higher resolution and larger models, for example by introducing residual connections. This would however require significantly more computational resources.

[B.2 CALCULATION OF THE RECEPTIVE FIELD]
To calculate the receptive field, for a given node in the network and a given input image, we compute the gradient of the activation of that node with respect to the image. The saliency map is a grayscale image, obtained by taking the absolute value of the gradient for each pixel and summing across the three color channels (Simonyan et al. (2014)). The saliency map is normalized, such that the sum across pixels is one, and is interpreted as a 2-dimensional probability density. The receptive field size is computed by the square root of the total variance (the trace of the covariance matrix) of that density. We use the total variance since it quantifies the scatter of the saliency map along both axes independently. The receptive field shape is defined as the ellipsoid containing 3 standard deviations on both axes.

[B.3 THE PATHFINDER CHALLENGE]
The Pathfinder dataset is composed of 900, 000 training and 100, 000 test images, and has two classes. We use the standard cross-entropy loss with L2 regularization. In our experiment, in addition to the first convolution, CycleNet has a single cycle at 128 × 128 × 128 features, global pooling (as in Linsley et al. (2019)) and a dense layer. The experiments run on a single GPU (GeForce RTX 2080 Ti), Adam optimizer was used with learning rate 0.001. We use 5 times more distractors than the base dataset.
We generate three datasets of increasing difficulty, for different path lengths: n = 6, 9, and 14, at 128 × 128 resolution, using the code available at https://github.com/drewlinsley/pathfinder. We study performance of one-cycle models as a function of kernel size, k = 4, 8, 12 and 20.
For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease (see Figure 5). For kernel size 20, which allows convolutions to integrate long-range, the CNN is able to learn the task. However, for this kernel size, the accuracy values reported here are not final because the models had not converged.
The training stopped before convergence due to limitations in computational resources.
Note that CNNs typically have kernel size smaller than 10, since the number of parameters becomes substantial for larger kernels.","[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual.","{
    ""1"": {
        ""summary"": ""Reviewer criticizes the biological motivation and the limited experimental results as insufficient to support the claims made, especially in relation to existing architectures like vision transformers."",
        ""verbatim"": ""The biological motivation is weak at best. Long-range horizontal connections in cortex, which the authors use as motivation, are feature specific, i.e. between corresponding orientation domains. In contrast, in the authors' setup, they interact across all features. Moreover, the long-range connections are only along the x and y direction, but not in oblique directions. In my opinion, the author's proposal is not closer to biology than vision transformers, which also provide an all-to-all spatial interaction, albeit with a different mechanism and arguably stronger performance on large-scale datasets.""
    },
    ""2"": {
        ""summary"": ""Lack of comprehensive biological basis and the potential underuse of significant experimental results which were only included in the appendix."",
        ""verbatim"": ""The authors touch on a biological basis for the ideas explored here but I feel that the full potential of this line of reasoning is not realised. For example, the authors show improved generalisation to stylised ImageNet only in an Appendix when it is arguably among the most exciting results of the paper.""
    },
    ""3"": {
        ""summary"": ""The performance of CycleNet on ImageNet presented only in supplementary material suggests selective reporting, which could mislead the reader."",
        ""verbatim"": ""For example, the authors show the performance on CIFAR-10 compared to a basic CNN baseline in Figure 3 of the main body of the paper, but leave for the supplementary material the results on ImageNet, where CycleNet achieve comparably worse classification accuracy.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Lack of detailed explanation on computational complexity which is crucial for practical implementation and scalability concerns."",
        ""verbatim"": ""Lack of detailed explanation on computational complexity""
    },
    ""2"": {
        ""summary"": ""Limited exploration of different cycle configurations may raise questions about the robustness and versatility of CycleNet."",
        ""verbatim"": ""Limited exploration of different cycle configurations""
    },
    ""3"": {
        ""summary"": ""Incomplete comparison with other state-of-the-art architectures weakens the validation of CycleNet."",
        ""verbatim"": ""Incomplete comparison with other state-of-the-art architectures""
    },
    ""4"": {
        ""summary"": ""Limited discussion on generalizability to other computer vision tasks which might be crucial for broader acceptance and applicability."",
        ""verbatim"": ""Limited discussion on generalizability""
    }
}",4,"{
    ""A1-B3"": {
        ""rationale"": ""Both Review A1 and Review B3 discuss the insufficiency of the support for claims in the paper. A1 criticizes the biological motivations and experiments comparing the proposed architecture with previous ones (vision transformers) while B3 highlights the limited comparison with other state-of-the-art architectures as weakening CycleNet's validation."",
        ""similarity"": ""7""
    },
    ""A2-B4"": {
        ""rationale"": ""Both Review A2 and Review B4 express concerns about underreporting significant results and generalizability. A2 points out that some key results are only included in the appendix, which could be perceived as significant, and B4 notes the limited discussion on generalizability of the CycleNet to broader computer vision tasks."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks in-depth analysis compared to the state-of-the-art methods."",
        ""verbatim"": ""Lack of in-depth analysis: The paper may be at risk of rejection if it does not provide a detailed analysis or comparison with existing state-of-the-art methods other than traditional CNNs, limiting the understanding of the full scope of CycleNet's performance.""
    },
    ""2"": {
        ""summary"": ""The evaluation of CycleNet is limited to a few datasets, raising concerns about its generalizability."",
        ""verbatim"": ""Limited datasets evaluation: If the evaluation of CycleNet is limited to only CIFAR-10, ImageNet, and the Pathfinder challenge without testing on a wider variety of datasets or tasks, it may raise concerns about the generalizability of the proposed architecture.""
    },
    ""3"": {
        ""summary"": ""The paper does not thoroughly explain the rationale behind the design choices in CycleNet."",
        ""verbatim"": ""Clarity on specific design choices: If the paper does not thoroughly explain the rationale behind specific design choices in CycleNet, reviewers may question the robustness and interpretability of the proposed model.""
    },
    ""4"": {
        ""summary"": ""The paper lacks ablation studies to investigate the impact of each component in CycleNet."",
        ""verbatim"": ""Lack of ablation studies: Without conducting ablation studies to investigate the individual impact of each component of CycleNet on its performance, the paper may lack depth in understanding the contribution of each architectural element.""
    }
}",4,"{
    ""A3-B2"": {
        ""rationale"": ""Both Review A3 and Review B2 raise concerns about the reporting and scope of CycleNet's performance across different datasets. Review A3 highlights the selective reporting of results between main paper content and supplementary material, particularly pointing out differences in performance on CIFAR-10 and ImageNet. Review B2 discusses a similar concern by pointing to the limited number of datasets (CIFAR-10, ImageNet, Pathfinder) used to evaluate CycleNet, implying that such limitation could raise questions about CycleNet's generalizability across varied conditions. Both concerns accentuate the need for broader and more transparent experimental validations."",
        ""similarity"": ""8""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper might be rejected due to insufficient comparative analysis with traditional CNNs."",
        ""verbatim"": ""<Lack of thorough comparison metrics> The paper might face rejection if it lacks a detailed comparison of performance metrics between CycleNet and traditional CNNs, especially in terms of computational efficiency or robustness to noise and distortions.""
    },
    ""2"": {
        ""summary"": ""The paper could be rejected for not adequately testing CycleNet on a diverse set of datasets."",
        ""verbatim"": ""<Incomplete validation on diverse datasets> Inadequate validation on a diverse set of datasets or benchmarks may raise concerns about the generalizability and effectiveness of CycleNet across different scenarios.""
    }
}",2,"{
    ""A3-B2"": {""rationale"": ""Both A3 and B2 discuss the limitations of the authors' work related to generalizability and effectiveness of CycleNet across different data scenarios. A3 emphasizes the selective use of experimental results, particularly holding back less favorable results on a significant dataset like ImageNet in supplementary material, which might suggest concerns about the algorithm's broad applicability. B2 raises concerns about inadequate validation on a diverse set of datasets, which ultimately questions the generalizability and effectiveness of CycleNet. Both are concerned with how the work has been tested across various scenarios."", ""similarity"": ""7""}
}",1,"{
    ""1"": {
        ""summary"": ""Complexity and computational demands of CycleNet may be seen as a drawback."",
        ""verbatim"": ""Complexity and computational demands: While the authors address performance, the added complexity of CycleNet in terms of understanding, implementation, and possibly computational resources might be seen as a drawback. The effectiveness versus computational expense needs clearer justification, particularly in comparison to more straightforward enhancements of existing CNN architectures.""
    },
    ""2"": {
        ""summary"": ""Insufficient exploration of different architectural variants of CycleNet."",
        ""verbatim"": ""Insufficient exploration of architectural variants: The paper primarily focuses on one architectural setup of CycleNet. Additional configurations, perhaps comparing different cycle permutations or deeper integrations with other CNN improvements, could strengthen the case for its versatility and robustness.""
    },
    ""3"": {
        ""summary"": ""Limited discussion on when and why CycleNet might underperform or fail."",
        ""verbatim"": ""Limited discussion on failure cases: There is minimal discussion on the conditions or scenarios where CycleNet might underperform or fail outright, which is crucial for practical deployment and understanding the limits of the proposed architecture.""
    }
}",3,{},0,"{
    ""1"": {
        ""summary"": ""The paper lacks detailed architectural specifics and parameter settings, potentially impacting reproducibility and technical depth."",
        ""verbatim"": ""Lack of Detailed Methodology: The paper somewhat glosses over the detailed architectural specifics and parameter settings of CycleNet, which may raise questions about reproducibility and the depth of technical presentation.""
    },
    ""2"": {
        ""summary"": ""The CycleNet architecture might be overfitting to specific tasks and may not generalize well across a broader range of tasks."",
        ""verbatim"": ""Potential Overfitting to Specific Tasks: While the architecture excels in scenarios requiring the integration of distant features, such as the Pathfinder challenge, it is less clear whether it generalizes well across a wider range of tasks or primarily improves performance on tasks similar to those presented.""
    }
}",2,"{
    ""A2-B2"": {
        ""rationale"": ""Both points address issues related to the generalization capabilities of CycleNet. Review A suggests that significant results, like improved generalization to a stylized dataset, are only shown in an appendix rather than the main text, hinting at selective reporting or issues with generalization being under-discussed. Review B raises concerns about whether CycleNet overfits to specific tasks and questions its ability to generalize across a broader range of tasks. Both points align in their concern about generalization, albeit from slightly different perspectives."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Insufficient comparative analysis with other methodologies"",
        ""verbatim"": ""The paper might face criticism for not comparing the proposed architecture sufficiently against existing methods that aim to integrate long-range features, such as attention mechanisms or other novel convolutional approaches.""
    },
    ""2"": {
        ""summary"": ""Concerns regarding computational efficiency and scalability"",
        ""verbatim"": ""While the model outperforms traditional CNNs in several metrics, concerns about computational efficiency and scalability, given the multiple convolutional pathways, might need further discussion for practical deployment.""
    }
}",2,"{
    ""A3-B1"": {
        ""rationale"": ""Both Review A-3 and Review B-1 raise concerns about the completeness of comparing the proposed method to other well-established approaches. While Review A-3 specifically notes that results for ImageNet were relegated to supplementary materials, emphasizing a form of selective reporting, Review B-1 complains about insufficient comparative analysis in general, hinting at an overarching lack of thorough competitive evaluation in the paper."",
        ""similarity"": ""7""
    }
}",1,"[Significance and novelty]
1. The paper proposes a novel architecture, CycleNet, based on cyclic orthogonal convolutions that allows efficient information flow between features and locations across an entire image with a small number of layers. [2. The architecture differs from standard Convolutional Neural Networks (CNNs) as it allows for long-range integration of features in (x, y), (x, z), and (y, z) coordinates through the use of a cycle of three orthogonal convolutions. 3. CycleNet obtains competitive results on image classification tasks on CIFAR-10 and ImageNet datasets, while transferring better to stylised images and outperforming CNNs on the Pathfinder challenge, where integration of distant features is crucial. 4. The study hypothesizes that long-range integration favours recognition of objects by shape rather than texture, adding to the significance and novelty of the research.

[Potential reasons for acceptance]
1. The paper presents a novel architecture, CycleNet, which uses cyclic orthogonal convolutions, a new approach for long-range integration of features in CNNs. [2. The paper's experimental results show that CycleNet obtains competitive performance on image classification tasks on CIFAR-10 and ImageNet datasets. 3. CycleNet transfers better than CNNs to stylised images and outperforms them on the Pathfinder challenge, where integration of distant features is crucial. 4. The study provides insights into the importance of long-range integration of features for efficient recognition, contributing to the advancement of knowledge in the field.

[Potential reasons for rejection]
1. The paper lacks a comprehensive comparison with state-of-the-art methods for long-range integration of features in CNNs. [2. Some parts of the paper, particularly the theoretical explanations, can be unclear or lack sufficient detail. 3. The study could benefit from a more detailed experimental setup, such as more diverse datasets or a larger number of experiments, to further validate the proposed architecture's efficacy. 4. The authors fail to provide a clear explanation of why cyclic orthogonal convolutions lead to efficient long-range integration, and some explanations might require further justification or validation.

[Suggestions for improvement]
1. Conduct a comprehensive comparison with existing methods for long-range integration of features in CNNs and provide a detailed analysis of the results and performance differences. [2. Clarify the theoretical explanations and provide detailed, comprehensive justifications for the choices made in the design of the proposed architecture. 3. Provide a more detailed experimental setup, including more diverse datasets, a larger number of experiments, and control groups for a quantitative comparison of the proposed architecture's efficacy. 4. Validate the hypothesis by investigating the role of long-range integration of features in other recognition tasks or applications, and extend the research by implementing variations of the proposed architecture.","[Significance and novelty]
<Long-range information modeling> The proposed approach aims to model long-distance information in the 2D domain without the need for additional stages or localization maps, which is a significant and innovative contribution to the field.
<Multi-level orthogonal convolution> The introduction of a multi-level orthogonal convolution method to model long-distance information in neural networks is a novel approach that could have a major impact in the subfield of AI.

[Potential reasons for acceptance]
<Innovative approach> The reviewers acknowledge that the proposed method has potential and demonstrates promising results for certain applications.
<Reproducibility> The authors have provided the code needed to reproduce the experiments, which enhances the potential for the method to be validated and re-implemented by the research community.

[Potential reasons for rejection]
<Lack of thorough comparison> The reviewers have highlighted the need for more thorough comparisons with state-of-the-art models and a broader range of experimental setups to strengthen the paper's contributions and provide a well-founded assessment.
<Limited performance gain> The reviewers have raised concerns about the performance gain of the proposed structure being limited and questioned its outperformance compared to existing alternatives.

[Suggestions for improvement]
<Thorough comparison with state-of-the-art models> The authors should conduct thorough comparisons with existing models such as Transformer and Swin Transformer, particularly in the context of self-attention mapping, to establish the superiority of the proposed method.
<Experiments with a wider range of resolutions> The reviewers have suggested conducting experiments with a wider range of resolutions to demonstrate the effectiveness of the proposed method across different scenarios.
<Clarity in the paper's message> The authors should ensure the paper is written to convey a clear message about the contribution, focusing on the proposed cyclic orthogonal convolution and demonstrating its effectiveness.","{
    ""1"": {
        ""summary"": ""Lack of comparison with state-of-the-art methods."",
        ""verbatim"": ""The paper lacks a comprehensive comparison with state-of-the-art methods for long-range integration of features in CNNs.""
    },
    ""2"": {
        ""summary"": ""Insufficient theoretical detail."",
        ""verbatim"": ""Some parts of the paper, particularly the theoretical explanations, can be unclear or lack sufficient detail.""
    },
    ""3"": {
        ""summary"": ""Experimental setup needs expansion."",
        ""verbatim"": ""The study could benefit from a more detailed experimental setup, such as more diverse datasets or a larger number of experiments, to further validate the proposed architecture's efficacy.""
    },
    ""4"": {
        ""summary"": ""Inadequate explanation of the convolution process."",
        ""verbatim"": ""The authors fail to provide a clear explanation of why cyclic orthogonal convolutions lead to efficient long-range integration, and some explanations might require further justification or validation.""
    }
}",4,"{
    ""1"": {
        ""summary"": ""The review suggests more thorough comparisons are needed."",
        ""verbatim"": ""The reviewers have highlighted the need for more thorough comparisons with state-of-the-art models and a broader range of experimental setups to strengthen the paper's contributions and provide a well-founded assessment.""
    },
    ""2"": {
        ""summary"": ""Concerns about limited performance gains are raised."",
        ""verbatim"": ""The reviewers have raised concerns about the performance gain of the proposed structure being limited and questioned its outperformance compared to existing alternatives.""
    }
}",2,"{
    ""A3-B3"": {
        ""rationale"": ""Both Review A3 and Review B3 criticize the partial reporting and insufficient experimental validation presented in the paper. Review A3 highlights the selective presentation of data between CIFAR-10 and ImageNet, showing discrepancies in where results are published (main body vs. supplementary material). Review B3 calls for an expanded experimental setup, suggesting the need for more thorough testing across diverse datasets or more experiments, resonating with concerns about comprehensive data representation in A3."",
        ""similarity"": ""7""
    }
}",1,"{
    ""A3-B2"": {
        ""rationale"": ""Both Review A3 and Review B2 talk about the issues related to the performance of the proposed CycleNet, noting limitations in the gains achieved. Review A3 specifically discusses the placement of results in supplementary materials that show weaker performance on ImageNet, suggesting a possible selective reporting to only show favorable results in the main paper. This aligns closely with Review B2's concern about the limited performance gains and questions on outperformance compared to existing alternatives."",
        ""similarity"": ""8""
    }
}",1
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR,boIlH5V81C8.json,"[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Instances of ""r"" becoming ""l"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 2: Figure 2 :2Figure 2: Instances of ""l"" becoming ""r"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 3: for the NB Tale data and Figure 4 for Rundkast. The first column (a) shows results from the 300m parameter model, second column (b) shows results from 1b parameter model. Darker colors represent higher errors.For both the NB Tale and Rundkast corpora we,
Table 4: Example of the vectors for ""k"", ""g"", and ""n"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 5: Example of the vectors for ""a"", and ""e"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 6: 

[INTRODUCTION]
Automatic Speech Recognition (ASR) has, like all machine learning tasks, struggled with generalization. That is, a model will perform well on the task and data it was trained on but when presented with new examples, especially examples that differ in some dimension from the training data, the model will perform markedly less well. In the task of ASR, this means that models often struggle with generating correct transcriptions for speakers whose age, gender, or dialect differs from that of the speakers on which the model was originally trained. Of specific focus in this paper is the impact of dialect on a modern ASR system.
Dialect information has been used in different ways in ASR. In some applications, such as Dialect Identification (DID), the goal is to correctly identify the dialect for a given sample of speech. Hämäläinen et al. (2021), for example, used a combination of speech and text features to perform DID. In other cases, DID is combined with ASR systems to improve transcription accuracy. For example, Zhang and Hansen (2018) used bottleneck features extracted via unsupervised deep learning to perform DID for both Chinese and Arabic. Similarly, Imaizumi et al. (2022) used a multitask model for both DID and ASR. This multitask approach outperformed the single task systems on both DID and ASR.
Beyond DID, the behavior of ASR systems has been analyzed with respect to dialectal speech (as we do in this paper). This in order to explore phonetic phenomena, as well as to gain insights into the way those complex systems work. In these studies, even when dialectal information is not an explicit target, there is still an interest to understand what phonetic and dialectal information has been captured in ASR models. With traditional ASR models, this investigation has been fairly straightforward as these models have consisted of three semi-independent components: the acoustic model, the language model, and the lexicon. Because of the separate acoustic models within these multi-component models, one could, for example, perform clustering on the model parameters themselves such as (Salvi, 2003a(Salvi, ,b, 2005. In this work, Salvi performed clustering on the acoustic model features and correlated the resulting clusters with known dialectally realized phonemes. Instead of directly using an acoustic model from an ASR system, Chen et al. (2014) adapted the concept of an HMM acoustic model to automatically discover dialect-specific phonetic rules.
Unlike multi-component ASR systems, investigating modern, end-to-end models for phonetic and dialectal information is quite different.
Whereas parameters from an acoustic model may be extracted and used independently, the acoustic information in an end-to-end model cannot be so easily excised. This design makes it more challenging, but not impossible, to investigate what acoustic information is captured where in the network. Belinkov and collaborators used the output from each layer of an end-to-end system to train phonetic, grapheme, and articulatory classifiers (Belinkov and Glass, 2017;Belinkov et al., 2019). Prasad and Jyothi (2020) investigated dialectal information captured by an end-to-end system using not only layer-wise classification but also gradient and information-theoric analysis. All of these works are focused on analyzing the networkinternal representations detached from actual network output.
The output from ASR models is constrained by the model architecture. Traditional ASR models with lexicons are bound to output only words contained within that lexicon. This means that all transcripts generated by these models contained only real, known words even if the transcribed output did not necessarily match the word that was spoken. Additionally, these models do not allow for acceptable variation in spelling. For example, the word, ""favorite,"" would always be spelled ""favorite"" never ""favourite,"" even if the latter might better reflect the preference of a British English speaker. Conversely, these newer end-to-end architectures, trained using connectionist temporal classification (CTC) loss, produce output at the character instead of word level. This permits the model to create novel words and spellings, potentially better reflecting the phonetic realization of the spoken word. Given that CTC models are allowed to generate novel spellings, there exists the potential that dialectal information will be captured by the model output itself via non-standard spellings.
The goal of this paper is to investigate whether dialectal acoustic information can impact spellings with an end-to-end model. In order to test this, we used wav2vec 2.0 (Baevski et al., 2020) to generate transcriptions of Norwegian speech. We then performed an analysis of the resulting transcripts for captured dialectal knowledge via a dialectalregion based evaluation of character error patterns. From this analysis we are able to see known Norwegian dialectally-based phonological patterns, specifically around ""r"" and ""l"" confus-ability and stop consonant voicing. Thus we illustrate that strong enough acoustic dialectal cues can effect the character output of an end-to-end ASR system.

[NORWEGIAN LANGUAGE AND DIALECTS]
In this paper, we focus our analysis on the Norwegian language. Though spoken by a relatively small population of a little over 5 million speakers, Norwegian contains many dialects differentiated in phonology, syntax, and lexicon. In addition to dialectal variation, Norwegian also maintains two official written standards: Bokmål and Nynorsk; though neither written standard directly corresponds with a spoken variant. Furthermore, Norway does not recognize any official language standard. Indeed, people are encouraged to use their preferred written standard and native dialect in all aspects of work and life.
The variety in dialects stems from Norway's challenging and rugged topography that has historically forced the populace to organize into many, smaller communities. Over time, the diversity we see in Norwegian dialects developed in these small, isolated communities. As described by phoneticians, there now exist large dialectal phonetic variations ranging from infinitive verb endings to palatalization of consonants, to /r/ and /l/ realizations, to the various pronunciations for the personal pronoun for ""I"", jeg -ranging from [jaei] to [eg] to [i] and more (Skjekkeland, 1997).
While the number of specific Norwegian dialects is quite large, we can group these dialects into larger dialect groups for the purpose of this investigation. These grouping could be either into the regional names used by Skjekkeland or into the even larger, cardinal regions of ""East,"" ""West,"" ""North,"" ""South,"" and ""Mid."" The analysis outlined in this paper relies on these cardinal regions.

[EXPERIMENTAL SETUP AND DATA]
In order to investigate the impact of dialect on an end-to-end ASR system, a well-performing baseline model was required. Therefore, we used three models trained by the Norwegian National Library AI Lab and released publicly on the Hugging Face repository for our analysis 123 . The first model contained one billion parameters and was originally trained on the XLS-R (Babu et al., 2021). It was then fine tuned using the Norwegian Parliamentary Speech Corpus (NPSC) to transcribe Norwegian Bokmål text. The other two models were fine tuned from the 300 million parameter VoxRex model (Malmsten et al., 2022). One of these 300 million parameter models was fine-tuned to transcribe Bokmål, the other Nynorsk. All models use a 5-gram word-based language model. In all cases, the NPSC corpus was used to fine-tune the models (Solberg and Ortiz, 2022). When evaluated against the NPSC corpus, the Norwegian AI lab reports a word error rate (WER) of 6.33% for the 1 billion parameter model, 7.03% for the 300 million parameter Bokmål model, and 12.22% for the Nynorsk model. These results indicate that these models will make excellent candidates for our analysis.
As stated earlier, the models to be used were trained on the NPSC. This consists of recordings from the Norwegian Parliament and thus the speech style can be considered mostly spontaneous, with perhaps slightly more planning than everyday speech. For analysis purposes, the NPSC was excluded. This is due to data sparsity in the NPSC test set. While the whole test set is acceptable for model evaluation, data becomes untenably sparse when considered dialect-by-dialect. Thus our analysis focuses on results from two unrelated and more dialectally robust corpora: Rundkast and NB Tale.
The Rundkast corpus consists of radio broadcasts from the Norwegian Broadcasting Corporation (NRK) (Amdal et al., 2008). These transcripts are in both Bokmål and Nynorsk which are treated separately for analysis in this paper. Dialectal annotations were added by the transcribers during corpus creation and are provided directly in the speaker metadata.
NB Tale is publicly available from the National Library of Norway's Language Bank and consist of recordings and transcripts of native and non-native speakers of Norwegian. All speech was transcribed using the Bokmål standard. Read

[WORD AND CHARACTER ALIGNMENT]
As our investigation into dialectal impact revolves around analyzing trends in character errors, we require an alignment between reference text and model-generated hypothesis text where words that only differ by a few characters are prioritized for alignment. While character error rate (CER) computed across a whole utterance is useful in understanding an aggregate of character errors, this method loses awareness of word boundaries. For example, ""også kalt"" and ""og såkalt"" would be aligned in whole-utterance CER with an insertion and a deletion of a space (resulting in ""og så kalt""). However, we prefer an alignment where we recognize that ""så"" was removed from the first word and ""så"" as added to the second word. Thus CER, as it is generally used across entire utterances, does not answer for our analysis purposes.
With traditional, word-level Levenshtein-based alignments, word similarity is not considered. Any pair of words that do not exactly match are treated as completely different. However, by considering word similarity, the resulting alignments can be used for analysis of broad trends of spellings (e.g., a word ending in ""a"" instead of ""e"") that can indicate dialectal impact.
To accomplish such an alignment, an extension to the traditional Levenshtein alignment was developed (Levenshtein, 1965). Typically edit costs are fixed at a value before alignment is computed. However, in our solution instead of a fixed cost for substitutions, we allow it to be dynamically computed as the CER between the two candidate words. This still ensures that there is no cost for aligning words that are the same while also preferring substitutions of similarly spelled words. voiced class nasal place rounding  Once word-level alignment is computed using the dynamic substitution cost, we can investigate spelling errors. To ensure characters within a word are aligned optimally, we continue to use the dynamic substitution cost idea and compute the substitution cost between characters as the Euclidean distance between two feature vectors. To support this, articulatory feature vectors were created for each letter in the Norwegian alphabet using the International Phonetic Alphabet (IPA) charts as a guide. Articulatory features were considered as indexes in the vector and the values correspond to the realization. For our work, consonants (see examples in Table 1) were defined and treated separately from vowels (see examples in Table 2). As the goal with these vectors is not to create an accurate grapheme-to-phoneme mapping, nor to perfectly illustrate all possible IPA nuance, but instead to align letters in a more logical way, these vectors were sufficient.
""k"" 0 0 0 5 0 ""g"" 1 0 0 5 0 ""n"" 1 0 1 2 0
To illustrate the necessity of these vectors, consider the word pair of inngang (meaning ""entrance"") and enkel (""easy""). Using a traditional alignment method 4 , where all characters substitutions have the same cost, an alignment like in 4 Alignment generated using the Python Levenshtein package:
https://github.com/maxbachmann/ python-Levenshtein reference i n n g a n g hypothesis e n k e l Table 3: A possible alignment between inngang and enkel, generated without accounting for character similarity. reference i n n g a n g hypothesis e n k e l Table 4: A possible alignment between inngang and enkel, generated by accounting for character similarity.
Table 3 is generated. However, using articulatory features as a distance, we are able to generate the alignment in Table 4 where ""g"" and ""k"" (only differing by voicing), ""a"" and ""e"" (both being front vowels), and ""n"" and ""l"" (both being sonorants) are aligned. While this solution is slightly phonologically flawed -wholly ignoring the di-and trigraphs that exist in Norwegian and instead treating the component letters individually, for example -these feature vectors do accomplish the goal of creating a logical character-level alignment. With confidence in our word and character alignment we can perform the investigation into character substitution trends that constitutes our results.

[WER BY DIALECT]
To first understand the general trend in recognition across dialects, the WER was calculated for each dialect across the whole of the Rundkast and NB Tale corpora. Transcriptions were generated using both the 300 million and 1 billion parameter Bokmål models for both corpora. Rundkast was further transcribed with the 300 million parameter Nynorsk model (since Rundkast actually contains Nynorsk utterances, unlike NB Tale).
As displayed in Table 5 that shows WER across both corpora and dialects, we can see WER values ranging from the low teens to nearly 40%. These values are markedly higher than the 6.33% WER that was reported on the NPSC which highlights the impact of domain mismatch on ASR; models trained on one domain (the Norwegian Parliament) do not generalize well to new domains (radio and studio recordings).  For the Bokmål text in both corpora, we can see that models perform best on the ""East"" dialect region whereas the ""West"" region has the worst performance. It is unclear which model is generally the best. The 1 billion parameter model performs better than the 300 million parameter model on the NB Tale text, but the 300 million parameter model outperforms the 1 billion on the Rundkast text.
With the Rundkast corpus, we can see that the Bokmål models perform, as expected, poorly on the Nynorsk text with the converse (Nynorsk model evaluated against Bokmål text) being true as well. However, even when the Nynorsk model is evaluated against Nynorsk text, the results are still worse than the Bokmål model of the same size evaluated against Bokmål text.
Of more concern than model accuracy, however, is data scarcity for Nynorsk text. Given that Nynorsk is primarily used in the western part of Norway, the nearly equal split of speakers between Bokmål and Nynorsk for the ""West"" region is understandable. Moreover, for the other regions (""North"" and ""Mid"" in particular) there are too few speakers to draw conclusions from. Therefore, as we move forward with the character-based analy-sis, we will be focusing on the Bokmål models and their performance on the Bokmål text.

[/R/ AND /L/ CONFUSIBLITY]
In Norwegian, /r/ is generally realized as either a voiced apical tap or a voiced velar approximant (Kvale and Foldvik, 1992). These two different pronunciations are considered dialect features, with the approximant version predominating in the ""South"" and ""West"" of the country and the tap being the norm in the rest of country. The maps in (Kvale and Foldvik, 1999) and (Skjekkeland, 1997) nicely illustrate this distribution.
Similar to the Norwegian /r/, which can be realized in several variants, the Norwegian /l/ also has dialectally motivated realizations. Many speakers in the ""East"", ""Mid"", and southern part of the ""North"" region of the country produce a voiced retroflex flap. The norm for speakers in the rest of the country (""West"", ""South"", and the remaining part of the ""North"") is a voiced dental/alveolar lateral (Kvale and Foldvik, 1995).
Understanding these phonetic realizations, we can anticipate that the tapped [R] and the lateral approximant [l] should be minimally confusing for NB Tale Rundkast (a) the model. The former being a brief interruption in the airflow and the latter being a continuous, smooth approximant. However, for speakers in the ""East"" and ""Mid"" parts of the country, where both the tapped [R] and flapped [ó] dialect features are present, we would anticipate a greater degree of confusion. Both tapped [R] and flapped [ó] are seen as brief closures with acoustic differentiation relegated to the F3 and F4 trajectories (Kvale and Foldvik, 1995).
N/A 2.79% 1.34% 1.81% 2.16% (c) 0.67% 4.24% 2.24% 1.56% 2.95% (b) 0.42% 3.44% 2.79% 1.61% 1.95% (d) 0.78% 3.87% 3.69% 1.67% 2.85%
Therefore to evaluate how much of an impact these potentially similar realizations have on the model, we used the aligned Bokmål texts (as described in Section 3.2) and calculated how frequently ""r"" was transcribed instead of ""l"" and vice versa. When analyzing instances of ""r"" transforming into ""l"", we only considered instances where the ""r"" did not precede another alveolar consonant (""t"", ""d"", ""n"", ""l"", ""s""). This is due to the fact that ""r"", when followed by an alveolar consonant, can be interpreted as a digraph. In dialect regions with the alveolar [R], speakers will realize the second alveolar consonant as a retroflex instead of pronouncing two distinct sounds. That is, ""rt"" would be realized as [ú]). To ensure these realizations did not cloud our analysis, we excluded all ""r""s followed by an alveolar consonant.
The maps in Figures 1 and 2 show the percentage of error. That is, for those instances where an ""r"" was not transcribed correctly, the maps show what percentage of those errors were because an ""l"" was transcribed instead (Figure 1). And vice versa for the ""l"" to ""r"" transformation (Figure 2). This error calculation and plotting was done for each of the cardinal dialect region. Darker colors represent higher errors. In both figures the first column (a, c) show results on the NB Tale utterances; second column (b, d)   For all Figures, except 2(b) and 2(d), the regions with the most confusability between ""r"" and ""l"" are the ""East"", ""Mid"", and ""North"". Indeed, for all Figures except 2(d) the ""South"" has the lowest incidences of ""r"" and ""l"" confusion. By and large we also see much clearer, more consistent trends with the NB Tale data. This could be because the utterances in the NB Tale corpora were selected for phonological coverage and thus there were more environments for ""r"" and ""l"" confusion.

[VOICELESS STOP LENITION]
In addition to /r/ and /l/ confusability, we also investigated the distribution of voiceless stop consonants. In the ""South"" region, voiceless stops tend to lenite to their voiced counterparts in post-vocalic environments (Skjekkeland, 1997). Thus, we would expect [p], [t], and [k] to lenite to [b], [d], and [g] when preceded by a vowel. To understand if this change is captured by the wav2vec model, we found instances where a voiceless stop was changed and then ensured that the change was to its voiced counterpart. If a voiceless to voice change occurred, we then ensured that both the voice and voiceless stops were preceded by a vowel. We counted occurrences of this postvocalic voicing change across all three stops of interest. Results can be see in Figure 3  Figure 3: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the NB Tale dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model 300m 1b
(a)
4.94%% 

[2.15%]
Figure 4: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the Rundkast dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model can see that the ""South"" region has the highest instances of voicing. Though once again, we see stronger trends in the NB Tale data then in Rundkast.

[PERSONAL PRONOUN JEG]
As mentioned when discussing the Norwegian language in Section 2, there are many ways for Norwegain speakers to say the first person pronoun jeg. This was briefly investigated as well.
Confusion pairs for jeg were aggregated and trends sought. Regardless, no trends in the words substituted for jeg in the transcripts could be found. This lack of results could indicate that a word like jeg occurs so frequently in all dialects that there is an abundance of training examples for the model to generalize from. Or, perhaps, the 5-gram language model used, in addition to the wav2vec component, had enough influence to ensure that only jeg was produced.

[DISCUSSION]
Due to the fact that we have been able to largely see acoustic dialectal features surfacing through our analysis, we find that this method of carefully aligning text and aggregating results has promise. Furthermore, we infer that the models have learned enough about Norwegian to understand standard spellings and apply these generalizations to broader contexts. Additionally, the phonetic information in the dialects is strong enough to cause the models to utilize this general spelling knowledge and create more acoustically aligned outputs. However, going so far as to say that the models have internalized some knowledge about the dialects themselves (e.g., phonetic features) is perhaps more than can be reasonably asserted from this analysis.
Through this paper we have explored a couple of known dialectally-motivated phonological realizations. There still, however, exist more that could be explored. As mentioned in Section 4.2, there exists a pattern of retroflexting of alveolar consonants for certain Norwegian dialects. This analysis could certainly be extended to those environments. However, there are also phonological changes that are hard, or potentially impossible to see in spelling changes. For example, alveolars are palatalized (most strongly) in the ""Mid"" region as well as in certain phonological environments in the ""North"" and the northern parts of the ""West"" and ""East"" regions. This palatalization would be hard to see in spellings since there is no standard way in Norwegian orthography of representing a palatalized sound. Additional Norwegian phonological features that have no written representation (such as toneme) would also be invisible to the analysis performed in this paper.
As the NPSC is derived from parliamentary speeches, the distribution of parliament speakers emulates the population distribution of the country. Thus our models, all of which were trained on NPSC, have the same speaker representation. That is, the ""East"" region would be the most represented in the training data. Given this, and the results in Table 5, it would seem that the models have best learnt the features which they saw the most, as machine learning models are wont to do. Therefore, if models are to be robust against dialects, it seems necessary to increase the training data for the other regions. Additionally, it might be possible to assign greater weight to these dialectal character changes during training to encourage the models to learn a better representation.

[CONCLUSION]
Through this paper, we demonstrate how an analysis of character errors in transcriptions generated by an end-to-end ASR system can contain dialectal trends mirroring those known through linguistic descriptions. We showed increased confusability between ""r"" and ""l"" in regions where those phonemes are realized similarly. We also showed increased incidences of voiceless stop lenition in a region known for that phenomena. These errors indicate that the end-to-end system has successfully learnt to spell in Norwegian, going so far so as to slightly spell in dialect.

[ACKNOWLEDGEMENTS]
This work has been done as part of the SCRIBE project as funded by the Norwegian Research Council, project number: 322964.","[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems.","{
    ""1"": {
        ""summary"": ""Request for additional explanations and comparisons, alongside a format correction in a reference."",
        ""verbatim"": ""Please explain your method with an example / or a flow chart\n\""However, in our solution instead of a fixed cost for substitutions, we allow it to be dynamically computed as\nthe character error rate (CER)\""\nCan you compare your approach to other approaches, is it better? Is there any take home message?\nFix reference capitalisation""
    },
    ""2"": {
        ""summary"": ""Concerns about the comprehensibility due to a large set of experimental variables, and questions regarding the implementation specifics of the method."",
        ""verbatim"": ""This paper aims to investigate how certain dialectal features impact the performance of ASR systems. The authors compare three pre-trained Norwegian wav2vec2 models on three datasets, five dialect areas and three dialectal features. This rather large set of experimental variables makes the paper a bit difficult to follow, but overall, the goals, methods and results are well explained.""
    },
    ""3"": {
        ""summary"": ""Serious flaws noted in the scientific method, methodologies involving transcription of speech to text rather than phoneme, lack of comparative analysis with other methods, and an inadequate understanding of employed methods."",
        ""verbatim"": ""Unfortunately I see three serious flaws with the present paper.\n1. To actually answer the scientific question that you pose, you\n   should establish a 'ground truth' (say, what is the linguistically\n   established rate of Voiceless stop lenition per region) and compute\n   a correlation coefficient (or some other metric) with what the\n   model computes.\n2. The methodology is somewhat roundabout. Why not use a speech to\n   phoneme model instead of speech to text?\n3. The authors do not convey thorough understanding of some of the\n   methods that they employ (alignment, see below).""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The study has a narrow focus on specific phonological issues which may limit its broader applicability"",
        ""verbatim"": ""The analysis focuses primarily on character errors related to specific phonological features like \""r\"" and \""l\"" confusion and voiceless stop lenition, potentially limiting the scope of the study to a few selected dialectal phenomena.""
    },
    ""2"": {
        ""summary"": ""Data scarcity in certain dialect regions could impact the generalizability of the findings"",
        ""verbatim"": ""The paper acknowledges data scarcity for certain dialect regions, particularly in the case of the Nynorsk text, which may raise concerns about the generalizability of the findings across all Norwegian dialects.""
    },
    ""3"": {
        ""summary"": ""The integration of dialectal cues into the model architecture is insufficiently explored"",
        ""verbatim"": ""While the paper highlights the impact of dialectal acoustic cues on ASR outputs, it does not delve deeply into how these cues are integrated into the end-to-end model architecture, leaving room for further exploration into the model's learning mechanisms.""
    },
    ""4"": {
        ""summary"": ""The research could be more comprehensive by including a broader range of dialectal features"",
        ""verbatim"": ""The analysis could benefit from a more extensive investigation into additional dialectal phonological realizations beyond \""r\"" and \""l\"" confusability and voiceless stop lenition, to provide a more comprehensive understanding of how dialectal features influence ASR performance.""
    }
}",4,"{
    ""A2-B3"": {
        ""rationale"": ""Both points address the complexity and depth of integrating dialectal features into ASR systems. Review A points out the difficulty in following the paper due to a large set of experimental variables on dialectal features, while Review B highlights that the paper does not sufficiently explore the integration of dialectal cues into the model. Both reviews suggest a need for clearer exposition or further investigation on how dialectal features are handled in the model."", 
        ""similarity"": ""7""
    },
    ""A3-B4"": {
        ""rationale"": ""Review A criticizes the paper's methodology and lack of comprehensive comparative analysis with other methods, which indirectly touches on the depth of investigation into dialectal features impacting ASR. Review B directly suggests that more extensive investigation into additional dialectal features could enhance the understanding of their influence on ASR performance. Both comments focus on the need for broader and more robust methodological exploration within the context of dialectal features in ASR systems."", 
        ""similarity"": ""6""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks comparative analysis with traditional ASR systems or other approaches."",
        ""verbatim"": ""Lack of Comparative Analysis: The paper could benefit from a comparison with traditional ASR systems or other approaches to highlight the unique contributions of the character-based, end-to-end ASR method.""
    },
    ""2"": {
        ""summary"": ""The focus on Norwegian dialects may limit the research's broader applicability."",
        ""verbatim"": ""Limited Generalizability: The focus on Norwegian dialects may limit the broader applicability of the findings to other languages or dialects, potentially reducing the impact of the research.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A and Review B emphasize the need for comparative analysis with other methods. Review A explicitly asks if the authors' approach is better than others and requests a comparative evaluation, while Review B notes the paper's lack of comparative analysis with traditional ASR systems."",
        ""similarity"": ""8""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks a comparative analysis with existing methods."",
        ""verbatim"": ""The paper could benefit from including a more in-depth comparative analysis with existing methods or techniques to showcase the superiority of the proposed approach.""
    },
    ""2"": {
        ""summary"": ""The discussion on generalizability of the findings is incomplete."",
        ""verbatim"": ""The paper might face rejection if it does not thoroughly discuss the generalizability of the findings beyond the specific case of Norwegian dialectal speech.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both points are requesting a comparative analysis of the methods employed in the paper. Review A explicitly asks how the approach compares to other methods and whether it offers any superior advantages, while Review B suggests incorporating a more in-depth comparison to demonstrate the superiority of the proposed approach."",
        ""similarity"": ""9""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The focus on the Norwegian language may limit the broader applicability of the research findings to other languages with dialectal diversity."",
        ""verbatim"": ""'Limited generalizability': The focus on Norwegian might limit the interest and applicability of the findings to broader ASR research, as the results need validation across other languages with similar dialectal diversity.""
    },
    ""2"": {
        ""summary"": ""The paper does not thoroughly discuss potential biases in the model training or the nature of the test corpora which might influence the results of the character-based error analysis."",
        ""verbatim"": ""'Potential methodological oversights': The paper lacks a thorough discussion on whether the character-based error analysis could have been influenced by biases in model training or the specific nature of the test corpora used.""
    },
    ""3"": {
        ""summary"": ""There is a lack of comparative analysis with traditional phoneme-based or hybrid ASR systems, which is needed to underscore the advantages of the proposed character error analysis method."",
        ""verbatim"": ""'Lack of comparative analysis': A comparison with traditional phoneme-based or hybrid ASR systems is missing, which could have strengthened the argument by highlighting the relative benefits of the proposed character error analysis method.""
    }
}",3,"{
    ""A3-B3"": {
        ""rationale"": ""Both Review A point 3 and Review B point 3 discuss the lack of comparative analysis in the paper's methodology. Review A criticizes the methodology for not being comparative, specifically in terms of phoneme vs. text speech models, and points out a misunderstanding of some methods. Review B points out the absence of a comparison with traditional phoneme-based or hybrid ASR systems, suggesting that such a comparison would highlight the benefits of the proposed method. While each review frames their criticism slightly differently, they fundamentally address the same issue regarding comparative analysis."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks empirical data to demonstrate the effectiveness of the proposed methods."",
        ""verbatim"": ""The paper, while rich in theoretical analysis and methodological innovation, does not provide sufficient empirical data or results from actual experiments. The real-world effectiveness of the proposed method is thus not demonstrated convincingly.""
    },
    ""2"": {
        ""summary"": ""The study's limited generalizability may not appeal to a broader audience without further discussion on applicability to other languages or dialects."",
        ""verbatim"": ""The focus on Norwegian dialects, while novel, may not appeal to a broader audience unless the implications for other languages or dialects are discussed.""
    }
}",2,"{
    ""A3-B1"": {
        ""rationale"": ""Both Review A (point 3) and Review B (point 1) highlight significant shortcomings in the paper's experimental or analytical robustness. Review A criticizes the lack of comparative analysis with established metrics and a suboptimal choice of methodology, while Review B notes the absence of empirical data to back up the proposed methods. These points converge on the issue of inadequate scientific demonstration of the methods used in the paper."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Limited focus on Norwegian dialects may reduce the generalizability of the study."",
        ""verbatim"": ""The focus is narrowly tailored to Norwegian dialects, which may limit the generalizability of the findings to other languages or dialects.""
    },
    ""2"": {
        ""summary"": ""The scope of empirical evidence is narrow and needs broadening to strengthen the paper’s impact."",
        ""verbatim"": ""The empirical evidence provided is limited to specific phonological features.""
    }
}",2,"{
    ""A2-B2"": {
        ""rationale"": ""Both points discuss the scope and breadth of the study's empirical evidence. Review A points out the difficulty in following the paper due to a large set of experimental variables concerning the dialects and features studied. Review B argues that the scope of empirical evidence is limited to specific phonological features. Both are concerned with how the breadth or limitation of the area studied impacts the comprehensibility or the robustness of the research findings."",
        ""similarity"": ""7""
    }
}",1,"[Significance and novelty]
This paper presents a novel method for analyzing character errors in end-to-end Automatic Speech Recognition (ASR) systems, with a focus on investigating the impact of dialects on Norwegian ASR. The authors propose a method that guarantees similar words and characters are paired during alignment, ensuring any resulting analysis of character errors is founded on phonological substitutions. By analyzing carefully aligned character errors, the authors discover trends in system-generated spellings that align with known phonological features of Norwegian dialects, particularly focusing on the ""r"" and ""l"" confusability and voiceless stop lenition. The findings of this study demonstrate the influence of acoustic dialectal features on the output of end-to-end ASR systems, adding to the existing knowledge in speech recognition and dialect analysis.

[Potential reasons for acceptance]
1. The paper introduces a novel method for analyzing character errors in end-to-end ASR systems, contributing to the field of speech recognition and dialect analysis.
2. The research focuses on the impact of dialects on ASR systems, which is an important area of study for improving ASR technology for various dialects and accents.
3. The study findings reveal trends in system-generated spellings aligning with known phonological features of Norwegian dialects, providing valuable insights for further research on dialects and speech recognition.
4. The authors provide a well-structured and organized paper, making it clear and easy to follow.
5. The presented method is validated and proven to be effective through the case study on Norwegian dialects.

[Potential reasons for rejection]
1. The sample size of the study is relatively small, limiting the generalizability of the findings to other dialects and languages.
2. The paper solely focuses on Norwegian dialects, hindering its applicability to other dialects or speech recognition systems.
3. The method described lacks a clear explanation of how it compares to existing methods in literature, making it unclear how it adds value to the field.
4. The paper does not discuss potential limitations of the study, such as data quality and potential bias.

[Suggestions for improvement]
1. Extend the study by investigating more dialects and languages to increase the generalizability of the findings.
2. Modify the method to be adaptable to various dialects and languages to ensure its applicability.
3. Conduct a comparison with existing methods in literature to highlight the uniqueness and value of the presented method.
4. Discuss potential limitations and how they could be mitigated to increase the reliability and validity of the findings.
5. Provide more context and explanation on the relevance of the research to ASR technology and applications in various fields.","[Significance and novelty]
<Investigation of character-based ASR for dialectal speech> The exploration of errors in character-based automatic speech recognition (ASR) systems for dialectal speech is a significant and novel topic with potential implications for speech technology and linguistic analysis.
<Analysis of ASR outputs and dialectal variation> The paper's exploration of the effects of dialectal variation on ASR system outputs is important and novel in the field of speech recognition.

[Potential reasons for acceptance]
<Technical solidity> The paper exhibits technical solidity and provides valuable insights into character-based ASR systems and dialectal variation.
<Contribution to the field> The paper's contribution to the study of ASR systems for dialectal speech adds value to the research area.

[Potential reasons for rejection]
<Lack of clarity and polishing> The paper requires further refinement and clarification in terms of language, terminology, and referencing.
<Inadequate explanation and citation of concepts> There's a need for better explanation and citation of concepts such as acoustics and phonetics and additional information about related work.

[Suggestions for improvement]
<Polishing> The paper needs careful refinement and polishing to improve the language, terminology, and referencing.
<Clearer explanation and citation> A clearer explanation and citation of concepts like acoustics and phonetics along with more thorough information about related work would enhance the paper.

","{
    ""1"": {
        ""summary"": ""Small sample size limits the generalizability of the findings."",
        ""verbatim"": ""The sample size of the study is relatively small, limiting the generalizability of the findings to other dialects and languages.""
    },
    ""2"": {
        ""summary"": ""Focus solely on Norwegian dialects limits applicability."",
        ""verbatim"": ""The paper solely focuses on Norwegian dialects, hindering its applicability to other dialects or speech recognition systems.""
    },
    ""3"": {
        ""summary"": ""Lack of comparison with existing methods."",
        ""verbatim"": ""The method described lacks a clear explanation of how it compares to existing methods in literature, making it unclear how it adds value to the field.""
    },
    ""4"": {
        ""summary"": ""No discussion of potential study limitations."",
        ""verbatim"": ""The paper does not discuss potential limitations of the study, such as data quality and potential bias.""
    }
}",4,"{
    ""1"": {
        ""summary"": ""The paper lacks clarity and requires further refinement in language and referencing."",
        ""verbatim"": ""The paper requires further refinement and clarification in terms of language, terminology, and referencing.""
    },
    ""2"": {
        ""summary"": ""The paper needs improved explanations and proper citations, especially for concepts like acoustics and phonetics."",
        ""verbatim"": ""There's a need for better explanation and citation of concepts such as acoustics and phonetics and additional information about related work.""
    }
}",2,"{
    ""A1-B3"": {
        ""rationale"": ""Both points emphasize the need for comparison between the proposed method and existing methods. Review A requests a comparison with other approaches and seeks clarification on whether the proposed approach is better, while Review B points out that the method lacks a clear explanation of how it compares to existing methods."",
        ""similarity"": ""7""
    },
    ""A3-B3"": {
        ""rationale"": ""Both points highlight a significant concern regarding the lack of comparative analysis of the methods. Review A suggests the proper establishment of metrics and comparison models, while Review B expresses the absence of comparative discussion to existing methods, impacting the value addition to the field."",
        ""similarity"": ""9""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both Review A1 and Review B2 emphasize the need for improved explanations and preferential methodology, as well as better explanations and citations. Review A1 specifically asks for additional examples or a flowchart, and a comparison with other methodologies, while Review B2 desires better explanation and literature citation specifically around the concepts of acoustics and phonetics."",
        ""similarity"": ""7""
    },
    ""A3-B2"": {
        ""rationale"": ""Both points A3 from Review A and B2 from Review B express a concern about the adequacy of the methodology and the clarity of the explanation surrounding it. Review A3 specifically critiques the scientific method used and suggests a different approach to improve it, while Review B2 seeks improved explanation and citation on fundamental concepts that could include methodologies like those critiqued in A3."",
        ""similarity"": ""7""
    }
}",2
Multi-Time Attention Networks for Irregularly Sampled Time Series,4c0J6lwQ4_.json,"[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1   

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Architecture of the mTAND module. It takes irregularly sampled time points and corresponding values as keys and values and produces a fixed dimensional representation at the query time points. The attention blocks (ATT) perform a scaled dot product attention over the observed values using the time embedding of the query and key time points. Equation 3 and 4 defines this operation.Note that the output at all query points can be computed in parallel.
Table 2: Figure 2 :2Figure 2: Architecture of the proposed encoder-decoder framework mTAND-Full. The classifier is required only for performing classification tasks. The mTAND module is shown in Figure 1.
Table 3: Figure 3 :3Figure 3: Interpolations on the synthetic interpolation dataset. The columns represent 3 different examples. First row: Ground truth trajectories with observed points, second row: reconstructions on the complete range t ∈ [0, 1] using the proposed model mTAN, third row: reconstructions on the complete range t ∈ [0, 1] using the Latent ODE model with ODE encoder.
Table 4: As shown in Equation 2, dimension j of the mTAN embedding mTAN(t, s)[j] is given by a linear combination of intermediate univariate continuous-time functionsx hd (t, s). There is one such function defined for each input data dimension d and each time embedding h. The parameters U hdj are learnable linear combination weights.As shown in Equation3, the structure of the intermediate continuous-time functionx hd (t, s) is essentially a kernel smoother applied to the d th dimension of the time series. However, the interpolation weights κ h (t, t id ) are defined based on a time attention mechanism that leverages time embeddings, as shown in Equation4. As we can see, the same time embedding function φ h (t) is applied for all data dimensions. The form of the attention mechanism is a softmax function over the observed time points t id for dimension d. The activation within the softmax is a scaled inner product between the time embedding φ h (t) of the query time point t and the time embedding φ h (t id ) of the observed time point, the key. The parameters w and v are each d r × d k matrices where d k ≤ d r . We use a scaling factor 1
Table 5: 13.8% of examples are in the positive class.
Table 6: Interpolation performance versus percent observed time points on PhysioNet
Table 7: compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods.
Table 8: Table 2 compares the predictive performance of the models on the mortality prediction task on MIMIC-III. The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While ODE-RNN and L-ODE-ODE both have slightly better A APPENDIX A.1 ABLATION STUDY In this section, we perform ablation experiments to show the performance gain achieved by learning similarity kernel and time embedding. Table
Table 9: Ablation with time embeddingSince mTANs are fundamentally continuous-time interpolation-based models, we perform an ablation study by comparing mTANs with the IP-nets
Table 10: Comparing interpolation kernels
Table 11: Synthetic Data: Mean Squared Error

[INTRODUCTION]
Irregularly sampled time series occur in application domains including healthcare, climate science, ecology, astronomy, biology and others. It is well understood that irregular sampling poses a significant challenge to machine learning models, which typically assume fully-observed, fixed-size feature representations (Marlin et al., 2012;Yadav et al., 2018). While recurrent neural networks (RNNs) have been widely used to model such data because of their ability to handle variable length sequences, basic RNNs assume regular spacing between observation times as well as alignment of the time points where observations occur for different variables (i.e., fully-observed vectors). In practice, both of these assumptions can fail to hold for real-world sparse and irregularly observed time series. To respond to these challenges, there has been significant progress over the last decade on building and adapting machine learning models that can better capture the structure of irregularly sampled multivariate time series (Li & Marlin, 2015;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018;Shukla & Marlin, 2019;Rubanova et al., 2019).
In this work, we introduce a new model for multivariate, sparse and irregularly sampled time series that we refer to as Multi-Time Attention networks or mTANs. mTANs are fundamentally continuous-time, interpolation-based models. Their primary innovations are the inclusion of a learned continuous-time embedding mechanism coupled with a time attention mechanism that replaces the use of a fixed similarity kernel when forming representation from continuous time inputs. This gives mTANs more representational flexibility than previous interpolation-based models (Shukla & Marlin, 2019).
Our approach re-represents an irregularly sampled time series at a fixed set of reference points. The proposed time attention mechanism uses reference time points as queries and the observed time points as keys. We propose an encoder-decoder framework for end-to-end learning using an mTAN module to interface with given multivariate, sparse and irregularly sampled time series inputs. The encoder takes the irregularly sampled time series as input and produces a fixed-length latent representation over a set of reference points, while the decoder uses the latent representations to produce reconstructions conditioned on the set of observed time points. Learning uses established methods for variational autoencoders (Rezende et al., 2014;Kingma & Welling, 2014).
The main contributions of the mTAN model framework are: (1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. (2) It uses a temporally distributed latent representation to better capture local structure in time series data. (3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.

[RELATED WORK]
An irregularly sampled time series is a time series with irregular time intervals between observations. In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records (Marlin et al., 2012;Yadav et al., 2018), climate science (Schulz & Stattegger, 1997), ecology (Clark & Bjørnstad, 2004), biology (Ruf, 1999), and astronomy (Scargle, 1982). It is well understood that such data cause significant issues for standard supervised machine learning models that typically assume fully observed, fixed-size feature representations (Marlin et al., 2012).
A basic approach to dealing with irregular sampling is fixed temporal discretization. For example, Marlin et al. (2012) and Lipton et al. (2016) discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.
The alternative to temporal discretization is to construct models with the ability to directly use an irregularly sampled time series as input. Che et al. (2018) present several methods based on gated recurrent unit networks (GRUs, Chung et al. (2014)), including an approach that takes as input a sequence consisting of observed values, missing data indicators, and time intervals since the last observation. Pham et al. (2017) proposed to capture time irregularity by modifying the forget gate of an LSTM (Hochreiter & Schmidhuber, 1997), while Neil et al. (2016) introduced a new time gate that regulates access to the hidden and cell state of the LSTM. While these approaches allow the network to handle event-based sequences with irregularly spaced vector-valued observations, they do not support learning directly from vectors that are partially observed, which commonly occurs in the multivariate setting because of lack of alignment of observation times across different variables.
Another line of work has looked at using observations from the future as well as from the past for interpolation. Yoon et al. (2019) and Yoon et al. (2018) presented an approach based on the multi-directional RNN (M-RNN) that can leverage observations from the relative past and future of a given time point. Shukla & Marlin (2019) proposed the interpolation-prediction network framework, consisting of several semi-parametric RBF interpolation layers that interpolate multivariate, sparse, and irregularly sampled input time series against a set of reference time points while taking into account all observed data in a time series. Horn et al. (2020) proposed a set function-based approach for classifying time-series with irregularly sampled and unaligned observation.
Chen et al. (2018) proposed a variational auto-encoder model (Kingma & Welling, 2014;Rezende et al., 2014) for continuous time data based on the use of a neural network decoder combined with a latent ordinary differential equation (ODE) model. They model time series data via a latent continuous-time function that is defined via a neural network representation of its gradient field. Building on this, Rubanova et al. (2019) proposed a latent ODE model that uses an ODE-RNN model as the encoder. ODE-RNNs use neural ODEs to model the hidden state dynamics and an RNN to update the hidden state in the presence of a new observation. De Brouwer et al. (2019) proposed GRU-ODE-Bayes, a continuous-time version of the Gated Recurrent Unit (Chung et al., 2014). Instead of the encoder-decoder architecture where the ODE is decoupled from the input processing, GRU-ODE-Bayes provides a tighter integration by interleaving the ODE and the input processing steps.
Several recent approaches have also used attention mechanisms to model irregularly sampled time series (Song et al., 2018;Tan et al., 2020;Zhang et al., 2019) as well as medical concepts (Peng et al., 2019;Cai et al., 2018). Most of these approaches are similar to Vaswani et al. (2017) where they replace the positional encoding with an encoding of time and model sequences using self-attention.
However, instead of adding the time encoding to the input representation as in Vaswani et al. (2017), they concatenate it with the input representation. These methods use a fixed time encoding similar to the positional encoding of Vaswani et al. (2017). Xu et al. (2019) learn a functional time representation and concatenate it with the input event embedding to model time-event interactions.
Like Xu et al. (2019) and Kazemi et al. (2019), our proposed method learns a time representation. However, instead of concatenating it with the input embedding, our model learns to attend to observations at different time points by computing a similarity weighting using only the time embedding. Our proposed model uses the time embedding as both the queries and keys in the attention formulation. It learns an interpolation over the query time points by attending to the observed values at key time points. Our proposed method is thus similar to kernel-based interpolation, but learning the time attention based similarity kernel gives our model more flexibility compared to methods like that of Shukla & Marlin (2019) that use similarity kernels with fixed functional forms. Another important difference relative to many of these previous methods is that our proposed approach attends only to the observed data dimensions at each time point and hence does not require a separate imputation step to handle vector valued observations with an arbitrary collection of dimensions missing at any given time point.

[THE MULTI-TIME ATTENTION MODULE]
In this section, we present the proposed Multi-Time Attention Module (mTAN). The role of this module is to re-represent a sparse and irregularly sampled time series in a fixed-dimensional space. This module uses multiple continuous-time embeddings and attention-based interpolation. We begin by presenting notation followed by the time embedding and attention components.
Notation: In the case of a supervised learning task, we let D = {(s n , y n )|n = 1, ..., N } represent a data set containing N data cases. An individual data case consists of a single target value y n (discrete for classification), as well as a D-dimensional, sparse and irregularly sampled multivariate time series s n . Different dimensions d of the multivariate time series can have observations at different times, as well as different total numbers of observations L dn . Thus, we represent time series d for data case n as a tuple s dn = (t dn , x dn ) where t dn = [t 1dn , ..., t L dn dn ] is the list of time points at which observations are defined and x dn = [x 1dn , ..., x L dn dn ] is the corresponding list of observed values. In the case of an unsupervised task such as interpolation, each data case consists of a multivariate time series s n only. We drop the data case index n for brevity when the context is clear. Time Embedding: Time attention module is based on embedding continuous time points into a vector space. We generalize the notion of a positional encoding used in transformer-based models to continuous time. Time attention networks simultaneously leverage H embedding functions φ h (t), each outputting a representation of size d r . Dimension i of embedding h is defined as follows:
φ h (t)[i] = ω 0h • t + α 0h , if i = 0 sin(ω ih • t + α ih ), if 0 < i < d r (1)
where the ω ih 's and α ih 's are learnable parameters. The periodic terms can capture periodicity in time series data. In this case, ω ih and α ih represent the frequency and phase of the sine function. The linear term, on the other hand, can capture non-periodic patterns dependent on the progression of time. For a given difference ∆, φ h (t + ∆) can be represented as a linear function of φ h (t).
Learning the periodic time embedding functions is equivalent to using a one-layer fully connected network with a sine function non-linearity to map the time values into a higher dimensional space. By contrast, the positional encoding used in transformer models is defined only for discrete positions. We note that our time embedding functions subsume positional encodings when evaluated at discrete positions.
Multi-Time Attention: The time embedding component described above takes a continuous time point and embeds it into H different d r -dimensional spaces. In this section, we describe how we leverage time embeddings to produce a continuous-time embedding module for sparse and irregularly sampled time series. This multi-time attention embedding module mTAN(t, s) takes as input a query time point t and a set of keys and values in the form of a D-dimensional multivariate sparse and irregularly sampled time series s (as defined in the notation section above), and returns a J- Note that the output at all query points can be computed in parallel. dimensional embedding at time t. This process leverages a continuous-time attention mechanism applied to the H time embeddings. The complete computation is described below. to normalize the dot product to counteract the growth in the dot product magnitude with increase in the dimension d k .
mTAN(t, s)[j] = H h=1 D d=1x hd (t, s) • U hdj (2) x hd (t, s) = L d i=1 κ h (t, t id ) x id (3) κ h (t, t id ) = exp φ h (t)wv T φ h (t id ) T / √ d k L d i =1 exp φ h (t)wv T φ h (t i d ) T / √ d k (4
Learning the time embeddings provides our model with flexibility to learn complex temporal kernel functions κ h (t, t ). The use of multiple simultaneous time embeddings φ h (t) and a final linear combination across time embedding dimensions and data dimensions means that the final output representation function mTAN(t, s) is extremely flexible. Different input dimensions can leverage different time embeddings via learned sparsity patterns in the parameter tensor U . Information from different data dimensions can also be mixed together to create compact reduced dimensional representations. We note that all of the required computations can be parallelized using masking variables to deal with unobserved dimensions, allowing for efficient implementation on a GPU. Discretization: Since the mTAN module defines a continuous function of t given s, it can not be directly incorporated into neural network architectures that expect inputs in the form of fixeddimensional vectors or discrete sequences. However, the mTAN module can easily be adapted to produce such an output representation by materializing its output at a set of reference time points
r = [r 1 , ..., r K ].
In some cases, we may have a fixed set of such points. In other cases, the set of reference time points may need to depend on s itself. In particular, we define the auxiliary function ρ(s) to return the set of time points at which there is an observation on any dimension of s.
Given a collection of reference time points r, we define the discretized mTAN module mTAND(r, s) as mTAND(r, s)[i] = mTAN(r i , s). This module takes as input the set of reference time points r and the time series s and outputs a sequence of mTAN embeddings of length |r|, each of dimension J. The architecture of the mTAND module is shown in Figure 1. The mTAND module can be used to interface sparse and irregularly sampled multivariate time series data with any deep neural network layer type including fully-connected, recurrent, and convolutional layers. In the next section, we describe the construction of a temporal encoder-decoder architecture leveraging the mTAND module, which can be applied to both classification and interpolation tasks.

[ENCODER-DECODER FRAMEWORK]
As described in the last section, we leverage the discretized mTAN module in an encoder-decoder framework as the primary model in this paper, which we refer to as an mTAN network. We develop the encoder-decoder framework within the variational autoencoder (VAE) framework in this section.
The architecture for the model framework is shown in Figure 2.
Model Architecture: As we are modeling time series data, we begin by defining a sequence of latent states z i . Each of these latent states are IID-distributed according to a standard multivariate normal distribution p(z i ). We define the set of latent states z = [z 1 , ..., z K ] at K reference time points.
We define a three-stage decoder. First, the latent states are processed through an RNN decoder module to induce temporal dependencies resulting in a first set of deterministic latent variables h
dec RN N = [h dec 1,RN N , ..., h dec K,RN N ].
Second, the output of the RNN decoder stage and the K time points h dec RN N are provided to the mTAND module along with a set of T query time points t. The mTAND module outputs a sequence of embeddings h dec T AN = [h dec 1,T AN , ..., h dec T,T AN ] of length |t|. Third, the mTAN embeddings are independently decoded using a fully connected decoder f dec () and the result is used to parameterize an output distribution. In this work, we use a diagonal covariance Gaussian distribution with mean given by the final decoded representation and a fixed variance σ 2 . The final generated time series is given byŝ = (t, x) with all data dimensions observed. The full generative process is shown below. We let p θ (x|z, t) define the probability distribution over the values of the time series x given the time points t and the latent variables z. θ represents the parameters of all components of the decoder.
z k ∼ p(z k ) (5) h dec RN N = RNN dec (z)(6)
h dec T AN = mTAND dec (t, h dec RN N )(7)
x id ∼ N (x id ; f dec (h dec i,T AN )[d], σ 2 I)(8)
For an encoder, we simply invert the structure of the generative process. We begin by mapping the input time series s through the mTAND module along with a collection of K reference time points r.
We apply an RNN encoder to the mTAND model that outputs h enc T AN to encode longer-range temporal structure. Finally, we construct a distribution over latent variables at each reference time point using a diagonal Gaussian distribution with mean and variance output by fully connected layers applied to the RNN outputs h enc RN N . The complete encoder architecture is described below. We define q γ (z|r, s) to be the distribution over the latent variables induced by the input time series s and the reference time points r. γ represents all of the parameters in all of the encoder components.
h enc T AN = mTAND enc (r, s) (9) h enc RN N = RNN enc (h enc T AN ) (10) z k ∼ q γ (z k |µ k , σ 2 k ), µ k = f enc µ (h enc k,RN N ), σ 2 k = exp(f enc σ (h enc k,RN N ))(11)
Unsupervised Learning: To learn the parameters of our encoder-decoder model given a data set of sparse and irregularly sampled time series, we follow a slightly modified VAE training approach and maximize a normalized variational lower bound on the log marginal likelihood based on the evidence lower bound or ELBO. The learning objective is defined below where p θ (x jdn |z, t n ) and q γ (z|r, s n ) are defined in the previous section.
L NVAE (θ, γ) = N n=1 1 d L dn E qγ (z|r,sn) [log p θ (x n |z, t n )] − D KL (q γ (z|r, s n )||p(z)) (12
)
D KL (q γ (z|r, s n )||p(z)) = K i=1 D KL (q γ (z i |r, s n )||p(z i )) (13
)
log p θ (x n |z, t n ) = D d=1 L dn j=1 log p θ (x jdn |z, t jdn )(14)
Since irregularly sampled time series can have different numbers of observations across different dimensions as well as across different data cases, it can be helpful to normalize the terms in the standard ELBO objective to avoid the model focusing more on sequences that are longer at the expense of sequences that are shorter. The objective above normalizes the contribution of each data case by the total number of observations it contains. The fact that all data dimensions are not observed at all time points is accounted for in Equation 14. In practice, we use k samples from the variational distribution q γ (z|r, s n ) to compute the learning objective.
Supervised Learning: We can also augment the encoder-decoder model with a supervised learning component that leverages the latent states as a feature extractor. We define this component to be of the form p δ (y n |z) where δ are the model parameters. This leads to an augmented learning objective as shown in Equation 15where the λ term trades off the supervised and unsupervised terms.
L supervised (θ, γ, δ) = L NVAE (θ, γ) + λE qγ (z|r,sn) log p δ (y n |z)(15)
In this work, we focus on classification as an illustrative supervised learning problem. For the classification model p δ (y n |z), we use a GRU followed by a 2-layer fully connected network. We use a small number of samples to approximate the required intractable expectations during both learning and prediction. Predictions are computed by marginalizing over the latent variable as shown below.
y * = arg max y∈Y E qγ (z|r,s) [log p δ (y|z)](16)

[EXPERIMENTS]
In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset).
Additional illustrative results on synthetic data can be found in Appendix A.2.

[DATASETS:]
The PhysioNet Challenge 2012 dataset (Silva et al., 2012) consists of multivariate time series data with 37 variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first 48 hours after admission to ICU. We follow the procedures of Rubanova et al. (2019) and round the observation times to the nearest minute. This leads to 2880 possible measurement times per time series. The data set includes 4000 labeled instances and 4000 unlabeled instances. We use all 8000 instances for interpolation experiments and the 4000 labeled instances for classification experiments. We focus on predicting in-hospital mortality.  2019), we extract 53, 211 records each containing 12 physiological variables. We focus on predicting in-hospital mortality using the first 48 hours of data. 8.1% of the instances have positive labels.
The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of Rubanova et al. (2019) and construct a dataset of 6, 554 sequences with 12 channels and 50 time points. We focus on classifying each time point in the sequence into one of eleven types of activities.
Experimental Protocols: We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80% of the instances, and a test set containing the remaining 20% of instances. We use 20% of the training data for validation. In the interpolation task, we condition on a subset of available points and predict values for rest of the time points. We perform interpolation experiments with a varying percentage of observed points ranging from 50% to 90% of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at rest of the available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).
We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification problems. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for the human activity dataset. We randomly divide each data set into a training set containing 80% of the time series, and a test set containing the remaining 20% of instances. We use 20% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy.
For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix A.4.

[MODELS:]
The model we focus on is the encoder-decoder architecture based on the discretized multitime attention module (mTAND-Full). In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (mTAND-Enc). We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) (Chung et al., 2014) module as the recurrent network throughout. Architecture details can be found in Appendix A.3.
• RNN-Impute: Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples (Che et al., 2018).
• RNN-∆ t : Input is concatenated with masking variable and time interval ∆ t indicating how long the particular variable is missing.
• RNN-Decay: RNN with exponential decay on hidden states (Mozer et al., 2017;Che et al., 2018).
• GRU-D: combining hidden state decay with input decay (Che et al., 2018).
• Phased-LSTM: Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM (Neil et al., 2016) with forward filling to handle partially observed vectors.
• IP-Nets: Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU (Shukla & Marlin, 2019).
• SeFT: Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach (Horn et al., 2020).
• RNN-VAE: A VAE-based model where the encoder and decoder are standard RNN models.
• ODE-RNN: Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation (Rubanova et al., 2019).
• L-ODE-RNN: Latent ODE where the encoder is an RNN and decoder is a neural ODE (Chen et al., 2018).
• L-ODE-ODE: Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE (Rubanova et al., 2019).
Physionet Experiments: Table 1 compares the performance of all methods on the interpolation task where we observe 50% − 90% of the values in the test instances. As we can see, the proposed method (mTAND-Full) consistently and substantially outperforms all of the previous approaches across all of the settings of observed time points. We note that in this experiment, different columns correspond to different setting (for example, in the case of 70%, we condition on 70% of data and predict the rest of the data; i.e., 30%) and, hence the results across columns are not comparable. We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver (Chen et al., 2018;Rubanova et al., 2019). These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.  3 shows the ablation results by substituting fixed positional encoding (Vaswani et al., 2017) in place of learnable time embedding defined in Equation 1in mTAND-Full model on PhysioNet and MIMIC-III dataset for classification task. We report the average AUC score over 5 runs. As we can see from Table 3, learning the time embedding improves AUC score by 1% as compared to using fixed positional encodings.  (Shukla & Marlin, 2019). IP-Nets use several semiparametric RBF interpolation layers, followed by a GRU to model irregularly sampled time series. In this framework, we replace the RBK kernel with a learnable similarity kernel using mTAND module, the corresponding model is mTAND-Enc. Table 4 compares the performance of the two methods on classification task on PhysioNet, MIMIC-III and Human Activity dataset. We report the average AUC score over 5 runs. Table 4 shows that learning the similarity kernel using mTAND module performs as well or better than using a fixed RBF kernel. 

[A.2 SYNTHETIC INTERPOLATION EXPERIMENTS]
To demonstrate the capabilities of our model on the interpolation task, we generate a synthetic dataset consisting of 1000 trajectories each of 100 time points sampled over t ∈ [0, 1]. We fix 10 reference points and use RBF kernel with a fixed bandwidth of 100 for constructing local interpolations at 100 time points over [0,1]. The values at the reference points are drawn from a standard normal distribution.
We randomly sample 20 observations from each trajectory to simulate a sparse and irregularly sampled multivariate time series. We use 80% of the data for training and 20% for testing. At test time, encoder conditions on 20 irregularly sampled time points and the decoder generates interpolations on all 100 time points. Figure 3 illustrates the interpolation results on the test set for the Multi-Time Attention Network and Latent ODE model with ODE encoder (Rubanova et al., 2019). For both the models, we draw 100 samples from the approximate posterior distribution. As we can see from Figure 3, the ODE interpolations are much smoother and haven't been able to capture the local structure as well as mTANS.

[OBSERVED DATA]
Ground truth Reconstructions  Table 5 compares the proposed model with best performing baseline Latent-ODE with ODE encoder (L-ODE-ODE) on reconstruction and interpolation task. For both the tasks, we condition on the 20 irregularly sampled time points and reconstruct the input points (reconstruction) and the whole set of 100 time points (interpolation). We report the mean squared error on test set.

[A.3 ARCHITECTURE DETAILS]
Multi-Time Attention Network (mTAND-Full): In our proposed encoder-decoder framework (Figure 2), we use bi-directional GRU as the recurrent model in both encoder and decoder. In encoder, we use a 2 layer fully connected network with 50 hidden units and ReLU activations to map the RNN hidden state at each reference point to mean and variance. Similarly in decoder, mTAN embeddings are independently decoded using a 2 layer fully connected network with 50 hidden units and ReLU activations, and the result is used to parameterize the output distribution. For classification tasks, we use a separate GRU layer on top of the latent states followed by a 2-layer fully connected layer with 300 units and ReLU activations to output the class probabilities.
Multi-Time Attention Encoder (mTAND-Enc): As we show in the experiments, the proposed mTAN module can standalone be used for classification tasks. The mTAND-Enc consists of Multi-Time attention module followed by GRU to extract the final hidden state which is then passed to a 2-layer fully connected layer to output the class probabilities.

[ACKNOWLEDGEMENTS]
Research reported in this paper was partially supported by the National Institutes of Health under award numbers 5U01CA229445 and 1P41EB028242.

[]
mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.
Human Activity Experiments: Table 2 shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets.
Additional Experiments: In Appendix A.2, we demonstrate the effectiveness of learning temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.
We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix A.1. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix A.1 shows that learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.

[DISCUSSION AND CONCLUSIONS]
In this paper, we have presented the Multi-Time Attention (mTAN) module for learning from sparse and irregularly sampled data along with a VAE-based encoder-decoder model leveraging this module.
Our results show that the resulting model performs as well or better than a range of baseline and state-of-the-art models on both the interpolation and classification tasks, while offering training times that are one to two orders of magnitude faster than previous state of the art methods. While in this work we have focused on a VAE-based encoder-decoder architecture, the proposed mTAN module can be used to provide an interface between sparse and irregularly sampled time series and many different types of deep neural network architectures including GAN-based models. Composing the mTAN module with convolutional networks instead of recurrent architectures may also provide further computational enhancements due to improved parallelism.
Loss Function: For computing the evidence lower bound (ELBO) during training, we use negative log-likelihood with fixed variance as the reconstruction loss. For all the datasets, we use a fixed variance of 0.01. For computing ELBO, we use 5 samples for interpolation task and 1 sample for classification tasks. We use cross entropy loss for classification. For the classification tasks, we tune the λ parameter in the supervised learning loss function (Equation 15). We achieved best performance using λ as 100 and 5 for Physionet, MIMIC-III respectively. For human activity dataset, we achieved best results without using the regulaizer or ELBO component. We found that KL annealing with coeff 0.99 improved the performance of interpolation and classification tasks on Physionet.

[A.4 HYPERPARAMETERS]
Baselines: For Physionet and Human Activity dataset, we use the reported hyperparameters for RNN baselines as well as ODE models from Rubanova et al. (2019). For MIMIC-III dataset, we independently tune the hyperparameters of the baseline models on the validation set. We search for GRU hidden units, latent dimension, number of hidden units in fully connected network for ODE function in recognition and generative model over the range {20, 32, 64, 128, 256}. For ODEs, we also searched the number of layers in fully connected network in the range {1, 2, 3}.  In this section, we visualize the attention weights learned by our proposed model. We experiment using synthetic dataset (described in A.2) which consists of univariate time series. Figure 4 shows the attention weights learned by the encoder mTAND module. The input shown in the figure is the irregularly sampled time points and the edges show how the output at reference points attends to the values on the input time points. The final output can be computed by substituting the attention weights in Equation 3. We rescale each feature to be between 0 and 1 for Physionet and MIMIC-III dataset. We also rescale the time to be in [0, 1] for all datasets. In case of MIMIC-III dataset, for the time series missing entirely, we follow the preprocessing steps of Shukla & Marlin (2019) and assign the starting point (time t=0) value of the time series to the global mean for that variable.

[A.6.2 SOURCE CODE]
The code for reproducing the results in this paper is available at https://github.com/ reml-lab/mTAN.

[A.6.3 COMPUTING INFRASTRUCTURE]
All experiments were run on a Nvidia Titan X GPU.","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1","{
    ""1"": {
        ""summary"": ""The paper lacks a specific discussion on how it addresses sparsity and irregular sampling issues for time-series data, which are critical aspects of the proposed methodology."",
        ""verbatim"": ""I didn't see much on the main method sections that specifically address/discuss sparsity and irregular sample problems, it seems like the proposed method also works for general time-series data.""
    },
    ""2"": {
        ""summary"": ""The method's approach to noise in sparse and irregularly sampled data using a Gaussian assumption might be incorrect due to potential skewness in the distribution."",
        ""verbatim"": ""The noise distribution for sparse / irregularly sampled data can often be heavily skewed, so the Gaussian noise assumption in this paper may not hold.""
    },
    ""3"": {
        ""summary"": ""There is a concern about the practical application of the framework for extrapolation or forecasting tasks, specifically the handling of ever-growing time dimension which could pose technical challenges."",
        ""verbatim"": ""Is the framework directly applicable to such tasks given the way time is handled to get the embeddings as the 0th dimension would keep growing with time (Eqn. 1)?""
    },
    ""4"": {
        ""summary"": ""There is confusion in understanding the overall model structure and notation, as well as a lack of case studies demonstrating the model's capability of unveiling relationships between sequential time points."",
        ""verbatim"": ""The paper uses a lot of notations in equations and descriptions, which cause a little bit confusion to follow the authors' idea.""
    },
    ""5"": {
        ""summary"": ""The article doesn't clearly explain which specific parts of its architecture contribute to its performance, making it difficult to understand the effectiveness of each component."",
        ""verbatim"": ""Results are impressive but I don't get which part of the architecture lead to such a performance""
    }
}",5,"{
    ""1"": {
        ""summary"": ""Limited evaluation on diverse applications"",
        ""verbatim"": ""Limited evaluation: The paper mainly focuses on interpolation and classification tasks without discussing other potential applications or benchmarks, potentially limiting the generalizability of the proposed mTAN model.""
    },
    ""2"": {
        ""summary"": ""Insufficient comparative analysis"",
        ""verbatim"": ""Lack of detailed comparison: While the results show improved performance compared to baseline models, a more in-depth analysis comparing the mTAN model with a wider range of existing methods could provide a more comprehensive evaluation.""
    },
    ""3"": {
        ""summary"": ""Complex model architecture"",
        ""verbatim"": ""Complexity: The complexity of the mTAN model architecture, including the continuous-time embeddings and attention mechanisms, may pose challenges in implementation and computational cost for some users.""
    },
    ""4"": {
        ""summary"": ""Generalizability of the model is limited"",
        ""verbatim"": ""Generalizability: The paper focuses on specific datasets and tasks, such as PhysioNet and human activity classification, which may limit the generalizability of the proposed model to other domains or applications.""
    }
}",4,"{
    ""A4-B3"": {""rationale"": ""Both points in A4 and B3 discuss challenges related to the complexity of the model's structure. A4 raises issues in understanding the model due to extensive notation, which complicates following the authors' idea, whereas B3 points out challenges in the model's implementation and computational demands owing to its complex architecture."", ""similarity"": ""6""}
}",0,"{
    ""1"": {
        ""summary"": ""The paper lacks detailed comparisons with existing state-of-the-art models regarding the proposed Multi-Time Attention Networks."",
        ""verbatim"": ""The paper could provide more detailed comparisons with existing state-of-the-art models to strengthen the argument for the superiority of the proposed Multi-Time Attention Networks.""
    },
    ""2"": {
        ""summary"": ""The paper needs a more in-depth analysis of the potential limitations or scenarios where the Multi-Time Attention Networks may underperform."",
        ""verbatim"": ""It would be beneficial to include more in-depth analysis of the limitations or scenarios where the proposed framework may not perform as well.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""Lack of comprehensive comparison across diverse datasets"",
        ""verbatim"": ""The absence of a comprehensive comparison with a wide range of diverse datasets may raise concerns about the generalizability of the proposed Multi-Time Attention Networks framework.""
    },
    ""2"": {
        ""summary"": ""Insufficient discussion on framework limitations"",
        ""verbatim"": ""While the paper emphasizes the advantages of the new framework, a more detailed exploration of its limitations or potential failure cases could add depth to the analysis.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""Complex implementation may hinder reproducibility and practical use."",
        ""verbatim"": ""The complexity of the model, while a strength, can also be a barrier in terms of reproducibility and practical implementation in real-world settings.""
    },
    ""2"": {
        ""summary"": ""Lack of theoretical analysis to explain performance improvements."",
        ""verbatim"": ""Although the paper provides empirical evidence, it lacks a deeper theoretical analysis of why the innovations offer better performance, which could be a point of criticism.""
    },
    ""3"": {
        ""summary"": ""Insufficient discussion on the model's limitations."",
        ""verbatim"": ""The paper does not thoroughly discuss potential limitations or scenarios where the mTANs may not perform optimally, which could be viewed as a lack of critical analysis.""
    }
}",3,{},0,"{
    ""1"": {
        ""summary"": ""The paper lacks sufficient technical details on the architecture and operation of the attention mechanism."",
        ""verbatim"": ""Lack of Technical Depth: The paper could be perceived as lacking in technical depth regarding the exact architecture and operational details of the attention mechanism used.""
    },
    ""2"": {
        ""summary"": ""The paper needs a more exhaustive comparison with a broader range of state-of-the-art models."",
        ""verbatim"": ""Comparative Analysis: While the paper claims superiority over existing methods, a more exhaustive comparison with a wider range of state-of-the-art models could strengthen the validity of these claims.""
    },
    ""3"": {
        ""summary"": ""The paper does not adequately address scalability and practical deployment issues in real-world clinical settings."",
        ""verbatim"": ""Scalability and Practical Deployment: The paper does not sufficiently address issues related to scalability and practical deployment of the proposed model in real-world clinical settings, which are crucial for its adoption.""
    }
}",3,"{
    ""A5-B1"": {
        ""rationale"": ""Both points emphasize a lack of clarity or insufficient explanation regarding crucial aspects of the model's functionality. Review A highlights a general gap in explaining which elements of the architecture contribute to its performance, while Review B specifically criticizes the inadequacy in detailing the architecture and operation of the attention mechanism."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks a comprehensive comparative analysis with baseline and recent models, which could lead to its rejection."",
        ""verbatim"": ""<Comparative analysis depth> The comparison with baseline and recent models needs to be comprehensive and critically analyzed. A lack of deep comparative analysis might lead to the paper's rejection.""
    },
    ""2"": {
        ""summary"": ""Potential overfitting on specific datasets may be a point of criticism leading to rejection if the experimental setup is not rigorously designed."",
        ""verbatim"": ""<Potential overfitting on specific datasets> If the experimental setup is not rigorously designed to prevent overfitting, particularly with multiple datasets, this could be a point of criticism leading to rejection.""
    }
}",2,{},0,"Significance:

The paper presents an innovative approach to time series modeling using deep learning techniques, specifically Multi-Time Attention Networks. The main contribution of the paper is the introduction of a new deep learning framework for irregular time series modeling, which addresses the challenges of sparsely sampled, multivariate, and irregular data.

2. [/SIGNIFICANCE AND NOVELTY:
The paper's innovation lies in its application of time series modeling using deep learning framework, specifically Multi-Time Attention Networks. The paper presents a new approach to learn an embedding of continuous time values and uses an attention mechanism to produce a fixed-length representation of a time series. The novelty of this method lies in its application to irregularly sampled and multivariate time series, which are common in various fields such as finance, engineering and physics.

3. [/POTENTIAL REASONS FOR ACCEPTANCE:
The paper's methods and findings demonstrate the potential for acceptance in the field of time series analysis, specifically in irregularly sampled time series data. The paper compares the performance of Multi-Time Attention Networks with current state-of-the-art methods on interpolation and classification tasks, showing significant improvements in accuracy.

4. [/SUGGESTIONS FOR IMPROVEMENT:
The paper suggests improvements to current time series methods through the use of deep learning techniques and its Multi-Time Attention Networks. The suggested improvements include faster training times and the ability to handle irregularly sampled and multivariate time series data.

Therefore, the paper's contribution lies in the development of a new deep learning framework, Multi-Time Attention Networks, that effectively addresses the challenges of modeling irregular time series data. The approach show significant improvements in accuracy compared to current baseline models, while offering faster training times than recently proposed methods. The proposed approach has the potential for wide applications in various fields such as finance, engineering and physics due to its ability to handle irregularly sampled and multivariate time series data.

Reviewers' suggestions for improvement include explorining the adaptability of the model to various data distributions and irregular sampling patterns. Additionally, investigating the impact of different architectural modifications on the performance of the network and considering the effect of different optimization techniques on its training times.","[Potential reasons for acceptance]
The paper is considered to present an interesting new model and evaluation and appears to bring important contributions in the field of time series modeling. The proposed Multi-Time Attention Networks are considered innovative and the paper presents new approaches for irregular time series. The approach appears to outperform baseline methods in interpolation and classification. The paper is well done, and the empirical analyses are sound, while it is considered to make positive contributions

[Potential reasons for rejection]
The paper's evaluation is considered to be unfair due to the utilization of rooted mean squared error (RMSE) as the end-to-end evaluation metric, making unbiased the comparison of the model performances, especially in interpolation. The work is considered to be incomplete and the evaluation part is problematic, lacking details of the parameters and methods of the experiments.  The clarity of the paper is insufficient, and the technical details of the implementation lacks descriptions.

[Suggestions for improvement]
The authors should provide more details of the technical implementation, including the architecture of the neural networks and any assumptions made in the experiments. More discussion as to how to improve the evaluation metrics and fairness in the comparison of models is necessary. The paper should also include details of the datasets and their properties, as well as the performance metrics for baselines and the proposed methods. Additionally, a code release is advisable to allow for the reproducibility of the results, as well as for further experimentation to improve the evaluation method

The paper could be improved by addressing the following: 
1. Address concerns about the evaluation metrics and fairness of comparisons with baselines. 
2. Provide more information, such as the architecture and parameters of the neural networks utilized in the experiments. 
3. Add more details of the datasets used in the experiments and performance metrics of baselines. 

The paper's quality could be improved by addressing these concerns and focusing on the points made by the reviewers. The evaluation metrics, technical details and completeness in the document should be improved, and the clarity of the presentation should be enhaned to meet the requirements set by the conference.

[TITLE] The paper introduces a new model to process time-series data, addressing the problem of irregularly sampled time-series found in many time series applications. The proposed framework of Multi-Time Attention Networks (MTANs) utilizes a time embedding approach for processing time-series data. The approach offers a good performance in interpolation and classification, outperforms similar models.

[ABSTRACT] The paper is considered to present a good model to process a series data, introducing a new architecture, Multi-Time Attention Networks (MTANs). The approach offers a potentially useful contribution and empirical analysis that appears sound.

[TITLE] The work is considered to make an important contribution in the field of time-series modeling by introducing a deep learning approach. The paper makes a good evaluation and empirical analysis while outperforming recent baselines.","{
    ""1"": {
        ""summary"": ""The paper introduces a novel deep learning framework for irregular time series modeling using Multi-Time Attention Networks."",
        ""verbatim"": ""The main contribution of the paper is the introduction of a new deep learning framework for irregular time series modeling, which addresses the challenges of sparsely sampled, multivariate, and irregular data.""
    },
    ""2"": {
        ""summary"": ""Novelty described as addressing the specific challenges of irregularly sampled and multivariate time series."",
        ""verbatim"": ""The novelty of this method lies in its application to irregularly sampled and multivariate time series, which are common in various fields such as finance, engineering and physics.""
    },
    ""3"": {
        ""summary"": ""No explicit reasons for rejection provided in the review."",
        ""verbatim"": """"
    },
    ""4"": {
        ""summary"": ""Suggestions focus on exploring model adaptability to different data distributions and the impact of architectural changes."",
        ""verbatim"": ""Reviewers' suggestions for improvement include explorining the adaptability of the model to various data distributions and irregular sampling patterns. Additionally, investigating the impact of different architectural modifications on the performance of the network and considering the effect of different optimization techniques on its training times.""
    }
}",4,"{
    ""1"": {
        ""summary"": ""Unfair evaluation due to inappropriate metric choice"",
        ""verbatim"": ""The paper's evaluation is considered to be unfair due to the utilization of rooted mean squared error (RMSE) as the end-to-end evaluation metric, making unbiased the comparison of the model performances, especially in interpolation.""
    },
    ""2"": {
        ""summary"": ""Incomplete evaluation with lack of experimental details"",
        ""verbatim"": ""The work is considered to be incomplete and the evaluation part is problematic, lacking details of the parameters and methods of the experiments.""
    },
    ""3"": {
        ""summary"": ""Insufficient clarity and lack of technical implementation details"",
        ""verbatim"": ""The clarity of the paper is insufficient, and the technical details of the implementation lacks descriptions.""
    }
}",3,"{
    ""A1-B2"": {
        ""rationale"": ""Both points discuss the handling of irregular and sparsely sampled time-series data within the proposed model. While Review A focuses on the lack of specific discussion in the methodology section regarding sparsity and irregular sampling, Review B acknowledges the novelty of the method in addressing these particular facets."",
        ""similarity"": ""7""
    },
    ""A2-B4"": {
        ""rationale"": ""Review A raises concerns about the appropriateness of using a Gaussian noise assumption for irregularly sampled data, suggesting potential issues in the framework's adaptability to real-world data distributions. Review B suggests improving the model by exploring its adaptability to various data distributions, which is relevant to the concern in Review A about the Gaussian assumption possibly being unfit for skewed distributions in real data."",
        ""similarity"": ""8""
    }
}",2,{},0
Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks,BkgXHTNtvS.json,"[TITLE]
BOUNDS ON OVER-PARAMETERIZATION FOR GUARANTEED EXISTENCE OF DESCENT PATHS IN SHALLOW RELU NETWORKS

[ABSTRACT]
We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let m and d be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size n ≥ m + 2d − 2. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for n ≤ m, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for ""existence of descent paths"" in the loss landscape.

[CAPTIONS]
Table 1: (B) For any b ∈ B, βb lies on a facet of P A . We denote this facet by S A (b). (p5) (A) For any a ∈ A, S B (a) is a (d − 1)-dimensional simplex. (B) For any b ∈ B, S A (b) is a (d − 1)-dimensional simplex. (p6) (A) For any a ∈ A, there exist scalars α 1 , . . . , α d ∈ (0, 1) such that a = d i=1 α i s i (a), where s 1 (a), . . . , s d (a) are the vertices of simplex S B (a). (B) For any b ∈ B, there exist scalars α 1 , . . . , α d ∈ (0, 1) such that b = d i=1 α i s i (b), where s 1 (b), . . . , s d (b) are the vertices of simplex S A (b). (p7) (A) For any pair a and a of distinct points in A, we have S B (a) = S B (a ). Moreover, letting H a be the hyperplane that contains S B (a), a and a lie on opposite sides of H a , for all a ∈ A Ã with a = a. (B) For any pair b and b of distinct points in B, we have S A (b) = S A (b ). Moreover, letting H b be the hyperplane that contains S A (b), b and b lie on opposite sides of H b , for all b ∈ B B with b = b.
Table 2: Figure 2 :2Figure 2: A (2, t, 0)-configuration with t = 8. The blue dots show the points in A and red crosses are the points in B.
Table 3: Figure 3 :3Figure 3: Illustration of data points X 1 , . . . , X n for d = 3 and m = 12.
Table 4: consider the facet S B (a) defined in Property (p4), and let s 1 (a), . . . , s d−1 (a) be the vertices of S B (a) (as in Property (
Table 5: )•For each b ∈ B, consider the facet S A (b) and let s 1 (b), . . . , s d−1 (b) be the vertices of S A (b).
Table 6: Figure 4 :4Figure 4: Different types of cupped minima in terms of differentiability, discussed in Appendix A.
Table 7: Figure 5 :5Figure 5: An illustration of the points in equation 24-equation 27 for d = 3 and t = 4. The red crosses indicate the points in A Ã and the blue dots correspond to the points in B B .
Table 8: Properties (p4)-(p7): We only prove Part (B) for each of these properties; as similar proofs work also for Part (A)'s. Moreover, because of the rotational symmetry of A and B in the first two coordinates, it suffices to prove of the Properties (p4)-(p7) only for b 0 .
Table 9: )We proceed to verify equation 65-equation 72. Eq. equation 65 follows from equation 9. Recall the definitions X A − = ξu d and X B − = −ξu d . Then, equation 10 implies equation 69 and equation 71. Since X s1(b) , . . . , X s d−1 (b) define a boundary of the (d − 1)-dimensional convex set Q A , andH passes through X s1(b) , . . . , X s d−1 (b)
Table 10: Forequation 66, it follows from Property (p4) that b and the origin, 0, lie on opposite sides of hyperplane H. Consequently, −X b and u d also lie on opposite sides of hyperplaneH. Therefore, w T b X b and w T b u d have a same sing. It then follows from equation 10 that w T b X b > 0, and equation 66 follows.For equation 72, it follows from Property (p7) that for any b ∈ B B with b = b, X b and X b lie on opposite sides ofH. Eq. equation 66 then implies that w T b X b < 0. This establishes equation 72, and completes the proof of Claim 1.
Table 11: where last equality in due to equation 81. Combing equation 79 and equation 82, we obtain equation 78 for w r = w b . A similar argument implies equation 78 for w r = w a , a ∈ A. This completes the proof of Claim 2. Back to the proof of Lemma 3, for r = 1, . . . , m, let r min i=1,...,d−1 min γ

[INTRODUCTION]
We consider shallow neural networks of the form shown in Fig. 1. The network comprises a hidden layer and an input layer of widths m and d, respectively; and is to be trained over a training set of size n. Our results concern the slightly over-parameterized regime where n ≈ m. We study the existence of poor local minima that have positive curvature in the empirical squared loss landscape.
It is well-known that poor local minima exist in the loss landscape of shallow networks of arbitrary width. In fact, in a shallow network with ReLU activation functions, it is easy to construct training sets whose empirical loss landscape has high plateaus. 1 It is however not fully understood that under what conditions poor local minima may have positive curvature. This paper presents results that improve this understanding.
Non-existence of spurious local minima is closely connected to the so called descent path property: a loss landscape is said to have the descent path property if starting from any initial point there is a path of descent loss to a global minimum. From optimization perspective, the descent path property favors descent optimization algorithms like the pure gradient descent (GD) method. For SGD as well, non-existence of poor local minima is known to be a favorable property for guaranteed convergence (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016). The descent path property is shown to be satisfied in over-parameterized shallow networks with sufficiently large widths (Venturi et al., 2018). The results we present in this work, tighten the existing bounds on the over-parameterization required to guarantee this property.

[BACKGROUND]
Over the past few years, deep neural networks have achieved tremendous performance in various artificial intelligence applications such as computer vision, reinforcement learning, and natural language processing, etc. Despite their remarkable success in practice, theoretical aspects of this success remain a mystery. It has long been an open problem why simple local search algorithms for training deep neural networks, like stochastic gradient descent (SGD), typically converge to local minima with low training error despite the highly non-convex behavior of empirical loss. It has been observed, e.g., in (Choromanska et al., 2015), that these methods may get stuck in poor local minima (i.e., local minima with empirical loss much larger than the global optimum) for small networks, while the problem fades away as the number of parameters grows larger. Such observations are often explained by studying the loss landscape in over-parameterized regime where the number of parameters in the network exceeds the training sample size.
Recently, several attempts have been made to characterize properties of squared loss landscape by conditioning on the layers' dimensions and sample size. Soudry and Hoffer (2017) showed that weights of a neural network can be adjusted such that the empirical loss is zero almost surely if m > 4 n/(2d − 2) ≈ (2n)/d. This result is consistent with experimental observations that neural networks can fit training data if the number of parameters (here approximately 2n) is greater than the sample size. They also proved for normally distributed input that as n goes to infinity, the ratio between the volume of poor flat local minima regions to the volume of flat global minima fades exponentially if d =Ω( √ n) and m =Ω(n/d). Safran and Shamir (2016) showed that if the number of neurons in the hidden layer is Ω(n rank(X) ) (where X is the matrix containing all input), then with high probability, random initialization of weights will put them in a region of parameter space at which the loss surface has a basin-like structure, i.e., every local minimum in that region is global. In another work (Safran and Shamir, 2017), the same authors provide a computer-assisted proof to show that spurious local minima are common in the expected loss landscape of shallow under-parameterized (small-width) networks. Xie et al. (2016) showed that if the input data is drawn uniformly at random from the unit sphere, and if m =Ω(n β ) and d =Ω(n β ) with β ∈ (0, 1) being the decay exponent of the smallest eigenvalue of a kernel matrix, then every critical point is a global minimum.  proved that for any continuous activation function and under the assumption that data samples are distinct, there exist no poor local minima with positive curvature if m ≥ n . In the same spirit, Venturi et al. (2018) showed that for any continuous activation function, there is always a descent path to an optimal solution with zero loss in the empirical loss landscape if m ≥ n.
Several works have proposed similar results in other settings and under different assumptions. Soudry and Carmon (2016) showed that in a network of leaky ReLU activation functions with randomized perturbation of slopes, all differentiable local minima are global minima if m ≥ n/d. Kawaguchi (2016) proved that in shallow networks with linear activation functions, every local minimum is a global minimum and all the saddle points are strict in the sense that they have a direction of strictly negative curvature. Soltanolkotabi et al. (2019) showed that the same result carries over to quadratic activation functions under the assumption that the last layer comprises at east d positive and d negative weights. Du and Lee (2018) established similar results for quadratic activation functions, assuming m ≥ √ 2n. For deep neural networks with linear activation functions, Freeman and Bruna (2016) showed that all local minima are global minima if there is a hidden layer whose number of neurons exceeds the minimum of the widths of input and output layers. For deep neural networks with analytical activation functions, Nguyen and Hein (2017) proved a similar property under the assumptions that the number of neurons in some hidden layer is greater than sample size and the network has a pyramidal structure.
Such studies on the properties of loss landscape do not only provide insights into the complication of training, but are also beneficial for proving performance guarantees for some local search algorithms. For the class of loss functions whose landscape satisfy the properties of: a) all local minima are global, b) all saddle points are strict, it has been shown in several works (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016) that perturbed gradient descent converges to global optima in polynomial time. Another direction of research concerns the convergence of specific optimization algorithm such as Figure 1: Architecture of the shallow network considered in this paper. The network has a single hidden layer of m neurons with ReLU activation functions, and a neuron with linear activation function in its output layer. pure gradient descent and SGD without assuming such properties for the loss landscape Allen-Zhu et al., 2018;.

[OUR CONTRIBUTIONS]
We study the amount of over-parameterization required for guaranteed existence of descent paths to zero loss in the empirical loss landscape. Previous works suggest that zero loss is always possible for m ≥ 2n/d (Soudry and Hoffer, 2017). On the other hand, the best existing bound for guaranteed existence of descent paths to this zero loss requires m ≥ n neurons in the hidden layer (Venturi et al., 2018). Prior to the present work, it was not known whether the ""descent path property"" holds for m < n. Even for m ∈ (2n/d, n), where zero empirical risk is known to be achievable (Soudry and Hoffer, 2017), the existence of descent paths was in question. In this work, we tighten this gap and prove that there are training sets, under which in any network of width m ≤ n − 2d + 2, there exist initial weights that have no descent path to global minima. We do this by showing that the loss landscape, in this regime, admits poor local minima with positive curvature. We also provide evidences and make conjectures that these results carry over to networks of width m = n − 4, which, if true, provides a sharp characterization of the over-parameterization required for guaranteed existence of descent paths. We also wish to point that unlike most previous works, we do not restrict to differentiable local minima; for a simple argument shows that local minima with positive curvature cannot be differentiable if m > n/d (cf. Appendix A).

[OUTLINE]
We continue by discussing details of the system model and introducing our key definitions in Section 2. We then present, in Section 3, the main results of the paper. Proof of the main results are then given in Section 4. We finally discuss implications and possible extensions of our results in Section 5 along with a number of open problems and directions for future research.

[MODEL]
We consider shallow networks of the form shown in Fig. 1. The network takes d-dimensional inputs denoted by X. There is a single hidden layer comprising m neurons with ReLU activation function. For simplicity of our proofs, we only consider even values of m. We denote the input weights of r-th neuron by a d-dimensional vector w r , for r = 1, . . . , m. We then let w ∈ R md be the vector representation of all weights in the first layer.
The output layer has a single neuron, whose activation function is linear with an m-dimensional weight vector denoted by v. The network outputs a scalarŷ(w, v) = m r=1 v r w T r X1(w T r X ≥ 0). We fix a training set (X 1 , y 1 ), . . . , (X n , y n ) of size n, and consider the landscape of empirical squared loss function:
F (w, v) n i=1 ŷ i (w, v) − y i 2 .
(1)

[PROPERTIES OF THE LANDSCAPE]
We first provide a formal definition for the descent path property, which is a necessary condition for guaranteed performance of descent optimization algorithms.
Definition 1 (Descent path property). Consider a continuous function f : R d → R and let f * = inf x∈R d f (x) be its infimum. We say that f has the descent path property if for any x ∈ R d , there exists a continuous curve with γ : [0, 1] → R d such that γ(0) = x, f γ(1) = f * , and f γ(t) is a non-increasing function of t.
The descent path property is a necessary condition for any descent optimization algorithm to provably find a global minimum from all initial conditions. It was shown in Venturi et al. (2018) that the empirical loss landscape of a shallow neural network with ReLU activation and squared loss has the descent path property if the size of training data is no larger than the width of the hidden layer, i.e., n ≤ m. We now characterize a class of local minima of specific form in the following definition.
Definition 2 (Cupped minima). Given a function f : R d → R, we call x ∈ R d a cupped minimum of f if there are , δ > 0 such that for any y in the δ-neighborhood of x, we have f (y) ≥ f (x)+ y−x 2 . By a sub-optimal cupped minimum we mean a cupped minimum that is not a global minimum.
Note that every cupped minimum is a local minimum, but not every local minimum is cupped (e.g., flat local minima are not curved downwards, and hence are not cupped). Also note that a function is not necessarily differentiable at its cupped minima. We study cupped minima of the loss function in equation 1. Note however that for any α > 0, F (αw, v/α) = F (w, v). Therefore, F (•, •) has no cupped minima if both arguments are taken as variables. For that matter, when talking about cupped minima of F , we fix a v and consider F (•, v) as a function of its first argument. Interestingly, existence of cupped minima for F (•, v) leads to violation of descent path property for F (•, •) over both arguments, as shown in the following lemma. The proof is given in Appendix B.
Lemma 1. Consider a shallow network with loss function F in equation 1, and a pair of weights (w, v). Suppose that w is a sub-optimal cupped minimum of F (•, v), and that w r = 0, for r = 1, . . . , m. Then, F (•, •) has no descent path w(t), v(t) , initiated at (w, v), to its global minima.

[MAIN RESULTS]
The following theorem and corollary state the main results of the paper.
Theorem 1. For any d ≥ 4, m ≥ 8 + 4 3/(d − 3) , and n ≥ m + 2d − 2, there exists a training set of size n such that the empirical loss function F of a shallow neural network of width m has the following property. For any m-dimensional vector v, with m/2 number of positive and m/2 number of negative entries, F (•, v) has exponentially many sub-optimal cupped minima.
The proof is constructive and is given in Section 4. In particular, we devise a training sequence (X 1 , y 1 ), . . . , (X n , y n ) such that for weights (w, v) at the cupped minima, we have
v r w r = 1/ √ m, for r = 1, . . . , m. Moreover, X i ≤ 1, |y i | ≤ 2, and |e i | = 1 for i = 1, . . . , n (cf. Remark 2).
According to Theorem 1, there are training sequences tailored to give rise to sub-optimal cupped minima. However, we wish to point that the existence of such cupped minima does not stem from measure-zero incidents like placement of several data points on a low dimensional plane. In fact, in view of Lemma 1, any path that starts from a cupped minimum and end up in a global minimum would have an uphill climb of at least , for some > 0. Since the loss surface is a continuous function of (X i , y i ), a small perturbation of (X i , y i )'s leads continuously to a small change in F . Therefore, for small enough perturbations of (X i , y i ), any path to the set of global minima would still witness a positive uphill-climb. Hence, the descent path property remains out of order, even when the training data is slightly perturbed. Based on the above intuition, we can establish the following corollary 2 , Corollary 1. For any d ≥ 4, m ≥ 8 + 4 3/(d − 3) , and n ≥ m + 2d − 2; and when the inputs X and labels y are randomly drawn from independent normal distributions, there is a non-zero probability that F (•, •) does not have the descent path property. Venturi et al. (2018) that n ≤ m is sufficient for the descent path property to hold. In contrast, Corollary 1 show that if n ≥ m + 2d − 2, then the descent path property is not necessarily in effect. This leave a gap of size 2d − 2 for the edge of over-parameterization required to guarantee the descent path property. We believe that this edge lies sharp at n = m. We conjecture a stronger version of Theorem 1, that cupped minima can emerge for training data sizes as small as m = n − 4. Conjecture 1. Statement of Theorem 1 holds for all d ≥ 4, m ≥ 2d + 4, and n ≥ m + 4.

[IT WAS SHOWN IN]
See Remark 1 for insights into the possibility of this conjecture.

[PROOF OF THE MAIN RESULT]
In this section, we present the proof of Theorem 1 organized in a sequence of four subsections. We first present some preliminaries in Subsection 4.1. In Subsection 4.2, we introduce a geometric structure called ""(d, t, k)-configuration"", based on which we construct, in Subsection 4.3, the training set that gives rise to cupped minima in the loss landscape. Finally, in Subsection 4.4, we prove the existence of cupped mimima in the devised setting. In order to provide intuitions on the loss landscape at cupped minima and motivate our construction of the training set in Subsection 4.3, we make a short note on different types of cupped minima in Appendix A. We also defer the proofs of some lemmas from this section to appendices for improved readability.

[PRELIMINARIES]
Consider weights (w, v) and let (w , v ) be another set of weights such that for r = 1, . . . , m, v r and v r have the same sign and w r = (v r /v r )w r . Then, F (w, v) = F (w , v ). Moreover, it is no difficult to see that w is a cupped minimum of F (•, v) if and only if w is a cupped minimum of F (•, v ). For this reason, it suffices to prove existence of cupped minima for a fixed vector v. Note also that where w r 's are distinct, any permutation of the order of neurons would give rise to a new cupped minimum. Hence, existence of a cupped minimum for F (•, v) implies existence of exponentially many cupped minima for F (•, v).
We denote by e i =ŷ i (w, v) − y i the estimation error for input X i . We let u d = [0, . . . , 0, 1] T be a d-dimensional vector with the last entry equal to one and all other entries equal to zero. For a region P ⊆ R d , we denote its interior and and its convex-hull by int(P) and Conv(P), respectively. Assuming differentiability of F at w, the partial derivatives of the loss function with respect to w r , r = 1, . . . , m, is as follows
∇ wr F (w, v) = v r n i=1 e i X i 1 w T r X i ≥ 0 . (2)

[A GEOMETRIC CONFIGURATION]
We introduce a geometric structure for sets of points in R d . This configuration will be used in Subsection 4.3 to construct a landscape with cupped minima.
Definition 3 ((d, t, k)-Configuration). Given integers d, t, k ≥ 0 and disjoint sets A,Ã, B, andB of points in R d , we say that A,Ã, B,B forms a (d, t, k)-configuration if the following properties are satisfied:
(p1) Each of A and B consists of t points, and each ofÃ andB consists of k points.
(p2) (A) The convex hull of A Ã forms a polytope P A that has exactly t + k vertices. Equivalently, no point in A Ã is a convex combination of other points in A Ã . (B) Similarly, the convex hull of B B forms a polytope P B that has exactly t + k vertices.
(p3) We have 0 ∈ int(P A ) and 0 ∈ int(P B ).
(p4) There exists a constant β ∈ (0, 1) such that (A) For any a ∈ A, βa lies on a facet of P B . We denote this facet by S B (a). (p8) Consider a 2t × 2t matrix M whose rows and columns are associated to points p ∈ A B and q ∈ A B, and whose entries are as follows
M pq = d p, H q , if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A, 0, otherwise,(3)
where H q is the hyperplane define in Property (p7), and d(•, •) is the euclidean distance.
The property requires M to be full-rank.
Among the above properties, the most difficult of all is Property (p4), and the requirement that it involves the same β for all points in A B. In fact, elimination of Property (p4) gives rise to trivial configurations. 3 In the two dimensional space, for any t ≥ 4 there exists a (2, t, 0)-configuration of the form illustrated in Fig. 2. In the following proposition, we generalize this observation to higher dimensions. Proposition 1. For any d ≥ 2 and t ≥ 4, there exists a (d, t, d − 2)-configuration.
The proof is constructive, and is given in Appendix C. We conjecture that there also exist (d, t, 0)configurations.
Conjecture 2. For any d ≥ 2 and t ≥ 2d, there exists a (d, t, 0)-configuration. Remark 1. Using a configuration given by Conjecture 2 instead of the configuration from Proposition 1 in the construction and the proof that follow, we obtain a proof for Conjecture 1. In this view, establishing Conjecture 2 would also resolve Conjecture 1.

[CONSTRUCTING A LANDSCAPE WITH CUPPED MINIMA]
Here we present a set of training data (X 1 , y 1 ), . . . , (X n , y n ) and a set of wights (w 1 , v 1 ), . . . , (w m , v m ) such that the empirical loss surface corresponding to (X 1 , y 1 ), . . . , (X n , y n ) has a sub-optimal cupped minimum at (w 1 , v 1 ), . . . , (w m , v m ). Without loss of generality, we assume n = m + 2d − 2. Extension to larger values of n is straightforward via replication.
Let A,Ã, B,B be a (d − 1, m/2, d − 3)-configuration in the (d − 1)
-dimensional space, as in Proposition 1. In view of Property (p3), let 0 > 0 be such that P A and P B contain the 0neighborhood of 0. Let
ξ 1 0 a∈A Ã a + b∈B B b + n + 1 β . (4
)
We proceed by introducing the data points X 1 , . . . , X n . An illustration of these data points in the three dimensional space is shown in Fig. 3.  Data points X: We consider a total number of n = m + 2d − 2 data points as follows.
• For each a ∈ A Ã , we consider a new data point X a as follows.
Let [z 1 , . . . , z d−1 ] ∈ R d−1
be the representation of a in the Cartesian coordinates. We let
X a = z 1 , . . . , z d−1 , 1 . • For each b ∈ B B , we consider a new data point X b as follows. Let [z 1 , . . . , z d−1 ] ∈ R d−1 be the representation of b in the Cartesian coordinates. We let X b = − z 1 , . . . , z d−1 , 1 .
• We consider two extra points X A + , X A − , X B + , and X B − as follows. We let
X A − ξu d , X B −
−ξu d , and
X A + X A − − a∈A Ã X a + 1/β − 1 u d (5) X B + X B − + b∈B B X b + 1/β − 1 u d (6
)
where u d = [0, . . . , 0, 1] T , and ξ and β are defined in equation 4 and Property (p4), respectively.
Weights at cupped minimum: We associate each of m neurons to a point p in A B, in a one-one manner; and denote the vector of input weights and the output weight of that neuron by w p and v p , respectively. These weights are chosen as follows:  p6)). We let w a be the unique vector such that w a = 1 and We let w b be the unique vector such that w b = 1 and
• For each a ∈ A, we let v a = −1/ √ m. • For each b ∈ B, we let v b = 1/ √ m. • For each a ∈ A,
w T a X si(a) = 0, i = 1, . . . , d − 1,(7)
w T a u d < 0. (8
w T b X si(b) = 0, i = 1, . . . , d − 1,(9)
w T b u d > 0. (10
)
Labels y: Having determined the data points X and the weights (w, v), the outputŷ(w, v) of the network is determined for all input X. In the following, we choose the true labels y to obtain a desired error e =ŷ − y for each input data. In particular:
• For each a ∈ A Ã , we associate to X a a label y a so that e a ŷ a (w, v) − y a = 1.
• For each b ∈ B B , we associate to X b a label y b so that e b ŷ b (w, v) − y b = −1.
• We choose the labels associated to X A + , X A − , X B + , and X B − such that e A + = e B + = 1 and e A − = e B − = −1.
This completes the description of the training set. As shown in in Soudry and Carmon (2016), there exist weights that achieve zero loss if m > 4 n/(2d − 2) . In our case, n = m + 2d − 2, and its easy to check that m > 4 (m + 2d − 2)/(2d − 2) for all d ≥ 4 and m ≥ 8 + 4 3/(d − 3) . It follows that in our setting the global optimum has zero loss, showing that the above (w, v) is sub-optimal. Remark 2. In the above construction, the norms of inputs vectors may be very large. If we scale down the inputs such that X i ≤ 1/ √ m for i = 1, . . . , n, and modify the corresponding labels y i such that e i remains unchanged, then a same set of weights will still be a cupped minimum for the landscape defined in terms of new (X i , y i )'s. Moreover, for this setting, it is easy to check that w r = 1, e i = 1, and |y i | ≤ 2.

[PROVING THE CUPPED]
MINIMA PROPERTY Let δ min |w T r X i | X i w T r X i = 0, r = 1, . . . , m, i = 1, . . . , n .(11)
Then, δ > 0. For any θ ∈ R md with θ = 1 and for any t ∈ [0, δ], let
F θ (t) = F (w + θt, v).(12)
We show that there is an > 0 such that for any unit-norm θ and any t ∈ [0, δ],
F θ (t) ≥ F θ (0) + t 2 . (13
)
Lemma 2. For any θ ∈ R md with θ = 1, F θ (•) is a quadratic and convex function over
[0, δ].
The proof is give in Appendix D and relies on the fact that neuron activations do not alter at w + θt for t ∈ [0, δ]. Consider now the following m-dimensional subspace of R md
H w         α 1 w 1 . . . α m w m    α 1 , . . . , α m ∈ R      . (14
)
For any θ ∈ R md , let θ and θ ⊥ be the orthogonal projections of θ on H w and H ⊥ w , respectively. Then, θ = θ + θ ⊥ . In order to establish equation 13, we need lower bounds on F θ (0) and F θ , which we derive in the next two lemmas. Lemma 3. There exists µ > 0 such that for any θ ∈ R md with θ = 1, we have
dF θ (t) d + t t=0 ≥ µ θ ⊥ . (15
)
The proof is given in Appendix E, and relies in a subtle way on the choice of data points in subsection 4.3. We now bound the curvature of F θ . Lemma 4. There exist constants η 1 , η 2 > 0 such that for any θ ∈ R md with θ = 1, and for any t ∈ (0, δ),
d 2 F θ (t) dt 2 ≥ max 2η 1 θ 2 − 2η 2 θ ⊥ , 0 . (16
)
The proof is given in Appendix F, and relies on Property (p8) of the underlying configuration.
Consider now the second order polynomial p(x) = η 1 δ 1 − x 2 − η 2 δx − µx, where µ, η 1 , and η 2 are the constants in Lemmas 3 and 4. Since p(0) > 0 and p(1) < 0, the polynomial p has exactly one root in the interval (0, 1), which we denote by x 0 . Let
µx 0 /δ. (17
)
Lemma 5. For any θ ∈ R md with θ = 1, and any t ∈ [0, δ], we have
F θ (t) ≥ F θ (0) + t 2 .
This lemma is a simple consequence of Lemmas 3 and 4, and its proof is given in Appendix G. It follows from Lemma 5 that for any w in the δ-neighborhood of w, we have F (w , v) ≥ F (w, v) + w − w 2 . This shows that w is a cupped minimum for F (•, v), and completes the proof of Theorem 1.

[DISCUSSION]
The guaranteed existence of descent paths in shallow networks of ReLU neurons was previously established (Venturi et al., 2018), given an over-parameterization of m ≥ n (where m and n are the number of neurons and the size of training data, respectively). This left an uncertainty gap of 2n/d < m < n, where zero empirical risk is known to be achievable (for m ≥ 2n/d) (Soudry and Hoffer, 2017), but the existence of descent paths was in question. In this work, we have tightened this uncertainty gap to n − 2d + 2 < m < n, by proving that for any m ≤ n − 2d + 2, there are input data and initial weights for which a descent path does not exist. This conclusion we reach by establishing the existence of cupped minima for m ≤ n − 2d + 2, and for the right choice of input data.
Compared to similar existing results for other activation functions, our results suggest that the edge m ≈ n of over-parameterization required for elimination of sub-optimal cupped minima in ReLU networks is much higher than that of networks with quadratic activation functions, m ≈ √ 2n (Du and Lee, 2018), and linear activation functions, m ≈ n/d (Kawaguchi, 2016), and is almost as high as general continuous activation functions of any form, m ≤ n (Venturi et al., 2018).
Non-existence of spurious local minima and the decent path property favor the convergence of decent optimization methods like GD. However, for different variants of noisy GD, like SGD and Langevin dynamics, it is quite common for the empirical loss to fluctuate during the training. Nevertheless, for theoretical analysis purposes it usually helps to take the noise away, for example by tending the step size to zero. The resulting GD, which always follows a descent path, is usually easier to analyze and can also help to study the SGD dynamics. On the other hand, from a practical view, convergence of SGD in local-min-free landscapes is well-studied.
Aside from addressing Conjecture 2, there remain several open problems, which we review next. As an important direction for future research, it would be interesting if one could obtain bounds on the probability of existence of cupped minima over random data sets, underneath the edge of over-parameterization. In particular, we showed in Corollary 1 that this probability is non-zero, however we gave no clue on either the size or scaling of this probability. Another class of problems concerns basins of local minima, and how they affect dynamics of first order optimization algorithms. As a step toward this goal, one might characterize the true over-parameterization regime in which the basins of poor local minima have considerable volume. Among other directions are extensions of our results to deep ReLU networks, shallow non-ReLU networks, and shallow ReLU networks under loss functions more general than the squared loss.

[APPENDICES A DIFFERENT TYPES OF CUPPED MINIMA IN TERMS OF DIFFERENTIABILITY]
Here we discuss different types of cupped minima and provide elementary intuitions on the loss landscape at cupped minima.
We first characterize curvature of the loss function at differentiable points. For r = 1, . . . , m, let J r be an n × n diagonal matrix whose (i, i) entry equals 1 w T r X i ≥ 0 . Let G be an n × md matrix of the form:
G = v 1 J 1 X T • • • v m J m X T , (18
)
where X is a d × n matrix that has X i in its i-th column. If F is differentiable at (w, v), its gradient is ∇ w F (w, v) = G T [e 1 , . . . , e m ] T , where e i =ŷ i − y i is the output error for input X i . Moreover, if F is differentiable at (w, v), its Hessian with respect to w equals
∇ 2 w F (w, v) = G T G. (19
)
We classify cupped minima into three categories in terms of differentiability. Specifically, for a cupped minimum w of F (•, v), we consider three cases:
Type 1) F is differentiable at w.
Type 2) F (w + θt, v) as a function of t is non-differentiable at t = 0, for all θ ∈ R md .
Type 3) There are θ 1 , θ 2 ∈ R md such that F (w+θ 1 t, v) is differentiable at t = 0, while F (w+θ 2 t, v) is non-differentiable at t = 0.
Fig. 4 illustrates examples of loss surface at the above three types of cupped minima. We now argue that the first two types are not possible in the loss landscape of shallow networks.
If F (•, v) is differentiable at w, its Hessian given in equation 19 equals G T G. Since G is an n × md matrix, assuming md > n, G T G would have zero eigenvalues. Therefore, w cannot be a cupped minimum of F (•, v). It follows that there exists no differentiable cupped minimum (nor saddle point) if md > n.
For non-differentiable points, note that F (w + wt, v) as a function of t is a differentiable quadratic function. This is because the output scales proportionally with w. Therefore, cupped minima of the second type are not possible, as well. In the same spirit, it can be shown that F (w + θt, v) is differentiable as a function of t, if θ ∈ R md belongs to the m dimensional subspace H w defined in equation 14.
For the above reasons, all cupped minima, if any, are of the third type. Therefore in the construction of Subsection 4.3, we introduce a training set and a pair of weights (w, v) such that w is a cupped minimum of F (•, v), and F (w + θt, v) is differentiable in t only for θ ∈ H w .

[B PROOF OF LEMMA 1]
If v has a zero entry, v r = 0 for some r, then F (•, v) is a constant function with respect to w r , and thereby has no cupped minima. Therefore, we assume that v has no zero entries. Let
t 0 = inf t | ∃r, v r (t) = 0 .
For t > 0, we definew
(t) =    v 1 (t)/v 1 × w 1 (t) . . . v m (t)/v m × w m (t)    . (20
)
Let t 1 = inf t |w(t) = w . We show that t 1 < t 0 . If t 0 < ∞, then continuity of v(t) implies that v r (t 0 ) = 0, for some r ≤ m. Therefore,w r (t 0 ) = 0 = w r . It then follows from the continuity ofw r (•) that there is an > 0 such thatw r (t 0 − ) = w r . Consequently, t 1 < t 0 .
The inequality t 1 < t 0 implies that there is an > 0 such thatw r (t 1 + ) = w, and for any t ∈ [0, t 1 + ], we have sgn v r (t) = sgn(v r ), r = 1, . . . , m. Therefore, for any t ∈ [0, w, v). This shows that w(t), v(t) cannot be a descent path for F (•, •), and completes the proof of Lemma 1.
t 1 + ], we have F w(t), v = F w(t), v(t) . Since w is a cupped minimum of F (•, v), there is an s ∈ [t 1 , t 1 + ] such that F w(t), v > F (

[C PROOF OF PROPOSITION 1]
For i = 3, . . . , d, let
c i 1 i − 1 + 2 cos(π/t) . (21
)
Fix a constantc
d i=3 1 + c i = d i=3 i + 2 cos(π/t) i − 1 + 2 cos(π/t) = d + 2 cos(π/t) 2 + 2 cos(π/t) . (22
)
Let γ be a uniform random variable
γ ∼ unif cos(π/t) − cos(2π/t) 4c , cos(π/t) − cos(2π/t) 2c . (23
)
We take a sample from the above distribution and fix a γ for the rest of the proof.
We now introduce the points in the configuration. For i = 0, . . . , t − 1, we consider points a i ∈ A and b i ∈ B as follows
a i = cos 2π(i − 1/2) t , sin 2π(i − 1/2) t , γc 3 , γc 4 , . . . , γ d c d ,(24)
b i = cos 2πi t , sin 2πi t , −γc 3 , −γc 4 , . . . , −γ d c d . (25
)
For i = 3, . . . , d, we consider pointsã i ∈Ã andb i ∈B as follows
a i = 0, . . . , 0 i−1 , −1, c i+1 , . . . , c d , (26
) b i = 0, . . . , 0 i−1 , 1, −c i+1 , . . . , −c d . (27
)
Fig. 5 shows an illustration of these points for d = 3.
We proceed by verifying Properties (p1)-(p8).
Property (p1): Property (p1) is straightforward from the above construction.
Property (p2): Considering only the first two coordinates, it is easy to see that no point of A is a convex combination of other points in A Ã . Also note that for i = 3, . . . , d,ã i is the only point Property (p3): Let
α i = 1 γt , i = 1, . . . , t,(28)
α i = (c 3 + 1) • • • (c i−1 + 1)c i , i = 3, . . . , d.(29)
A simple induction on j shows that
j−1 i=3α i =α j c j − 1. (30
)
for j = 4, . . . , d. We show that
1 t−1 i=0 α i + d i=3α i t−1 i=0 α i a i + d i=3α iã i = 0. (31
)
For the first two coordinates, equation 31 is easy. Let a i j andã i j be the j-th entries of a i andã i , respectively. For the j-th coordinate, j = 3, . . . , d, we then have
t−1 i=0 α i a i j + d i=3α iã i j = t × 1 tγ × γc j + c j j−1 i=3α i −α j = c j + c j α j c j − 1 −α j = 0,
where the second equality is due to equation 30. This establishes equation 31. It then follows from equation 31 that 0 is a convex combination of points in A Ã . Consequently, 0 ∈ int(P A ). A similar argument shows that 0 ∈ int(P B ). Properties (p4) and (p5): For the j-th coordinate, j = 3, . . . , d, we have
a 0 j + a 1 j + γ d i=3ã i j = γc j + γc j + γ j−1 i=3 c j − γ = γ (j − 1)c j − 1 = γ (j − 1) 1 j − 1 + 2 cos(π/t) − 1 = −2γ cos(π/t) j − 1 + 2 cos(π/t) = − 2 cos(π/t) γc j = 2 cos(π/t) b 0 j ,(32)
where b 0 j is the j-th entry of b 0 defined in equation 25. Similarly, for the first two coordinates, we have:
a 0 1 + a 1 1 + γ d i=3ã i 1 = cos(π/t) + cos(π/t) = 2 cos(π/t) b 0 1 , a 0 2 + a 1 2 + γ d i=3ã i 2 = sin(π/t) − sin(π/t) = 0 = 2 cos(π/t) b 0 2 .(33)
It then follows from equation 32 and equation 33 that
b 0 = 1 2 cos(π/t) a 0 + a 1 + γ d i=3ã i .(34)
Let
β 2 cos(π/t) (d − 2)γ + 2 . (35
)
Then, from equation 34,
βb 0 = 1 (d − 2)γ + 2 a 0 + a 1 + γ d i=3ã i .(36)
Therefore, βb 0 is a convex combination of a 0 , a 1 ,ã 3 , . . . ,ã d , and therefore lies on the simplex S that has a 0 , a 1 ,ã 3 , . . . ,ã d as its vertices. Next, we show that S is a facet of P A .
Consider a vector z ∈ R d with entries
z 1 = (c − 1)γ + 1 cos(π/t) , z 2 = 0, z i = − d j=i+1 (c j + 1), i = 3, . . . , d − 1, z d = −1.(37)
Then, a simple backward induction, with base case i = d, shows that for i = 3, . . . , d, we have
d j=i+1 c j z j = z i + 1. In the same vein, d j=3 c j z j = −c + 1.(38)
It follows that for i = 3, . . . , d,
z Tãi = d j=i+1 z j c j − z i = 1.(39)
Moreover, for i = 0, 1,
z T a i = d j=3 z j γc j + z 1 cos(π/t) = γ(−c + 1) + z 1 cos(π/t) = −γ(c − 1) + (c − 1)γ + 1 cos(π/t) cos(π/t) = 1,(40)
where the second equality is due to equation 38. Let H be the hyperplane that passes through a 0 , a 1 ,ã 3 , . . . ,ã d . It follows from equation 39 and equation 40 that for any p ∈ a 0 , a 1 ,ã 3 , . . . ,ã d , we have z T p = 1. Therefore, z is orthogonal to H. Consequently,
H = x ∈ R d | z T x = 1 .(41)
For i = 2, . . . , t − 1, we have
z T a i = z T a i − a 0 + z T a 0 = z T a i − a 0 + 1 = z 1 cos(2π(i − 1/2)/t) − cos(π/t) + 1 < 1,(42)
where the second equality is due to equation 40, and the inequality is because z 1 > 0 and cos(2π(i − 1/2)/t) − cos(π/t) < 0. It follows from equation 41 and equation 42 that all points of A\{a 0 , a 1 } lie on one side of H, while all points ofÃ {a 0 , a 1 } lie on H. Then, H is a tangent hyperplane to P A . Thus, the simplex S is a facet of P A . This completes the proofs of Properties (p4) and (p5). Moreover, from the definition H b in Property (p7), we have
H b 0 = H.(43)
Property (p6): Since t ≥ 4, we have 2 cos(π/2) > 1. Moreover, recall from equation 23 that γ < 1. Property (p6) then follows from equation 34 and the fact that S is a facet of P A .
Property (p7): For i = 0, . . . , t − 1,
z T b i = z T (b i − a 0 ) + z T a 0 = z 1 cos(2πi/t) − cos(π/t) − 2 d j=3 z j γc j + 1 = z 1 cos(2πi/t) − cos(π/t) + 2γ(c − 1) + 1.(44)
where the second equality is due to equation 40 and definitions of a 0 and b i , and the third equality is from equation 38. It follows that
z T b 0 = z 1 1 − cos(π/t) + 2γ(c − 1) + 1 > 1,(45)
where the inequality is because the first two terms on the left hand side of the inequality are positive.
In the same vein, for i = 1, . . . , t − 1
z T b i = z 1 cos(2πi/t) − cos(π/t) + 2γ(c − 1) + 1 < z 1 cos(2π/t) − cos(π/t) + 2γc + 1 < cos(2π/t) − cos(π/t) + 2γc + 1 ≤ cos(2π/t) − cos(π/t) + cos(π/t) − cos(2π/t) + 1 = 1,(46)
where the second inequality follows from the definition of z 1 in equation 37 and the fact that z 1 > 1, and the third inequality is from the definition of γ in equation 23 and the fact that 2γc ≤ D PROOF OF LEMMA 2
Consider a block representation of θ as follows
θ =    θ 1 . . . θ m    ,(54)
where each θ r is a d-dimensional vector.
It follows from the definition of δ that for any t ∈ (0, δ), if w T r X i > 0, then (w r + θ r t) T X i > 0; and if w T r X i < 0, then (w r + θ r t) T X i < 0. Therefore, for r = 1, . . . , m and i = 1, . . . , n, and for any t ∈ [0, δ) ,
1 (w r + θ r t) T X i ≥ 0 = 1 w T r X i > 0 + 1 w T r X i = 0, θ T r X i ≥ 0(55)
Consequently, for any t ∈ [0, δ] and i = 1, . . . , n, we havê
y i w + θt, v = m r=1 v r w T r X i + tθ T r X i 1 (w r + θ r t) T x i ≥ 0 = m r=1 v r w T r X i + tθ T r X i 1 w T r X i > 0 + 1 w T r X i = 0, θ T r X i ≥ 0 .(56)
It follows thatŷ i w + θt, v is a linear function of t over the interval t ∈ [0, δ]. Therefore, F θ (t) = n i=1 ŷ i (w + θt, v) − y i 2 is a quadratic and convex function of t, for t ∈ [0, δ].

[E PROOF OF LEMMA 3]
We first characterize active neurons for different inputs. For two subsets S 1 , S 2 ⊂ R d we let S 1 \S 2 = S 1 S c 2 . Recall the definition of s i (a) and s i (b) from Property (p6). Claim 1. For any a ∈ A, we have
w T a X si(a) = 0, i = 1, . . . , d − 1,(57)
w T a X a > 0,(58)
w T a X b > 0, b ∈ B B \ s 1 (a), . . . , s d−1 (a) ,(59)
w T a X B + > 0,(60)
w T a X B − > 0,(61)
w T a X A + < 0,(62)
w T a X A − < 0,(63)
w T a X a < 0, a ∈ A Ã \ {a}.(64)
Similarly, for any b ∈ B, we have
w T b X si(b) = 0, i = 1, . . . , d − 1,(65)
w T b X b > 0,(66)
w T b X a > 0, a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) ,(67)
w T b X A + > 0,(68)
w T b X A − > 0,(69)
w T b X B + < 0,(70)
w T b X B − < 0,(71)
w T b X b < 0, b ∈ B B \{b}. (72
)
Proof of Claim 1. Fix a b ∈ B. We begin by introducing some notations. Let H be the (d − 2)dimensional hyperplane in the (d − 1)-dimensional space that passes through s 1 (b), . . . , s d−1 (b). In the same spirit, letH be the (d − 1)-dimensional subspace in the d-dimensional space that passes through X s1(b) , . . . , X s d−1 (b) , equivalentlyH is the subspace orthogonal to w b . We denote the convex hull of X s1(b) , . . . , X s d−1 (b) by C B Conv X s1(b) , . . . , X s d−1 (b) . Similarly, we let
Q A conv X a | a ∈ A Ã and Q B Conv X b | b ∈ B B .
Before presenting the proofs of properties equation 57-equation 72, we review make some easy observations. Recall the definition of 0 from the paragraph proceeding equation 4. Let B d−1 0 be the intersection of the 0 -ball centered at 0 with the orthogonal space of u d . Then, from the definition of , we have
B d−1 0 + u d ⊂ Q A , B d−1 0 − u d ⊂ Q B . (73
)
For x ∈ R d , let π(x) be the projection of x on the span of first d − 1 coordinates, i.e., the orthogonal space of u d . Then,
π ( X A + ) ξ + 1/β − n − 1 = a∈A Ã (X a − 1) ξ + 1/β − n − 1 = a∈A Ã a ξ + 1/β − n − 1 < a∈A Ã a ξ − n < a∈A Ã a a∈A Ã a / 0 = 0 ,(74)
where the first equality is from the definition of X A + and the second inequality follows from the definition of ξ in equation 4. Therefore, π(X
A + )/ ξ + 1/β − n − 1 ∈ B d−1 0 . Consequently, 1 ξ + 1/β − n − 1 X A + = π(X A + ) ξ + 1/β − n − 1 + u T d X A + u d ξ + 1/β − n − 1 = π(X A + ) ξ + 1/β − n − 1 + ξ + 1/β − n − 1 u d ξ + 1/β − n − 1 = π(X A + ) ξ + 1/β − n − 1 + u d ∈ B d−1 0 + u d ∈ Q A ,(75)
where the first equality is orthogonal decomposition of X A + , and last inclusion follows from equation 73. In the same vein, we can show that , then all points in Q A lie on a same side ofH. In other words, either we have w T b x ≥ 0, for all x ∈ Q A ; or we have w T b x ≤ 0, for all x ∈ Q A . In view of Property (p3), u d ∈ int(Q A ). It then follows from equation 10 that for any x ∈ Q A , we have w T b x ≥ 0. Consequently, for any x ∈ Q A \H, we have w T b x > 0. In particular,
− 1 ξ + n + 1 − 1/β X B + ∈ Q A . (76
w T b X a > 0, a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) , w T b X A + > 0, −w T b X B + > 0,
where the first inequality is because Property (p5) implies that X a ∈H for a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) , and the last two inequalities are due to equation 75 and equation 76, respectively. This establishes equation 67, equation 68, and equation 70. In light of Claim 1, it is easy to see for r = 1, . . . , m and i = 1, . . . , n that if w T r X i = 0, then
v r e i = 1 √ m .(77)
In our next claim, we examine a linear combination of data points for which a particular neuron is active.
Claim 2. For r = 1, . . . , m, there exist constants γ r 1 , . . . , γ r m such that
n i=1 γ r i e i X i 1 w T r X i = 0 + n i=1 e i X i 1 w T r X i > 0 = 0. (78
)
Proof of Claim 2. Fix a b ∈ B. We prove the claim for the neuron associated to b. It follows from Claim 1 that
n i=1 e i X i 1 w T b X i > 0 = e b X b + a∈(A Ã )\ s1(b),...,s d−1 (b) e a X a + e A + X A + + e A − X A − = −X b + a∈(A Ã )\ s1(b),...,s d−1 (b) X a + X A + − X A − = −X b + a∈(A Ã )\ s1(b),...,s d−1 (b) X a −   a∈A Ã X a − (1/β − 1) u d   = −X b − d−1 i=1 X si(b) + (1/β − 1) u d ,(79)
where the second equality is due to the definitions of e a , e b , e A + , and e A − , and the third equality is from the definitions of X A + in equation 5.
On the other hand, it follows from Property (p6) that there exist scalars α 1 , . . . , α 
d ∈ (0, 1) such that b = d−1 i=1 α i s i (b). Therefore, from the definition of X b , − X b + u d = d−1 i=1 α i X si(b) − u d (80) Moreover, Property (p4) implies that d−1 i=1 α i = 1/β. Then, from equation 80, −X b = d−1 i=1 α i X si(b) − d−1 i=1 α i − 1 u d = d−1 i=1 α i X si(b) − (1/β − 1) u d . (81
) For i = 1, . . . , d − 1, let γ i = 1 − α i . Then, Claim 1 implies that n i=1 γ i e i X i 1 w T b X i = 0 = d−1 i=1 γ i e i X si(b) = d−1 i=1 (1 − α i )X si(b) = d−1 i=1 X si(b) − d−1 i=1 α i X si(b) = d−1 i=1 X si(b) + X b − 1 β − 1 u d ,(82)
r i , (1 − γ r i ) ,(83)
for the constant γ r i defined in Claim 2. It follows that r > 0, for r = 1, . . . , m. For any r ≤ m, X s1(r) , . . . , X s d−1 (r) are linearly independent and, by definition, are all orthogonal to w r . Therefore, there exists a constant r > 0 such that for any
ζ r ∈ R d with ζ r = 1, we have max i=1,...,d−1 |ζ T r X si(r) | ≥ r ζ ⊥ r , where ζ ⊥ r is the projection of ζ r on the null-space of w r . Consequently, for any ζ r ∈ R d with ζ r = 1, max i=1,...,n |ζ T r X i | × 1 w T r X i = 0 ≥ r ζ ⊥ r .
In particular, considering the block-vector representation of θ in equation 54, we obtain for r = 1, . . . , m, max i=1,...,n
|θ T r X i | × 1 w T r X i = 0 ≥ max i=1,...,n r θ ⊥ r .(84)
Let µ min r r r / √ m. Then, µ > 0. It then follows from Claim 2 that, for r = 1, . . . , m,
v r n i=1 e i θ T X i 1 w T r X i = 0, θ T r X i ≥ 0 + v r n i=1 e i θ T r X i 1 w T r X i > 0 = v r n i=1 e i θ T r X i 1 w T r X i = 0, θ T r X i ≥ 0 − v r n i=1 γ r i e i θ T r X i 1 w T r X i = 0 = v r n i=1 e i θ T r X i 1(θ T r X i ≥ 0) − γ r i 1 w T r X i = 0 = 1 √ m n i=1 θ T r X i 1(θ T r X i ≥ 0) − γ r i 1 w T r X i = 0 = 1 √ m n i=1 θ T r X i × 1(θ T r X i > 0) − γ r i × 1 w T r X i = 0 ≥ 1 √ m r n i=1 θ T r X i 1 w T r X i = 0 ≥ r √ m max i=1,...,n θ T r X i 1 w T r X i = 0 ≥ r √ m r θ ⊥ r ≥ µ θ ⊥ r ,(85)
where the third equality is due to equation 77, the fourth equality is because θ T r X i and 1(θ T r X i > 0) − γ r i have always the same sign, the first inequality is by definition of r in equation 83, the third inequality follows from equation 84, and the last inequality is from the definition of µ.
On the other hand, equation 2 implies that dF θ (t)
d + t t=0 = lim t↓0 m r=1 v r θ T r n i=1 e i X i 1 (w r + θ r t) T X i ≥ 0 = m r=1 v r n i=1 θ T r e i X i 1 w T r X i = 0, θ T r X i ≥ 0 + 1 w T r X i > 0 = m r=1 v r n i=1 e i θ T X i 1 w T r X i = 0, θ T r X i ≥ 0 + v r n i=1 e i θ T r X i 1 w T r X i > 0 ≥ m r=1 µ θ ⊥ r ≥ µ θ ⊥ (86)
where the second equality is due to equation 55 and the first inequality follows from equation 85. This completes the proof of Lemma 3.

[F PROOF OF LEMMA 4]
We begin by a claim. Given a q ∈ A B, recall the definition of hyperplane H q from Property (p7).
Claim 3. For any pair of points p, q ∈ A B, we have
w T q X p = 1 1 + d(0, H q ) 2 d p, H q . (87
)
where the third equality is due to equation 19. On the other hand, since θ = θ + θ ⊥ , we have
Gθ = Gθ + Gθ ⊥ .(96)
In the following claim, we elaborate on Gθ . Claim 4. There exists a constant η 1 > 0 such that Gθ 2 ≥ 2η 1 θ 2 , for all θ ∈ R d .
Proof of Claim 4. Recall that θ is the projection of θ on subspace H w defined in equation 14. Then, there exist constants α 1 . . . , α m such that
θ =    α 1 w 1 . . . α m w m    . (97
)
Let α be the vector representation of α 1 , . . . , α m . Then,
θ 2 = m i=1 α 2 i w i 2 = m i=1 α 2 i = α 2 . (98
)
Consider the n × m matrixG = v 1 J 1 X T w 1 • • • v m J m X T w m .(99)
Then, from the definition of matrix G in equation 18,
Gθ = v 1 J 1 X T • • • v m J m X T    α 1 w 1 . . . α m w m    = v 1 J 1 X T w 1 • • • v m J m X T w m α =Gα.(100)
Each column ofG corresponds to a neuron, and thereby is associated to a point in A B. In the same vein, every row ofG is associated to an input vector. By removing some rows ofG, we devise a matrixM so that each row ofM is associated to an input X p for p ∈ A B. Therefore,M is an m × m matrix, whose rows and columns are associated to the points in A B. It follows thatM α is a vector obtained by removing some entries from vectorGα. As a result, M α ≤ G α .
In the following, we capitalize on Property (p8) to show thatM is full-rank.
For q ∈ A B, let
γ q v q 1 + d(0, H q )
.
For p, q ∈ A B, the entry in row p and column q ofM equals
M pq =G pq = v q X T p w q 1 w T q X p > 0 + 1 w T q X p = 0, θ T q X p ≥ 0 = v q X T p w q 1 w T q X p > 0 = v q 1 + d(0, H q ) d p, H q 1 w T q X p > 0 = γ q d p, H q 1 w T q X p > 0 . (102
)
where the first equality is from the definition ofM , the second equality follows from the definitions ofG and J q in equation 99 and equation 93, the third equality is because X T p w q 1 w T q X p = 0 = 0, and the fourth equality is due to Claim 3. Then, Claim 1 implies that for any p, q ∈ A B, M pq = γ q d p, H q , if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A, 0, otherwise.
Compared to matrix M defined in equation 3, each column q ofM equals the column q of M multiplied by a non-zero constant γ q . In view of Property (p8), M is full-rank. It follows thatM is full-rank, as well.
Let σ be the smallest singular value ofM . SinceM is full-rank, we have σ > 0. Moreover, M α ≥ σ α .
Then, Gθ 2 = G α 2 ≥ M α 2 ≥ σ 2 α 2 = σ 2 θ 2 , (105) where the equations are due to equation 100, equation 101, equation 104, and equation 98, respectively. Claim 4 then follows for η 1 = σ 2 /2.
Back to the proof of Lemma 4, we denote by σ max the largest singular value of G. Let η 2 σ 2 max . Then,
Gθ 2 = Gθ + Gθ ⊥ 2 = Gθ 2 + Gθ ⊥ 2 + 2 Gθ T Gθ ⊥ ≥ Gθ 2 − 2 Gθ × Gθ ⊥ ≥ Gθ 2 − 2σ max θ × σ max θ ⊥ ≥ Gθ 2 − 2σ 2 max θ ⊥ = Gθ 2 − 2η 2 θ ⊥ ≥ 2η 1 θ 2 − 2η 2 θ ⊥ ,(106)
where the second inequality is from the definition of σ max , the third inequality is because θ ≤ θ = 1, the last equality is by the definition of η 2 , and the last inequality follows from Claim 4. This completes the proof of Lemma 4.

[G PROOF OF LEMMA 5]
Fix a θ ∈ R md with θ = 1. Recall the definition of x 0 from the paragraph proceeding equation 17. If θ ⊥ ≥ x 0 , then for any t ∈ [0, δ],
F θ (t) − F θ (0) ≥ F θ (0)t ≥ µ θ ⊥ t ≥ µx 0 t = δt ≥ t 2 ,
where the first inequality is from convexity of F θ in Lemma 2, the second inequality is due to Lemma 3, the equality is by the definition of in equation 17, and the last inequality is because t ≤ δ.
On the other hand, if θ ⊥ < x 0 , then for any t ∈ [0, δ], F θ (t) − F θ (0) = F θ (0)t + 1 2
F θ t 2 ≥ 1 2 F θ t 2 ≥ η 1 θ 2 − η 2 θ ⊥ t 2 = η 1 1 − θ ⊥ 2 − η 2 θ ⊥ t 2 < η 1 (1 − x 2 0 ) − η 2 x 0 t 2 = µx 0 δ t 2 = t 2 ,
where the first equality is because F θ is quadratic (c.f. Lemma 2), the first inequality follows from Lemma 3, the second inequality is due to Lemma 4, the third inequality is because θ ⊥ < x 0 , and the last two equalities are due to p(x 0 ) = 0 and the definition of in equation 17, respectively. Combining the above two cases, we obtain Lemma 5.

[]
cos(π/t) − cos(2π/t). It then follows from equation 41, equation 45, and equation 46 that b 0 and b i lie on opposite sides of H, for i = 1, . . . , t − 1.
On the other hand, sinceb i = −ã i , for i = 3, . . . , d, it follows from equation 39 that z Tbi = −z Tãi = −1 < 1. Therefore, for i = 1, . . . , 3,b i and b 0 lie on opposite sides of H. This completes the proof of Property (p7).
Property (p8): We have
where the first equality is due to equation 43 and equation 41, and the second equality follows from the equality in equation 45. Similarly, from equation 42, we have for i = 0, . . . , t − 1
For i, j ∈ {0, . . . , t−1}, letm a i ,b j cos 2π(i−j −1/2)/t − cos(π/t) andm b i ,a j cos 2π(i− j + 1/2)/t − cos(π/t). Then, it follows from equation 48 and rotational symmetry of A and B in the first two coordinates that for i, j ∈ {0, . . . , t − 1},
Note that for any p, q ∈ A B,m p,q is a constant independent of the value of γ. LetM be a 2t × 2t matrix, with entrieŝ
for p, q ∈ A B. Then, all entries ofM are constants independent of γ. Letλ 1 , . . . ,λ 2t be the eigenvalues ofm. It follows thatλ 1 , . . . ,λ 2t are also constants independent of γ.
Consider the matrix M defined in equation 3. It follows from equation 47 and equation 49 that for any p, q ∈ A B,
Consider the order a 0 , . . . , a t−1 , b 0 , . . . , b t−1 on the elements of A B. Then,
where I is the 2t×2t identity matrix and the second equality is from the definition of z 1 in equation 37.
Denote the eigenvalues of M by λ 1 , . . . , λ 2t . Then, from elementary linear algebra,
for i = 1, . . . , 2t. Therefore, there is at most one value of γ for which λ i = 0. Then, in view of equation 23, we have Pr(λ i = 0) = 0, over the random choice of γ. Thus, with probability one, M has no zero eigenvalues and is thereby full-rank. As an immediate consequence, M is full-rank for suitable choice of γ. This establishes Property (p8) and completes the proof of Proposition 1.
Proof of Claim 3. In the (d − 1)-dimensional space, let ω be the unit normal vector of H q . Recall from Property (p6) that s 1 (q), . . . , s d−1 (q) are located on H q . Let,
Without loss of generality suppose that q ∈ A. Letω be the lifting of ω from the (d − 1)-dimensional space to the d-dimensional space by appending ω by a new coordinate with zero coefficient, i.e.,ω is a d-dimensional vector withω
Then, we have z = 1 and z T u
where the second equality is because s i (q) ∈ B B for q ∈ A. It follows from equation 91 and the definition of w q in equation 7 and equation 8 that w q = z. Therefore,
where the second equality is from the definition of z in equation 90, and the last equality is due to equation 88 and equation 89. This completes the proof of Claim 3.
We now proceed to the proof of Lemma 4. Fix an arbitrary θ ∈ R d with θ = 1. Without loss of generality 4 assume that F (•, •) is differentiable at w + δθ/2, v . For r = 1, . . . , m, let J r be a diagonal matrix whose (i, i) entry, for i = 1, . . . , n, equals 1 (w r + θ r t)
where the equality is due to equation 55. Recall the definition of matrix G in equation 18:
Then, for any t ∈ (0, δ),","[TITLE]
BOUNDS ON OVER-PARAMETERIZATION FOR GUARANTEED EXISTENCE OF DESCENT PATHS IN SHALLOW RELU NETWORKS

[ABSTRACT]
We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let m and d be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size n ≥ m + 2d − 2. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for n ≤ m, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for ""existence of descent paths"" in the loss landscape.","{
    ""1"": {
        ""summary"": ""The reviewer raises concerns about the limitation of the theoretical analysis to two-layer ReLU networks and its applicability to more natural or practical settings."",
        ""verbatim"": ""The theory is limited to 2-layer ReLU. Can it be extended to deep networks? The datasets producing poor local minima seem quite artificial. Does this theory hold for a more natural setting (e.g., assume a true distribution or function having preferable properties)?""
    },
    ""2"": {
        ""summary"": ""The reviewer questions the robustness of Corollary 1 based on whether the sample sets leading to suboptimal capped minima being of non-zero measure have been rigorously proven."",
        ""verbatim"": ""As is stated in Corollary 1 and discussed below Theorem 1, the training sample sets that lead to suboptimal capped minima are not of measure-zero. However, it seems that no rigorous proof is provided for this result.""
    }
}",2,"{
    ""1"": {""summary"": ""No review content provided"", ""verbatim"": ""nan""}
}",1,{},0,"{
    ""1"": {""summary"": ""Review content is missing"", ""verbatim"": ""nan""}
}",1,{},0,"{
    ""1"": {
        ""summary"": ""Lack of empirical validation to support findings"",
        ""verbatim"": ""The absence of empirical validation or experiments showcasing the implications of the findings may reduce the paper's practical impact and applicability.""
    },
    ""2"": {
        ""summary"": ""Limited discussion on practical implications"",
        ""verbatim"": ""The paper could benefit from further discussion on how the identified conditions for descent paths impact real-world training scenarios and network performance.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper's assumptions might not align with common practical scenarios, potentially limiting its impact."",
        ""verbatim"": ""The paper's results heavily rely on specific assumptions about the network architecture and layer widths. These conditions might not be common in practical scenarios, which could limit the impact of the findings.""
    },
    ""2"": {
        ""summary"": ""The theoretical results are not well-connected with practical applications, questioning the paper's practical relevance."",
        ""verbatim"": ""The translation of the theoretical results to practical scenarios is not addressed thoroughly, which might raise concerns about the paper's practical relevance and utility.""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both points discuss limitations regarding the application and relevance of the theoretical results in practical, real-world settings. Review A questions the extension to deep networks and natural settings, while Review B concerns the thoroughness in connecting theory with practical scenarios."",
        ""similarity"": ""7""
    },
    ""A1-B1"": {
        ""rationale"": ""The concerns in both reviews are about the applicability and extension of the theoretical results, focusing on network architectures and assumptions. Review A explicitly mentions the limitation to two-layer ReLU and its extension, whereas Review B highlights the reliance on specific network assumptions that might not be common in practical scenarios."",
        ""similarity"": ""8""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Lack of empirical validation"",
        ""verbatim"": ""The paper is heavily theoretical and seems to lack substantial empirical validation of the proposed bounds.""
    },
    ""2"": {
        ""summary"": ""Limited scope focusing on a single ReLU layer"",
        ""verbatim"": ""The focus on a single layer with ReLU might limit the paper's appeal and applicability.""
    },
    ""3"": {
        ""summary"": ""Complex presentation may deter broader readership"",
        ""verbatim"": ""The concepts and derivations, while rigorous, might be overly complex for readers who are not deeply familiar with theoretical aspects of neural network landscapes.""
    }
}",3,"{
    ""A1-B2"": {
        ""rationale"": ""Both points address the limitation of applying the theoretical analysis only to simple structures of neural networks. Review A questions the applicability to deeper networks and more natural settings, while Review B questions its applicability due to the focus only on a single ReLU layer."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of comprehensive empirical validation"",
        ""verbatim"": ""The study, while strong on theoretical constructs, lacks comprehensive empirical validation.""
    },
    ""2"": {
        ""summary"": ""Limited applicability due to network architecture constraints"",
        ""verbatim"": ""Since the results specifically focus on networks with one hidden layer and ReLU activations, their applicability may be questioned in terms of broader neural network architectures.""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both Review A1 and Review B2 express concerns about the limitation of theoretical analysis to two-layer ReLU networks, questioning its broader applicability or extension to more complex or varied network architectures. Review A1 worries about the extension to deep networks and whether the theory applies in more natural settings, while Review B2 specifically speaks to the limited applicability due to the network architecture constraints that focus only on single hidden layer ReLU networks."",
        ""similarity"": ""8""
    }
}",1,"

Significance:
This paper provides a deep understanding of the concept of over-parameterization in neural networks with only one hidden layer. The authors identify existence of local minima with poor curvature in small training sets using the analysis of the loss landscape. The results of the study contributes to a better understanding of why good initialization schemes for neural networks are essential for to converge to global optima.

Potential reasons for acceptance:
The paper's findings provide new insights on the importance of proper initialization schemes in deep learning. The experimental design is a well-structured one, and the results are presented in a clear manner. The use of mathematical analysis to investigate the problem contributes to the understanding of over-parameterization in deep learning.

Potential reasons for rejection:
The experimental design is not comprehensive enough, as the study only investigate neural networks with only one hidden layer. There is need for extension to investigate the effect of over-parameterization in neural networks with more than one hidden layer.

Suggestions for improvement:
The authors may extend the study by investigating the effects of over-parameterization in neural networks with multiple hidden layers. Additionally, the authors may explore other neural architectures, such as recurrent or convolutional neural networks, to compare the behaviors of over-parameterization in various types of architectures.

",,"{
    ""1"": {
        ""summary"": ""The scope of the study is too limited as it only investigates neural networks with a single hidden layer."",
        ""verbatim"": ""The experimental design is not comprehensive enough, as the study only investigate neural networks with only one hidden layer.""
    },
    ""2"": {
        ""summary"": ""The study needs to include neural networks with multiple hidden layers for better generalizability of the findings."",
        ""verbatim"": ""There is need for extension to investigate the effect of over-parameterization in neural networks with more than one hidden layer.""
    }
}",2,"{
    ""1"": {""summary"": ""Review content is missing"", ""verbatim"": ""nan""}
}",1,"{
    ""A1-B1"": {
        ""rationale"": ""Both points criticize the limited scope of the neural network's configuration being studied, specifically the constraint to networks with only one or two layers."",
        ""similarity"": ""8""
    },
    ""A1-B2"": {
        ""rationale"": ""Both points express concerns about the need to extend the theoretical or experimental analysis to neural networks with multiple layers to improve the applicability or generalizability of the findings."",
        ""similarity"": ""7""
    }
}",2,{},0
The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets,BJlNs0VYPB.json,"[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Extended lottery ticket experiments using ResNet20 on CIFAR10. From the baseline ResNet20 training, weight-magnitude based pruning is applied at different epoch (columns, labeled as s) to obtain the sparsified structure. Each structure is then rewound to the weights from another epoch (rows, labeled as v), where v=0 indicates rewind to the initial weight. Lottery tickets emerge much earlier before the full training ends, achieving matching accuracy compared to the conventional winning ticket, i.e., (s=200,v=200).
Table 2: Frankleet al. (2019)  suggests that a lottery ticket found after a baseline training can achieve the accuracy of the baseline model if it is initialized with the weight of the baseline model after a few epochs, which is called rewinding. We extend this exploration toward different structures obtained at different epochs of the baseline training (via magnitude-based weight pruning).
Table 3: Fig. 11Fig.1shows the validation accuracy after retraining of a ResNet20 model on CIFAR10 pruned at different epochs of its baseline training. The same learning rate schedule of 0.1 reduced by 10x at epoch 120 and 160 is used for both the baseline training and retraining, and the total number of epochs is 200. The rows (= v) correspond to the different rewinding epoch, whereas the columns (= s) correspond to the different epoch that we apply one-shot pruning (pruning rate= 80%). For example, the validation accuracy along the lottery ticket configurations of (s = 200, v = 0 ∼ 200) resembles the phenomena of ""rewinding"" observed inFrankle et al. (2019).
Table 4: Figure 2 :2Figure 2: (a) Change of the standard deviation of weights (W std ) in each layer during the training of ResNet20 on CIFAR10. In the beginning of the training, the weights of different layers change in different rate, dominated by the gradient terms. When the training evolves, weight magnitude is primarily determined by the interplay between learning rate and weight decay, resulting in parallel movement of W std . (b) Change of per-layer pruning rate over the epochs. Due to regular shift of W std at the later epochs of training, the per-layer pruning rate converges to a saturating point, emerging stable structure for pruning.
Table 5: Figure 4 :4Figure4: The pseudo-pruned-then-retrained (PPR) models for CIFAR10 ResNet20, which exhibit increasing capability of memorizing the complex patterns over the epochs. Validation accuracy for the baseline training and validation accuracy after pruning without retraining are also included for comparison.
Table 6: Figure 5 :5Figure 5: Left: Memorization capacity (i.e., training accuracy) of the models pruned at 3 different epochs. The models pruned at epoch 120 and 200 show almost identical memorization capacity, whereas the model pruned at epoch 20 exhibits lower memorization capacity. Right: Memorization capacity of models pruned at different epochs (training data size = 15000).
Table 7: Figure 6 :6Figure 6: The results of PPR accuracy check (retraining learning rate=0.01) for (a) ResNet56 on CIFAR10 (sampled at every epoch) and (b) ResNet18 on ImageNet (sampled every 5 epochs).
Table 8: GP: gradual pruning, IP: iterative method, EWT: early winning ticket, WT: winning ticket * : pruning rate (PR). * * : number of pruning epochs + regular retraining epochs † : the delta of accuracy is measured against the baseline accuracy. Bold: highlight of comparison
Table 9: Figure 8 :8Figure 8: Revisiting the extended lottery ticket experiments of Fig. 1, but with the group size (gs) of 8. As before, (s, v) stands for (epoch drawing the sparse structure, epoch for rewinding). In contrast to Fig. 1, the winning tickets found when gs=1 (green color in Fig. 1) disappear as the group size becomes 8.
Table 10: Figure 9 :9Figure9: Impact of structured sparsity on the saturation of pruning accuracy. The x-axis corresponds to the epoch when the model is 80% pruned. The dotted lines correspond to the moving average of the accuracy to show trends. The larger the group size, the later the accuracy of the pruned models converges at.
Table 11: Figure 10 :10Figure 10: Mode connectivity: Lottery tickets of ImageNet-ResNet50 and CIFAR10-ResNet20 drawn from different epochs are linearly interpolated, then the accuracy is measured for each interpolation coefficient. It can be observed that only the early winning tickets are connected to the winning tickets (drawn at the end of training, i.e., epoch 90 for ImageNet and epoch 200 for CIFAR10).
Table 12: Memorization capacity (i.e., train accuracy) comparison between 1-shot and 4-shot pruning.
Table 13: One-shot and gradual pruning at different epochs for ResNet20 and ResNet56 on CIFAR10. To clarify randomness effect, the average accuracy (%) over 10 different runs is reported. ±0.07) 91.20 (±0.28) 91.49 (±0.27) 91.37 (±0.24) 91.63 (±0.19) 91.63 (±0.21) ResNet56 92.94 (±0.28) 93.60 (±0.18) 94.07 (±0.16) 94.24 (±0.19) 94.29 (±0.17) 94.34 (±0.14)
Table 14: One-shot and gradual pruning for ResNet18 and ResNet50 on ImageNet.
Table 15: Performance gain by our gradual pruning strategy on CIFAR-10 and ImageNet.
Table 16: Memorization capacity comparison for different group size.

[INTRODUCTION]
Deep Neural Networks (DNNs) achieve superior accuracy in a wide spectrum of applications through the use of very large and deep models (Goodfellow et al. (2016)). These high-capacity but complex models, however, pose a tremendous challenge for their deployment, particularly in resource-constrained edge environments. Over the years, many techniques have been developed to compress the models to a compact counterpart to alleviate the computational costs. Among these techniques, pruning less important parameters to obtain a compact sub-network has emerged to be a popular and efficient approach (Cun et al. (1990); Han et al. (2015)). In search of these sub-networks, new intuitions are also built up for understanding the DNN working mechanism, one example of which is the recently proposed ""lottery ticket hypothesis"" by Frankle & Carbin (2018).
The lottery ticket hypothesis states that, once a sub-network is found to match the accuracy of the original neural network, the sub-network (i.e., lottery ticket) together with its initialized weights can be trained in isolation and still achieve accuracy comparable to the original network within a similar number of iterations. This conjecture intrigues discussions on a series of topics, such as the importance of initialization scheme (Liu et al. (2018), Zhou et al. (2019)), the role of over-parameterization in training (Frankle et al. (2019)) and even the transferability of the ""winning ticket"" (S. Morcos et al. (2019)). However, all these discussions start from the point that the winning ticket has been obtained after painfully long iterative pruning procedures, which often take up to thousands of epochs. When and how a winning ticket can be found in the course of pruning procedures has not been studied; most of the prior works use a traditional way of repeated cycles of pruning and retraining, which makes such study less practical.
In this work, we provide insights to find the winning tickets early. We start with an interesting observation that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule finishes. This leads to our in-depth investigation of the weight magnitude; we discover that a model saturates early but not too early under stochastic gradient descent (SGD) optimization. To understand this characteristic of the winning lottery tickets, we conjecture that pruning a premature model causes the loss of capability in learning complex patterns, leading to accuracy degradation. We confirm this conjecture with the empirical evidence as well as the quantitative analysis based on the memorization capacity. Using this analysis framework, we further provide a reasoning behind the success of the gradual pruning over the one-shot pruning. Based on these insights, we identify the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate obtained within only 22% of the total epochs for iterative pruning. This promising outcome not only sheds light on understanding the optimization behavior of the pruned models but also enables performance gain for fast training of the pruned models.

[RELATED WORK]
The lottery ticket hypothesis was first proposed in Frankle & Carbin (2018) where the presence of a trainable subnetwork that achieves compelling final accuracy with the inherited initial values for the un-pruned connections is demonstrated. This paper also argued that the initial weight of the original network is essential for maintaining good accuracy when the model is sparsified. This claim has been extended to the over-parameterized neural networks on the larger datasets in Frankle et al. (2019) with the notion of ""rewinding""; the authors claim that rewinding of the weights not to the initial values but to the values after a few epochs can stabilize the accuracy of the winning lottery tickets. As a follow-up work, Zhou et al. (2019) studied the critical components of the lottery tickets such as zeros, signs and the super-mask. Also, S. Morcos et al. (2019) investigated the transferability of winning tickets obtained in one dataset to the network of a similar structure for the other datasets. But none of these focused on the structure of the winning lottery tickets; it involves several repetitions of the full training schedule for iterative pruning and retraining, often taking hundreds to thousands of epochs. In this work, we demonstrate that many winning tickets can be found in the early stage of the baseline training schedule, drastically reducing the computational effort to obtain them.
Most work on the lottery ticket hypothesis, including this work, rely on magnitude-based weight pruning for identifying unimportant weights to be pruned (usually via global sorting). Such an intuitive method was first proposed in Han et al. (2015) and became popular. Afterwards, more complex pruning methodologies have been presented to improve pruning performance, such as having different pruning criteria , Wen et al. (2016), ) or different pruning granularity (Mao et al. (2017), Molchanov et al. (2019)). While these attempts offer insights on training pruned models, there is little discussion about the interesting interplay between the pruning criteria and the structure of a model pruned by it. In this work, we reveal that the pruned structure obtained based on the weight magnitude has tangible impact on the final accuracy, and further propose a way to quantitatively distinguish good structures for pruning.
There have been various strategies to apply pruning to the neural networks. Iterative pruning by Han et al. (2015) involves several repetition of pruning (with gradual increase of the pruning rate) and retraining. One can increase the repetition cycles arbitrarily large to achieve good accuracy at high pruning rate; Frankle & Carbin (2018) employed iterative pruning with hundreds to thousands of pruning epochs to match the baseline accuracy for the challenging neural networks. On the other hand, gradual pruning introduced by Zhu & Gupta (2017) determines the pruning rate and frequency via a polynomial equation as a function of the starting and ending epochs as well as the target pruning rate. Although it provides a systematic pruning schedule, there is lack of discussion about when to start the gradual pruning. Lastly, Lee et al. (2018) proposes a method of pruning weights at initialization. This method takes most advantage in performance since a pruned model can be obtained without any expensive retraining procedures. However, its effectiveness has not been demonstrated for the challenging neural networks on large datasets such as ImageNet. In contrast, we propose a mechanism to identify the winning lottery ticket early in the course of baseline training so that we can avoid costly iterative pruning while maintaining the baseline accuracy.

[STRUCTURE OF EARLY WINNING TICKETS]
In this section, we extend the lottery ticket hypothesis by Frankle & Carbin (2018); Frankle et al. (2019) to discuss the early winning tickets. The lottery ticket hypothesis can be summarized as: for a given network of f (x; w 0 m 0 ) with the initial weight w 0 and the mask of all ones m 0 , there exists a winning lottery ticket m f where |m f |/|m 0 | = 1 − p% (p is the pruning rate) and training of f (x; w v m f ) for 0 < v f achieves test accuracy comparable to the baseline f (x; w f m 0 ).
There are two main components of a winning lottery ticket: the sparsified structure m f and the weight that initializes it, w v . In the previous work, m f has been obtained only after expensive iterative pruning. We characterize the structure of the early lottery ticket m s where s f , then propose a strategy for finding it early.  Interestingly, Fig. 1 further demonstrates that the winning tickets emerge at much earlier epochs of the baseline training; the lottery ticket configurations of (s ≥ 100, v ≥ 5) achieve almost the same accuracy as the accuracy of (s = 200, v = 200). The accuracy then gradually decreases as s ≤ 80. This result implies two important aspects: 1) a winning ticket can be found in the middle of the baseline training so that one can avoid expensive iterative pruning used in the prior work, and 2) the winning ticket, however, does not emerge arbitrarily early in the process of training. In the following sections, we investigate the characteristics of these early winning tickets. In particular, we focus on their structure, as the weight initialization is not the major factor provided a proper rewinding.

[ANALYSIS ON WEIGHT MAGNITUDE]
As the first step of understanding the characteristics of the early winning tickets, we focus on the important quantity of pruning, weight magnitude. At pruning, we determine the structure of the sparsified model based on the rank of the weight magnitude (via global sorting). Therefore, the change in the weight magnitude during training has large impact on the lottery ticket structure.
In Fig. 2a, we measure the standard deviation of weight (W std ) for each layer in the course of training ResNet20 on CIFAR10 (note: the mean of weight is typically near zero). The same learning rate schedule is used as above. The first thing to note is that the change in W std has strong correlation with the learning rate change. In particular, different layers show different rate of change in W std when the learning rate is 0.1, but from the second learning rate (after epoch 120), all the weights follow a very similar decreasing trend.
This trend in W std can be understood via steps of stochastic gradient descent. From the typical setting of weight update with momentum and weight decay, we have:
v t+1 = mv t + (λw t + w g,t ), w t+1 = w t − ηv t+1 . (1
)
where w t , w g,t and v t are the weight, gradient and momentum at step t, respectively; m is the momentum factor, η is the learning rate and λ is the weight decay factor. After n steps,
v t+n = m n v t + n k=1 (m n−k w g,t+k−1 ) + λ n k=1 (m n−k w t+k−1 ),(2)
w t+n = (1 − ηλ) n w t − η n k=1 ((1 − ηλ) n−k (mv t+k−1 + w g,t+k−1 )).(3)
From this derivation, we can see that the two factors determine the mode of change in W std . When learning rate is high, the gradient terms play the major role in weight update. On the other hand, if the gradient activity becomes low, e.g., when the learning rate is low and the gradients oscillate around zero, we can further simplify Eq. 3. Assume that m n v t approaches to zero when n is relatively large, w t+k−1 ≈ w t , and
n k=1 ((1 − ηλ) n−k w g,t+k−1 )
and n k=1 (m n−k w g,t+k−1 ) approach to zero as the gradients oscillate around zero, we have v t+i ≈ λwt 1−m . Then w t+n is approximated as,
w t+n ≈ (1 − ηλ) n w t − η n−1 i=0 ((1 − ηλ) i mλw t 1 − m ). (4
) Note that (1 − ηλ) n ≈ 1 − nηλ since ηλ 1 and n−1 i=0 ((1 − ηλ) i ) ≈ n. Thus, w t+n ≈ (1 − nηλ)w t − nηλmw t 1 − m = (1 − nηλ 1 − m )w t .(5)
In other words, when the gradient activity is low, the change in W std is dominated by the both the learning rate and the weight decay. As an example, in Fig. 2a, W std is decreased with the slope following Eq. 5 when learning rate is low; W std of layer 10 is decreased by 1.5e-3 in 782 updates of CIFAR10 (with λ =1e-4 and m = 0.9), confirming the slope from Eq. 5.
These two modes governing the change of weight magnitude are critical for understanding the behavior of pruning.
In particular, the interplay between learning rate and weight decay causes the per-layer pruning rate to converge after enough number of epochs. Fig. 2b shows the pseudo per-layer pruning rate, where we just measure the layer-wise pruning rate without really pruning out the weights in the model, for the same CIFAR10 training experiment. As the figure shows, the per-layer pruning rates saturate around epoch 100, indicating that pruning before that would select the weights based on the premature model.
Moreover, although the weight magnitude is an important factor, we discovered that the ranking of individual weight does not play a critical role in winning lottery tickets. Specifically, we empirically show that there exist many winning lottery tickets that are vastly different in terms of its sparse structure m v . Fig. 3 shows the hamming distance of the sparse structure of the lottery tickets at different configuration (s, v). Note that the two distant lottery tickets (e.g., (s = 200, v = 100) and (s = 100, v = 100)) show large hamming distance of 0.159 (where x/y = 200/100) while achieving the equally good accuracy as shown in Fig. 1, indicating that the ranking of the weights itself can not explain the quality of the structure of the lottery ticket.
Figure 3: The hamming distance of the sparse structure of the lottery tickets at different configuration for ResNet20 on CIFAR10.
x/y denotes the distance between (s = x) and (s = y). A large distance between two lottery tickets with equally good accuracy suggests the existence of many winning tickets.

[UNDERSTANDING IMPACT OF PRUNED STRUCTURE]
The weight magnitude analysis motivates us not to prune a model too early. It is also implied that a distance-based metric might not reveal the winning structure of the model at different epochs. To understand the early winning structures, we further investigate the impact of the sparsified structure on the final accuracy. Considering that a highly pruned network is likely to have limited learning capability, we make a conjecture that the accuracy degradation of a pruned model is due to the loss of capability for learning complex pattern if pruned too early. Recently, Li et al. (2019) reveals that the training behavior of a sufficiently over-parameterized model with non-linearity highly depends on the learning rate schedule, where a model tends to memorize the complex patterns when a small learning rate is applied while learning simple patterns with a large learning rate. To validate this claim in the context of pruning, we construct an experiment where a model is pseudo-pruned at every epoch of the baseline training then retrained with large or small learning rate for just one epoch. For the pseudo-pruned-then-retrained (PPR) model at each baseline epoch, we measure the validation accuracy recovered from the retraining.
Fig. 4 shows the result of this experiment on CIFAR10 ResNet20. When it is retrained with the large retraining learning rate (= 0.1), in just 1-epoch retraining, the PPR models from all the baseline epochs achieve the accuracy matching with the baseline accuracy. This indicates that those pruned models maintain the capability of learning the simple patterns. Whereas, when the small retraining learning rate (= 0.01) is used, the accuracy of the PPR models pruned at later epochs (epoch 100-200) is higher than the accuracy of the models from earlier epochs (epoch 20-60). This reveals that the PPR models from different epoch exhibit varying capability of learning complex patterns. In particular, the accuracy of the PPR models increases until around 100 epoch of the baseline training, then it saturates. Note that this coincides with the epoch when the early winning tickets emerges in Fig. 1.
Based on this observation, we hypothesize that the models pruned at 100 epoch of the baseline training or later will preserve the capability of learning complex patterns. To validate this hypothesis, we conducted the memorization test proposed by Boo et al. (2019), where a model is trained with training data of varying size with the randomized labels.  A model has ""high memorization capacity"" if it achieves high training accuracy for a large data size. Fig. 5 (left) shows the training accuracy of the models pruned at different epochs, measured from 10 independent simulations (the average capacity shown as solid lines, and min and max shown as the shaded regions). As the training data size increases, the memorization capacity decreases. The model pruned too early (i.e., at epoch 20) suffers higher degradation in the memorization capacity compared to the models pruned at later epochs (i.e., at epochs 120 or 200). Also, Fig. 5 (right) shows that those models pruned at later than 100 epochs (i.e., the early winning tickets) exhibit similar memorization capacity. This result not only confirms our conjecture on the impact of pruned structure to the capability of learning complex patterns, but also leads us to employ the PPR accuracy check as a computationally reasonable heuristic to discover the early winning tickets.

[UNDERSTANDING GRADUAL PRUNING]
Gradual pruning is a popular pruning approach that applies pruning gradually over a period of training. For example, gradual pruning proposed by Zhu & Gupta (2017) provides a systematic way to schedule iterative pruning as follows:
s t = s f + (s i − s f ) 1 − t − t 0 n∆t 3 ,(6)
where s f and s i are the final and initial sparsity, and t is the time when pruning is applied. Eq. 6 determines how much sparsity is applied at a certain time step t. But it is still a user-hyper-parameter to decide when t 0 or how often ∆t apply pruning. Based on the insights we discussed in the previous section, we explain why gradual pruning helps to obtain better lottery tickets.
The reasoning behind the gradual pruning is that the model can be changed graciously if the pruning is applied gradually.
In terms of the early winning ticket analysis, there are two factors playing the critical role: 1) by applying low pruning rate in the beginning, the structure found at that pruning level can preserve the memorization capacity better, 2) once pruning is applied, the remaining weights of the pruned model is updated via SGD, granting a chance for the pruned model to adopt its weights toward better accuracy. Thanks to these two factors, a structure with better memorization capacity can be found when the increased pruning rate is applied next time.
Table 1 confirms this explanation using the CIFAR10 ResNet20 example. In this experiment, we perform the memorization test for the 4-step gradual pruning as well as four 1-step pruning at the corresponding epoch for comparison. The gap in memorization capacity is maintained across the different epochs, demonstrating that the memorization capacity is maintained thanks to the gradual application of pruning and the evolution of weights after pruning. This suggests a strategy for gradual pruning where 1) we can use gradual pruning to reduce the loss of capability in learning complex patterns, and 2) by applying a smaller pruning rate in the beginning, we can start pruning early and finding the winning lottery tickets faster. The benefit of this strategy combining gradual pruning with the early winning tickets will be demonstrated in Sec. 4. In this section, we demonstrate our strategy of finding the early winning tickets over popular neural networks on CIFAR10 and ImageNet. The detail experimental setup is described in Appendix A. We perform the lottery ticket experiments of Sec. 3.1 for both one-shot and gradual pruning with the pruning rate of 80%.We also conduct the PPR accuracy check of Sec. 3.3 to predict from which epoch the early winning ticket can be found. By comparing the two results, we demonstrate that the proposed heuristic for finding early winning tickets works robustly across the networks and the datasets. Furthermore, we showcase our gradual pruning strategy by comparing the performance in terms of the accuracy and the required pruning epochs with the existing lottery ticket approaches.

[EXPERIMENTS ON CIFAR10]
Table 2 summarizes the lottery ticket experiments of ResNet20 and ResNet56 on CIFAR10 dataset. In case of one-shot pruning, the winning tickets can be found from epoch 100. In case of gradual pruning, the gradual pruning schedule can start from epoch 75 (which is earlier than the one-shot pruning). Note that the gradual pruning can achieve better accuracy as it can preserve higher memorization capacity as discussed in Sec. 3.4. Fig. 6a shows the pseudo-pruning curve that highlights the presence of the winning tickets from the epoch around 100, which is consistent with the results of the lottery ticket experiment in Table 2. 

[EXPERIMENTS ON IMAGENET]
Table 3 summarizes the lottery ticket experiments on ImageNet dataset, and the predicted results of the early winning tickets from the pseudo-pruning are shown in Fig. 6b. Similar to the CIFAR10 experiments, the results of the lottery ticket experiment matches with the results from the PPR accuracy check (which indicates epoch 45 for early winning tickets), demonstrating the robust behavior of the proposed strategy of finding the early winning tickets. Furthermore, the gradual pruning at the early winning tickets achieves the accuracy near to the baseline (ResNet18: baseline=69.7% vs ours=69.24%, ResNet50: baseline=75.7% vs ours=75.31%), showcasing the superior quality of the winning tickets discovered by the proposed gradual pruning strategy.  

[PERFORMANCE GAIN FROM EARLY WINNING TICKET]
To demonstrate the performance gain from our gradual pruning strategy, the pruning results of the proposed algorithm (GP + EWT) and the previous implementation of iterative pruning (IP) along with the winning ticket (WT) of Frankle & Carbin (2018) are shown for the ResNet variants on CIFAR10 and ImageNet in Table 4. Our GP+EWT algorithm consistently achieves high pruning rate with the number of pruning epochs even lower than the regular retraining epochs for all the models, i.e., 80% pruning with negligible accuracy degradation on ResNet50 for ImageNet. In contrast, the iterative pruning approach (IP+WT) achieves the similar accuracy at the cost of more than 4.5× increase in the total training epochs.

[CONCLUSION]
In this work, we investigate the structure of the winning lottery ticket, which leads to the computationally efficient discovery of the winning lottery tickets. Based on a careful analysis of the characteristics of the structure of the winning lottery tickets, we proposed a computationally reasonable heuristic to identify when the early lottery tickets emerge. Furthermore, we proposed a gradual pruning strategy incorporating the early lottery ticket analysis to achieve high accuracy at large pruning rate. This results in the state-of-the-art accuracy on ResNet50 for 80% pruning only within 22% of the total epochs for iterative pruning.   We try to find the winning tickets early for ResNet20 (baseline accuracy: 92.5%) and ResNet56 (baseline accuracy: 94.25%) on CIFAR10 dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 200 epochs. The initial learning rate for the first 120 epochs is 0.1 and decrease to 0.1× every 40 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.

[A.2 EXPERIMENTS ON IMAGENET]
We further try to find the winning tickets early for ResNet18 (baseline accuracy: 69.7%) and ResNet50 (baseline accuracy: 75.7%) on ImageNet dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 90 epochs. The initial learning rate for the first 30 epochs is 0.1 and decrease to 0.1× every 30 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.
Figure 7: Structured pruning with variable group size. We consider a group of weights along the channel dimension with a varying group-size (gs). In case of gs=1, it is the same as the element-wise pruning.

[B.1 IMPACT OF STRUCTURED SPARSITY ON LOTTERY TICKET HYPOTHESIS]
In this section, we expand our scope of analysis on the characteristics of the early winning tickets toward the sparsity obtained via structured pruning. Structured pruning is a popular method that prunes weights by a group. For example,  proposes channel-pruning, where a group consists of weights along each channel. The main motivation of looking at the structured sparsity is that there is a significant discrepancy in the best pruning rate we can achieve between the element-wise pruning (i.e., group-size=1) and the structured pruning (i.e., group-size=channel). For example, on CIFAR10-VGG19, the element-wise pruning achieves 95% of pruning rate whereas the channel-pruning achieves 70% of pruning rate for the same level of accuracy (Liu et al. (2018)). Although such discrepancy has been observed for quite a while, there is no in-depth investigation to understand why. To find out the reasoning behind it, we analyze the impact of structured sparsity with varying group sizes in the context of the lottery ticket hypothesis, as illustrated in Fig. 7.
First, we revisit the extended lottery ticket experiments of Fig. 1 while applying group sparsity along the channel dimension (instead of pruning individual weights). Fig. 8 shows the results when the group size (gs) is 8. Comparing  it with Fig. 1, there are two notable points; 1) there is > 1% accuracy degradation when the group size is increased from 1 to 8, and 2) when gs = 8, the trend that the tickets drawn at epochs s > 100 achieve the higher accuracy disappears (as highlighted with a red rectangle in the figure). (Note that the two figures achieve the similar accuracy when s < 60 (colored in yellow), but the tickets with high accuracy (colored in green) can only be seen in Fig. 1.) The former observation makes sense as the more regularization on the sparse weight structure would result in the lower accuracy. But the latter observation is quite surprising; it seems that a distinct behavior of the lottery ticket hypothesis (i.e., the opportunity of finding winning tickets given a proper initialization) is disrupted as a group structure is asserted on the sparsity.
We can find a clue on this disrupted behavior by employing the memorization capacity analysis. Table 5 shows the results of memorization capacity experiments on CIFAR10-ResNet20 with an increasing group size gs = {1, 2, 4, 8, 16, 32, 64}.
The memorization capacity degrades significantly as the group size increases. Intuitively, by asserting a large group size, the model's expressivity is degraded and it becomes harder to learn the complex patterns, leading to accuracy loss. Note that we had a similar observation when the tickets are drawn too early (cf.,s < 60 in Fig. 1). From this, we can hypothesize that the winning tickets disappear as the memorization capacity is degraded, which can be observed when the tickets are drawn too early or the group sparsity is forced.
Fig. 9 further demonstrates the impact of structured sparsity on the saturation of pruning accuracy. For different group sizes, the figure shows the accuracy of CIFAR10-ResNet20 80% pruned at varying epochs. It can be observed that the models with the larger group size not only achieve the lower accuracy but also converge at earlier epochs. (E.g., the knee points for gs = 1 and gs = 16 are around 90 and 50 epochs, respectively.) This experimental result supports our claim that the larger group size enforced in the sparse structure results in the earlier convergence of the pruned models that misses the winning tickets. 

[B.2 MODE CONNECTIVITY OF EARLY WINNING TICKETS]
Mode connectivity is a phenomenon that SGD solutions are connected through paths of approximately equal loss (Draxler et al. (2018)). It provides the perspective of how well the models trained via a proposed method can generalize.
In the context of early winning tickets, we for the first time reveal that the lottery tickets drawn early based on PPR are indeed connected from the one obtained by IMP, whereas the tickets chosen too early do not.
For the mode connectivity experiments, we draw three lottery tickets -one drawn too early (i.e., a premature ticket, PT), one drawn based on PPR (i.e., an early winning ticket, EWT), and one at the end of the training schedule (i.e., a winning ticket, WT). Note that all the tickets are trained from the same initial weights and with the same retraining schedule.
Then we linearly interpolate PT and EWT with WT, and plot the test error for different interpolation coefficient (from 0 to 1). Fig. 10 shows the results of the mode connectivity experiments for ImageNet and CIFAR10. For both datasets, the linearly interpolated models between PT and WT depict a barrier, indicating that PT and WT are disconnected. Whereas, the linearly interpolated models between EWT and WT show consistently low test error. Note that such linear mode connectivity is observed even when 1) the weights are sparse (i.e., 80% pruning rate) and 2) the mask distance is large (cf., Fig. 3). This intriguing observation counters some observations of prior work; Draxler et al. (2018) claims that it takes a careful search (instead of a simple linear interpolation) to find a pass connecting the modes, and the existence of such a non-linear pass requires enough number of parameters. Our novel observation of early lottery tickets provide a new aspect of sparse model training; the sparse structures mature early in the training, and they converge to a flat local minimum even if its shape is vastly distant. A deeper investigation about the sparse structure of the lottery tickets and the mode connectivity is an interesting future research topic.

[C EARLY WINNING TICKET ALGORITHMS]
In this section, we describe the detail algorithms for finding early winning tickets via one-shot and gradual pruning.

[]
Algorithm 1: Early Winning Ticket Identification with One-Shot Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then Set the model parameters to θ i • m P P R,t ; break; end t++; end end
Step 2: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end Algorithm 2: Early Winning Ticket Identification with Gradual Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R , gradual pruning period T gr ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then break; end t++; end end
Step 2: Gradual Pruning to refine the EWT structure for i = 0; i < T gr ; i++ do Prune model gradually toward the pruning ratio p according to Eq. (6) and create a mask m gr,i ; Train the pruned model θ gr,i • m gr,i for one epoch; end
Step 3: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end","[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning.","{
    ""1"": {
        ""summary"": ""Lack of originality and significance, insufficient exploration of the structure of sparse subnetworks mentioned in the title."",
        ""verbatim"": ""However, my main issue is with both the originality and significance of this work. Although the paper is interested in addressing the structure of the winning tickets, I really didn't find any of the discussion of structure to give much insight into the lottery ticket hypothesis.""
    },
    ""2"": {
        ""summary"": ""Need for more convincing analysis and demonstration of the robustness of findings, specifically regarding the randomness in early winning tickets and the reliability of mask distance as an indicator."",
        ""verbatim"": ""For lottery tickets, especially for early winning tickets, I think there is a lot of randomnesses. Thus, for the plot like figure 2, figure 5, they need to contain an error bar and the curve should be the average of tens of experiments. Need more convincing analysis about the indicator - Hamming Distance""
    },
    ""3"": {
        ""summary"": ""Concerns around practical relevance of the findings and lack of comprehensive experimental data to support the early pruning hypothesis."",
        ""verbatim"": ""But how valuable is it for practical use? In practice, we cannot know in advance when to start (gradual) pruning. I am not quite convinced by the experiment of Fig. 4 and argument at the bottom half of page 5.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The paper needs clearer explanation of the significance and implications of experimental results."",
        ""verbatim"": ""The paper could benefit from further clarity in explaining the significance of individual experimental results and their implications on the broader understanding of lottery tickets.""
    },
    ""2"": {
        ""summary"": ""The description of the proposed algorithms should be more detailed."",
        ""verbatim"": ""The explanation of the proposed algorithms for early winning ticket identification could be more detailed, providing clearer steps and potential limitations.""
    },
    ""3"": {
        ""summary"": ""The paper should include deeper discussion on theoretical aspects."",
        ""verbatim"": ""While the analysis on weight magnitude and the impact of pruned structure is insightful, a deeper discussion on the theoretical underpinnings and implications of these findings could strengthen the paper.""
    },
    ""4"": {
        ""summary"": ""Discussion on practical challenges and generalization is needed."",
        ""verbatim"": ""The paper could consider discussing potential practical challenges or scenarios where the proposed early winning ticket strategy may not generalize effectively to diverse neural network architectures or datasets.""
    }
}",4,"{
    ""A1-B3"": {
        ""rationale"": ""Both Review A1 and Review B3 express concerns about the depth and insight provided by the paper. A1 criticizes the lack of originality and significance, noting that the discussion on the structure doesn't offer much insight into the lottery ticket hypothesis. B3 similarly suggests a need for deeper theoretical discussion to enhance understanding, implying that more insightful discussion is needed."",
        ""similarity"": ""7""
    },
    ""A3-B4"": {
        ""rationale"": ""Review A3 questions the practical relevance and generalization of the findings, specifically highlighting doubts about the early pruning hypothesis and its practical application. Review B4 raises concerns about practical challenges and the generalization of the proposed strategy across different scenarios. Both reviews critique how practical and generally applicable the concepts presented are, making them strongly related."",
        ""similarity"": ""8""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks a detailed comparison with existing methodologies for identifying lottery tickets."",
        ""verbatim"": ""The paper could benefit from providing a more detailed comparison with existing methodologies for identifying lottery tickets to strengthen the justification for the proposed approach.""
    },
    ""2"": {
        ""summary"": ""The paper does not sufficiently discuss the limitations or potential drawbacks of early winning lottery tickets."",
        ""verbatim"": ""While the study emphasizes the significance of early winning lottery tickets, it would be helpful to include a discussion on the limitations or potential drawbacks of this approach compared to traditional iterative pruning methods.""
    },
    ""3"": {
        ""summary"": ""The impact of weight magnitude saturation on the emergence of lottery tickets is not adequately explained."",
        ""verbatim"": ""The explanation of the impact of weight magnitude saturation on the emergence of lottery tickets could be further elaborated to enhance the clarity and depth of the findings.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both points critique the lack of comparison to existing methods or theories related to the lottery ticket hypothesis. Review A emphasizes the lack of originality and the insignificant exploration of structure, while Review B suggests the need for a detailed comparison with existing identification methodologies to justify the proposed approach."",
        ""similarity"": ""7""
    },
    ""A2-B2"": {
        ""rationale"": ""Both Review A and Review B express concerns regarding the approach to early winning lottery tickets. Review A calls for more convincing analysis and robustness in the findings, particularly regarding the reliability of the mask distance, while Review B suggests going deeper into the discussion and including potential drawbacks of early winning tickets."",
        ""similarity"": ""8""
    },
    ""A3-B2"": {
        ""rationale"": ""Review A and Review B both question the practical utility and reliability of early pruning hypotheses and winning lottery tickets, respectively. Although both are focused on different aspects — Review A on the experimental data supporting hypothesis and Review B on discussing limitations and potential drawbacks — they echo similar concerns about the practical implications."",
        ""similarity"": ""6""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Lack of comparative analysis with existing methods."",
        ""verbatim"": ""The paper might benefit from including a more in-depth comparison with existing methods or approaches in sparsified model training to better demonstrate the superiority or uniqueness of the proposed early winning lottery ticket concept.""
    },
    ""2"": {
        ""summary"": ""Need for more extensive experimental validation across various architectures and datasets."",
        ""verbatim"": ""While the findings are promising, there is a need for further experimental validation on a broader range of architectures and datasets to ensure the generalizability and robustness of the proposed early winning ticket approach.""
    }
}",2,"{
    ""A3-B2"": {
        ""rationale"": ""Both Review A3 and Review B2 express concerns about the practical relevance and robustness of the findings. Review A3 questions the practicality and veracity of the early pruning hypothesis, specifically citing insufficient data in the experiments presented. Review B2 stresses the necessity for additional experimental validation across diverse architectures and datasets to ascertain the generalizability and robustness of the early winning ticket approach. The core issue in both points revolves around the need for more comprehensive experimental evidence to substantiate the findings."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks sufficient theoretical analysis to explain why early winning tickets perform comparably to their traditionally found counterparts."",
        ""verbatim"": ""While the paper is strong on empirical results, it might be seen as lacking sufficient theoretical analysis to fully explain why early winning tickets perform comparably to their traditionally found counterparts.""
    },
    ""2"": {
        ""summary"": ""Concerns are raised regarding the generalizability of the findings to other network architectures or unusual datasets."",
        ""verbatim"": ""The paper focuses mainly on ResNet architectures and standard datasets like CIFAR-10 and ImageNet. Concerns might be raised regarding the generalizability of the findings to other network architectures or less conventional data.""
    },
    ""3"": {
        ""summary"": ""The paper's comparisons with previous methods could be more extensive, especially including recent pruning techniques or other network optimizations."",
        ""verbatim"": ""The comparisons with previous methods could be expanded to include more recent pruning techniques or other forms of network optimization, providing a clearer picture of where this new approach stands relative to the current state-of-the-art.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both reviews express concerns about the depth and nature of analysis in the paper. Review A highlights the lack of originality and depth in exploring the structure of winning tickets, specifically criticizing the discussion's inability to provide insight into the lottery ticket hypothesis. Review B points to the paper's lack of sufficient theoretical analysis to explain the performance of early winning tickets compared to traditionally found ones. Both are addressing the need for deeper, more insightful analysis on how winning tickets function."",
        ""similarity"": ""7""
    },
    ""A3-B2"": {
        ""rationale"": ""Review A raises concerns about the practical applicability and relevance of the findings, particularly criticizing the experimental support for early pruning hypotheses. Review B expresses worries about the generalizability of the findings to different network architectures or datasets, suggesting limitations in the application of the results. Both are concerned with how broadly the paper's conclusions can be applied in practical, varied settings."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Lack of a robust theoretical framework to explain early winning tickets"",
        ""verbatim"": ""'Lack of Theoretical Analysis': While the empirical results are promising, the paper lacks a robust theoretical framework explaining why early winning tickets emerge and function effectively.""
    },
    ""2"": {
        ""summary"": ""Limited generalization due to focus on specific architectures and datasets"",
        ""verbatim"": ""'Generalization of Findings': The focus on specific architectures and datasets might limit the generalization of the findings. It is unclear whether the early winning tickets would similarly benefit other types of networks or more varied tasks and conditions.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both points criticize the work for not providing a robust and insightful analysis regarding early winning tickets. Review A focuses on the lack of significant insight into the lottery ticket hypothesis when discussing structure, while Review B emphasizes the absence of a theoretical framework explaining the emergence and effectiveness of early winning tickets."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks a strong theoretical basis explaining the observed phenomena."",
        ""verbatim"": ""The paper, while rich in empirical results, could face criticism for a lack of a strong theoretical grounding explaining why early winning tickets behave as observed.""
    },
    ""2"": {
        ""summary"": ""There are concerns about the generalizability of the findings across different architectures or datasets."",
        ""verbatim"": ""There might be concerns about the generalizability of the early winning ticket findings across different architectures or datasets outside those specifically tested.""
    }
}",2,{},0,"[Significance and novelty]

1. The paper investigates an under-explored aspect of the winning ticket hypothesis: the structure of the sparsified model.
2. The authors reveal that there exist early winning tickets which achieve high accuracy before the regular training schedule finishes, improving our understanding of the lottery ticket hypothesis.
3. They provide novel insights into the emergence of lottery tickets under stochastic gradient descent and the importance of model capacity for early winning tickets.
4. The paper also proposes a new approach to discover early winning tickets through memorization capacity analysis, contributing to the development of more efficient pruning strategies.

[Potential reasons for acceptance]
1. The research advances the understanding of the lottery ticket hypothesis by shedding new light on the structure of winning tickets.
2. The findings are significant because they present a new way to discover winning tickets without relying on computationally expensive iterative pruning, improving efficiency and reducing training time.
3. The paper showcases the state-of-the-art accuracy achieved with early winning tickets on CIFAR10 and ImageNet at a high pruning rate.

[Potential reasons for rejection]
1. The experimental setup only focuses on specific architectures, limiting the generalizability of the findings to other types of neural networks.
2. The paper might be considered to replicate prior work, as the authors also confirm the necessity of initialization, rewinding, and sign of initial weights.
3. The paper does not provide enough justification for why early winning tickets are necessarily better than regular winning tickets, leaving some ambiguity.

[Suggestions for improvement]
1. To strengthen the paper, the authors could expand their investigations to a broader range of neural network architectures, making their findings more universally applicable.
2. To improve the paper's discussion on the significance of early winning tickets, the authors could showcase specific advantages of early winning tickets over regular winning tickets, such as faster convergence rates, memory efficiency, or other desirable properties.
3. To further support their claims, the authors could include more thorough ablation studies and additional evaluation metrics, providing a more complete assessment of the advantages and disadvantages of early winning tickets.","[Significance and novelty]
<Investigation of winning tickets> The paper investigates the structure of winning tickets obtained by iterative magnitude pruning, focusing on the timing of pruning and the impact of pruning on the ability to learn complex patterns.
<Empirical findings on early winning tickets> The study provides significant empirical findings on early winning tickets, demonstrating that a winning ticket can be identified as early as 22% of the total epochs used for iterative pruning.

[Potential reasons for acceptance]
<Clear and focused writing> The paper has been praised for its clear and focused writing, making the research findings easy to understand and follow.
<Empirical contributions> The paper's strong empirical contributions have been highlighted, particularly in demonstrating the emergence of winning tickets early in training and the impact of gradual pruning methods.

[Potential reasons for rejection]
<Lack of theoretical depth> Reviewers have pointed out the paper's lack of theoretical depth and rigorous analysis, suggesting the need for more formal justifications and theoretical guarantees for the proposed methods and findings.
<Incomplete experimental analysis> Concerns have been raised about the completeness of the experimental analysis, particularly in terms of the thoroughness of the ablation studies and the use of non-standard hyperparameters.

[Suggestions for improvement]
<Theoretical foundation> The paper could benefit from a more solid theoretical foundation, including formal justifications and theoretical guarantees for the proposed methods and empirical findings.
<Comprehensive ablation studies> Address concerns related to the completeness of the ablation studies by conducting more thorough and standardized analyses to support the empirical findings.","{
    ""1"": {
        ""summary"": ""The paper's experimental setup is limited to specific architectures, which may restrict the generalization of the findings to other neural network types."",
        ""verbatim"": ""The experimental setup only focuses on specific architectures, limiting the generalizability of the findings to other types of neural networks.""
    },
    ""2"": {
        ""summary"": ""The paper might be seen as redundant because it confirms previously known aspects about the importance of initialization, rewinding, and sign of initial weights in neural networks."",
        ""verbatim"": ""The paper might be considered to replicate prior work, as the authors also confirm the necessity of initialization, rewinding, and sign of initial weights.""
    },
    ""3"": {
        ""summary"": ""The paper lacks sufficient justification for the superiority of early winning tickets over regular winning tickets, leaving some ambiguity."",
        ""verbatim"": ""The paper does not provide enough justification for why early winning tickets are necessarily better than regular winning tickets, leaving some ambiguity.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The paper lacks theoretical depth and rigorous analysis."",
        ""verbatim"": ""<Lack of theoretical depth> Reviewers have pointed out the paper's lack of theoretical depth and rigorous analysis, suggesting the need for more formal justifications and theoretical guarantees for the proposed methods and findings.""
    },
    ""2"": {
        ""summary"": ""The experimental analysis is incomplete, particularly in terms of ablation studies and use of non-standard hyperparameters."",
        ""verbatim"": ""<Incomplete experimental analysis> Concerns have been raised about the completeness of the experimental analysis, particularly in terms of the thoroughness of the ablation studies and the use of non-standard hyperparameters.""
    }
}",2,{},0,"{
    ""A1-B1"": {
        ""rationale"": ""Both points address the lack of depth or insight in the research work. Review A critizes the paper for lack of originality and not offering much insight into the structure of sparse subnetworks, while Review B emphasizes a general lack of theoretical depth and rigorous analysis."",
        ""similarity"": ""7""
    },
    ""A2-B2"": {
        ""rationale"": ""Review A is concerned with the robustness and reliability of the findings due to randomness in early winning tickets and requires more convincing analysis, using error bars and averages of experiments. Review B points out issues with the completeness of the experimental analysis, specifically mentioning the need for thorough ablation studies and issues with non-standard hyperparameters. Both points highlight deficiencies in the experimental rigor and validation."",
        ""similarity"": ""7""
    }
}",2
Assessing the validity of saliency maps for abnormality localization in medical imaging,02X3kfP6W4.json,"[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: (a) Visualizations from two independently trained InceptionV3 models (b) Visualizations from an InceptionV3 model (top row) and a DenseNet121 model (bottom row) (c) Comparison of dice score differences across saliency methods and architectures (d) Comparison of intra-vs. inter-architecture repeatability using Spearman rank correlation

[INTRODUCTION]
Saliency maps have become a popular approach for post-hoc interpretability of Convolutional Neural Networks (CNNs). (Adebayo et al., 2018) These maps are designed to highlight the salient components of the input images that are important to the model prediction. As a result, many deep learning medical imaging studies have used saliency maps to rationalize model prediction and provide localization. (Rajpurkar et al., 2017;Bien et al., 2018;Mitani et al., 2019) However, the validity of saliency maps has been called into question in a recent study showing that many popular saliency map approaches are not sensitive to model weight or label randomization for models evaluated on several datasets. (Adebayo et al., 2018) In this study, we extend this work by evaluating popular saliency map methods both quantitatively and qualitatively for classification models trained on the RSNA Pneumonia dataset. (Shih et al., 2019) Specifically, we assess the performance of these methods in localizing abnormalities in medical imaging by quantifying overlap with ground truth bounding boxes. Furthermore, we assess the effect of model weight and label randomization on localization performance. Lastly, we empirically study repeatability of the saliency maps, both within the same model architecture and across different model architectures.

[MODEL AND DATA RANDOMIZATION]
The saliency methods examined in our experiments are Gradient Explanation (Simonyan et al., 2013), Smoothgrad Integrated Gradients (IG) (Sundararajan et al., 2017), GradCAM (Selvaraju et al., 2016), XRAI (Kapishnikov et al., 2019), and Smoothgrad (Smilkov et al., 2017). Along with using Spearman rank correlation to compare maps before and after model weight and label randomization, we leverage the ground-truth bounding box coordinates Figure 1: a) Visualization of saliency maps under cascading randomization on InceptionV3 (performance before randomization: AUC=0.98, precision=0.92) (b) Dice scores under cascading randomization (c) Spearman rank correlation under cascading randomization provided in the RSNA Pneumonia dataset to establish a quantitative baseline using the dice metric. To investigate the sensitivity of saliency methods under changes to model parameters, we employ cascading randomization. (Adebayo et al., 2018) We observed that among these saliency techniques, GradCAM degraded with model randomization to a large degree whereas the other methods did not (Fig 1). This is also verified in a label randomization experiment shown in Fig 2(c) wherein we randomly flipped the labels and retrained the model to observe the difference in the dice scores of the saliency maps. In both the tests, it can be observed that gradient explanation, Smoothgrad IG, and XRAI do not degrade significantly under randomization, suggesting an undesirable invariance to model parameters and labels.

[REPEATABILITY AND REPRODUCIBILITY]
We also conducted repeatability tests on these saliency methods by comparing maps from a) models with the same architecture trained independently (intra-architecture repeatability) 

[DISCUSSION AND CONCLUSION]
In this study, we evaluated the performance of several popular saliency methods on the RSNA Pneumonia Detection dataset in regards to their localization capabilities, robustness to model parameter and label randomization, as well as repeatability and reproducibility with model architectures. It was found that GradCAM showed superior sensitivity to model parameter and label randomization, and was highly agnostic to model architecture. In future studies, we will further examine the effect of different model architectures on saliency maps and validate our findings on a separate medical imaging dataset.

[ACKNOWLEDGMENTS]
We would like to thank Julius Adebayo for providing us with the cascading randomization code used in his work. (Adebayo et al., 2018) Research","[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture.","{
    ""1"": {
        ""summary"": ""The paper lacks in-depth discussion on why GradCAM performs better and the images in the figures are too small and cluttered."",
        ""verbatim"": ""However, there was a lack of discussion of these results in this paper - are there any possible explanations for why GradCAM is performing better? Furthermore, the images in the figures hard to see. They should be larger and as much whitespace should be removed between images.""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of qualitative analysis on the interpretability of saliency maps"",
        ""verbatim"": ""One potential shortcoming could be the lack of analysis on the interpretability of the saliency maps generated by different methods.""
    },
    ""2"": {
        ""summary"": ""Absence of discussion on study limitations and dataset-specific issues"",
        ""verbatim"": ""The paper could be improved by including a discussion on the limitations of the study, such as the specific characteristics of the RSNA Pneumonia dataset that may affect the generalizability of the findings to other medical imaging datasets.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both points highlight a lack of analysis or discussion on critical aspects of the results generated by their respective methods (GradCAM and saliency maps). The main concern in both cases revolves around understanding the reasons behind the results produced by these methods and the need for a deeper qualitative analysis."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of comparison with similar studies could weaken context and relevance."",
        ""verbatim"": ""Lack of comparison with other similar studies in the domain of medical imaging and saliency maps, which could weaken the context and relevance of the findings.""
    },
    ""2"": {
        ""summary"": ""Limited discussion on generalizability may restrict impact."",
        ""verbatim"": ""Limited discussion on the generalizability of the results to other medical imaging datasets or conditions, potentially limiting the broader impact of the study.""
    },
    ""3"": {
        ""summary"": ""Absence of a comprehensive evaluation of limitations could affect robustness of conclusions."",
        ""verbatim"": ""The absence of a comprehensive evaluation of the limitations of the saliency map methods tested, which could raise concerns about the robustness of the conclusions drawn.""
    }
}",3,{},0,"{
    ""1"": {
        ""summary"": ""Study may lack generalizability beyond the specific dataset used"",
        ""verbatim"": ""The study may lack generalizability beyond the RSNA Pneumonia dataset, raising concerns about the broader applicability of the findings.""
    },
    ""2"": {
        ""summary"": ""Analysis lacks a comprehensive comparison of different saliency map methods"",
        ""verbatim"": ""The analysis could be strengthened by providing a more comprehensive comparison of different saliency map methods beyond GradCAM.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper's use of only the RSNA Pneumonia dataset may limit the generalizability of its findings across other medical conditions and imaging types."",
        ""verbatim"": ""Limited Diversity of Datasets: The study focuses solely on the RSNA Pneumonia dataset. This narrow scope might limit the generalizability of the findings across other types of medical imaging datasets.""
    },
    ""2"": {
        ""summary"": ""Concerns about the invariance of certain saliency methods to changes, questioning their reliability and utility in different clinical situations."",
        ""verbatim"": ""Invariance Issues: The invariance of certain saliency methods like gradient explanation and Smoothgrad IG to model parameter and label randomization might raise concerns about their utility and reliability in varying clinical conditions.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper focuses only on GradCAM and lacks a comprehensive comparison among various saliency map methods."",
        ""verbatim"": ""Limited scope regarding various saliency methods: The paper primarily focuses on GradCAM, and although it mentions 'various existing saliency map methods', there is a lack of comprehensive comparison across multiple types of saliency tools which might lead to a biased or incomplete view of the state of the art.""
    },
    ""2"": {
        ""summary"": ""The study's results might not be generalizable beyond the RSNA Pneumonia dataset used."",
        ""verbatim"": ""Potential data-specific findings: The findings might be heavily influenced by the characteristics of the RSNA Pneumonia dataset, raising questions about the generalizability of the results to other medical imaging tasks or datasets.""
    },
    ""3"": {
        ""summary"": ""The paper does not provide sufficient details on the experimental setup, affecting reproducibility."",
        ""verbatim"": ""Lack of implementation details: The paper might fall short in detailing the experimental setups, such as specific model architectures used and their configuration, which are crucial for reproducibility and validation of the findings by other researchers.""
    }
}",3,{},0,"{
    ""1"": {
        ""summary"": ""Study limited to only one dataset, raising concerns on generalizability"",
        ""verbatim"": ""The limitation of the study to only the RSNA Pneumonia dataset might raise concerns about the generalizability of the findings across different medical conditions or imaging types.""
    },
    ""2"": {
        ""summary"": ""Potential methodological shortcomings in the study"",
        ""verbatim"": ""The paper might face criticism for not addressing potential biases in the model training or the selection of saliency methods, which can significantly affect the reproducibility and reliability of the results.""
    }
}",2,{},0,"Our study compared several state-of-the-art methods, including U-Net, ResNet, and VGG-16, using the RSNA Pneumonia dataset. The experiments showed that GradCAM had the best overall performance, with the highest accuracy, specificity and sensitivity across all the classes. However, we identified several challenges in using saliency maps, including the need for more training data to improve the model's performance, and limitations in the ability to handle class imbalance in the data, as well as the small size of the dataset which reduces the confidence in the performance. [/Suggestons for improvement] To improve the performance of GradCAM, one approach that can be taken is to use additional training data. Another approach is to employ ensemblesion, where multiple saliency models are trained on the same data. This can help improve robustness and reduce overfitting. A third approach that could be taken is to use data augmentation, which involves generating synthetic data from the real data for training the model. Lastly, using transfer learning, which involve using a pretrained model on new data, can be used to improve performance on new data. Each of these approaches can be beneficial to improve performance and make saliency maps more effective in medical imaging applications. However, there is challenges remain in implementing these approaches in clinical practice due to limited availability and the high cost of acquiring and annotating large datasets and the complexity of implementing the techniques in real clinical workflows. Ongoing forward, continued research and developing will be needed to address these challenges and further improve the efficacy and robustness of saliency mapping techniques in clinical settings.

In the field of medical imaging, saliency maps have gained significant attention due to their ability to improve the diagnostic and prognostic accuracy in various medical conditions. These maps are computerized representations of medical images that can be used to assist radiologists in making more accurate diagnoseses and improving overall patient care. In medical imaging, saliency maps are often used to detect lesions, segment tumors, and diagnose diseases such as brain abnormalities.

However, there are limitations to saliency mapping techniques, including the need for large and high quality training datasets, the ability to handle class imbalance in the data, and limitations in the availability of these techniques in real clinical practice. To improve the performance of saliency maps, several approaches can be taken. One approach is to use additional training data to increase the size of the existing datasets, which can help improve the model's performance and reduce overfitting. Another approach is to employ ensemblesion, which involves training multiple saliency models on the same dataset and producing a consensus output to improve the model's robustness and reduce overfitting. A third approach is to use transfer learning, which involves using a pretrained model on new data to improve performance on new data. Furthermore, developing new and more advanced deep learning algorithms and techniques can be used to improve the perfomality of saliency maps.

In our study, we explored the validity and novelty of existing saliency map methods by comparing their performanacy on the RSNA Pneumonia dataset using the GradCAM, U-Net, and ResNet models. Our results showed that GradCAM outperformed the other methods in overall performance, specifically in terms of accuracy, specificity, sensitivity, and Matthews correlation coefficient. However, there are challenges in applying saliency maps in clinical practice due to their complexitiness and high cost of acquiring and annotating large datasets, as well as the limitations in handling class imbalance and size of the dataset which reduces the confidence in the performance. To improve the perfomality and robustness of saliency maps, several approaches can be taken. One approach is to use additional training data to increase the size of the existing datasets, thereby reducing overfitting and improving the model's performance. Another approach is to employ ensemblesion, which involves training multiple saliency models on the same dataset to generate consensus output and improve the model's robustness. A third approach is to use transfer learning, which involves using a pretrained model on new data to improve performance on new datasets.

The use of saliency maps in medical imaging applications has gained significant attention due to their ability to improve diagnostic and prognostic accuracy in various medical conditions. Medical imaging applications of saliency maps include lesion detection, tumor segmentation, and disease diagnoses, specifically in conditions such as brain abnormalities.

The limitations of saliency mapping techniques include the need for large and high quality training datasets, the ability to handle class imbalance in the data, and the availability of these techniques in clinical practice. To improve the performance and robustness of saliency maps, several approaches can be taken. One approach","[Potential reasons for acceptance]
The study's scope is limited to specific applications in medical imaging. The method is not novel, however the comparision of the proposed method with state-of-the-art methods is limited. The evaluation and contributions are suggested to have more clarity and details. The comparison to state-of-the-art methods in medical imaging is needed.

[Suggestions for improvement]
The paper needs more extensive experiments and analysis to strengthen its contributions, particularly in the section of discussing the validity of the proposed method and evaluation. The authors should expand discussion of comparision and application of the proposed method with other state-of-the-art methods in the field. Additional citations sources is needed within application of the proposed method in medical imaging.

[Concluding] The paper presents a method which leverages saliency maps for medical imaging based applications, but the study is limited only to one specific area. The paper should expand its scope by including other imaging applications in which saliency maps could be useful. The authors need to proof and reference all claims and statements in the paper. The paper could benefit from more extensive experimentation and analysis to strengthen the contributions. Additioinal citations are required in other applications beyond medical imaging. The authors need more details and clarrity in discussion of proposed method with other state-ofthe-art methods, in the field. Additionally, the study is limited to specific evaluations in medical imaging where the proposed method was applied. Further studies are suggested for generalization techniques and application of the proposed method beyond medical imaging and segmentation in magnetic resonance imaging.

","{
    ""1"": {
        ""summary"": ""Small dataset size reduces confidence in model performance"",
        ""verbatim"": ""the small size of the dataset which reduces the confidence in the performance""
    },
    ""2"": {
        ""summary"": ""High cost and complexity of acquiring and annotating large datasets"",
        ""verbatim"": ""the high cost of acquiring and annotating large datasets and the complexity of implementing the techniques in real clinical workflows""
    },
    ""3"": {
        ""summary"": ""Need for large and high quality training datasets"",
        ""verbatim"": ""the need for large and high quality training datasets""
    },
    ""4"": {
        ""summary"": ""Challenges in handling class imbalance"",
        ""verbatim"": ""the ability to handle class imbalance in the data""
    },
    ""5"": {
        ""summary"": ""Limitations of saliency mapping availability in clinical practice"",
        ""verbatim"": ""limitations in the availability of these techniques in real clinical practice""
    }
}",5,"{
    ""1"": {
        ""summary"": ""The method is not novel and comparison to state-of-the-art methods is limited."",
        ""verbatim"": ""The method is not novel, however the comparison of the proposed method with state-of-the-art methods is limited.""
    },
    ""2"": {
        ""summary"": ""Lack of clarity and detail in evaluation and contributions."",
        ""verbatim"": ""The evaluation and contributions are suggested to have more clarity and details.""
    },
    ""3"": {
        ""summary"": ""Insufficient experiments and analysis to support the contributions."",
        ""verbatim"": ""The paper needs more extensive experiments and analysis to strengthen its contributions.""
    },
    ""4"": {
        ""summary"": ""Need for broader discussion and comparison with other state-of-the-art methods."",
        ""verbatim"": ""The authors should expand discussion of comparison and application of the proposed method with other state-of-the-art methods in the field.""
    },
    ""5"": {
        ""summary"": ""Additional citation sources required within the application of the proposed method."",
        ""verbatim"": ""Additional citations sources is needed within application of the proposed method in medical imaging.""
    },
    ""6"": {
        ""summary"": ""Study focus is too narrow, limited to one specific area."",
        ""verbatim"": ""The study is limited only to one specific area.""
    },
    ""7"": {
        ""summary"": ""Claims and statements in the paper are unsubstantiated and lack references."",
        ""verbatim"": ""The authors need to proof and reference all claims and statements in the paper.""
    },
    ""8"": {
        ""summary"": ""Further studies suggested for generalization beyond specific applications."",
        ""verbatim"": ""Further studies are suggested for generalization techniques and application of the proposed method beyond medical imaging and segmentation in magnetic resonance imaging.""
    }
}",8,{},0,"{
    ""A1-B4"": {
        ""rationale"": ""Both points highlight the need for a broader discussion and comparison regarding the effectiveness of the methods utilized, specifically asking for further explanation on performance and including comparative analysis with state-of-the-art methods."",
        ""similarity"": ""7""
    }
}",1
Explanation-Based Attention for Semi-Supervised Deep Active Learning,SyxKiVmedV.json,"[TITLE]
EXPLANATION-BASED ATTENTION FOR SEMI-SUPERVISED DEEP ACTIVE LEARNING

[ABSTRACT]
We introduce an attention mechanism to improve feature extraction for deep active learning (AL) in the semi-supervised setting. The proposed attention mechanism is based on recent methods to visually explain predictions made by DNNs. We apply the proposed explanation-based attention to MNIST and SVHN classification. The conducted experiments show accuracy improvements for the original and class-imbalanced datasets with the same number of training examples and faster long-tail convergence compared to uncertainty-based methods.

[CAPTIONS]
Table 1: Figure 2 :Figure 3 :23Figure 2: MNIST test dataset accuracy for 3 class imbalance ratios: a) 1 (no imbalance), b) 10 and c) 100. Total 9 AL iterations (b = 10) are performed each with P = 250 pool size.
Table 2: SVHN. The dataset train/validation/test split is 500K/104K/26K. A typical 8-layer CNN is used with the following hyperparameters: epochs=35, batch-size=25, lr=0.1, lr-decay=0.1 every 15 epochs, uncert methods and IG EBA use K = 128 and L is 256 for single-scale (before fc1 layer) and 384 for two-scale descriptors (+ layer before conv7). Figure3shows that the gap between random selection and the best EBA-based AL method grows from 2% to more than 12% when the unlabeled training collection has more class imbalance. The gap between full training dataset accuracy increases for larger-scale SVHN as well. This results in even faster convergence for the proposed AL relative to random selection. Accuracies of the uncert methods are closer to each other than for MNIST, which may signal their declining effectiveness for large-scale data. The proposed EBA-based methods outperform all uncertainty-based methods for SVHN in the first AL iterations (up to +2.5%) and later arrive at approximately equal results.5 CONCLUSIONS AND FUTURE WORKWe applied recent image retrieval feature-extraction techniques to deep AL and introduced a novel EBA mechanism to improve feature-similarity matching. First feasibility experiments on MNIST and SVHN datasets showed advantages of EBA to improve density-based AL. Rather than performing AL for the well-picked training datasets, we also considered more realistic and challenging scenarios with class-imbalanced training collections where the proposed method emphasized the importance of additional feature supervision. In future research, EBA could be evaluated with other types of data distortions and biases: within-class bias, adversarial examples, etc. Furthermore, such applications as object detection and image segmentation may benefit more from EBA because multiscale attention can focus on spatially-important features.

[INTRODUCTION]
Deep active learning (AL) minimizes the number of expensive annotations needed to train DNNs by selecting a subset of relevant data points from a large unlabeled dataset (Lewis & Gale, 1994). This subset is annotated and added to the training dataset in a single pool of data points or, more often, in an iterative fashion. The goal is to maximize prediction accuracy while minimizing the product of pool size × number of iterations. A proxy for this goal could be the task of matching feature distributions between the validation and the AL-selected training datasets.
In density-based AL approaches, data selection is typically performed using a simple L 2 -distance metric (Sener & Savarese, 2018). The image retrieval field (Zhou et al., 2017) has advanced much further in this area. For example, recent state-of-the-art image retrieval systems are based on DNNbased feature extraction (Babenko & Lempitsky, 2015) with attention mechanisms (Noh et al., 2017). The latter estimates an attention mask to weight importance of the extracted features and it is trained along with the feature extraction.
Inspired by this, we employ image retrieval techniques and propose a novel attention mechanism for deep AL. Unlike supervised self-attention in (Noh et al., 2017;Vaswani et al., 2017), our attention mechanism is not trained with the model. It relies on recent methods to generate visual explanations and to attribute feature importance values (Sundararajan et al., 2017). We show the effectiveness of such explanation-based attention (EBA) mechanism for AL when combined with multi-scale feature extraction on a number of image classification datasets. We also conduct experiments for distorted class-imbalanced training data which is a more realistic assumption for unlabeled data.

[RELATED WORK]
AL is a well-studied approach to decrease annotation costs in a traditional machine learning pipelines (Settles, 2010). Recently, AL has been applied to data-demanding DNN-based systems in semi-supervised or weakly-supervised settings. Though AL is an attractive direction, existing methods struggle to deal with high-dimensional data e.g. images. We believe this is related to the lack of class and instance-level feature importance information as well as the inability to capture spatially-localized features. To overcome these limitations, we are interested in estimating spatiallymultiscale features and using our EBA mechanism to select only the most discriminative features. softmax Figure 1: Conventional multi-scale feature extraction and the proposed EBA extension (dashed). Wang et al. (2017) proposed to augment the training dataset by labeling the least confident data points and heuristically pseudo-labeling high confidence predictions. We believe the softmax output is not a reliable proxy for the goals of AL i.e. for selecting images using feature distribution matching between validation and train data. Unlike (Wang et al., 2017), we use pseudo labels only to estimate EBA vectors and find similarities between discriminative features. Gal et al. (2017) introduced a measure of uncertainty for approximate Bayesian inference that can be estimated using stochastic forward passes through a DNN with dropout layers. An acquisition function then selects data points with the highest uncertainty which is measured at the output of softmax using several metrics. Recent work (Beluch et al., 2018) extended this method by using an ensemble of networks for uncertainty estimation and achieved superior accuracy.
Sener & Savarese (2018) formulated feature similarity-based selection as a geometric core-set approach which outperforms greedy k-center clustering. Though their method can complement our approach, we are focusing on the novel feature extraction. For instance, they employed a simple L 2 distance similarity measure for the activations of the last fully-connected layer.
The most similar work to ours, by Vodrahalli et al. (2018), uses the gradients as a measure of importance for dataset subsampling and analysis. However, our approach formulates the problem as a multi-scale EBA for AL application and goes beyond a less robust single-step gradient attention. Other related works are online importance sampling methods (Ren et al., 2018) and the influence functions approach in (Koh & Liang, 2017). Online importance sampling upweights samples within the mini-batch during supervised training using gradient similarity while influence functions analyze data point importance using computationally challenging second-order gradient information.

[METHOD]
Pool-based AL. Let (X, y) be an input-label pair. There is a validation dataset {(X v i , y v i )} i∈M of size M and a collection of training pairs {(X i , y i )} i∈N of size N for which, initially, only a small random subset or pool of labels indexed by N 1 is known. The validation dataset approximates the distribution of test data. At every bth iteration the AL algorithm selects a pool of P new labels to be annotated and added to existing training pairs which creates a training dataset indexed by N b .
A DNN Φ(X, θ) is optimized by minimizing a loss function (N b ) −1 i∈N b L(ŷ i , y i ) w.r.t. to model parameters θ. However, the actual task is to minimize validation loss expressed by
M −1 i∈M L(ŷ v i , y v i ).
Therefore, an oracle AL algorithm achieves minimum of validation loss using the smallest b × P product. In this work, we are interested not in finding an oracle acquisition function, but in a method to extract relevant features for such function. We use a low-complexity greedy k-center algorithm to select the data points in the unlabeled training collection which are most similar to the misclassified entries in the validation dataset.
Feature descriptors. Let F j i ∈ R C×H×W , where C, H, and W are the number of channels, the height, and the width, respectively be the output of the jth layer of DNN for input image X i . Then, a feature vector or descriptor of length L can be defined as d i = φ(F i ) ∈ R L×1 , where function φ(•) is a conventional average pooling operation from (Babenko & Lempitsky, 2015). In a multi-scale case, descriptor is a concatenation of multiple feature vectors
d i = [φ j (F j i ), • • • , φ l (F l i )].
A descriptor matrix for the validation dataset V d ∈ R L×M and training dataset S d ∈ R L×N can be calculated using forward passes. Practically, descriptors can be compressed for storage efficiency reasons using PCA, quantization, etc. Then, a match kernel (Lee, 1999), e.g. cosine similarity, can be used to match features in both datasets. Assuming that vectors d i are L 2 -normalized, the cosine-similarity matrix is simply R d = V T d S d . Explanation-based attention. Feature maps F i extracted by Φ(X, θ) and pooled by φ(•) contain features that: a) are not class and instance-level discriminative (in other words, not disentangled), b) spatially represent features for a plurality of objects in the input. We would like to upweight discriminative features that satisfy a) and b) using an attention mechanism. One approach would be to use self-attention (Vaswani et al., 2017) at the cost of modifying network architecture and intervening into the training process. Instead, we propose to use EBA that is generated only for feature selection. The EBA mechanism attributes feature importance values w.r.t. to the output predictions. Unlike a visual explanation task, which estimates importance heatmaps in the input (image) space, we propose to estimate feature importance tensors A i of the internal DNN representations F i . Attention tensors A i can be efficiently calculated using a series of backpropagation passes. Using one of backpropagation-based methods called integrated gradients (IG) from (Sundararajan et al., 2017) ,  A j i can be estimated as
A j i = 1 K K k=1 ∂L(ŷ i (k), y i ) ∂F j i = 1 K K k=1 ∂L(Φ(kX i /K, θ), y i ) ∂F j i ,(1)
where K is the number of steps to approximate the continuous integral by a linear path. Other forms of (1) are possible: from the simplest saliency method for which K = 1 (Simonyan et al., 2014) to more advanced methods with randomly sampled input features (Gudovskiy et al., 2018).
Due to lack of labels y i in (1), we use common pseudo-labeling strategy: y i = 1 arg maxŷi . It is schematically shown in Figure 1. Unlike (Wang et al., 2017), pseudo-labels are used only to calculate similarity without additional hyperparameters rather than to perform a threshold-selected greedy augmentation. The EBA A i can be converted to multi-scale attention vector using the same processing a i = φ(A i ) ∈ R L×1 , which, by analogy, forms validation V a ∈ R L×M and train attention matrices S a ∈ R L×N . The latter processing is implemented in most modern frameworks and, therefore, the complexity to generate A i is only K forward-backward passes.
Summary for the proposed method. A random subset of N 1 training data points is annotated and a DNN Φ(X, θ) optimized for this subset. Then, the AL algorithm iteratively (b = 2, 3 . . .) performs following steps: 1) generates descriptor-attention matrix pairs
(V d , V a ), (S d , S a ), 2) calculates similarity matrix R = R d R a = (V T d S d ) (V T a S a )
, where is element-wise product, 3) selects P relevant data points from the remaining subset using acquisition function arg max i∈N\N b−1 (R(X i ), Φ) and 4) retrains Φ(X, θ) using augmented subset N b .

[EXPERIMENTS]
Our method as well as uncertainty-based methods from (Gal et al., 2017) are applied to the MNIST and SVHN classification. We evaluate AL with the original and distorted training data because unlabeled collection of data points cannot be a-priori perfectly selected. Hence, we introduce a class imbalance which is defined as the ratio of {0 . . . 4} to {5 . . . 9} digits. The following methods have been employed: random sampling, uncertainty-based (uncert), greedy selection using similarity matching without (top-P:none) and with EBA. The latter is estimated by saliency (top-P:grad) or IG (top-P:ig). We rerun experiments 10 times for MNIST and 5 times for SVHN with all-randomized initial parameters. Mean accuracy and standard deviation are reported. DNN parameters are trained from scratch initially and after each AL iteration. Mini-batch size is chosen by cross-validation.
MNIST. The dataset train/val/test split is 50K/10K/10K. The LeNet is used with the following hyperparameters: epochs=50, batch-size=25, lr=0.05, lr-decay=0.1 every 15 epochs, uncert methods and IG EBA use K = 128 passes and L is 20 for single-scale (before fc1 layer) and 90 for multiscale descriptors (all layers are concatenated). Figure 2(a) shows that feature-only matching (top-P:none L20) outperforms random selection by ≈ 1% while EBA (top-P:ig L90) adds another 1% of accuracy when there is no class imbalance. High class imbalance (Figure 2(c)) increases that gap: up to 20% for feature-only matching and 25% with EBA. The highest accuracy is achieved by multi- Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 0.5 1.0 1.5 2.0 2.5 3.0 3. Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 0.5 1.0 1.5 2.0 2.5 3.0 3. Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 scale EBA estimated by IG. EBA-based methods outperform the best uncertainty-based variation ratio (uncert:varMC) approach for all class imbalance settings except the last one where its accuracy is higher by less than 1% when b = 4. This might be related to small-scale MNIST and pseudo-label noise for EBA. To study the effects of pseudo-labeling, we plot true-label configurations (marked by ""Abl"") as well. The accuracy gap between EBA using true-and pseudo-labels is small with no class imbalance, but much larger (up to 25%) when class imbalance ratio is 100 during first AL iterations.","[TITLE]
EXPLANATION-BASED ATTENTION FOR SEMI-SUPERVISED DEEP ACTIVE LEARNING

[ABSTRACT]
We introduce an attention mechanism to improve feature extraction for deep active learning (AL) in the semi-supervised setting. The proposed attention mechanism is based on recent methods to visually explain predictions made by DNNs. We apply the proposed explanation-based attention to MNIST and SVHN classification. The conducted experiments show accuracy improvements for the original and class-imbalanced datasets with the same number of training examples and faster long-tail convergence compared to uncertainty-based methods.","{
    ""1"": {
        ""summary"": ""Concerns about the clarity of the methodology particularly regarding the use of integrated gradients and pseudo-labels, the justification for certain assumptions, and the breadth of the experimental validation."",
        ""verbatim"": ""My major concerns are two-fold: 1) the intuition of utilizing integrated gradients (IG) and pseudo-labels is not super clear to me; 2) experiments should be more extensive. The authors assume that the way of using IGs as EBA for evaluating sample similarity by multiplying themselves with descriptor matrices can upweight features that \""a) are not class and instance-level discriminative, b) spatially represent features for a plurality of objects in the input. \"" The assumption needs more justification. For example, a) why to use average pooling function for both gradients and features is reasonable, b) the derivation of R_b is of what properties such that the distribution between training data and validation set are more similar iteratively (so we can believe the set of b-th iteration is better than the set of {b-1}-th). Also, experiments can justify the assumption as well with more visual explanations on why the proposed AL method is better and reasonable.""
    }
}",1,"{
    ""1"": {
        ""summary"": ""More detailed explanations or examples of the limitations or scenarios where EBA may not be effective are needed."",
        ""verbatim"": ""The paper could benefit from providing more detailed explanations or examples on the limitations or specific scenarios where the proposed Explanation-based Attention (EBA) mechanism may not perform as effectively.""
    },
    ""2"": {
        ""summary"": ""Need for more information on the computational complexity and limitations of EBA compared to existing methods."",
        ""verbatim"": ""While the experiments show accuracy improvements, the paper could further elaborate on the computational complexity or limitations of the proposed method compared to existing approaches.""
    },
    ""3"": {
        ""summary"": ""The paper lacks an extensive comparison with other state-of-the-art methods."",
        ""verbatim"": ""It would be helpful to include a more extensive comparison with other state-of-the-art methods in feature extraction and selection for deep active learning to showcase the superiority of the proposed EBA mechanism.""
    },
    ""4"": {
        ""summary"": ""Insufficient analysis of the scalability of EBA to larger datasets."",
        ""verbatim"": ""The paper lacks a detailed analysis of the scalability of the proposed method to larger datasets beyond MNIST and SVHN, which could impact its generalizability and applicability to real-world scenarios.""
    }
}",4,"{
    ""A1-B3"": {
        ""rationale"": ""Both Review A and Review B point to the need for more extensive experimental comparison with other methods. Review A highlights this within the context of better validating assumptions by extending experiments, while Review B specifically mentions lacking a comparison with state-of-the-art methods."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Insufficient comparison with diverse baselines"",
        ""verbatim"": ""Lack of comparison with more diverse baselines: While the paper compares the proposed method with uncertainty-based techniques, a more comprehensive comparison with a wider range of state-of-the-art methods in semi-supervised deep active learning could strengthen the evaluation of the approach.""
    },
    ""2"": {
        ""summary"": ""Experiments limited to basic datasets"",
        ""verbatim"": ""Limited datasets: The experiments are conducted on MNIST and SVHN datasets, which are standard benchmarks. Including experiments on more diverse or challenging datasets could help in assessing the generalizability of the proposed method.""
    },
    ""3"": {
        ""summary"": ""Lack of in-depth theoretical analysis"",
        ""verbatim"": ""Limited theoretical analysis: The paper could benefit from a more extensive theoretical analysis of the proposed attention mechanism to elucidate the underlying principles and mechanisms guiding its effectiveness.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both points emphasize the need for a broader and more precise evaluation of the methodology. Review A discusses the need for more extensive experiments specifically related to the utilization of integrated gradients and pseudo-labels, pointing out that the assumptions and method should be better justified through experimental evidence. Review B criticizes the limited scope of comparison with baselines and suggests an expansion to include a broader range of methods, which could help in evaluating the proposed approach more comprehensively. Both concerns relate to strengthening the empirical evaluation part of the research to ensure the approach is robust and well supported."",
        ""similarity"": ""7""
    },
    ""A1-B2"": {
        ""rationale"": ""Both points discuss limitations concerning the extensiveness and diversity of experimentation. Review A calls for more extensive experiments to support the methodology, while Review B specifically identifies the need to apply the proposed method to more diverse or challenging datasets than just MNIST and SVHN. Both are concerned with the comprehensive testing of the proposed method to demonstrate its applicability and effectiveness more convincingly."",
        ""similarity"": ""8""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper might be rejected due to the lack of thorough theoretical analysis."",
        ""verbatim"": ""The paper may face rejection if it lacks a detailed theoretical analysis supporting the effectiveness and generalizability of the proposed explanation-based attention mechanism.""
    },
    ""2"": {
        ""summary"": ""The paper might be rejected due to insufficient discussion on scalability and complexity concerns."",
        ""verbatim"": ""If the paper fails to address the scalability and potential complexity issues associated with implementing the attention mechanism in larger datasets or real-world applications, it could be rejected.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""Concern about the limited variety of datasets used in experiments, questioning the generalizability of the method."",
        ""verbatim"": ""Limited Variety of Datasets: While the results on MNIST and SVHN are promising, the generalization of the proposed method to other datasets or more complex scenarios beyond image classification is not demonstrated.""
    },
    ""2"": {
        ""summary"": ""Issue of complexity and scalability due to multiple backpropagation passes, which could increase computational overhead and limit practical application in larger datasets or real-time systems."",
        ""verbatim"": ""Complexity and Scalability Issues: The incorporation of EBA, while novel, involves multiple backpropagation passes which could potentially increase computational overhead. The scalability of this method when applied to larger, more complex datasets or in different contexts (such as real-time systems) isn't addressed.""
    },
    ""3"": {
        ""summary"": ""Risk of model overfitting to specific features that are highlighted by the visual explanation methods, potentially compromising the model's effectiveness on class discriminative features."",
        ""verbatim"": ""Potential Overfitting to Specific Features: The reliance on explanation-based features for active learning could lead the model to overfit to visual explanations which might not always correlate directly with the underlying class discriminative features.""
    }
}",3,{},0,"{
    ""1"": {
        ""summary"": ""The experiments in the paper are limited to simple and extensively studied datasets, potentially reducing the generalizability and impact of the results."",
        ""verbatim"": ""'Limited Experimental Scope': The experiments are limited to MNIST and SVHN datasets, which are relatively simple and extensively studied. More complex or real-world datasets might present different challenges not addressed in the current study.""
    },
    ""2"": {
        ""summary"": ""The paper lacks a comprehensive comparative analysis with a wide range of active learning strategies, which could weaken the strength of its conclusions."",
        ""verbatim"": ""'Lack of Comparative Analysis': While the paper states improvements over certain methods, a comprehensive comparison across a broader range of active learning strategies is missing. This comprehensive analysis could solidify the contribution beyond the specific instances reported.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A and Review B express concerns regarding the scope and depth of experimental validation. Review A notes that experiments should be more extensive, suggesting the need for more robust experimental evidence, while Review B points out that the experiments are limited to simple datasets, which could affect generalizability. Both reviews ultimately critique the limitations in experimental design and validation."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Concern about the limited testing on simpler datasets, raising questions on generalizability"",
        ""verbatim"": ""The experiments are limited to MNIST and SVHN, which are relatively simple and overused in machine learning research. The lack of results on more complex or diverse datasets might raise concerns about the generalizability of the proposed method.""
    },
    ""2"": {
        ""summary"": ""Concern regarding potential reproducibility issues of the experiments"",
        ""verbatim"": ""The paper might face reproducibility challenges if it does not sufficiently detail the implementation of the attention mechanism and the experimental setup, which is critical for validating the claimed improvements.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A and Review B raise concerns about the limitations of the experiments in providing a robust proof of concept for the proposed methods. Review A criticizes the need for more extensive experimentation and the use of the integration gradient and pseudo-labels, while Review B pinpoints the issue with using only simple, well-understood datasets (MNIST, SVHN) which may not validate the method's effectiveness across a broader range of applications."",
        ""similarity"": ""7""
    }
}",1,"Significance:
The paper presents an novel approach to improve the performance of deep learning models in the semi-supervised setting. The proposed mechanism uses explanation-based attention to improve the predictions made by deep learning models, which is a recent trend in the machine learning research field. The results show improvements in accuracy and faster convergence speed compared to uncertainty-based methods.

Potential reasons for acceptance:
The paper contributes to the ongoing research in deep learning, particularly in the area of explainability and the application of deep learning models in the semi-supervised setting. The use explanation-based attention mechanism is a recent trend in the machine learning research and is becoming increasingly popular for its ability to improve the performance of deep learning models. The paper's experimental setup follows the standard methods in the research and provides clear and detailed explanations for its methods.

Potential reasons for rejection:
The given paper lacks a clear statement of the research question and the problem being addressed. The experimental setup is not well described and lacks enough context to evaluate the proposed method effectively. The paper does not sufficiently compare the results with relevant previous work in the field, making it difficult to assess its novelty and significance.

Suggestions for improvement:
The authors can provide a clearer statement of the research question and the problem being addressed. They can include more detail on the experimental setup, including the dataset used, preprocessing techniques, and training details. The authors can make more comparisons with previous work in the field to establish the novelty and significance of their findings.

To summarize each point within the given criteria, please consider the following:

1. Significance and novelty:

The paper presents a novel approach to explainability in the semi-supervised deep learning models using a explanation-based attention mechanism. The application of this mechanism is a current trend in the deep learning research area, and has become increasingly popular due to its potential improve performance and interpretability of deep learning models.

2. Potential reasons for acceptance:

The paper contributes to the deep learning research, particularly in the explainability and improvement of deep learning models in the semi-supervised setting. The proposed attention mechanism aligns with recent trends in the deep learning research and is likely to be of interest to the deep learning community. The study design and methodology are clearly described and well documented, increasing the confidence in the reliability and validity of the results.

3. Potential reasons for rejection:

The research question and problem statement are not clear and well defined. The experiments lack clear and sufficient detail, making it difficult to evaluate the methodology effectively. The paper lacks sufficient comparison with relevant previous work and the field, making it unclear its novelty and significance.

4. Suggestions for improvement:

The authors can provide a clearer statement of the research question and problem being addressed. They can include more detail on the experimental setup and methods, including the specifications of datasets used, preprocessing techniques, and training settings. The authors can include more comparisons with relevant previous work and the field. They can clearly summarize their main contributions and the results, and discuss its implications for future research. They can provide code and data for reproducibility to support their findings. In addition, they can address any potential criticisms and limitations of their study and suggest potential avenues for future research.",,"{
    ""1"": {
        ""summary"": ""Lack of clear research question and problem statement"",
        ""verbatim"": ""The given paper lacks a clear statement of the research question and the problem being addressed.""
    },
    ""2"": {
        ""summary"": ""Insufficient experimental setup description"",
        ""verbatim"": ""The experimental setup is not well described and lacks enough context to evaluate the proposed method effectively.""
    },
    ""3"": {
        ""summary"": ""Inadequate comparison with relevant previous work"",
        ""verbatim"": ""The paper does not sufficiently compare the results with relevant previous work in the field, making it difficult to assess its novelty and significance.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Review is empty or not provided"",
        ""verbatim"": ""nan""
    }
}",1,"{
    ""A1-B2"": {
        ""rationale"": ""Both points address the concerns related to the clarity and adequacy of experimental descriptions. Review A discusses the unclear intuition and methodology behind using integrated gradients and pseudo-labels as well as the assumption made in the experimental setup. Similarly, Review B mentions that the experimental setup is not well described, lacking sufficient context for effective evaluation."",
        ""similarity"": ""7""
    }
}",1,{},0
Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet Recognition through the Lens of Robustness,zZcCINENgm.json,"[TITLE]
WHY DEEP SURGICAL MODELS FAIL?: REVISITING SURGICAL ACTION TRIPLET RECOGNITION THROUGH THE LENS OF ROBUSTNESS

[ABSTRACT]
Surgical action triplet recognition provides a better understanding of the surgical scene. This task is of high relevance as it provides the surgeon with contextaware support and safety. The current go-to strategy for improving performance is the development of new network mechanisms. However, the performance of current state-of-the-art techniques is substantially lower than other surgical tasks. Why is this happening? This is the question that we address in this work. We present the first study to understand the failure of existing deep learning models through the lens of robustness and explainability. Firstly, we study current existing models under weak and strong δ −perturbations via an adversarial optimisation scheme. We then analyse the failure modes via feature based explanations. Our study reveals that the key to improving performance and increasing reliability is in the core and spurious attributes. Our work opens the door to more trustworthy and reliable deep learning models in surgical data science. https://yc443.github.io/robustIVT/Minimally Invasive Surgery (MIS) has become the gold standard for several procedures (i.e., cholecystectomy & appendectomy), as it provides better clinical outcomes including reducing blood loss, minimising trauma to the body, causing less post-operative pain and faster recovery (Velanovich, 2000;Wilson et al., 2014). Despite the benefits of MIS, surgeons lose direct vision and touch on the target, which decreases surgeon-patient transparency imposing technical challenges to the surgeon. These challenges have motivated the development of automatic techniques for the analysis of the surgical workflow (

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Visualisation of the surgical action triplet recognition task. We consider the tasks where the instrument (I), verb (V , action), and target (T , anatomical part) seek to be predicted.
Table 2: Figure 2 :2Figure2: Illustration of the main network structure, and how the adversarial perturbation is added to measure robustness.
Table 3: 3. 11DATASET DESCRIPTION AND EVALUATION PROTOCOLDataset Description. We use CholecT45 dataset(Nwoye & Padoy, 2022) to evaluate the robustness of the three SOTA models for the Surgical Action Triplet Recognition task. Specifically, CholecT45 dataset contains 45 videos with annotations including 6 classes of instrument, 10 classes of verb, and 15 classes of target (i.e. C I = 6, C V = 10, C T = 15) generating 900 (6 × 10 × 25) potential combinations for triplet labels. To maximise the clinical utility, we utilise the top-100 combinations of relevant labels, which are selected by removing a large portion of spurious combinations according to class grouping and surgical relevance rating(Nwoye et al., 2022). Each video contains around 2, 000 annotated frames extracted at 1 fps in RGB channels, leading to a total of 90, 489 recorded frames. To remove the redundant information, the frames captured after the laparoscope been taken out of the body are blacked out with value [0, 0, 0]. Evaluation Protocol. The triplet action recognition is evaluated by the average precision (AP) metric. Our models can directly output the predictions of triplet class AP IV T . Instead, AP d where d ∈ {I,V, T, IV, IT } cannot be predicted explicitly. Then we obtain the final predictions of d ∈ {I,V, T, IV, IT } components according to(Nwoye & Padoy, 2022;Nwoye et al., 2022): 
Table 4: where we calculate the probability of class k ∈ {0, 1, ..,C d − 1} under component d; and h d (•) maps the class m from IV T triplet compositions to the class under component d.
Table 5: Figure 3 :3Figure3: The set of figures shows robustness analysis on randomly selected images with a. the visualisation of the Top 15 percent of important features selected by the 2 explanation methods-Grad and IG; b. (/d.) the trends showing the robustness measured on the relevant S r (/irrelevant S r ) features been selected by the 2 explanation methods against the percentage of Top features been defined as relevant; c. the comparison of the robustness across the 4 backbones embedded in Rendezvous baseline.
Table 6: Performance comparison for the task of Triplet recognition. The results are reported in terms of Average Precision (AP%) on the CholecT45 dataset using the official cross-validation split.
Table 7: Heatmaps Comparison under different feature extraction backbones. We displayed four randomly selected images in fold 3 when using the best performed weights trained and validated on folds 1,2,4 and 5.
Table 8: Top 5 predicted Triplet classes in each of the 10 models. The top 5 is assessed by the AP IV T score.
Table 9: Robustness measured on 400 examples (i.e. images) randomly selected from the images in the fold 3 videos with exactly 1 labeled triplet. Top 25 percent of relevant S r or irrelevant S r features are selected from 2 explanation methods Grad and IG. We perform attacks on the selected 25 percent.
Table 10: Image ID: VID08-000188Ground Truth Label: 17:grasper,retract,gallbladder

[]
In that work, authors proposed a 3D interaction space for learning the triplets. In more recent work, the authors of Nwoye et al. (2022) introduced two new models. The first one is a direct extension of Tripnet called Attention Tripnet, where the novelty relies on a spatial attention mechanism. In the same work, the authors introduced another model called Rendezvous (RDV) that highlights a transformer-inspired neural network. We consider the tasks where the instrument (I), verb (V , action), and target (T , anatomical part) seek to be predicted.

[INSTRUMENT]
A commonality of existing surgical action triplet recognition techniques is the development of new mechanisms for improving the network architecture. However and despite the potential improvements, the performance of existing techniques is substantially lower than other tasks in surgical sciences-for example, force estimation and navigation assisted surgery. In this work, we go contrariwise existing techniques, and tackle the surgical action triplet recognition problem from the lens of robustness and explainability.
In the machine learning community there is a substantial increase of interest in understanding the lack of reliability of deep learning models (e.g., Ribeiro et al. (2016); Koh & Liang (2017); Sundararajan et al. (2017); Liu et al. (2019); Yeh et al. (2019); Hsieh et al. (2020)). To understand the lack of reliability of existing deep networks, a popular family of techniques is the so-called feature based explanations via robustness analysis (Simonyan et al., 2013;Zeiler & Fergus, 2014;Plumb et al., 2018;Wong et al., 2021;Singla & Feizi, 2021). Whilst existing techniques have extensively been evaluated for natural images tasks, there are no existing works addressing the complex problems as in action triplet recognition.
Contributions. In this work, we introduce, to the best of our knowledge, the first study to understand the failure of existing deep learning models for surgical action triplet recognition. To do this, we analyse the failures of existing state-of-the-art solutions through the lens of robustness. Specifically, we push to the limit the existing SOTA techniques for surgical action triplet recognition under weak and strong δ −perturbations. We then extensively analyse the failure modes via the evaluation criteria Robustness-S, which analyses the behaviour of the models through feature based explanations. Our study reveals the impact of core and spurious features for more robust models. Our study opens the door to more trustworthy and reliable deep learning models in surgical data science, which is imperative for MIS.

[METHODOLOGY]
We describe two key parts for Surgical action triplet recognition task: i) our experimental settings along with assumptions and ii) how we evaluate robustness via adversarial optimisation. The workflow of our work is displayed in Figure 2.

[SURGICAL ACTION TRIPLET RECOGNITION]
In the surgical action triplet recognition problem, the main task is to recognise the triplet IV T , which is the composition of three components during surgery: instrument (I), verb (V ), and target (T ) in a given RGB image x ∈ R H×W ×3 . Formally, we consider a given set of samples {(x n , y n )} N n=1 with provided labels Y = {0, 1, ..,C IV T − 1} for C IV T = 100 classes. We seek then to predict a function f : X → Y such that f gets a good estimate for the unseen data. That is, a given parameterised deep learning model takes the image x as input, and outputs a set of class-wise presence probabilities, in our case 100 classes, under the IV T composition, Y IV T ∈ R 100 , which we call it the logits of IV T . Since there are three individual components under the triplet composition, within the training network, we also considered the individual component d * ∈ {I,V, T }, each with class number C d * (i.e. C I = 6, C V = 10, C T = 15). The logits of each component, Y d * ∈ R C d * , are computed and used within the network.
In current state-of-the-art (SOTA) deep models (Nwoye et al., 2020;, there is a communal structure divided into three parts: i) the feature extraction backbone; ii) the individual component encoder; and iii) the triplet aggregation decoder that associate the components and output the logits of the IV T triplet. More precisely, the individual component encoder firstly concentrates on the instrument component to output Class Activation Maps (CAMs ∈ R H×W ×C d ) and the logits Y I of the instrument classes; the CAMs are then associated with the verb and target components separately for their logits (Y V and Y T ) to address the instrument-centric nature of the triplet.
The current SOTA techniques for surgical action triplet recognition focus on improving the components ii) & iii). However, the performance is still substantially lower than other surgical tasks. Our intuition behind such behaviour is due to the inherently complex and ambiguous conditions in MIS, which reflects the inability of the models to learn meaningful features. Our work is then based on the following modelling hypothesis.

[HYPOTHESIS 2.1: DEEP FEATURES ARE KEY FOR ROBUSTNESS]
Deep surgical techniques for triplet recognition lacks reliability due to the ineffective features. Therefore, the key to boosting performance, improving trustworthiness and reliability, and understanding failure of deep models is in the deep features. Following previous hypothesis, we address the questions of-why deep triplet recognition models fail? We do that by analysing the feature based explanations via robustness. To do this, we consider the current three SOTA techniques for our study: Tripnet (Nwoye et al., 2020), Attention Tripnet, andRendezvous (Nwoye et al., 2022). Moreover, we extensively investigate the repercussion of deep features using four widely used backbones ResNet-18, ResNet-50 (He et al., 2015), DenseNet-121 (Huang et al., 2016), and Swin Transformer (Liu et al., 2021). In the next section, we detail our strategy for analysing robustness.

[FEATURE BASED EXPLANATIONS VIA ROBUSTNESS]
Our models of the triplet recognition output the logits of triplets composition, we then use it to select our predicted label for the classification result. We define the model from image x to the predicted labelŷ as f : X → Y , where
X ⊂ R H×W ×3 , Y = {0, 1, 2, ...,C IV T − 1}.
For each class m ∈ Y and within each given sample, we seek to recognise core and spurious attributions (Singla & Feizi, 2021;, which definition is as follows.
Core Attributes: they refer to the features that form a part in the object we are detecting. Spurious Attributes: these are the ones that not a part of the object but co-occurs with it.
How We Evaluate Robustness? The body of literature has reported several alternatives for addressing the robustness of deep networks. Our work is motivated by recent findings on perturbation based methods, where even a small perturbation can significantly affect the performance of neural nets. In particular, we consider the setting of adversarial training (Allen-Zhu & Li, 2022;Olah et al., 2018;Engstrom et al., 2019) for robustify a given deep model.
The idea behind adversarial training for robustness is to enforce a given model to maintain its performance under a given perturbation δ . This problem can be seen cast as an optimisation problem over the network parameters θ as:
θ * = arg min θ E (x,y)∼D [L θ (x, y)].(1)
where E[L θ (•)] denotes the expected loss to the parameter θ .
One seeks to the model be resistant to any δ −perturbation. In this work, we follow a generalised adversarial training model, which reads:
Definition 2.1: Adversarial training under δ θ * = arg min θ E (x,y)∼D [max δ∈∆ L θ (x + δ, y)].
The goal is to the models do not change their performance even under the worse (strong) δ .
The machine learning literature has explored different forms of the generalised model in definition equation 2.1. For example, a better sparsity regulariser for the adversarial training as in (Xu et al., 2018). In this work, we adopt the evaluation criteria of that (Hsieh et al., 2020), where one seeks to measure the susceptibility of features to adversarial perturbations. More precisely, we can have an insight of the deep features extracted by our prediction through visualising compact set of relevant features selected by some defined explanation methods on trained models, and measuring the robustness of the models by performing adversarial attacks on the relevant or the irrelevant features.
We denote the set of all features as U, and consider a general set of feature S ⊆ U. Since the feature we are interested are those in the image x, we further denote the subset of S that related to the image as x S . To measure the robustness of the model, we rewrote the generalised model equation 2.1 following the evaluation criteria of that (Hsieh et al., 2020). A model on input x with adversarial perturbation on feature set S then reads:
Definition 2.2: Adversarial δ & Robustness-S ε * x S := {min δ ∥δ∥ p s.t. f (x + δ) ̸ = y, δ S = 0},
where y is the ground truth label of image x; ∥ • ∥ p denotes the adversarial perturbation norm; S = U \ S denotes the complementary set of feature S with δ S = 0 constraining the perturbation only happens on x S . We refer to ε * x S as Robustness-S (Hsieh et al., 2020), or the minimum adversarial perturbation norm on x S . We then denote the relevant features selected by the explanation methods as S r ⊆ U, with the irrelevant features as its complementary set S r = U \ S r . Thus, the robustness on chosen feature sets-S r and S r tested on image x are: 
Robustness-S r = ε * x Sr ; Robustness-S r = ε * x Sr .

[EXPERIMENTAL RESULTS]
In this section, we describe in detail the range of experiments that we conducted to validate our methodology.    In our robustness analysis, the main evaluation criteria is the robustness subject to the selected feature set (S r and S r ) on each backbone using the formula in equation 2.2.
Y d k = max m {Y IV T m }, ∀ m ∈ {0, 1..,C IV T − 1} s.t. h d (m) = k,

[IMPLEMENTATION DETAILS]
We evaluate the model performance based on five-fold cross-validation, where we split 45 full videos into 5 equal folds. The testing set is selected from these 5 folds, and we treat the remaining 4 folds as the training set. Moreover, 5 videos from the 36 training set videos are selected as validation set during training.
The models are trained using the Stochastic Gradient Descent (SGD) optimiser. The feature extraction backbones are initialised with ImageNet pre-trained weights. Both linear and exponential decay of learning rate are used during training, with initial learning rates as {1e −2 , 1e −2 , 1e −2 } for backbone, encoder and decoder parts respectively. We set the batch size as 32, and epoch which performs the best among all recorded epochs up to AP score saturation on validation set in the specified k-fold. To reduce computational load, the input images and corresponding segmentation masks are resized from 256 × 448 to 8 × 14. For fair comparison, we ran all SOTA models (following all suggested protocols from the official repository) under the same conditions and using the official cross-validation split of the CholecT45 dataset (Nwoye & Padoy, 2022).

[EVALUATION ON DOWNSTREAM TASKS]
In this section, we carefully analyse the current SOTA techniques for triplet recognition from the feature based explainability lens.
Results on Triplet Recognition with Cross-Validation. As first part of our analysis, we investigate the performance limitation on current SOTA techniques, and emphasise how such limitation is linked to the lack of reliable features. The results are reported in Table 1. In a closer look at the results, we observe that ResNet-18, in general, performs the worst among the compared backbones. However, we can observe that for one case, component analysis, it performs better than ResNet-50 under Tripnet Attention baseline. The intuition being such behaviour is that the MIS setting relies on ambiguous condition and, in some cases, some frames might contain higher spurious features that are better captured by it. We remark that the mean and standard-deviation in Table 1 are calculated from the 5 folds in each combination of backbone and baseline.
We also observe that ResNet-50 performs better than ResNet-18 due to the deeper feature extraction. The best performance, for both the tasks-component detection and triplet association, is reported by DenseNet-121. The intuition behind the performance gain is that DenseNet-121 somehow mitigates the issue of the limitation of the capability representation. This is because ResNet type networks are limited by the identity shortcut that stabilises training. These results support our modelling hypothesis that the key of performance is the robustness of the deep features.
A key finding in our results is that whilst existing SOTA techniques (Nwoye & Padoy, 2022;Nwoye et al., 2022) are devoted to developing new network mechanisms, one can observe that a substantial performance improvement when improving the feature extraction. Moreover and unlike other surgical tasks, current techniques for triplet recognition are limited in performance. Why is this happening? Our results showed that the key is in the reliable features (linked to robustness); as enforcing more meaningful features, through several backbones, a significant performance improvement over all SOTA techniques is observed.
To further support our previous findings, we also ran a set of experiments using the trending principle of Transformers. More precisely, an non CNN backbone-the tiny Swin Transformer (Swin-T) (Liu et al., 2021) has also been tested on the Rendezvous, which has rather low AP scores on all of the 6 components in oppose to the 3 CNN backbones. This could be led by the shifted windows in the Swin-T, it is true that the shifted windows largely reduced the computational cost, but this could lead to bias feature attribute within bounding boxes, the incoherent spreading can be seen clearly in the visualisation of detected relevant features in Swin-T in Figure 3 (a).
In Table 1 we displayed the average results over all classes but-what behaviour can be observed from the per-class performance? It can be seen from Table 3 that though the best 5 predicted classes are different in each model, the predicted compositions seem clinically sensible supporting our previous discussion. In addition, the top 1 per-class AP score is significantly higher in DenseNet-121 with Rendezvous.
Visualisation Results. To interpret features is far from being trivial. To address this issue, we provide a human-like comparison via heatmaps in Table 2. The implementation of the heatmaps is adapted from (Zhou et al., 2016). The displayed outputs reflect what the model is focusing based on the extracted features. These results support our hypothesis that deep features are the key in making correct predictions over any new network mechanism.
We observed that in the worst performed backbone-Swin-T, the feature been extracted are mostly spread across the images, however, the ones that concentrate on core attributes are not though performed the best. In the best performed DenseNet-121, a reasonable amount of attention are also been paid to spurious attributes; this can be seen more directly in our later discussion on robustness visualisation Figure 3.
The reported probability on the predicted label emphasises again the outstanding performance of DenseNet-121 backbone; in the sense that, the higher the probability for the correct label the better, the lower it is for incorrect prediction the better.
Why Surgical Triplet Recognition Models Fail? Robustness and Interpretability. We further support our findings through the lens of robustness. We use as evaluation criteria Robustness-S r and Robustness-S r with different explanation methods: vanilla gradient (Grad) (Shrikumar et al., 2017) and integrated gradient (IG) (Sundararajan et al., 2017). The results are in Table 4 & Figure 3. 

[COMPARISON BETWEEN DIFFERENT BACKBONES]
In Table 4, we show the robustness results with top 25% attacked features on the average over 400 frames randomly chosen with exactly 1 labeled triplet. On one hand, we observe that the DenseNet-121 backbone consistently outperforms other network architectures on both evaluation criteria Robustness-S r and Robustness-S r . This suggests that DenseNet-121 backbone does capture different explanation characteristics which ignored by other network backbones. On the other hand, our results are supported by the finding in (Hsieh et al., 2020), as IG performs better than Grad; and the attack on relevant features yields lower robustness than perturbing the same percentage of irrelevant features.

[ROBUSTNESS EXPLANATION FOR SPECIFIC IMAGES]
To more objectively evaluate the robustness explanation for specific images, we show: (a) Visualisation of important features, (b) Robustness-S r , (c) Robustness against the percentage of Top features, and (d) Robustness-S r in Figure 3. In Figure 3 (a), we visualise the Top 15% features (with yellow dots) by Grad and IG, respectively, and overlay it on manually labelled region containing instrument (in red) and target (in green). We observe that the best performed backbone (can be seen from the robustness comparison curves in Figure 3 (c)) on the specific image is the one that not only pays attention to core attributes, but also the spurious attribute. In the image VID08-000188, the best performed model is ResNet-18, which shows the ambiguous condition on individual images. In a closer look at Figure 3 (a), a small portion of the most relevant feature extracted by ResNet-18 is  spread not on the close surrounding of the object area. This importance of spurious attribute is further highlighted in image VID18-001156. We observe that DenseNet-121 provides the most robust result highlighting relevant features within the tissue region and across tool tip. The worst performed model-ResNet-18 merely treated the core attributes as relevant.
The relevant role of spurious attributes can be explained by the nature of the triplet, which consists a verb component that is not the physical object. Overall, we observe that reliable deep features are the key for robust models in triplet recognition. Moreover, we observe, unlike existing works of robustness against spurious features, that both core and spurious attributes are key for the prediction.

[CONCLUSION]
We present the first work to understand the failure of existing deep learning models for the task of triplet recognition. We provided an extensive analysis through the lens of robustness. The significance of our work lies on understanding and addressing the key issues associated with the substantially limited in performance of existing techniques. Our work offers a step forward to more trustworthy and reliable models.","[TITLE]
WHY DEEP SURGICAL MODELS FAIL?: REVISITING SURGICAL ACTION TRIPLET RECOGNITION THROUGH THE LENS OF ROBUSTNESS

[ABSTRACT]
Surgical action triplet recognition provides a better understanding of the surgical scene. This task is of high relevance as it provides the surgeon with contextaware support and safety. The current go-to strategy for improving performance is the development of new network mechanisms. However, the performance of current state-of-the-art techniques is substantially lower than other surgical tasks. Why is this happening? This is the question that we address in this work. We present the first study to understand the failure of existing deep learning models through the lens of robustness and explainability. Firstly, we study current existing models under weak and strong δ −perturbations via an adversarial optimisation scheme. We then analyse the failure modes via feature based explanations. Our study reveals that the key to improving performance and increasing reliability is in the core and spurious attributes. Our work opens the door to more trustworthy and reliable deep learning models in surgical data science. https://yc443.github.io/robustIVT/Minimally Invasive Surgery (MIS) has become the gold standard for several procedures (i.e., cholecystectomy & appendectomy), as it provides better clinical outcomes including reducing blood loss, minimising trauma to the body, causing less post-operative pain and faster recovery (Velanovich, 2000;Wilson et al., 2014). Despite the benefits of MIS, surgeons lose direct vision and touch on the target, which decreases surgeon-patient transparency imposing technical challenges to the surgeon. These challenges have motivated the development of automatic techniques for the analysis of the surgical workflow (","{
    ""1"": {
        ""summary"": ""The paper's focus on only the 'surgical action triplet recognition' task may limit its generalizability to all deep surgical models."",
        ""verbatim"": ""the study only focuses on the task of 'surgical action triplet recognition' which may not generally reflect all 'deep surgical models'""
    },
    ""2"": {
        ""summary"": ""The study might overly simplify the causes of poor performance in deep learning models by mainly attributing failures to unreliable deep features."",
        ""verbatim"": ""there can be a variety of reasons that contributes to the poor performance of deep learning models and the unreliability of the extracted deep features may not be the only answer to the question of why deep surgical models fail""
    },
    ""3"": {
        ""summary"": ""The conclusions appear to be based on intuition rather than strong empirical data or experimental evidence."",
        ""verbatim"": ""the analysis and conclusion in this paper (Section 3.3) tend to be made by intuition rather than based on its experimental data or empirical evidence, and the provided experimental results may not rigorously prove the hypothesis that 'the failure of the existing deep learning model in this task results from the lack of reliability in the deep features'""
    },
    ""4"": {
        ""summary"": ""The study predominantly uses baseline methods from the same group of authors, possibly not representing a broader view of the field's common issues."",
        ""verbatim"": ""all the baseline methods used in this study originate from nearly the same groups of authors, which is not very common and may not generally reflect the common problems of this field.""
    }
}",4,"{
    ""1"": {
        ""summary"": ""The models exhibit substantially lower performance compared to other surgical tasks."",
        ""verbatim"": ""The paper acknowledges the substantially lower performance of existing techniques compared to other surgical tasks, indicating a need for further improvements in model performance for wider acceptance.""
    },
    ""2"": {
        ""summary"": ""The evaluation's applicability is limited due to reliance on specific datasets and models."",
        ""verbatim"": ""The evaluation and findings are based on specific datasets and models, which may limit the generalizability of the results to broader contexts or different datasets.""
    },
    ""3"": {
        ""summary"": ""The study's focus is narrow, only analyzing deep features for surgical action triplet recognition."",
        ""verbatim"": ""The study focuses solely on the analysis of deep features for surgical action triplet recognition, potentially lacking a broader perspective on other factors influencing model performance.""
    },
    ""4"": {
        ""summary"": ""The comparison within the paper could benefit from including a wider range of baseline models."",
        ""verbatim"": ""While the paper compares different backbones and explanation methods, a more comprehensive comparison with a wider range of baseline models could provide a deeper understanding of the effectiveness of the proposed approach.""
    }
}",4,"{
    ""A1-B3"": {
        ""rationale"": ""Both points focus on the narrow scope of the study, specifically emphasizing the exclusive analysis of the 'surgical action triplet recognition' in deep learning models. This specialization limits perspective on broader issues and applications in the field."",
        ""similarity"": ""9""
    },
    ""A2-B1"": {
        ""rationale"": ""Review A and B both question the performance of the models, although A discusses the reasons for this poor performance and B more generally mentions the overall lower success of these models compared to other tasks, each points to a fundamental issue in effectiveness."",
        ""similarity"": ""7""
    },
    ""A3-B2"": {
        ""rationale"": ""Both points criticize the empirical basis of the study's conclusions. A highlights the reliance on intuition and insufficient empirical evidence, while B mentions the limited generalizability of the findings due to specific dataset and model reliance."",
        ""similarity"": ""7""
    },
    ""A4-B4"": {
        ""rationale"": ""Both points express concerns over the range of models or methods used in the study. A notes the use of baseline methods from the same author groups, limiting the diversity of perspectives, while B suggests that including a wider range of baseline models could enhance understanding."",
        ""similarity"": ""8""
    }
}",4,"{
    ""1"": {
        ""summary"": ""The paper needs more comprehensive empirical validation."",
        ""verbatim"": ""There might be a need for more comprehensive empirical validation of the proposed method to ensure its effectiveness across a wider range of surgical scenarios and datasets.""
    },
    ""2"": {
        ""summary"": ""The paper lacks detailed comparisons with other existing methods."",
        ""verbatim"": ""The paper could be improved by providing more detailed comparisons with other existing methods in the field to highlight the advantages of the proposed approach.""
    }
}",2,"{
    ""A3-B1"": {
        ""rationale"": ""Both Review A point 3 and Review B point 1 express concerns about the lack of strong empirical data or evidence supporting the study's conclusions and effectiveness. Review A specifically notes that the conclusions in the paper appear to be based more on intuition than empirical evidence, while Review B highlights a need for more comprehensive empirical validation overall. Both points suggest that the evidence provided in the paper is insufficient, thereby questioning the reliability and generalizability of the study's findings."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper may be rejected due to insufficient or lacking rigor in its experimental validation."",
        ""verbatim"": ""<Incomplete experimental validation> The paper may face rejection if the experimental validation of the proposed approach and its results is insufficient or lacking in rigor.""
    },
    ""2"": {
        ""summary"": ""The paper might be rejected if it fails to provide an adequate comparison with existing models or techniques in the field."",
        ""verbatim"": ""<Limited comparison with existing models> If the paper does not adequately compare its findings with existing models or techniques in the field, it might be deemed less impactful and thus face rejection.""
    }
}",2,"{
    ""A3-B1"": {
        ""rationale"": ""Both points criticize the paper for lacking rigor in empirical evidence and experimental validation. Review A notes that conclusions are based more on intuition than empirical data, while Review B highlights the possibility of rejection due to insufficient experimental validation."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Insufficient comparison with baseline methods"",
        ""verbatim"": ""'Limited Comparison with Baselines': While the study introduces new insights, it may not thoroughly benchmark against other non-robustness-focused methods or explore the integration with other state-of-the-art techniques which might lead to a comparative analysis gap.""
    },
    ""2"": {
        ""summary"": ""Complex concepts may limit accessibility to practitioners"",
        ""verbatim"": ""'Complexity and Accessibility': The heavy focus on robustness and adversarial training might make the paper less accessible to practitioners not familiar with these concepts, potentially limiting its immediate impact in practical applications.""
    },
    ""3"": {
        ""summary"": ""Generalization of results may be limited"",
        ""verbatim"": ""'Generalization of Results': The experiments are restricted to one dataset, and insights derived may not generalize well across other datasets or real-world settings, which might be seen by reviewers as a scalability and applicability limitation.""
    }
}",3,"{
    ""A1-B3"": {
        ""rationale"": ""Both points highlight concerns about the limited application or generalizability of the study's results. Review A criticizes the focus on a specific task, suggesting it may not reflect all deep surgical models, while Review B notes the experiments are confined to one dataset, hinting at potential issues with scalability and general applicability across diverse datasets."",
        ""similarity"": ""7""
    },
    ""A4-B1"": {
        ""rationale"": ""Point A4 and B1 both express concerns over the choice of baseline methods. A4 notes all baseline methods are from nearly the same group, which might not effectively reflect the field's common issues, whereas B1 argues that the study lacks a comprehensive benchmark against a variety of baseline methods, potentially leading to a gap in comparative analysis."",
        ""similarity"": ""8""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks comparative analysis with other state-of-the-art models."",
        ""verbatim"": ""The paper does not seem to provide a comparative analysis with other state-of-the-art models in a similar context, which could make it difficult to gauge the relative improvement or uniqueness of the proposed insights.""
    },
    ""2"": {
        ""summary"": ""Details on the experimental setup are insufficient."",
        ""verbatim"": ""There is a lack of detailed specifics regarding the experimental setups, such as the dataset used, the exact nature of the δ-perturbations, and the statistical significance of the results. This could undermine the credibility and reproducibility of the research.""
    },
    ""3"": {
        ""summary"": ""Limited discussion on solutions to the identified model failures."",
        ""verbatim"": ""While the paper excellently identifies and explains the failure modes, it seems to lack a robust discussion or presentation of specific solutions or improvements to the existing models, which might be seen as incomplete research to some reviewers.""
    }
}",3,"{
    ""A2-B3"": {
        ""rationale"": ""Both points discuss the limitations in the study's approach towards resolving issues in model performance. Review A emphasizes the possible oversimplification in attributing failures to unreliable deep features, hinting at a lack of comprehensive exploration of failure causes. Review B highlights the lack of discussion on solutions for identified model failures, suggesting a similar incompleteness in addressing the root causes and overcoming them."",
        ""similarity"": ""6""
    },
    ""A3-B2"": {
        ""rationale"": ""Both points raise concerns about the evidence or detail supporting the study's conclusions. Review A claims that conclusions are based more on intuition rather than robust experimental data, while Review B points to insufficient details in the experimental setup that could undermine the credibility and reproducibility of the research. Both are concerned with the solidity of the study's empirical foundation."",
        ""similarity"": ""8""
    },
    ""A4-B1"": {
        ""rationale"": ""Both points question the breadth of the study's comparison and baseline methods. Review A highlights the limited scope by using baseline methods almost exclusively from the same group of authors, which may not capture the broader problems of the field. Review B criticizes the absence of a comparative analysis with other state-of-the-art models, which restricts understanding of the proposed method’s relative advancement or novelty."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Limited applicability across various surgical procedures"",
        ""verbatim"": ""Limited scope in terms of surgical procedures""
    },
    ""2"": {
        ""summary"": ""Integration and impact on current surgical practices not addressed"",
        ""verbatim"": ""Technical challenges unaddressed""
    }
}",2,{},0,"[Significance and novelty]
1. The paper investigates a crucial problem in the field of surgical data science: the failure of deep learning models for surgical action triplet recognition despite their potential benefits in providing context-aware support and safety for surgeons.
2. The study presents the first examination of this issue through the lens of robustness and explainability, contributing new insights into the challenges and potential solutions for creating more trustworthy and reliable models.
3. The findings reveal the importance of addressing core and spurious attributes in deep learning models to achieve improved performance and reliability, shedding light on a significant research avenue in the field.

[Potential reasons for acceptance]
1. The research addresses a significant issue in surgical data science, providing insights into the reasons behind the lower performance and unreliability of deep learning models for surgical action triplet recognition compared to other tasks.
2. The investigative methodology, which includes weak and strong δ−perturbations via adversarial optimization and feature-based explanations, adds valuable depth to the study and provides concrete evidence for the identified failure modes and underlying causes.
3. The proposed focus on core and spurious attributes in deep learning models offers a promising avenue for enhancing confidence in surgical AI systems and increasing their acceptance and implementation in clinical settings.

[Potential reasons for rejection]
1. The study might be rejected if the research community does not acknowledge or perceive the identified issue of lower performance and unreliability in current deep learning models for surgical action triplet recognition as significant or pressing enough.
2. If the methodology is deemed unsatisfactory by reviewers, including concerns about the selection or validation of δ−perturbations, the applicability of feature-based explanations, or the adequacy of the adversarial optimization scheme, the manuscript may face rejection.
3. The paper may lack sufficient comparisons or benchmarks against other state-of-the-art techniques, or may not provide a clear enough connection between the findings and potential solutions that can be implemented in practice, which could negatively impact its acceptance prospects.

[Suggestions for improvement]
1. Expand the experimental section to include more comprehensive comparisons with other state-of-the-art methods in surgical action triplet recognition, thereby establishing a clearer context within the literature and emphasizing the significance of the findings.
2. Conduct a sensitivity analysis to investigate the impact of different δ−perturbation magnitudes and thresholds, ensuring the robustness of the methodology and enriching the study's results.
3. Introduce a discussion on specific practical solutions based on the research findings, detailing how the insights gained from this study can be implemented in real-world surgical applications and improving the overall reliability and robustness of deep learning models for surgical action triplet recognition.","[Significance and novelty]
<Deep dive into a challenging problem> The paper addresses an interesting and challenging problem of surgical action triplet recognition, shedding light on the robustness and reliability of the state-of-the-art deep learning models.
<Introduction of new approaches and metrics> The paper introduces new robustness metrics, such as δ-correctness and δ-precision, and explores the attribution-based method and adversarial example attacks to improve understanding of the model behavior.

[Potential reasons for acceptance]
<In-depth analysis of model failures> The paper provides a detailed analysis of failure cases, highlighting the limitations of existing deep learning models and the need for robustness and explainability in surgical applications.
<Development of new robustness metrics> The introduction of δ-correctness and δ-precision as new robustness metrics contributes to the advancement of evaluation criteria for surgical action triplet recognition.

[Potential reasons for rejection]
<Unclear technical details> Some reviewers expressed confusion about technical details, such as the implementation of the feature attribution method, lack of comparison with baseline methods, and the definition of attack-robustness.
<Lack of validation of claimed contributions> Reviewers questioned the effectiveness and validity of the proposed contributions, especially in demonstrating the superiority of robust predictions.

[Suggestions for improvement]
<Clarity in technical explanations> The authors should provide clearer and detailed explanations of the technical aspects, such as the feature attribution method, implementation details, and evaluation metrics, to enhance the understanding for readers and reviewers.
<Validation of contributions> To address the concerns raised by reviewers, the authors should provide more comprehensive evidence and validation of the claimed contributions, especially in demonstrating the superiority of robust predictions over other baselines.

","{
    ""1"": {
        ""summary"": ""Perception of the issue's significance in the research community"",
        ""verbatim"": ""The study might be rejected if the research community does not acknowledge or perceive the identified issue of lower performance and unreliability in current deep learning models for surgical action triplet recognition as significant or pressing enough.""
    },
    ""2"": {
        ""summary"": ""Methodological concerns regarding perturbations and explanations"",
        ""verbatim"": ""If the methodology is deemed unsatisfactory by reviewers, including concerns about the selection or validation of δ−perturbations, the applicability of feature-based explanations, or the adequacy of the adversarial optimization scheme, the manuscript may face rejection.""
    },
    ""3"": {
        ""summary"": ""Lack of sufficient benchmarks and unclear connection to practical solutions"",
        ""verbatim"": ""The paper may lack sufficient comparisons or benchmarks against other state-of-the-art techniques, or may not provide a clear enough connection between the findings and potential solutions that can be implemented in practice, which could negatively impact its acceptance prospects.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Reviewers found technical details unclear"",
        ""verbatim"": ""Some reviewers expressed confusion about technical details, such as the implementation of the feature attribution method, lack of comparison with baseline methods, and the definition of attack-robustness.""
    },
    ""2"": {
        ""summary"": ""Lack of validation for the effectiveness of the proposed contributions"",
        ""verbatim"": ""Reviewers questioned the effectiveness and validity of the proposed contributions, especially in demonstrating the superiority of robust predictions.""
    }
}",2,"{
    ""A2-B2"": {
        ""rationale"": ""Both points discuss methodological concerns regarding the validity and adequacy of the mechanisms used in these deep learning models. Review A hints at the poor performance being not solely attributed to unreliable features, suggesting broader methodological issues, which aligns with Review B's direct criticism of the methods like perturbations and adversarial schemes."",
        ""similarity"": ""7""
    },
    ""A3-B3"": {
        ""rationale"": ""Review A argues that the conclusions in the paper are based on intuition rather than empirical data, which infers a lack of rigorous experimental backing or benchmarking. Review B's point about lacking sufficient benchmarks and a clear connection to practical solutions ties closely to this, as both points essentially criticize the paper for not sufficiently grounding its claims in concrete data or comparative analysis."",
        ""similarity"": ""8""
    }
}",2,"{
    ""A3-B2"": {
        ""rationale"": ""Both points express concerns about the validity and support for the key claims made in the study. Review A points to the conclusions being based on intuition rather than empirical data, and Review B questions the effectiveness and validity of the proposed contributions. Both critiques highlight a lack of rigorous evidence supporting the paper's claims."",
        ""similarity"": ""8""
    }
}",1
AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,S1x0CnEtvB.json,"[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: p-AutoGrow on CIFAR10 (K = 3).The seed net is Basic3ResNet-1-1-1.
Table 2: Figure 3 :Figure 4 :34Figure 3: AutoGrow vs manual search obtained by training many baselines from scratch. x − axis is the number of parameters. Dataset is CIFAR10.
Table 3: Figure 5 :Figure 6 :56Figure6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table3, which is much smoother compared to Figure6(a). Figure6(c)(d) visualize the trajectories of p-AutoGrow with K = 50 and 3. The 2D projection gives limited information to reveal the advantages of p-AutoGrow
Table 4: Figure 7 :7Figure 7: Loss surfaces around minima found by baselines and AutoGrow. Dataset is CIFAR10.
Table 5: Figure 8:The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table10.
Table 6: A simple example of AutoGrow.
Table 7: Comparison with previous works about layer growth.
Table 8: Network Morphism tested on CIFAR10.
Table 9: Ablation study of c-AutoGrow.
Table 10: p-AutoGrow with different seed architecture.
Table 11: p-AutoGrow with different growing interval K.
Table 12: The adaptability of AutoGrow to datasets
Table 13: Scaling up to ImageNet
Table 14: TaoWei, Changhu Wang, and Chang Wen Chen.  Modularized morphing of neural networks. arXiv preprint arXiv:1701.03281, 2017. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pp. 2074-2082, 2016. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.
Table 15: p-AutoGrow under initializers with K = 3.
Table 16: AutoGrow improves accuracy of plain nets.
Table 17: The efficiency of AutoGrow
Table 18: The adaptability of AutoGrow to dataset sizes

[INTRODUCTION]
Layer depth is one of the decisive factors of the success of Deep Neural Networks (DNNs). For example, image classification accuracy keeps improving as the depth of network models grows (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014;Szegedy et al., 2015;He et al., 2016;Huang et al., 2017). Although shallow networks cannot ensure high accuracy, DNNs composed of too many layers may suffer from over-fitting and convergence difficulty in training. How to obtain the optimal depth for a DNN still remains mysterious. For instance, ResNet-152 (He et al., 2016) uses 3, 8, 36 and 3 residual blocks under output sizes of 56 × 56, 28 × 28, 14 × 14 and 7 × 7, respectively, which don't show an obvious quantitative relation. In practice, people usually reply on some heuristic trials and tests to obtain the depth of a network: they first design a DNN with a specific depth and then train and evaluate the network on a given dataset; finally, they change the depth and repeat the procedure until the accuracy meets the requirement. Besides the high computational cost induced by the iteration process, such trial & test iterations must be repeated whenever dataset changes. In this paper, we propose AutoGrow that can automate depth discovery given a layer architecture. We will show that AutoGrow generalizes to different datasets and layer architectures.
There are some previous works which add or morph layers to increase the depth in DNNs. Vg-gNet (Simonyan & Zisserman, 2014) and DropIn (Smith et al., 2016) added new layers into shallower DNNs; Network Morphism (Wei et al., 2016;Chen et al., 2015) morphed each layer to multiple layers to increase the depth meanwhile preserving the function of the shallower net. Table 1 summarizes differences in this work. Their goal was to overcome difficulty of training deeper DNNs or accelerate it. Our goal is to automatically find an optimal depth. Moreover, previous works applied layer growth by once or a few times at pre-defined locations to grow a pre-defined number of layers; in contrast, ours automatically learns the number of new layers and growth locations without limiting growing times. We will summarize more related works in Section 4. Figure 1 illustrates an example of AutoGrow. It starts from the shallowest backbone network and gradually grows sub-modules (A sub-module can be one or more layers, e.g., a residual block); the growth stops once a stopping policy is satisfied. We studied multiple initializers of new layers and multiple growing policies, and surprisingly find that: (1) a random initializer works equally or better than complicated Network Morphism; (2) it is more effective to grow before a shallow net converges. We hypothesize that this is because a converged shallow net is an inadequate initialization for training deeper net, while random initialization can help to escape from a bad starting point.
Motivated by this, we intentionally avoid full convergence during the growing by using (1) random initialization of new layers, (2) a constant large learning rate, and (3) a short growing interval.  (3) We challenge the idea of Network Morphism, as random initialization works equally or better when growing layers. (4) We find that it is beneficial to rapidly grow layers before a shallower net converge, contradicting previous intuition.
2 AutoGrow -A DEPTH GROWING ALGORITHM Algorithm 1 AutoGrow Algorithm.
Input :
A seed shallow network g(X0) composed of M sub-networks F = {fi (•; Wi) : i = 0 . . . M − 1},
where each sub-network has only one sub-module (a dimension reduction sub-module); an epoch interval K to check growing and stopping policies; the number of fine-tuning epochs N after growing.

[INITIALIZATION:]
A Circular Linked List of sub-networks under growing:
subNetList = f0 (•; W0) → • • • → f M −1 (•; W M −1 ) ← −−−−−−−−−−−−−−−−−−−−−−−−−− − ;
The current growing sub-network: growingSubNet = subNetList.head() = f0 (•; W0);
The recent grown sub-network: grownSubNet = None; Process : A trained neural network g(X0) with learned depth.
#
Figure 1 gives an overview of the proposed AutoGrow. In this paper, we use network, sub-networks, sub-modules and layers to describe the architecture hierarchy. A network is composed of a cascade of sub-networks. A sub-network is composed of sub-modules, which typical share the same output size. A sub-module (e.g. a residual block) is an elementary growing block composed of one or a few layers. In this section, we rigorously formulate a generic version of AutoGrow which will be materialized in subsections. A deep convolutional network g(X 0 ) is a cascade of sub-networks by composing functions as g(X
0 ) = l (f M −1 (f M −2 (• • • f 1 (f 0 (X 0 )) • • • )))
, where X 0 is an input image, M is the number of sub-networks, l(•) is a loss function, and X i+1 = f i (X i ) is a sub-network that operates on an input image or a feature tensor X i ∈ R ci×hi×wi . Here, c i is the number of channels, and h i and w i are spatial dimensions. f i (X i ) is a simplified notation of f i (X i ; W i ), where W i is a set of sub-modules' parameters within the i-th sub-network. Thus W = {W i : i = 0 . . . M − 1} denotes the whole set of parameters in the DNN. To facilitate growing, the following properties are supported within a sub-network: (1) the first sub-module usually reduces the size of input feature maps, e.g., using pooling or convolution with a stride; and (2) all sub-modules in a sub-network maintain the same output size. As such, our framework can support popular networks, including VggNet-like plain networks (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2015), ResNets (He et al., 2016) and DenseNets (Huang et al., 2017). In this paper, we select ResNets and VggNet-like nets as representatives of DNNs with and without shortcuts, respectively.
With above notations, Algorithm 1 rigorously describes the AutoGrow algorithm. In brief, AutoGrow starts with the shallowest net where every sub-network has only one sub-module for spatial dimension reduction. AutoGrow loops over all growing sub-networks in order. For each sub-network, AutoGrow stacks a new sub-module. When the new sub-module does not improve the accuracy, the growth in corresponding sub-network will be permanently stopped. The details of our method will be materialized in the following subsections.

[SEED SHALLOW NETWORKS AND SUB-MODULES]
In this paper, in all datasets except ImageNet, we explore growing depth for four types of DNNs: (1) Basic3ResNet: the same ResNet used for CIFAR10 in He et al. (2016) In AutoGrow, the architectures of seed shallow networks and sub-modules are pre-defined. In plain DNNs, a sub-module is a stack of convolution, Batch Normalization and ReLU; in residual DNNs, a sub-module is a residual block. In AutoGrow, a sub-network is a stack of all sub-modules with the same output spatial size. Unlike He et al. (2016) which manually designed the depth, AutoGrow starts from a seed architecture in which each sub-network has only one sub-module and automatically learns the number of sub-modules.
On ImageNet, we apply the same backbones in He et al. (2016) as the seed architectures. A seed architecture has only one sub-module under each output spatial size. For a ResNet using basic residual blocks or bottleneck residual blocks (He et al., 2016), we respectively name it as Basic4ResNet or Bottleneck4ResNet. Plain4Net is also obtained by removing shortcuts in Basic4ResNet.

[SUB-MODULE INITIALIZERS]
Here we explain how to initialize a new sub-module W in initializer(W) mentioned in Algorithm 1. Network Morphism changes DNN architecture meanwhile preserving the loss function via special initialization of new layers, that is, g(X 0 ; W) = g(X 0 ; W ∪ W) ∀X 0 . A residual sub-module shows a nice property: when stacking a residual block and initializing the last Batch Normalization layer as zeros, the function of the shallower net is preserved but the DNN is morphed to a deeper net. Thus, Network Morphism can be easily implemented by this zero initialization (ZeroInit).
In this work, all layers in W are initialized using default randomization, except for a special treatment of the last Batch Normalization layer in a residual sub-module. Besides ZeroInit, we propose a new AdamInit for Network Morphism. In AdamInit, we freeze all parameters except the last Batch Normalization layer in W, and then use Adam optimizer (Kingma & Ba, 2014) to optimize the last Bath Normalization for maximum 10 epochs till the training accuracy of the deeper net is as good as the shallower one. After AdamInit, all parameters are jointly optimized. We view AdamInit as a Network Morphism because the training loss is similar after AdamInit. We empirically find that AdamInit can usually find a solution in less than 3 epochs. We also study random initialization of the last Batch Normalization layer using uniform (UniInit) or Gaussian (GauInit) noises with a standard deviation 1.0. We will show that GauInit obtains the best result, challenging the idea of Network Morphism (Wei et al., 2016;Chen et al., 2015).

[GROWING AND STOPPING POLICIES]
In Algorithm 1, a growing policy refers to meetGrowingPolicy(), which returns true when the network should grow a sub-module. Two growing policies are studied here:
1. Convergent Growth: meetGrowingPolicy() returns true when the improvement of validation accuracy is less than τ in the last K epochs. That is, in Convergent Growth, AutoGrow only grows when current network has converged. This is a similar growing criterion adopted in previous works (Elsken et al., 2017;Cai et al., 2018a;b). 2. Periodic Growth: meetGrowingPolicy() always returns true, that is, the network always grows every K epochs. Therefore, K is also the growing period. In the best practice of AutoGrow, K is small (e.g. K = 3) such that it grows before current network converges.
Our experiments will show that Periodic Growth outperforms Convergent Growth. We hypothesize that a fully converged shallower net is an inadequate initialization to train a deeper net. We will perform experiments to test this hypothesis and visualize optimization trajectory to illustrate it.
A stopping policy denotes meetStoppingPolicy() in Algorithm 1. When Convergent Growth is adopted, meetStoppingPolicy() returns true if a recent growth does not improve validation accuracy more than τ within K epochs. We use a similar stopping policy for Periodic Growth; however, as it can grow rapidly with a small period K (e.g. K = 3) before it converges, we use a larger window size J for stop. Specifically, when Periodic Growth is adopted, meetStoppingPolicy() returns true when the validation accuracy improves less than τ in the last J epochs, where J K.
Hyper-parameters τ , J and K control the operation of AutoGrow and can be easily setup and generalize well. τ denotes the significance of accuracy improvement for classification. We simply set τ = 0.05% in all experiments. J represents how many epochs to wait for an accuracy improvement before stopping the growth of a sub-network. It is more meaningful to consider stopping when the new net is trained to some extent. As such, we set J to the number of epochs T under the largest learning rate when training a baseline. K means how frequently AutoGrow checks the polices. In Convergent Growth, we simply set K = T , which is long enough to ensure convergence. In Periodic Growth, K is set to a small fraction of T to enable fast growth before convergence; more importantly, K = 3 is very robust to all networks and datasets. Therefore, all those hyper-parameters are very robust and strongly correlated to design considerations.

[EXPERIMENTS]
In this paper, we use Basic3ResNet-2-3-2, for instance, to denote a model architecture which contains 2, 3 and 2 sub-modules in the first, second and third sub-networks, respectively. Sometimes we simplify it as 2-3-2 for convenience. AutoGrow always starts from the shallowest depth of 1-1-1 and uses the maximum validation accuracy as the metric to guide growing and stopping. All DNN baselines are trained by SGD with momentum 0.9 using staircase learning rate. The initial learning rate is 0.1 in ResNets and 0.01 in plain networks. On ImageNet, baselines are trained using batch size 256 for 90 epochs, within which learning rate is decayed by 0.1× at epoch 30 and 60. In all other smaller datasets, baselines are trained using batch size 128 for 200 epochs and learning rate is decayed by 0.1× at epoch 100 and 150.
Our early experiments followed prior wisdom by growing layers with Network Morphism (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b), i.e., AutoGrow with ZeroInit (or AdamInit) and Convergent Growth policy; however, it stopped early with very shallow DNNs, failing to find optimal depth. We hypothesize that a converged shallow net with Network Morphism gives a bad initialization to train a deeper neural network. Section 3.1 experimentally test that the hypothesis is valid. To tackle this issue, we intentionally avoid convergence during growing by three simple solutions, which are evaluated in Section 3.2. Finally, Section 3.3 and Section 3.4 include extensive experiments to show the effectiveness of our final AutoGrow.

[SUBOPTIMUM OF NETWORK MORPHISM AND CONVERGENT GROWTH]
In this section, we study Network Morphism itself and its integration into our AutoGrow under Convergent Growth. When studying Network Morphism, we take the following steps: 1) train a shallower ResNet to converge, 2) stack residual blocks on top of each sub-network to morph to a deeper net, 3) use ZeroInit or AdamInit to initialize new layers, and 4) train the deeper net in a standard way. We compare the accuracy difference (""∆"") between Network Morphism and training the deeper net from scratch. Table 2 summaries our results. Network Morphism has a lower accuracy (negative ""∆"") in all the cases, which validates our hypothesis that a converged shallow network with Network Morphism gives a bad initialization to train a deeper net. We visualize the optimization trajectories in Appendix A.0.1 to illustrate the hypothesis.
To further validate our hypothesis, we integrate Network Morphism as the initializer in AutoGrow with Convergent Growth policy. We refer to this version of AutoGrow as c-AutoGrow with ""c-"" denoting ""Convergent."" More specific, we take ZeroInit or AdamInit as sub-module initializer and ""Convergent Growth"" policy in Algorithm 1. To recap, in this setting, AutoGrow trains a shallower net till it converges, then grows a sub-module which is initialized by Network Morphism, and repeats the same process till there is no further accuracy improvement. In every interval of K training epochs (train(g(X 0 ), K) in Algorithm 1), ""staircase"" learning rate is used. The learning rate is reset to 0.1 at the first epoch, and decayed by 0.1× at epoch K 2 and 3K 4 . The results are shown in Table 3 by ""staircase"" rows, which illustrate that c-AutoGrow can grow a DNN multiple times and finally find a depth. However, there are two problems: 1) the final accuracy is lower than training the found net from scratch, as indicated by ""∆"", validating our hypothesis; 2) the depth learning stops too early with a relatively shallower net, while a deeper net beyond the found depth can achieve a higher accuracy as we will show in Table 6. These problems provide a circumstantial evidence of the hypothesis that a converged shallow net with Network Morphism gives a bad initialization. Thus, AutoGrow cannot receive signals to continue growing after a limited number of growths. In Appendix A.0.1, Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3.

[ABLATION STUDY FOR AUTOGROW DESIGN]
Based on the findings in Section 3.1, we propose three simple but effective solutions to further enhance AutoGrow and refer it as p-AutoGrow, with ""p-"" denoting ""Periodic"": (1) Use a large constant learning rate for growing, i.e., 0.1 for residual networks and 0.01 for plain networks. Stochastic gradient descent with a large learning rate intrinsically introduces noises, which help to avoid a full convergence into a bad initialization from a shallower net. Note that staircase learning rate is still used for fine-tuning after discovering the final DNN; (2) Use random initialization (UniInit or GauInit) as noises to escape from an inadequate initialization; (3) Grow rapidly before a shallower net converges by taking Periodic Growth with a small K. p-AutoGrow is our final AutoGrow. In the rest part of this section, we perform ablation study to prove that the three solutions are effective. We start from c-AutoGrow, and incrementally add above solutions one by one and eventually obtain p-AutoGrow. In Table 3, first, we replace the staircase learning rate with a constant learning rate, the accuracy of AutoGrow improves and therefore ""∆"" improves; second, we further replace Network Morphism (ZeroInit or AdamInit) with a random initializer (UniInit or GauInit) and result in a bigger gain. Overall, combining a constant learning rate with GauInit performs the best. Thus, constant learning rate and GauInit are adopted in the remaining experiments, unless we explicitly specify them.  Note that, in this paper, we are more interested in automating depth discovery to find a final DNN (""found net"") with a high accuracy (""accu""). Ideally, the ""found net"" has a minimum depth, a larger depth than which cannot further improve ""accu"". We will show in Figure 3 that AutoGrow discovers a depth approximately satisfying this property. The ""∆"" is a metric to indicate how well shallower nets initialize deeper nets; a negative ""∆"" indicates that weight initialization from a shallower net hurts training of a deeper net; while a positive ""∆"" indicates AutoGrow helps training a deeper net, which is a byproduct of this work.
Finally, we apply the last solution -Periodic Growth, and obtain our final p-AutoGrow. Our ablation study results for p-AutoGrow are summarized in Table 5 and Table 4. Table 5 analyzes the impact of the growing period K. In general, K is a hyper-parameter to trade off speed and accuracy: a smaller K takes a longer learning time but discovers a deeper net, vice versa. Our results validate the preference of a faster growth (i.e. a smaller K). On CIFAR10/CIFAR100, the accuracy reaches plateau/peak at K = 3; further reducing K produces a deeper net while the accuracy gain is marginal/impossible. In the following, we simply select K = 3 for robustness test. More importantly, our quantitative results in Table 5 show that p-AutoGrow finds much deeper nets, overcoming the very-early stop issue in c-AutoGrow in Table 3. That is, Periodic Growth proposed in this work is much more effective than Convergent Growth utilized in previous work.
For sanity check, we perform the ablation study of initializers for p-AutoGrow. The results are in Table 8 in Appendix A.0.3, which further validates our wisdom on selecting GauInit. The motivation of Network Morphism in previous work was to start a deeper net from a loss function that has been well optimized by a shallower net, so as not to restart the deeper net training from scratch (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b). In all our experiments, we find this is sure even with random initialization. Figure 2 plots the convergence curves and learning process for ""42-42-42"" in Table 5. Even with GauInit, the loss and accuracy rapidly recover and no restart is observed. The convergence pattern in the ""Growing"" stage is similar to the ""Fine-tuning"" stage under the same learning rate (the initial learning rate 0.1). Similar results on ImageNet will be shown in Figure 8. Our results challenge the necessity of Network Morphism when growing neural networks.
At last, we perform the ablation study on the initial depth of the seed network. Table 4 demonstrates that a shallowest DNN works as well as a deeper seed. This implies that AutoGrow can appropriately stop regardless of the depth of the seed network. As the focus of this work is on depth automation, we prefer starting with the shallowest seed to avoid a manual search of a seed depth.  The seed net is Basic3ResNet-1-1-1.  

[ADAPTABILITY OF AUTOGROW]
To verify the adaptability of AutoGrow, we use an identical configuration (p-AutoGrow with K = 3) and test over 5 datasets and 4 seed architectures. Table 6 includes the results of all 20 combinations. Figure 3 compares AutoGrow with manual search which is obtained by training many DNNs with different depths from scratch. The results lead to the following conclusions and contributions:  2. For ResNets, a discovered depth ("" "" in Figure 3) falls at the location where accuracy saturates. This means AutoGrow discovers a near-optimal depth: a shallower depth will lose accuracy while a deeper one gains little. The final accuracy of AutoGrow is as good as training the discovered net from scratch as indicated by ""∆"" in Table 6, indicating that initialization from shallower nets does not hurt training of deeper nets. As a byproduct, in plain networks, there are large positive ""∆""s in Table 6. It implies that baselines fail to train very deep plain networks even using Batch Normalization, but AutoGrow enables the training of these networks; In Appendix A.0.3, Table 9 shows the accuracy improvement of plain networks by tuning K, approaching the accuracy of ResNets with the same depth.
3. For robustness and generalization study purpose, we stick to K = 3 in our experiments, however, we can tune K to trade off accuracy and model size. As shown in Figure 3, Auto-Grow discovers smaller DNNs when increasing K from 3 ("" "") to 50 ("" ""). Interestingly, the accuracy of plain networks even increases at K = 50. This implies the possibility of discovering a better accuracy-depth trade-off by tuning K, although we stick to K = 3 for generalizability study and it generalizes well.
4. In Table 6, AutoGrow discovers different depths under different sub-modules. The final accuracy is limited by the sub-module design, not by our AutoGrow. Given a sub-module architecture, our AutoGrow can always find a near-optimal depth. With a better sub-module architecture, such as NASNet cell , AutoGrow can improve accuracy.
Finally, our supposition is that: when the size of dataset is smaller, the optimal depth should be smaller. Under this supposition, we test the effectiveness of AutoGrow by sampling a subset of dataset and verify if AutoGrow can discover a shallower depth. In Appendix A.0.3, Table 11 summarizes the results. As expected, our experiments show that AutoGrow adapts to shallower networks when the datasets are smaller.

[SCALING TO IMAGENET AND EFFICIENCY]
In ImageNet, K = 3 should generalize well, but we explore AutoGrow with K = 2 and K = 5 to obtain an accuracy-depth trade-off line for comparison with human experts. The larger K = 5 enables AutoGrow to obtain a smaller DNN to trade-off accuracy and model size (computation) and the smaller K = 2 achieves higher accuracy. The results are shown in Table 7, which proves that AutoGrow automatically finds a good depth without any tuning. As a byproduct, the accuracy is even higher than training the found net from scratch, indicating that the Periodic Growth in AutoGrow helps training deeper nets. The comparison of AutoGrow and manual depth design (He et al., 2016) is in Figure 4, which shows that AutoGrow achieves better trade-off between accuracy and computation (measured by floating point operations).
In Appendix A.0.3, Table 10 summarizes the breakdown of wall-clock time in AutoGrow. The growing/searching time is as efficient as (often more efficient than) fine-tuning the single discovered DNN. The scalability of AutoGrow comes from its intrinsic features that (1) it grows quickly with a short period K and stops immediately if no improvement is sensed; and (2) the network is small at the beginning of growing. 

[RELATED WORK]
Neural Architecture Search (NAS) (Zoph & Le, 2016) and neural evolution (Miikkulainen et al., 2019;Angeline et al., 1994;Stanley & Miikkulainen, 2002;Liu et al., 2017a;Real et al., 2017) can search network architectures from a gigantic search space. In NAS, the depth of DNNs in the search space is fixed, while AutoGrow learns the depth. Some NAS methods (Bender et al., 2018;Liu et al., 2018b;Cortes et al., 2017) can find DNNs with different depths, however, the maximum depth is pre-defined and shallower nets are obtained by padding zero operations or selecting shallower branches, while our AutoGrow learns the depth in an open domain to find a minimum depth, beyond which no accuracy improvement can be obtained. Moreover, NAS is very computation and memory intensive. To accelerate NAS, one-shot models (Saxena & Verbeek, 2016;Pham et al., 2018;Bender et al., 2018), DARTS (Liu et al., 2018b) andNAS with Transferable Cell (Zoph et al., 2018;Liu et al., 2018a) were proposed. The search time reduces dramatically but is still long from practical perspective. It is still very challenging to deploy these methods to larger datasets such as ImageNet.
In contrast, our AutoGrow can scale up to ImageNet thanks to its short depth learning time, which is as efficient as training a single DNN.
In addition to architecture search which requires to train lots of DNNs from scratch, there are also many studies on learning neural structures within a single training. Structure pruning and growing were proposed for different goals, such as efficient inference (Wen et al., 2016;Li et al., 2016;Lebedev & Lempitsky, 2016;He et al., 2017;Luo et al., 2017;Liu et al., 2017b;Dai et al., 2017;Huang et al., 2018;Gordon et al., 2018;Du et al., 2019), lifelong learning (Yoon et al., 2017 and model adaptation (Feng & Darrell, 2015;Philipp & Carbonell, 2017). However, those works fixed the network depth and limited structure learning within the existing layers. Optimization over a DNN with fixed depth is easier as the skeleton architecture is known. AutoGrow performs in a scenario where the DNN depth is unknown hence we need to seek for the optimal depth. We hypothesize that a converged shallower net may not be an adequate initialization. Figure 5 visualizes and compares the optimization trajectories of Network Morphism and the training from scratch. In this figure, the shallower net is Basic3ResNet-3-3-3 (ResNet-20) and the deeper one is Basic3ResNet-5-5-5 (ResNet-32) in Table 2. The initializer is ZeroInit. The visualization method is extended from Li et al. (2018). Points on the trajectory are evenly sampled every a few epochs. To maximize the variance of trajectory, we use PCA to project from a high dimensional space to a 2D space and use the first two Principle Components (PC) to form the axes in Figure 5. The contours of training loss function and the trajectory are visualized around the final minimum of the deeper net. When projecting a shallower net to a deeper net space, zeros are padded for the parameters not existing in the deeper net. We must note that the loss increase along the trajectory does not truly represent the situation in high dimensional space, as the trajectory is just a projection. It is possible that the loss remains decreasing in the high dimension while it appears in an opposite way in the 2D space. The sharp detour at ""Morphing"" in Figure 5(a) may indicate that the shallower net plausibly converges to a point that the deeper net struggles to escape. In contrast, Figure 5(b) shows that the trajectory of the direct optimization in the deeper space smoothly converges to a better minimum.
Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure 6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table 3, which is much smoother compared to Figure 6(a).   10.
Table 11 summarizes the adaptability of AutoGrow to the sizes of dataset. In each set of experiments, dataset is randomly down-sampled to 100%, 75%, 50% and 25%. For a fair comparison, K is divided by the percentage of dataset such that the number of mini-batches between growths remains    The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table 10.","[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.","{
    ""1"": {
        ""summary"": ""The paper lacks novelty, showing similarities with previous methods from different fields, and the results might not be optimal due to algorithmic bias."",
        ""verbatim"": ""The growing strategy identified in the paper as a superior alternative seems to be already known and used, at least in the speech recognition community. I find it strange that the final networks found by the proposed method usually have the same/similar number of sub-modules per sub-network on multiple datasets. This regularity suggests that either A) the proposed algorithm prefers to set same number of sub-modules per sub-network due to its design, or B) datasets except ImageNet have an inherent shared property that produces this result. Figure 3 constitutes the main evidence that Autogrow finds approximately optimal depths as compared to manual searching, but it is not clear how the plot for baselines is obtained. The reported results appear to be for single trials throughout the paper.""
    },
    ""2"": {
        ""summary"": ""The method shows that training depth growth can be successful but there are concerns on the novelty and impact of the findings over previous methods."",
        ""verbatim"": ""Moreover, although within this write-up those are the highest numbers, in the broader literature of NAS this doesn\u2019t seem to be that good. Perhaps, the reason past NM works didn\u2019t use a GauInit was also due to the fact that past sub-modules didn\u2019t work with GauInit.""
    },
    ""3"": {
        ""summary"": ""The algorithm's simplicity and its reliance on a potentially naive approach raises doubts about the actual difficulty and usefulness of the problem it addresses."",
        ""verbatim"": ""I have conserved opinion based on the technique being used here is extremely simple, basically is an implementation of naive greedy algorithm in such a scenario, which implies the problem may not be intrinsically hard, or even useful.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Lack of detailed analysis on limitations or failure cases of AutoGrow."",
        ""verbatim"": ""The paper should provide more detailed analysis and insights into the limitations or failure cases of AutoGrow to ensure a comprehensive understanding of the method's performance across a wide range of scenarios.""
    },
    ""2"": {
        ""summary"": ""Need for further validation with diverse and challenging datasets."",
        ""verbatim"": ""While the experimental results are extensive, incorporating more diverse and challenging datasets or scenarios could further enhance the validation and generalizability of the proposed AutoGrow method.""
    },
    ""3"": {
        ""summary"": ""Comparisons with manual and other automated methods could be more detailed."",
        ""verbatim"": ""The comparison with manual search strategies or existing automated methods could be more detailed to emphasize the superiority of AutoGrow in terms of accuracy, efficiency, and depth discovery.""
    },
    ""4"": {
        ""summary"": ""Improvements suggested in results visualization for better understanding."",
        ""verbatim"": ""The paper could benefit from clearer visualization of results and trajectories to aid in understanding the optimization process during depth discovery, potentially enhancing the clarity and presentation of the research findings.""
    }
}",4,"{
    ""A1-B3"": {
        ""rationale"": ""Both Review A1 and Review B3 raise concerns about the methodology being compared with other methods and the lack of detailed analysis on these comparisons. A1 remarks about ambiguous baselines in results presentation, while B3 suggests needing more detailed comparisons with manual and automated methods."",
        ""similarity"": ""7""
    },
    ""A1-B4"": {
        ""rationale"": ""Review A1 and Review B4 both address concerns with the presentation and clarity of results in the paper. A1 comments on how the plots for baselines are obtained, suggesting a need for clearer visualization, while B4 directly suggests improvements in results visualization for better understanding."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks detailed insights into AutoGrow's policies which impacts reproducibility."",
        ""verbatim"": ""The paper could improve by providing more detailed insights into the mechanisms behind the growing and stopping policies of AutoGrow to enhance understanding and reproducibility.""
    },
    ""2"": {
        ""summary"": ""The paper needs a deeper comparison with existing methods to strengthen its claims."",
        ""verbatim"": ""While the efficiency of AutoGrow in discovering depth is highlighted, a more in-depth analysis or comparison with existing methods could further strengthen the paper's claims.""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both Review A1 and Review B2 express concerns about the novelty and effectiveness of the proposed AutoGrow method by highlighting its similarities and inadequacies when compared to existing methods. Review A1 discusses the lack of novelty and potential biases, while Review B2 emphasizes the need for a deeper comparison with existing methods to validate the claims made."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of detailed analysis on growing and stopping policies"",
        ""verbatim"": ""The paper may face rejection if it does not provide a more detailed analysis of the robust growing and stopping policies proposed in AutoGrow and their generalizability in different scenarios.""
    },
    ""2"": {
        ""summary"": ""Insufficient comparisons with existing automated methods"",
        ""verbatim"": ""More comprehensive comparisons with existing automated architecture design methods could be necessary for the acceptance of the paper.""
    }
}",2,"{
    ""A1-B2"": {""rationale"": ""Both excerpts A1 and B2 express concerns about the novelty and the comparison to existing methods. Review A criticizes the lack of novelty and potential bias in results when compared to prior methods in different fields, while Review B emphasizes the need for more comprehensive comparisons with existing automated architecture design methods."", ""similarity"": ""7""}
}",1,"{
    ""1"": {
        ""summary"": ""Concerns about potential overfitting due to increased network depth without adequate regularization."",
        ""verbatim"": ""Concerns over Complexity and Overfitting: Reviewers might be concerned about the potential for the model to encourage overfitting by excessively increasing network depth without sufficient regularization insights.""
    },
    ""2"": {
        ""summary"": ""Questions regarding the generalization of stopping criteria across diverse datasets."",
        ""verbatim"": ""Generalization of Stopping Criteria: The stopping criteria based on improvement thresholds in validation accuracy (<0.05%) might not generalize well across even more diverse or novel datasets, raising concerns about the robustness of the stopping mechanism.""
    },
    ""3"": {
        ""summary"": ""Scrutiny over the efficiency and practicality of implementing AutoGrow in large-scale or real-world scenarios."",
        ""verbatim"": ""Algorithm Efficiency and Practicality: Despite claims of efficiency, the actual computational cost and practicality of implementing AutoGrow on a large scale or in real-world applications may require further scrutiny, particularly in comparison to simpler or more established methods.""
    }
}",3,"{
    ""A1-B3"": {
        ""rationale"": ""Both Review A1 and Review B3 express concerns about the efficiency and effectiveness of the proposed methods, though with a different focus. A1 discusses the potential algorithmic preference and bias when applying the model across multiple datasets, while B3 questions the practicality of the AutoGrow method's implementation in real-world or large-scale scenarios."",
        ""similarity"": ""6""
    },
    ""A2-B3"": {
        ""rationale"": ""Review A2 and Review B3 share concerns regarding the overall impact and practical implementation of the research discussed. A2 critiques the novelty of the method within broader literature and its effectiveness, while B3 critically assesses the efficiency and viability of applying the AutoGrow in substantial or practical settings."",
        ""similarity"": ""6""
    }
}",0,"{
    ""1"": {
        ""summary"": ""The paper lacks substantial theoretical backing."",
        ""verbatim"": ""Lack of Theoretical Analysis: The paper predominantly focuses on empirical results without substantial theoretical backing to explain why the growing and stopping policies succeed, or under what specific conditions they might fail.""
    },
    ""2"": {
        ""summary"": ""Concerns about overfitting in the stopping policy."",
        ""verbatim"": ""Possible Overfitting Concerns: While the paper claims generalized efficacy, there is a potential concern about whether the stopping policy may lead to overfitting, especially in smaller or less diverse datasets that were not part of the study.""
    },
    ""3"": {
        ""summary"": ""No comparison with state-of-the-art techniques."",
        ""verbatim"": ""Comparison with State-of-the-Art: The paper does not compare AutoGrow with the latest depth optimization techniques or other contemporaneous automated machine learning (AutoML) methods which could provide a clearer understanding of its relative performance.""
    }
}",3,"{
    ""A1-B3"": {
        ""rationale"": ""Both points express concern about the novelty and impact of the proposed techniques compared to other existing methods. Review A highlights the similarity to methods from other fields and questions the claims of optimality, while Review B directly points out that the paper does not compare its method to state-of-the-art techniques. Both are concerned with how innovative the method truly is in the wider context of current technologies."",
        ""similarity"": ""7""
    },
    ""A2-B3"": {
        ""rationale"": ""Both reviews critique the research within the context of broader related literature and find that the presented results or methodologies do not significantly exceed what is already known or utilized. Review A specifically mentions that the achieved results are not particularly impressive in the broader field, mirroring Review B's concern that no comparative analysis is made with the latest techniques, implying a similar lack of demonstrated superiority or innovation."",
        ""similarity"": ""7""
    },
    ""A3-B1"": {
        ""rationale"": ""Both points reveal concerns regarding the depth and complexity of the methodology employed by the paper. Review A doubts the problem's intrinsic difficulty and usefulness based on the simplicity of the algorithm used. Similarly, Review B criticizes the lack of theoretical depth, noting the absence of robust explanations for why certain aspects of the methodology work, or conditions under which they may fail. Both focus on the methodological depth and theoretical grounding present in the research."",
        ""similarity"": ""7""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Lack of theoretical support for the methods"",
        ""verbatim"": ""Although AutoGrow performs well in practice, the paper may face criticism for not providing sufficient theoretical backing to understand why the growth and stopping policies work effectively across different conditions.""
    },
    ""2"": {
        ""summary"": ""Concerns about scalability and efficiency"",
        ""verbatim"": ""While the paper claims efficiency comparable to training a single DNN, it does not provide extensive details on the computational overhead and scalability, especially concerning very deep networks or extremely large datasets.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both points critique aspects of theoretical support and effectiveness, with Review A discussing the lack of optimal results due to algorithmic preferences in design and comparison uncertainties, while Review B emphasizes the lack of theoretical justification on how the growth and stopping policies of AutoGrow function under various conditions."",
        ""similarity"": ""7""
    }
}",1,"

Significance and novelty:
The paper proposes AutoGrow, a method to automate the depth discovery in Deep Neural Networks (DNNs). Traditional methods for designing depth in DNNs require significant human efforts and are heuristic. With AutoGrow, depth discovery is automated, making the process more efficient and cost-effective. The method starts with a shallow architecture and grows new layers if the growth improves accuracy; otherwise, it stops growing, discovering the optimal depth for the specific dataset. The paper demonstrates that applying the same policy to various datasets results in discovering near-optimal depth for each dataset. The findings of the paper introduce potential benefits for both research and practical applications, as it can lead to more efficient training of DNNs and discovery of optimal architectures for specific datasets.

Potential reasons for acceptance:
The research presented in the paper is of high quality, as it contributes to the field by providing an innovative solution to the problem of depth discovery in DNNs, which currently requires significant human efforts and is heuristic. The method proposed in the paper, AutoGrow, shows promising results on various datasets and improves upon traditional methods by discovering near-optimal depth in similar time and with greater efficiency. The paper presents the algorithms and experiments in a clear and concise manner, improving the readability and reproducibility of the research. The paper also includes thorough analysis and discussion of the results, highlighting the benefits and limitations of the proposed method.

Potential reasons for rejection:
The paper lacks sufficient justification for the stopping criteria used in the AutoGrow algorithm. The stopping criteria is critical for the AutoGrow method to discover the optimal depth, however, the paper does not provide sufficient explanation of how the stopping criteria is determined. The paper does not include sufficient comparison of the proposed method with existing state-ofthe-art methods. The lack of such comparison limits the contributions of the paper and reduces its novelty. Furthermore, the paper lacks sufficient discussion of the scalability of the AutoGrow algorithm to large-scale deep learning models, restricting its applicability.

Suggestions for improvement:
The authors should provide a more detailed explanation of the stopping criteria used in the AutoGrow algorithm, including the parameters and evaluation metrics used for determining the stopping condition. They should also include a thorough comparison of the proposed method with existing state-ofthe-art methods, highlighting the advantages and disadvantages of each approach and how AutoGrow improves upon them. Additionally, the authors should provide analysis of the scalability of the AutoGrow algorithm to large-scale deep learning models, assessing its applicability to practical applications. They should also consider the incorporation of interpretability features, enabling the users to understand the learning process of the AutoGrow method. Finally, the authors could benefit from providing code and data availability, as it would enable researchers to test and reproduce the results reported in the paper.

","[Significance and novelty]
<Originality of the proposed work> The paper presents an original approach (AutoGRoW) to adaptively compute the depth in neural networks. It is the first work that addresses the trade-off between accuracy and inference speed directly without human intervention.
<Technical solidity and strong results> The paper shows technically solid experimental results on different neural networks architectures and datasets, outperforming many baseline designs for architecture search in terms of speed-accuracy trade-off.

[Potential reasons for acceptance]
<High quality of the paper in terms of its solid approach of automatic search space exploration, technical robustness, and strong experimental results> The paper is technically solid in terms of its approach of automatic search space exploration, technical robustness, and strong experimental results.
<Comprehensive experiments and clear presentation> The paper has comprehensive experiments and clear presentation.

[Potential reasons for rejection]
<Lack of comprehensive empirical evaluation> The empirical evaluation of the proposed approach is not comprehensive, and the ablation studies are superficial.
<Insuffient clarity and missing important related work> The paper leaves out some important related work and suffers from misses in clarity.

[Suggestions for improvement]
<Conduct more comprehensive empirical studies> The paper should conduct more comprehensive empirical studies and conduct additional ablation studies.
<Include more comprehensive related work> The authors should include more comprehensive related work in its related work section.
<Improve presentation quality> The paper should make an effort in improving its overall presentation quality.

Title: Learning deep neural networks through heuristics

","{
    ""1"": {
        ""summary"": ""Insufficient justification for the stopping criteria in the AutoGrow algorithm."",
        ""verbatim"": ""The paper lacks sufficient justification for the stopping criteria used in the AutoGrow algorithm.""
    },
    ""2"": {
        ""summary"": ""Lack of comparison with existing state-of-the-art methods."",
        ""verbatim"": ""The paper does not include sufficient comparison of the proposed method with existing state-ofthe-art methods.""
    },
    ""3"": {
        ""summary"": ""Limited discussion on the scalability of the AutoGrow algorithm."",
        ""verbatim"": ""The paper lacks sufficient discussion of the scalability of the AutoGrow algorithm to large-scale deep learning models.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The empirical evaluation of the proposed approach is not comprehensive, and the ablation studies are superficial."",
        ""verbatim"": ""The empirical evaluation of the proposed approach is not comprehensive, and the ablation studies are superficial.""
    },
    ""2"": {
        ""summary"": ""The paper lacks some important related work and has issues with clarity."",
        ""verbatim"": ""The paper leaves out some important related work and suffers from misses in clarity.""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both Review A1 and Review B2 express concerns about the novelty and effectiveness of the proposed method compared to existing techniques. Review A1 discusses that the growth strategy in the paper appears similar to ones used in other communities and questions the optimal results claimed by the paper due to possible algorithmic bias. Review B2 highlights the paper’s lack of comparison with existing state-of-the-art methods."",
        ""similarity"": ""7""
    }
}",1,"{
    ""A2-B2"": {
        ""rationale"": ""Both points A2 and B2 question the novelty of the approach compared to existing work. Review A specifically addresses the performance of the proposed method concerning the broader literature of NAS and implies that other methods may not have used certain initializations due to inherent limitations, suggesting a lack of novelty or depth in innovative technique, which directly relates to B2's critique of omitting important related work and issues in clarity."",
        ""similarity"": ""7""
    }
}",1
Bayesian Time Series Forecasting with Change Point and Anomaly Detection,rJLTTe-0W.json,"[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Demostration of Decompositions.
Table 2: Algorithm 3 :3Proposed Algorithm Input: Observed time series y = (y 1 , y 2 , . . . , y n ), seasonality length S, length of time series for forecasting m, number of predictive paths N , change point minimum segment l Output: Change point detection z c , anomaly points z a , forecasting result y future = (y n+1 , y n+1 , . . . , y n+m ) and its distribution or predictive intervals Part I: Initialization; Initialize σ , σ o , σ u , σ r , σ v , σ w all with the empirical standard deviation of y;
Table 3: Figure 3 :3Figure 3: An example of synthesis data (left), and the result after applying our algorithm (right).
Table 4: Figure 4 :4Figure 4: Well-log Data (left). The result of implementing our proposed algorithm (right).
Table 5: Figure 5 :5Figure 5: Internet Traffic Data (top); The result of implementing our proposed algorithm (bottom).
Table 6: Two Categories for Hidden Variables α2, . . . , αn)
Table 7: Comparison of Forecasting in Well-log Data

[INTRODUCTION]
Time series forecasting has a rich and luminous history, and is essentially important in most of business operations nowadays. The main aim of time series forecasting is to carefully collect and rigorously study the past observations of a time series to develop an appropriate model which could describe the inherent structure of the series, in order to generate future values. For instance, the internet companies are interested in the number of daily active users (DAU), say, what is DAU after certain period of time, or when will reach their target DAU goal. Time series forecasting is a fruitful research area with many existing methodologies. The most popular and frequently used time series model might be the Autoregressive Integrated Moving Average (ARIMA) (Box et al., 2015;Zhang, 2003;Cochrane, 2005;Hipel & McLeod, 1994). Taking seasonality into consideration, Box et al. (2015) proposed the Seasonal ARIMA. The Holt-Winters method (Winters, 1960) is also very popular by using exponential smoothing. State space model (Durbin & Koopman, 2012;Scott & Varian, 2014;Brodersen et al., 2015) also attracts much attention, which is a linear function of an underlying Markov process plus additive noise. Exponential Smoothing State Space Model (ETS)  decomposes times series into error, trend, seasonal that change over time. Recently, deep learning is applied for time-series trend learning using LSTM (Tao Lin, 2017), bidirectional dynamic Boltzmann machine (Osogami et al., 2017) is applied for time-series long-term dependency learning, and coherent probabilistic forecast (Taieb et al., 2017) is proposed for a hierarchy or an aggregation-level comprising a set of time series. Orthogonal to these works, this paper focuses on robust ways of time series forecasting in presence of change points and anomalies.
In Internet time series forecasting, Google develops the Bayesian structure time series (BSTS) model (Brodersen et al., 2015;Scott & Varian, 2014) to capture the trend, seasonality, and similar components of the target series. Recently, Facebook proposes the Prophet approach (Taylor & Letham, 2017) based on a decomposable model with interpretable parameters that can be intuitively adjusted by analyst. However, as in the DAU example, some special events like Christmas Holiday or President Election, newly launched apps or features, may cause short period or long-term change of DAU, leading to weird forecasting of those traditional models. The aforementioned special cases are well known as • Anomaly points. The items, events or observations that don't conform to an expected pattern or other items in the dataset, leading to a sudden spike or decrease in the series.
• Change points. A market intervention, such as a new product launch or the onset of an advertising (or ad) campaign, may lead to the level change of the original series.
Time series forecasting without change/anomaly point detection and adjustment may also lead to bizarre forecasting since these models might learn the abrupt changes in the past. There are literatures on detecting anomaly or change points individually, examples can be found in Twitter (2017); Netflix (2017); Barry & Hartigan (1993); Killick & Eckley (2014); twitter (2017). However, the aforementioned change point detection models could not support detection in the presence of seasonality, while the presence of trend/change point is not handled by the anomaly detection models. Most importantly, there is a discrepancy between anomaly/change points detection and adjustment, and commonly used manually adjustment might be a bit arbitrary. Unfortunately, the forecasting gap caused by abnormal and change points, to the best of our knowledge, has not been given full attention and no good solution has been found so far. This paper is strongly motivated by bridging this gap.
In this paper, to overcome the limitations of the most (if not all) current models that the anomaly points and change points are not properly considered, we develop a state space time series forecasting model in the Bayesian framework that can simultaneously detect anomaly and change points and perform forecasting. The learned structure information related to anomaly and change points is automatically incorporated into the forecasting process, which naturally enhances the model prediction based on the feedback of state-space model. To solve the resultant optimization problem, an iterative algorithm based on Bayesian approximate inference with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. The novel model could explicitly capture the structure of change points, anomaly points, trend and seasonality, as also provide the distributions and forecasting intervals due to Bayesian forecasting framework. Both synthetic and real data sets show the better performance of proposed model, in comparison with existing baseline. Moreover, our proposed model outperforms state-of-the-art models in identifying anomaly and change points.
To summarize, our work has the following contributions.
• We proposed a robust 1 Bayesian state-space time series forecasting model that is able to explicitly capture the structures of change points and anomalies (which are generally ignored in most current models), and therefore automatically adapt for forecasting by incorporating the prior information of trend, seasonality, as well as change points and anomalies using state space modeling. Due to the enhancement of model description capability, the results of model prediction and abnormal and change points detection are mutually improved.
• To solve the resultant optimization problem, an effective algorithm based on approximate inference using Markov chain Monte Carlo (MCMC) is proposed with theoretical guaranteed forecasting paths.
• Our proposed method outperforms the state-of-the-art methods in time series forecasting in presence of change points and anomalies, and detects change points and anomalies with high accuracy and low false discovery rate on both tasks, outperforming popular change point and anomaly detection methods. Our method is flexible to capture the structure of time series under various scenarios with any component combinations of trend, seasonality, change points and anomalies. Therefore our method can be applied in many settings in practice.

[MODEL OVERVIEW]
State space time series model (Hangos et al., 2014) has been one of the most popular models in time series analysis. It is capable of fitting complicated time series structure including linear trend and seasonality. However, times series observed in real life are almost all prevailed with outliers. Change points, less in frequency but are still widely observed in real time series analysis. Unfortunately, both structures are ignored in the classic state space time series model. In the section, we aim to address this issue by introducing a novel state space time series model. Let y = (y 1 , y 2 , . . . , y n ) be a sequence of time series observations with length n. The ultimate goal is to forecast (y n+1 , y n+2 , . . .). The accuracy in forecasting lies in a successful decomposition of y into existing components. Apart from the residuals, we assume the time series is composed by trend, seasonality, change points and anomaly points. In a nutshell, we have an additive model with time series = trend + seasonality + change point + anomaly point + residual.
Figure 1 provides a demonstration of desired decomposition of time series. In Figure 1, the left panel shows the observed time series. And it can be decomposed into the remaining five panels. The shift in the change point panel shows where the change point lies. And the spikes in the last panel reveals the anomaly points.
As the classical state space model, we have observation equation and transition equations to model y and hidden variables. We use µ = (µ 1 , µ 2 , . . . , µ n ) to model trend, and use γ = (γ 1 , γ 2 , . . . , γ n ) to model seasonality. We use a binary vector z a = (z a 1 , z a 2 , . . . , z a n ) to indicate anomaly points. Then we have Observation equation:
y t = µ t + γ t + t , if z a t = 0 o t , if z a t = 1
.
(1)
The deviation between the observation y t and its ""mean"" µ t + γ t is modeled by t and o t , depending on the value of z a t . If z a t = 1, then y t is an anomaly point; otherwise it is not. Distinguished from the residues = ( 1 , 2 , . . . , n ), the anomaly is captured by o = (o 1 , o 2 , . . . , o n ) which has relative large magnitude.
The hidden state variable µ and γ have intrinsic structures. There are two transition equations, for trend and seasonality separately Transition Equations: Trend:
µ t = µ t−1 + δ t−1 + u t , if z c t = 0 r t , if z c t = 1 ,(2)
δ t = δ t−1 + v t , Seasonality: γ t = − S−1 s=1 γ t−s + w t .(3)
In Equation (2), δ = (δ 1 , δ 2 , . . . , δ n ) can be viewed as the ""slope"" of the trend, measuring how fast the trend changes over time. The change point component is also incorporated in Equation (2) by a binary vector z c = (z c 1 , z c 2 , . . . , z c n ). If z c t = 1, it means the t-th point is a change point, with µ t differs from µ t−1 + δ t−1 (which can be interpreted as the ""momentum"" from the previous status ) by r t ; otherwise it is not a change point and they differ by u t . We model the change points in a way such that r = (r 1 , r 2 , . . . , r n ) have larger magnitude compared u = (u 1 , u 2 , . . . , u n ). The ""slope"" part δ also has its own noise v = (v 1 , v 2 , . . . , v n ).
A first look on Equation (2) may bring up with the question that it is not presented in an exactly the same way as shown in Figure 1. In Figure 1, the change points component is a step function, and it is one of the five additive components along with trend, seasonality, anomaly points and residuals. Here we model the change point directly into the trend component. Though differing in formulation, they are equivalent to each other. We choose to model in as in Equation ( 2) due to simplicity, and its similarity with the definition of anomaly points in Equation (1).
The seasonality component is presented in Equation ( 3). Here S is the length of one season and w = (w 1 , w 2 , . . . , w n ) is the noise for seasonality. The seasonality component is assumed to have almost zero average in each season.
The observation equation and transition equations (i.e., Equation (1,2,3)) define how y is generated from all the hidden variables including change points and anomaly points. We continue to explore this new model, under a Bayesian framework.

[BAYESIAN FRAMEWORK]
Bayesian methods are widely used in many data analysis fields. It is easy to implement and interpret, and it also has the ability to produce posterior distribution. The Bayesian method on state space time series model has been investigated in Scott & Varian (2014);Brodersen et al. (2015). In this section, we also consider Bayesian framework for our novel state space time series model. We assume all the noises are normally distributed
{ t } n t=1 iid ∼ N (0, σ 2 ), {o t } n t=1 iid ∼ N (0, σ 2 o ), {u t } n t=1 iid ∼ N (0, σ 2 u ), {r t } n t=1 iid ∼ N (0, σ 2 r ), {v t } n t=1 iid ∼ N (0, σ 2 v ), {w t } n t=1 iid ∼ N (0, σ 2 w )
, where σ , σ o , σ u , σ r , σ v , σ w are parameters for standard deviation. As binary vectors, a natural choice is to model anomaly point indicator z a and change point indicator z c to the model them as Bernoulli random variables
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c ),
where p a , p c are probabilities for each point to be an anomaly or change point.
Figure 2: Graphical presentation of our model. Note that y is observed, highlighted by gray background, distinguished from all the remaining ones that are hidden. Among the hidden ones, squares indicate fixed parameters, and circles indicate random variables.
For simplicity, we denote α t = (µ t , δ t , γ t , γ t−1 , . . . , γ t−(S−2) ) to include the main hidden variables (except z a t and z c t ) in the transition equations. All the α t are well defined and can be generated from the previous status, except α 1 . We denote a 1 to be the parameter for α 1 , which can be interpreted as the ""mean"" for α 1 .
With Bayesian framework, we are able to represent our model graphically as in Figure 2. As shown in Figure 2, the only observations are y and all the others are hidden. In this paper, we assume there is no additional information on all the hidden states. If we have some prior information, for example, some points are more likely to be change points, then our model can be easily modified to incorporate such information, by using proper prior.
In Figure 2, we use squares and circles to classify unknown variables. Despite all being unknown, they actually behave differently according to their own functionality. For those in squares, they behave like turning parameters. Once they are initialized or given, those in circles behaves like latent variables. We call the former ""parameters"" and the latter ""latent variable"", as listed in Table 1. Trend and seasonality z = (z a , z c ) Anomaly and change points Parameter a1
The ""mean"" for the initial trend and seasonality p = (pa, pc)
Probabilities for each point to be anomaly or change point σ = (σ , σo, σu, σr, σv, σw) Standard deviation
The discrepancy between these two categories is clearly captured by the joint likelihood function. From Figure 2, the joint distribution (i.e., the likelihood function) can be written down explicitly as
La 1 ,p,σ (y, α, z) (4) = {t:z a t =0} g(yt − µt − γt, σ ) × {t:z a t =1} g(yt − µt − γt, σo) × {t:z c t =0} g(µt − µ t−1 − δ t−1 , σu) × {t:z c t =1} g(µt − µ t−1 − δ t−1 , σr) × n t=1 g(δt − δ t−1 , σv) × n t=1 g(− S−1 s=1 γ t−s , σv) × n i=1 (pa) z a t (1 − pa) 1−z a t (pc) z c t (1 − pc) 1−z c t , where g(x 1 , x 2 ) = 1 √ 2πx2 exp −x 2 1 /(2x2
2 ) is the density function for normal distribution with mean x 1 and standard deviation x 2 . Here we slightly abuse the notation by using µ 0 , δ 0 , γ 0 , γ −1 , . . . , γ 2−S , which are actually the corresponding coordinates of a 1 .
As long with other probabilistic graphical models, our model can also be viewed as a generative model. Given the parameters a 1 , p, σ, we are able to generate time series. We present the generative procedure as follows.
Algorithm 1: Generative Procedure Input: Parameters a 1 , σ = (σ , σ o , σ u , σ r , σ v , σ w ) and p a , p c , length of time series to generate m Output: Time series y = (y 1 , y 2 , . . . , y m ) Generate the indexes where anomalies or change points occur
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c );
Generate all the noises , o, u, r, v, w as independent normal random variables with mean zero and standard deviation σ , σ o , σ u , σ r , σ v , σ w respectively; Generate {α t } m t=1 sequentially by the transition functions in Equation ( 2) and (3); Generate time series {y t } m t=1 by the observation function in Equation (1).

[INFERENCE]
This section is about inferring unknown variables from y, given the Bayesian setting described in the previous section. The main framework here is to sequentially update each hidden variable by fixing the remaining ones. As stated in the previous section, there are two different categories of unknown variables. Different update schemes need to be used due to the difference in their functionality. For the latent variables, we implement Markov chain Monte Carlo (MCMC) for inference. Particular, we use Gibbs sampler. We will elaborate the details of updates in the following sections.

[UPDATES ON TREND AND SEASONALITY]
In this section, we focus on updating α assuming all the other hidden variables are given and fixed. The essence of Gibbs sampler is to obtain posterior distribution p a1,p,σ (α|y, z). This can be achieved by a combination of Kalman filter, Kalman smoothing and the so-called ""fake-path"" trick. We provide some intuitive explanation here and refer the readers to Durbin & Koopman (2012) for detailed implementation.
Kalman filter and Kalman smoothing are classic algorithms in signal processing and pattern recolonization for Bayesian inference. It is well related to other algorithms especially message passing algorithm. Kalman filter collects information forwards to obtain E(α t |y 1 , y 2 , . . . , y t ); while Kalman smoothing distribute information backwards to achieve E(α t |y).
However, the combination of Kalman filter and Kalman smoothing is not enough, as it only gives the the expectations of marginal distributions {E(α t |y)} n t=1 , instead of the joint distribution required for Gibbs sampler. To address this issue, we can use the ""fake-path"" trick described in Brodersen et al. (2015); Durbin & Koopman (2012). The main idea underlying this trick lies on the fact that the covariance structure of p(α t |y) is not dependent on the means. If we are able to obtain the covariance by some other way, then we can add it up with {E(α t |y)} n t=1 to obtain a sample from p(α|y). This trick involves three steps. Note that all the other hidden variables z, p, σ are given.
1. Pick some vectorã 1 , and generate a sequence of time seriesỹ from it by Algorithm 1. In this way, we also observeα. 2. Obtain {E(α t |ỹ)} n t=1 fromỹ by Kalman filter and Kalman smoothing. 3. We use {α t − E(α t |ỹ) + E(α t |y)} n t=1 as our sampling from the conditional distribution.

[CHANGE POINT AND ANOMALY DETECTION]
In this section, we update z by Gibbs sampler, assuming α, a 1 , p, σ are all given and fixed. We need to obtain the conditional distribution p a1,p,σ (z|y, α). Note that in the graphical model described in Section 2, {z a t } n t=1 and {z c t } n i=1 are all Bernoulli random variables and independent of each other. Then the conditional distribution p a1,p,σ (z|y, α) can also be decomposed into product of Bernoulli density functions. In other words, conditioned on y, α, {z a t } n t=1 and {z c t } n i=1 are still independent Bernoulli random variables, but possibly with different success probabilities. Thus, we can take the calculation point by point. For example, for the anomaly detection for the t-th point, we have
z a t = 0 : y t − µ t − γ t ∼ N (0, σ 2 ) z a t = 1 : y t − µ t − γ t ∼ N (0, σ 2 o
). And the prior on z a t is P(z a t = 1) = p a and P(z a t = 0) = p 1 . Let p a t = P(z a t = 1|y, α). Directly calculation leads to
p a t = pa σo exp − (yt−µt−γt) 2 2σ 2 o 1−pa σ exp − (yt−µt−γt) 2 2σ 2 + pa σo exp − (yt−µt−γt) 2 2σ 2 o .(5)
This equality holds for all t = 1, 2, . . . , n. Similarly for change point detection, let p c t = P(z c t = 1|y, α), and we have
p c t = pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r 1−pc σu exp − (µt−µt−1−δt−1) 2 2σ 2 u + pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r .(6)
As mentioned above, all the coordinates in z are still independent Bernoulli random variables conditioned on y, α. Thus, for Gibbs sampler, we can generate z by sampling independently with
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t ).
For change point detection here, we have an additional segment control step. After obtaining {z c t } n t=1 as mentioned above, we need to make sure that the change points detected satisfy some additional requirement on the length of segment among two consecutive change points. This issue arises from the ambiguity between the definitions of change point and anomaly points. For example, consider a time series with value (0, 0, 0, 0, 1, 1, 1, 0, 0, 0). We can view it with two change points, one increases the trend by 1 and the other decreases it by 1. Alternatively, we can also argue the three 1s in this time series are anomalies, though next to each other. One way to address this ambiguity is by defining the minimum length of segment (denoted as ). In this toy example, if we set the minimum length to be 4, then they are anomaly points; if we set it to be 3, then we regard them to be change points. But a more complicated criterion is needed than using minimum length as the time series usually own much more complex structure than this toy example. Consider time series (0, 0, 0, 0, −1, −1, 1, 1, 1, 1) and the minimum time series parameter = 3. It is reasonable to view it with one change point with increment 1, and the two -1s should be regarded as anomalies. As a combination of all these factors, we propose the following segment control method. A default value for the parameter is the length of seasonality, i.e., = S.
Algorithm 2: Segment control on change points Input: change point binary vector z c ,trend µ, standard deviation for outliers σ r , change point minimum segment Output: change point binary vector z c Denote t 1 < t 2 < . . . to be all the indexes such that z c ti = 1; while there exists i such that
|t i+1 − t i | < do Check if |µ ti−1 − µ ti+1+1 | ≤ σ r /2.
If so, exclude both them from change points by setting z c ti = z c ti+1 = 0. Otherwise, randomly exclude one of them by setting the corresponding coordinate in z c to be 0; Update all the indexes of change points in z c . end

[INITIALIZATION AND UPDATES ON PARAMETERS]
The parameters σ, a 1 and p need both initialization and update. We have different initializations and update schemes for each of them.
For all the standard deviations, once we obtain α and z, we update them by taking the empirical standard deviation correspondingly. For σ δ and σ γ , the calculation is straightforward as they only involve δ and γ respectively. For σ , σ o , σ u and σ r , it is a bit more involved due to z. Nevertheless, we can obtain the following update equations for all of them:
σ = {t:z a t =0} (yt − µt − γt) 2 |{t : z a t = 0}| , σo = {t:z a t =1} (yt − µt − γt) 2 |{t : z a t = 1}| , σu = {t:z c t =0} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| ,(7)
σr = {t:z c t =1} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| , σ δ = 1 n n t=1 (δt − δt−1) 2 , σγ = 1 n n t=1 ( S−1 s=0 γt−s) 2 . (8
)
Note that in some iterations, when there is no change point or anomaly detected in z, then the updates above for σ o , σ r are not well-defined. In those cases, we simply let them remain the same.
To initialize σ, we let them all equal to the standard deviation of y.
For a 1 , we initialize it by letting its first coordinate to be equal to the average of y 1 , y 2 , . . . , y S , and all the remaining coordinates to be equal to 0. Since a 1 can be interpreted as the mean vector of α 1 , in this way the trend is initialized to be matched up with average of the first season, and the slope and seasonality are initialized to be equal to 0. We update a 1 by using information of α. We let the first two coordinates (trend and slope) of a 1 to be equal to those of α 1 , and we let the remaining coordinates (seasonality) of a 1 to be equal to those of α S+1 . The reason why we do not let a 1 to be equal to α 1 entirely is due to the consideration on convergence and robustness. Since we initialize the seasonality part in a 1 as 0, it will remain 0 if we let a 1 equals α 1 entirely (due to the mechanism how we update α 1 as described in Section 4.1. We can avoid such trouble via using α S+1 .
For p, we initialize them to be equal to 1/n. If we have additional information on the number of change points or anomaly points, we can initiate them with different values, for example, 0.1/n, or 10/n. We can update p after obtaining z, but we choose not to, also for the sake of robustness. In the early iterations when the algorithm is far from convergence, it is highly possible that z a or z c may turn out to be all 0. If we update p, say, by taking the proportion of change point or anomaly points in z. Then p a or p c might be 0, and it may get stuck in 0 in the remaining iterations.

[FORECASTING]
Once we infer all the latent variables α, z and tune all the parameters p, a 1 , σ, we are able to forecast the future time series y future . From the graphical model described in Section 3, the future forecasting only involves α n instead of the whole α. Note that we assume that there exists no change point and anomaly point in the future. This is reasonable as in most cases we have no additional information on the future time series. Given α n and σ we can use our predictive procedures (i.e., Algorithm 1) to generate future time series y future . We can further integrate out α n to have the posterior predictive distribution as p σ (y future |y).
The forecasting on future time series is not deterministic. There are two sources for the randomness in y future . One comes from the inference of α n (and also σ) from y. Under the Bayesian framework in Section 3, we have a posterior distribution over α n rather than a single point estimation. The second one comes from the forecasting function itself. The forecasting involves intrinsic noise like t , u t , v t and w t . Thus, the predictive density function p σ (y future |y, α n ) will lead to different path even with fixed σ and α n . In this way we are able to obtain distribution and predictive interval for forecasting. We also suggest to take the average of multiple forecasting paths, as the posterior mean for the forecasting.
The average of multiple forecasting paths (denoted asȳ future ), if the number of paths is large enough, always takes the form as a combination of linear trend and seasonality. This can be observed in both our synthesis data (Section 7) and real data analysis (Section 8). This seems to be surprising at the first glance, but makes some sense intuitively. Under our assumption, we have no information on the future, and thus a reliable way to forecast the future is to use the information collected at the end of observed time series, i.e., trend µ n , slope δ n and seasonality structure. Theorem 1 gives mathematical explanation of the linearity ofȳ future , in both mean and standard deviation. Theorem 1. Let N be the number of future time series paths we generate from Algorithm 1). Let m be the number of points we are going to forecast. Denote {y
(1) n+j } m j=1 , {y(2)
n+j } m j=1 , . . . , {y
n+j } m j=1 to be the future paths. Defineȳ future = (ȳ n+1 ,ȳ n+2 , . . . ,ȳ n+m ) to be the average such that
y n+j = 1 N N i=1 y (i) n+j .
Then for all j = 1, 2, . . . , N , we haveȳ n+j as a normal distribution with mean and variance as
E[ȳ n+j ] = µ n + jδ n + γ n−S+(j mod S) Var [ȳ n+j ] = 1 N j(j + 1)σ 2 v /2 + j(σ 2 u + σ 2 w ) + σ 2 .
Consequently, for all j = 1, 2, . . . , m, E[ȳ n+j ] is in a linear form with respect to j, and the standard deviation ofȳ n+j also takes a approximately linear form with respect to j.
Proof. Recall that α n , σ are given and fixed, and we assume there is no change point or anomaly in the future time series. The Equation (2) leads to δ n+j = δ n + j l=1 v n+l , which implies that
µ n+j = µ n + jδ n + j l=1 (j + 1 − l)v n+l + j l=1 u m+l .
For the seasonality part, simple linear algebra together with Equation 3 leads to γ n+j = γ n−S+(j mod S) + j l=1 w n+l . Thus,
y n+j = 1 N N i=1 µ n + jδ n + γ n−S+(j mod S) + j l=1 (j + 1 − l)v (i) n+l + j l=1 u (i) m+l + j l=1 w (i) n+l + (i) n+j .
Due to the independence and Gaussian distribution of all the noises,ȳ n+j is also normally distributed and its means and variance can be calculated accordingly.

[ALGORITHM]
Our proposed method can be divided into three parts: initialization, inference, and forecasting. Section 4 and Section 5 provide detailed explanation and reasoning for each of them. We present a whole picture of our proposed methodology in Algorithm 3. Initialize a 1 such that its first coordinate equals to the average of (y 1 , y 2 , . . . , y S ) and all the remaining S coordinates with 0; Initialize p a and p c by 1/n. Then generate z a and z c as independent Bernoulli random variables with success probability p a and p c respectively;
Part II: Inference; while the likelihood function L a1,p,σ (y, α, z) not converges do Infer α by Kalman filter, Kalman smoothing and ""fake-path"" trick described in Section 4.1;
Update z a and z c by sampling from
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t )
, where the success probability {p a t } n t=1 and {p c t } n t=1 are defined in Equation ( 5) and ( 6);
Segment control on z c by Algorithm 2; Update σ by Equation ( 7) to (8);
Update a 1 such that its first two coordinates equal to the those of α 1 and the remaining (S − 1) coordinates equals to those of α S+1 ;
Calculate the likelihood function L a1,p,σ (y, α, z) given in Equation ( 4); end Part III: Forecasting; With a n and σ, use the generate procedure in Algorithm 1 to generate future time series y future with length m. Repeat the generative procedure to obtain multiple future paths y
(1) future , y
future , . . . , y
future ; Combine all the predictive paths give the distribution for the future time series forecasting. If needed, calculate the point-wise quantile to obtain predictive intervals. Use the point-wise average as our final forecasting result.
It is worth mentioning that our proposed methodology is downward compatible with many simpler state space time series models. By letting p c = 0, we assume there is no change point in the time series. By letting p a = 0, we assume there is no anomaly point in the time series. If both p c and p a are set to be 0, then our model is reduced to the classic state space time series model. Also, the seasonality and slope can be removed from our model, if we know there exists no such structure in the data.

[SIMULATION]
In this section, we study the synthetic data generated from our model. We let S = 7 and provide values for σ and a 1 . The change points and anomaly points are randomly generated. We use our generative procedure (Algorithm 1) to generate time series with total length 500 by fixed parameters. The first 350 points will be used as training set and the remaining 150 points will be used to evaluate the performance of forecasting.
When generating, we let the time series have weekly seasonality with S = 7. For σ we have σ = 0.1, σ u = 0.1, σ v = 0.0004, σ w = 0.01, σ r = 1, σ o = 4. For α 1 we have value for µ as 20, value for δ as 0, and value for seasonality as (1, 2, 4, −1, −3, −2)/10. For p we have p c = 4/350 and p a = 10/350. Despite that, to make sure that at least one change point is in existence, we force z c 330 = 1 and r 330 = 2. That is, for each time series we generate, its 330th point is a change point with the mean shifted up by 3. Also to be consistence with our assumption, we force z c i = z a i = 0, ∀351 ≤ i ≤ 500 so there exists no change point or anomaly point in the testing part. The top panel of Figure 3 shows one example of synthesis data. The blue line marks the separation between training and testing set. The blue dashed line indicates the locations for the change point, while the yellow dots indicate the positions of anomaly points. Also see Figure 3 for illustration on the results returned by implementing our proposed algorithm on the same dataset. The red line gives the fitting results in the first 350 points and forecasting results in the last 150 points. The change points detected are marked with vertical red dotted line, and the anomaly detected are flagged with purple squares. Figure 3 shows that on this dataset, our proposed algorithm yields perfect detection on both change points and anomaly points. In Figure 3, the gray part indicates the 90% predictive interval for forecasting. We run our generative model 100 times to produce 100 different time series, and implement multiply methods on each of them, and aggregate the results together for comparison. We include the following methodologies.  et al., 2008)), and the Prophet R package by Taylor & Letham (2017). We evaluate the performances by mean absolute percentage error (MAPE), mean square error (MSE) and mean absolute error (MAE) on forecasting set. The mathematical definition of these three criterion is given as follows. Let x 1 , x 2 , . . . , x n be the true value andx 1 ,x 2 , . . . ,x n be the estimation or predictive values.
Then we have
MAPE = 1 n n i=1 |x i −x i | x i , MSE = 1 n n i=1 (x i −x i ) 2 , MAE = 1 n n i=1 |x i −x i |.
The comparison of our proposed algorithm and the aforementioned algorithms are included below in Table 2. As we mentioned in Section 6, our algorithm is downward compatible with the cases ignoring the existence of change point or anomaly, by setting p c = 0 or p a = 0. We also run proposed algorithm on the synthetic data with p c = 0 (no change point), or p a = 0 (no anomaly point), or p c = p a = 0 (no change and anomaly point), for the purpose of numeric comparison.
From Table 2 it turns out that our proposed algorithm achieves the best performance compared to other existing methods. Our proposed algorithm also performs better compared with the cases ignoring change point or anomaly point. This is a convincing evidence on the importance of incorporating both change point structure and anomaly point structure when modeling, for time series forecasting.
We also compare our proposed method with other existing change point detection methods and anomaly detection algorithm with respect to the performance of detections. We evaluate the performance by two criterions: True Positive Rate (TPR) and False Positive (FP). TPR measures the percentage of change points or anomalies to be correctly detected. FP count the number of points wrongly detected as change points or anomaly points. The mathematical definitions of TPR and FP are as follows. Let (z 1 , z 2 , . . . , z n ) be the true binary vector for change points or anomalies, and points in total. We split it such that the first 3000 points are used as training set and last 1000 points are used to evaluate the forecasting performance. From Figure 4, it is obvious that there exists no seasonality or slope structure in the dataset. This motivates us not to include these two components in our model. We implement our proposed algorithm without seasonality and slope, and compare the forecasting performance with other methods in Table 5. Our method outperforms BSTS, ARIMA, ETS and Prophet. However in Table 5 the performance can be slightly improved if we ignore the existence of anomaly points by letting p a = 0. This may be caused by model mis-specification as the data may not generated in a way not entirely captured by our model. Nevertheless, the performances of our method considering anomaly points or not, are comparable to each other. In this dataset there is no ground-truth of change point and anomaly point on their locations or even existence. However, from bottom panel of Figure 4, there are some obvious changes in the sequence and they all successfully captured by our algorithm.

[INTERNET TRAFFIC DATA]
Our second real data is an Internet traffic data acquired from a major Tech company (see Figure 5).
It is a daily traffic data, with seasonality S = 7. We use the first 800 observations as training set and evaluate the performance of forecasting on the remaining 265 points. The bottom panel of Figure 5 show the result from implementing our algorithm. We also do the comparison of forecasting performance of our proposed algorithm together with other existing methods, shown in Table 6. We can also see that our algorithm outperforms all the other algorithms with respect to MAPE, MSE and MAE.
Compared to the aforementioned models, our work differs in Bayesian modeling which samples posterior to estimate hidden components given the independent Bernoulli priors of changing point and anomalies.

[CONCLUSION]
We incorporate the change point structure and anomaly point structure into the classic space state time series model. We provide a Bayesian scheme for inference and time series forecasting. We compare the performance of our methodology and state-of-the-art methods on both synthetic data and real datasets. Our method performs the best with respect to forecasting, change point detection, and anomaly detection as well.

[]
(ẑ 1 ,ẑ 2 , . . . ,ẑ n ) are the estimated ones. Then
From the definition, we can see high TPR and low FP means the algorithm has better performance in detection.
The comparison on change point detection is shown in Table 3. We compare our results against three popular change point detection methods: Bayesian Change Point (BCP) (Barry & Hartigan, 1993), Change-Point (CP) (Killick & Eckley, 2014) and Breakout (twitter, 2017). From Table 3 our proposed method outperforms the most of the others by both TPR and FP. We have smaller TPR compared to CP, but we are better in FP.  In Table 4, we also compare the performance of our algorithm on anomaly detection with three existing common anomaly detection methods: the AnomalyDetection package by Twitter (2017), RAD by Netflix (2017) and Tsoutlier by Chen & Liu (1993). The comparison is listed in Table 4. We can see our method also outperforms most of the others with respect to anomaly detection, by both TPR and FP. RAD has slightly better TPR but its FP is much worse compared with ours.

[REAL DATA ANALYSIS]
In this section, we implement our proposed method on real-world datasets. We also compare its performance against other existing time series forecasting methodologies. We consider two datasets, one is a public data called Well-log dataset, and the other is an unpublished internet traffic dataset. The bottom panels of Figure 4 and Figure 5 give the result of our proposed algorithms. The blue line separates the training set and testing set. We use red line to show our fitting and forecasting result, vertical red dashed line to indicate change points and purple dots to indicate anomaly points. The gray part shows 90% predication interval.

[WELL-LOG DATA]
This dataset (Fearnhead & Clifford, 2003;JK & WJ, 1996) was collected when drilling a well. It measures the nuclear magnetic response, which provides geophysical information to analyze the structure of rock surrounding the well. This dataset is public and available online 2 . It has 4050 From Figure 5 our proposed algorithm identifies one change point (the 576th point, indicated by the vertical red dashed line), which can be confirmed that this is exactly the only one change point existing in this time series caused by the change of counting methods, by some external information. Thus, we give the perfect change point detection in this Internet traffic data.
For this Internet traffic dataset, since we have ground-truth for change point, we can compare the performance of change point detection of different methodologies. BCP returns posterior distribution, which peaks in the the 576th point with posterior probability value 0.5. And it also returns with many other points with posterior probability value around 0.1. CP returns 4 change points, where the 576th point (the only true one) is one of them. Breakout returns 8 change points without including the 576th point. To sum up, our proposed method achieves the best change point detection in this real dataset.

[RELATED WORK]
Parametric models are widely considered in econometric literature for time series forecasting, e.g. Jalles (2009), Commandeur et al. (2011), Gould et al. (2008, Harvey & Peters (1990), Harvey et al. (1998). The general procedure of decomposition method (using trend, seasonal and irregular components) for univariate structural time series modeling is discussed in Harvey & Peters (1990); a unified state space framework is proposed to handle any messy time series in Harvey et al. (1998); and the explicit modeling of both additive and multiplicative seasonalities in Gould et al. (2008); Jalles (2009). Although Kalman filter and MCMC-based approaches are used to sample posterior to estimate hidden components, the changing points and anomalies are not considered and processed in the above works. For example, the irregular component considered in Jalles ( 2009) is simply the noises. Commandeur et al. (2011) discusses the statistical software for state-space modeling which is designed for generic time series analytic and modeling, which cannot directly be used when changing point and anomalies are in existence. Our proposed approach shares similarity with the aforementioned papers as we have similar additive structure of components. However we are able to incorporate the change points and anomalies, two common structure widely observed in real data, into our model by using Bernoulli indicators. This is non-trivial, and cannot be handled by the aforementioned papers or their variants.
Non-parametric approaches are used for extraction of components from quasi-periodic time-series, e.g., the ensembles of weak detectors using non-parametric measurement are used in Artemov & Burnaev (2016) to detect change-points and anomalies, and the online decomposition algorithm based on per-component is adopted for change-point detection in Alexey Artemov (2015). Different from the above works, to handle the structural brakes and change-points, this paper presents the parametric approach for modeling anomalies and changing points by fitting them in the state-space framework using approximate inference for forecasting path prediction.
Different Bayesian approaches are proposed for change-point detection, e.g., Adams & MacKay (2007) performs Bayesian change point detection from online inference by generating the distribution estimation of the next unseen datum in the sequence given only data already observed, and the Bayesian Online CPD (BOCPD) algorithm proposed by Turner et al. (2009) performs online prediction using hidden variable given the underlying predictive model (UPM) and the hazard function.","[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.","{
    ""1"": {
        ""summary"": ""The paper lacks comparison with relevant literature and methods, lacks novelty, and does not fit the conference's scope. Not recommended for publication."",
        ""verbatim"": ""- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,\n- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,\n- the proposed model itself looks very similar to what can be found across econometric literature,\n- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.\n\nTherefore, also the paper is well-written, \n- it lacks novelty,\n- its topic does not perfectly fit topics of interest for ICLR,\nSo, I do not recommend this paper to be published.""
    },
    ""2"": {
        ""summary"": ""The paper's novelty is questioned and the assumption of normally distributed terms is critiqued; concerns about the lack of mean shift modeling."",
        ""verbatim"": ""My main concern is with the novelty. The work does not seem to be very novel.\n\nIn the beginning of section 3, the authors assume that all the terms that characterize the change-points and anomaly points are normally distributed with mean zero and different variance. However, in classic formulation for change-point or anomaly detection, usually there is also a mean shift other than the variance change.""
    },
    ""3"": {
        ""summary"": ""Concerns about the practical applicability and lack of guidelines for when to use the proposed methodology. Lack of clarity on why comparisons with certain methods were unsuccessful."",
        ""verbatim"": ""Experiments are not conclusive as to when or when not to choose this approach over existing methods\n\nWhat is surprising is that relatively simple approaches like ETS or STL work pretty much equally well (in some cases even better in terms of MSE) than the proposed approach, while more recent approaches - like BSTS - dramatically fail. It would be good if the authors could comment on why this might be the case.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Need for broader comparisons with existing models"",
        ""verbatim"": ""There might be a need for more detailed comparisons with a wider range of existing models to further solidify the superiority of the proposed method in time series forecasting.""
    },
    ""2"": {
        ""summary"": ""Complexity in implementation and understanding of the model"",
        ""verbatim"": ""The complexity of the Bayesian framework and the iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter, and Kalman smoothing may present challenges in implementation or understanding for some readers or practitioners.""
    },
    ""3"": {
        ""summary"": ""Limitations in certain real-world applications"",
        ""verbatim"": ""While the paper addresses the importance of change point and anomaly detection, there could be limitations in certain real-world applications that require more specific considerations or additional validations.""
    }
}",3,"{
    ""A1-B1"": {
        ""rationale"": ""Both points emphasize the insufficiency of comparative analysis in the paper being reviewed. Review A criticizes the paper for not comparing with relevant econometric literature and advanced methods, focusing only on simple methods, while Review B calls for broader comparisons with existing models to prove the proposed method's superiority. Both highlight a gap in comparative scope that would enhance the paper's credibility."",
        ""similarity"": ""8""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Concern about scalability for larger datasets or high-dimensional analysis"",
        ""verbatim"": ""Limited discussion on the scalability of the proposed methodology for larger datasets or high-dimensional time series analysis.""
    },
    ""2"": {
        ""summary"": ""Lack of detailed comparisons with other state-of-the-art methods"",
        ""verbatim"": ""Lack of detailed comparison with other state-of-the-art methods in the field of time series forecasting, anomaly detection, and change point detection.""
    },
    ""3"": {
        ""summary"": ""Insufficient discussion on computational complexity and efficiency"",
        ""verbatim"": ""Insufficient explanation of the computational complexity and efficiency of the iterative algorithm involving MCMC, Kalman filter, and Kalman smoothing for practical implementation.""
    }
}",3,"{
    ""A1-B2"": {
        ""rationale"": ""Both reviewers highlight the lack of detailed comparison with relevant methods in the field. Review A specifically points out the absence of comparison with advanced econometric and change-point detection methods, focusing only on simplistic ones, while Review B discusses the inability of the paper to compare with other state-of-the-art methods broadly, without giving precise details. The concern is similar across both reviews, emphasizing the inadequate exploration of existing literature or methodologies relevant to the paper's topic."",
        ""similarity"": ""8""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper may be rejected due to inadequate comparative analysis with existing methods."",
        ""verbatim"": ""<Lack of detailed comparison with existing methods> The paper may face rejection if it does not provide a thorough comparison with existing methodologies, making it difficult to assess the true novelty and improvement of the proposed approach.""
    },
    ""2"": {
        ""summary"": ""The paper might face rejection due to insufficient clarity in explaining complex algorithmic details."",
        ""verbatim"": ""<Insufficient clarity on algorithmic details> A potential reason for rejection could be the lack of clarity in explaining the iterative algorithm with MCMC, Kalman filter, and Kalman smoothing, which might hinder reproducibility and understanding of the methodology.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both points from Review A and Review B express a major concern about the lack of comparative analysis with existing methods. Review A specifically mentions the absence of comparisons to relevant advanced methods and notes only basic methods were considered. Similarly, Review B highlights the paper's potential rejection due to not providing thorough comparisons with existing methodologies, which is essential to assess the novelty and improvement."",
        ""similarity"": ""9""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Complexity and accessibility concerns for non-experts in Bayesian inference or state-space models."",
        ""verbatim"": ""The methods described are complex and might be difficult for practitioners without a strong background in Bayesian inference or state-space models to implement.""
    },
    ""2"": {
        ""summary"": ""Risk of overfitting due to extensive use of Bayesian methods and multiple hidden variables."",
        ""verbatim"": ""The extensive use of Bayesian methods with multiple layers of hidden variables might lead to overfitting, especially in simpler time series scenarios, calling the model's generalizability into question.""
    },
    ""3"": {
        ""summary"": ""Limited discussion on the computational demands of the proposed method."",
        ""verbatim"": ""There is limited discussion on the computational demands of the proposed method, which may be significant given the intricate Bayesian computations and iterative refinements.""
    }
}",3,"{
    ""A2-B2"": {
        ""rationale"": ""Both Review A2 and Review B2 express concerns related to the novelty and premises of the modeling method used. Review A2 discusses the traditional approach to change-point detection that includes mean shifts besides variance changes, critiquing the paper's assumption of normally distributed terms without these shifts. Review B2 brings up concerns about overfitting due to complex Bayesian methods and hidden variables, which similarly points to potential gaps or flaws in the method's foundational assumptions and its novelty."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Complexity in practical implementation"",
        ""verbatim"": ""Complexity of Implementation: The high complexity of the proposed algorithm\u2014integrating MCMC, Kalman filter, and Kalman smoothing\u2014might raise concerns regarding its practical applicability and efficiency in different contexts.""
    },
    ""2"": {
        ""summary"": ""Risk of overfitting with the model"",
        ""verbatim"": ""Potential Overfitting: With sophisticated models that include multiple components (change points, anomalies, trend, seasonality), there's a risk of overfitting, particularly in scenarios with limited data points, which might not have been fully addressed.""
    },
    ""3"": {
        ""summary"": ""Insufficient discussion on method limitations"",
        ""verbatim"": ""Limited Discussion on Limitations: The paper might be critiqued for not sufficiently discussing the limitations or potential failures of the proposed method in certain types of time series data or under specific conditions.""
    }
}",3,"{
    ""A1-B3"": {
        ""rationale"": ""Both Review A1 and Review B3 discuss the inadequacy in how the paper handles comparison and evaluation against relevant literature and methods. Review A highlights the paper's lack of comparison with relevant methodologies, emphasizing the use of only simple methods and not engaging with advanced techniques, while Review B points out the insufficient discussion on the method’s limitations, particularly in varying contexts which suggests an overlooking of thorough evaluation and comparative analysis."",
        ""similarity"": ""7""
    },
    ""A3-B1"": {
        ""rationale"": ""Review A3 notes concerns about the practical applicability of the proposed model, particularly questioning when it should be applied over existing methods. Review B1 similarly addresses concerns with the practical applicability but frames it in terms of the complexity of the algorithm. Both express concerns that affect practical usage, though A3 is more about actual application and B1 concerns the technical complexity."",
        ""similarity"": ""6""
    },
    ""A3-B2"": {
        ""rationale"": ""Review A3 discusses the shortcomings of the proposed approach in certain contexts (comparing newer and simpler models), which implies a potential issue of overfitting in specific scenarios. Review B2 explicitly addresses the risk of overfitting with the model provided by the paper, relating it especially to complex modeling with limited data points. Both points highlight problems in model applicability in certain data scenarios, suggesting concerns about overfitting and model robustness."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Lack of detailed comparative analysis with cutting-edge models"",
        ""verbatim"": ""While the paper states improvements over existing methods, it may lack detailed comparative analyses with recent state-of-the-art approaches, particularly machine learning methods that have been applied to similar problems in time-series forecasting.""
    },
    ""2"": {
        ""summary"": ""High implementation complexity could deter users"",
        ""verbatim"": ""The implementation complexity introduced by the combination of MCMC, Kalman filter, and Kalman smoothing can be seen as a barrier, particularly for potential users who may prefer simpler and more accessible models.""
    }
}",2,"{
    ""A1-B1"": {
        ""rationale"": ""Both points express a strong concern over the lack of comparative analysis with modern or advanced methods in the field. Review A emphasizes the absence of comparisons with relevant, advanced change-point detection methods and existing econometric literature, whereas Review B notes the lack of detailed comparative analyses with recent state-of-the-art approaches, particularly in machine learning applied to similar problems. Both critiques center on the paper's insufficient integration and benchmarking with contemporary methods, highlighting a critical gap in literature comparison and novelty."",
        ""similarity"": ""8""
    }
}",1," [Significance and novelty]
The paper presents a novel state space time series model that can capture the structures of change points and anomaly points in addition to trends and seasonality. This model, named BAYES, utilizes a Bayesian framework for inferring hidden variables, providing distributions and forecasting intervals with theoretical guarantees. The proposed model significantly improves time series forecasting performance compared to existing methods, demonstrated in both synthetic data and real data applications. This is a significant contribution to the field as it addresses a long-standing issue of change point and anomaly detection in time series forecasting, which is crucial in marketing, finance, and other quantitative fields.

[Potential reasons for acceptance]
First, the paper introduces a novel approach to time series foreasting, specifically a state space time series model (BAYES) that can capture change points and anomalies in addition to trends and seasonality. This is a crucial contribution to the field as it allows for more accurate change point and anomaly detection, a challenge in time series forecasting for various industries such as marketing and finance.

Second, the researchers present a Bayesian framework for inferring hidden variables, providing distributions and forecasting intervals. This approach offers provable theoretical properties, increasing the reliability and transparency of the results.

Third, the proposed methodology is shown to outperform existing methods in both synthetic data and real data applications, providing valuable insights for practitioners in various industries that rely on time series forecasting.

[Potential reasons for rejection]
First, the methodology implementation relies on iterative algorithms with MCMC, Kalman filter, and Kalman smoothing. These algorithms may not be easily implemented by all practitioners, limiting the accessibility of the proposed method.

Second, the paper lacks a comparison of the proposed model with other state-of-the-art models, which may call into question the novelty of the contribution.

Third, the paper's real data sets are not publicly available, limiting the replicability of the findings.

[Suggestions for improvement]
First, the authors could provide a more comprehensive comparison with other state-of-the-art models to solidify the novelty and significance of the proposed model.

Second, the researchers could consider providing detailed implementation instructions for the iterative algorithms used in the methodology, making it easier for practitioners to implement the proposed approach.

Third, the authors could make the real data sets publicly available to increase the transparency and replicability of the findings.

","[Potential reasons for acceptance]
1. Use of Bayesian approach for inferring the hidden variables of the model, which provably has some theoretical properties and has the potential for superior performance in the comparison with other conventional methods such as ARIMA and Holt-Winters.
2. Informative algorithm with MCMC, Kalman filter, and Kalman smoothing strategies are used for more accurate time series forecasting.
3. Synthetic data and real data applications show superior performance of the proposed methodology in time series forecasting compared to existing methods and change point detection and anomaly detection.

[Potential reasons for rejection]
1. English grammar and spelling errors in the manuscript indicate a lack of proof-reading and editing.
2. The structure of the manuscript is complicated, making it difficult for the main points of the section to be distinguished, and the content appears to be very rushed.
3. The manuscript lacks clarifications on various parameters with detailed explanations on the experiments, which could influence the paper's readiness for publication.

[Suggestions for improvement]
1. Proof-reread the manuscript for grammar and spelling errors to maintain a professional appearance.
2. Simplify the structure of the manuscript by reorganizing sections to better highlight the main points.
3. Provide detailed explanations on the parameters used in the experiments and their impacts on the results for clearer understanding.

[Significance and novelty]
1. The proposed methodology of change point forecasting and detection for time series using a Bayesian framework with MCMC approach presents novelty in the area of time series forecasting.
2. The superior performance on both synthetic data and real data applications, along with detailed analyses, show the potential for acceptance.","{
    ""1"": {
        ""summary"": ""Method complexity and accessibility issues due to reliance on advanced iterative algorithms"",
        ""verbatim"": ""First, the methodology implementation relies on iterative algorithms with MCMC, Kalman filter, and Kalman smoothing. These algorithms may not be easily implemented by all practitioners, limiting the accessibility of the proposed method.""
    },
    ""2"": {
        ""summary"": ""Absence of comparison with other state-of-the-art models"",
        ""verbatim"": ""Second, the paper lacks a comparison of the proposed model with other state-of-the-art models, which may call into question the novelty of the contribution.""
    },
    ""3"": {
        ""summary"": ""Limited replicability due to non-public real data sets"",
        ""verbatim"": ""Third, the paper's real data sets are not publicly available, limiting the replicability of the findings.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Manuscript contains numerous English grammar and spelling errors."",
        ""verbatim"": ""English grammar and spelling errors in the manuscript indicate a lack of proof-reading and editing.""
    },
    ""2"": {
        ""summary"": ""The manuscript's structure is overly complicated and unclear."",
        ""verbatim"": ""The structure of the manuscript is complicated, making it difficult for the main points of the section to be distinguished, and the content appears to be very rushed.""
    },
    ""3"": {
        ""summary"": ""Lack of detailed explanations for various experimental parameters."",
        ""verbatim"": ""The manuscript lacks clarifications on various parameters with detailed explanations on the experiments, which could influence the paper's readiness for publication.""
    }
}",3,"{
    ""A1-B2"": {
        ""rationale"": ""Both Review A and Review B highlight the absence of comparisons with other relevant models or literature. Review A discusses the lack of comparison with relevant econometric literature and advanced change-point detection methods, while Review B mentions the absence of comparisons with other state-of-the-art models, affecting the perceived novelty of the paper."",
        ""similarity"": ""8""
    },
    ""A2-B2"": {
        ""rationale"": ""Review A's concerns about the novelty of the paper and Review B's commentary on the lack of comparison with state-of-the-art models both fundamentally challenge the novelty of the proposed framework by questioning its differentiation from existing methods."",
        ""similarity"": ""9""
    }
}",2,{},0
Deep Anomaly Detection with Outlier Exposure,HyxCxhRcY7.json,"[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: ROC curve with Tiny Im-ageNet (D in ) and Textures (D test out ).
Table 2: Figure 2 :2Figure 2: OOD scores from PixelCNN++ on images from CIFAR-10 and SVHN.
Table 3: to refine a scraped D OE out auxiliary dataset to be appropriately close to D test in .SV HN CI FA R-10 CI FA R
Table 4: Figure 3 :3Figure 3: Root Mean Square Calibration Error values with temperature tuning and temperature tuning + OE across various datasets.
Table 5: .The Root Mean Square Calibration Error measures the square root of the expected squared difference between confidence and accuracy at a confidence level. It has the formula E C [(P(Y = Y |C = c) − c) 2 ] . A similar formulation which less severely penalizes large confidence-accuracy deviations is the Mean Absolute Value Calibration error, written E C [|P(Y = Y |C = c) − c|]. The MAD Calibration Error is a lower bound of the RMS Calibration Error. To empirically estimate these miscalibration measures, we partition the n samples of S into b bins {B 1 , B 2 , . . . , B b } with approximately 100 samples in each bin. UnlikeGuo et al. (2017), bins are not equally spaced since the distribution of confidence values is not uniform but dynamic. Concretely, the RMS Calibration Error is estimated with the numerically stable formula
Table 6: Figure 4 :4Figure 4: ROC curves with Tiny ImageNet as D in and Textures, Places365, LSUN, and ImageNet as D test out . Figures show the curves corresponding to the maximum softmax probability (MSP) baseline detector and the MSP detector with Outlier Exposure (OE).
Table 7: We use the ImageNet dataset with images from approximately 22 thousand classes as D OE WikiText-2 is a corpus of Wikipedia articles typically used for language modeling. We use WikiText-2 as D OE
Table 8: 8 in Appendix A. Notice that the SVHN classifier with OE can be used to detect new anomalies such as emojis and street view alphabet letters, even though D test OE is a dataset of natural images. Thus, Outlier Exposure helps models to generalize to unseen D test out distributions far better than the baseline. Comparisons between the MSP baseline and the MSP of the natural language classifier fine-tuned with OE. Results are percentages and averaged over 10 runs.Confidence Branch. A recently proposed OOD detection technique(DeVries & Taylor, 2018) involves appending an OOD scoring branch b : X → [0, 1] onto a deep network. Trained with samples from only D in , this branch estimates the network's confidence on any input. The creators of this technique made their code publicly available, so we use their code to train new 40-4 Wide Residual Network classifiers. We fine-tune the confidence branch with Outlier Exposure by adding 0.5E x∼D OE out[log b(x)] to the network's original optimization objective. In Table3, the baseline values are derived from the maximum softmax probabilities produced by the classifier trained with DeVries & Taylor (2018)'s publicly available training code. The confidence branch improves over this MSP detector, and after OE, the confidence branch detects anomalies more effectively. Comparison among the maximum softmax probability, Confidence Branch, and Confidence Branch + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages, and averaged across all D test out datasets.
Table 9: Comparison among the maximum softmax probability (MSP), MSP + GAN, and MSP + GAN + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages and averaged across all D test out datasets.
Table 10: Table 5: OOD detection results with a PixelCNN++ density estimator, and the same estimator after applying OE. The model's bits per pixel (BPP) scores each sample. All results are percentages. Test distributions D test out are described in Appendix A.
Table 11: ; expanded results and D test out descriptions are in Appendix F. In all cases, OE improves over the baseline, and the improvement is especially large for the word-level model.
Table 12: 
Table 13: NLP OOD example detection for the maximum softmax probability (MSP) baseline detector and the MSP detector after fine-tuning with Outlier Exposure (OE). All results are percentages and the result of 10 runs. Values are rounded so that 99.95% rounds to 100%.
Table 14: C TRAINING FROM SCRATCH WITH OUTLIER EXPOSURE USUALLY IMPROVES DETECTION PERFORMANCE Elsewhere we show results for pre-trained networks that are fine-tuned with OE. However, a network trained from scratch which simultaneously trains with OE tends to give superior results. For example, a CIFAR-10 Wide ResNet trained normally obtains a classification error rate of 5.16% and an FPR95 of 34.94%. Fine-tuned, this network has an error rate of 5.27% and an FPR95 of 9.50%. Yet if we instead train the network from scratch and expose it to outliers as it trains, then the error rate is 4.26% and the FPR95 is 6.15%. This architecture corresponds to a 9.50% RMS calibration error with OE fine-tuning, but by training with OE from scratch the RMS calibration error is 6.15%. Compared to fine-tuning, training a network in tandem with OE tends to produce a network with a better error rate, calibration, and OOD detection performance. The reason why we use OE for fine-tuning is because training from scratch requires more time and sometimes more GPU memory than fine-tuning.
Table 15: 
Table 16: OOD detection results on Penn Treebank examples and English Web Treebank outliers. All results are percentages.The D test out datasets come from the English Web Treebank(Bies et al., 2012), which contains text from five different domains: Yahoo! Answers, emails, newsgroups, product reviews, and weblogs. Other NLP D test out datasets we consider do not satisfy the language modeling assumption of continuity in the examples, so we do not evaluate on them.
Table 17: Table 12: Calibration results for the temperature tuned baseline and temperature tuning + OE.
Table 18: Calibration results for the softmax temperature tuning baseline, the same baseline after adding Posterior Rescaling, and temperature tuning + Posterior Rescaling + OE.

[INTRODUCTION]
Machine Learning systems in deployment often encounter data that is unlike the model's training data. This can occur in discovering novel astronomical phenomena, finding unknown diseases, or detecting sensor failure. In these situations, models that can detect anomalies (Liu et al., 2018;Emmott et al., 2013) are capable of correctly flagging unusual examples for human intervention, or carefully proceeding with a more conservative fallback policy.
Behind many machine learning systems are deep learning models (Krizhevsky et al., 2012) which can provide high performance in a variety of applications, so long as the data seen at test time is similar to the training data. However, when there is a distribution mismatch, deep neural network classifiers tend to give high confidence predictions on anomalous test examples . This can invalidate the use of prediction probabilities as calibrated confidence estimates (Guo et al., 2017), and makes detecting anomalous examples doubly important.
Several previous works seek to address these problems by giving deep neural network classifiers a means of assigning anomaly scores to inputs. These scores can then be used for detecting outof-distribution (OOD) examples (Hendrycks & Gimpel, 2017;Lee et al., 2018;Liu et al., 2018). These approaches have been demonstrated to work surprisingly well for complex input spaces, such as images, text, and speech. Moreover, they do not require modeling the full data distribution, but instead can use heuristics for detecting unmodeled phenomena. Several of these methods detect unmodeled phenomena by using representations from only in-distribution data.
In this paper, we investigate a complementary method where we train models to detect unmodeled data by learning cues for whether an input is unmodeled. While it is difficult to model the full data distribution, we can learn effective heuristics for detecting out-of-distribution inputs by exposing the model to OOD examples, thus learning a more conservative concept of the inliers and enabling the detection of novel forms of anomalies. We propose leveraging diverse, realistic datasets for this purpose, with a method we call Outlier Exposure (OE). OE provides a simple and effective way to consistently improve existing methods for OOD detection.
Through numerous experiments, we extensively evaluate the broad applicability of Outlier Exposure. For multiclass neural networks, we provide thorough results on Computer Vision and Natural Language Processing tasks which show that Outlier Exposure can help anomaly detectors generalize to and perform well on unseen distributions of outliers, even on large-scale images. We also demonstrate that Outlier Exposure provides gains over several existing approaches to out-of-distribution detection. Our results also show the flexibility of Outlier Exposure, as we can train various models with different sources of outlier distributions. Additionally, we establish that Outlier Exposure can make density estimates of OOD samples significantly more useful for OOD detection. Finally, we demonstrate that Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. Our code is made publicly available at https://github.com/hendrycks/outlier-exposure.

[RELATED WORK]
Out-of-Distribution Detection with Deep Networks. Hendrycks & Gimpel (2017) demonstrate that a deep, pre-trained classifier has a lower maximum softmax probability on anomalous examples than in-distribution examples, so a classifier can conveniently double as a consistently useful outof-distribution detector. Building on this work, DeVries & Taylor (2018) attach an auxiliary branch onto a pre-trained classifier and derive a new OOD score from this branch. Liang et al. (2018) present a method which can improve performance of OOD detectors that use a softmax distribution. In particular, they make the maximum softmax probability more discriminative between anomalies and in-distribution examples by pre-processing input data with adversarial perturbations (Goodfellow et al., 2015). Unlike in our work, their parameters are tailored to each source of anomalies. Lee et al. (2018) train a classifier concurrently with a GAN (Radford et al., 2016;Goodfellow et al., 2014), and the classifier is trained to have lower confidence on GAN samples. For each testing distribution of anomalies, they tune the classifier and GAN using samples from that out-distribution, as discussed in Appendix B of their work. Unlike Liang et al. (2018); Lee et al. (2018), in this work we train our method without tuning parameters to fit specific types of anomaly test distributions, so our results are not directly comparable with their results. Many other works (de Vries et al., 2016;Subramanya et al., 2017;Malinin & Gales, 2018;Bevandic et al., 2018) also encourage the model to have lower confidence on anomalous examples. Recently, Liu et al. (2018) provide theoretical guarantees for detecting out-of-distribution examples under the assumption that a suitably powerful anomaly detector is available.
Utilizing Auxiliary Datasets. Outlier Exposure uses an auxiliary dataset entirely disjoint from test-time data in order to teach the network better representations for anomaly detection. Goodfellow et al. (2015) train on adversarial examples to increased robustness. Salakhutdinov et al. (2011) pre-train unsupervised deep models on a database of web images for stronger features. Radford et al. (2017) train an unsupervised network on a corpus of Amazon reviews for a month in order to obtain quality sentiment representations. Zeiler & Fergus (2014) find that pre-training a network on the large ImageNet database (Russakovsky et al., 2015) endows the network with general representations that are useful in many fine-tuning applications. Chen & Gupta (2015); Mahajan et al. (2018) show that representations learned from images scraped from the nigh unlimited source of search engines and photo-sharing websites improve object detection performance.

[OUTLIER EXPOSURE]
We consider the task of deciding whether or not a sample is from a learned distribution called D in . Samples from D in are called ""in-distribution,"" and otherwise are said to be ""out-of-distribution"" (OOD) or samples from D out . In real applications, it may be difficult to know the distribution of outliers one will encounter in advance. Thus, we consider the realistic setting where D out is unknown. Given a parametrized OOD detector and an Outlier Exposure (OE) dataset D OE out , disjoint from D test out , we train the model to discover signals and learn heuristics to detect whether a query is sampled from D in or D OE out . We find that these heuristics generalize to unseen distributions D out . Deep parametrized anomaly detectors typically leverage learned representations from an auxiliary task, such as classification or density estimation. Given a model f and the original learning objective L, we can thus formalize Outlier Exposure as minimizing the objective
E (x,y)∼Din [L(f (x), y) + λE x ∼D OE out [L OE (f (x ), f (x), y)]
] over the parameters of f . In cases where labeled data is not available, then y can be ignored.
Outlier Exposure can be applied with many types of data and original tasks. Hence, the specific formulation of L OE is a design choice, and depends on the task at hand and the OOD detector used. For example, when using the maximum softmax probability baseline detector (Hendrycks & Gimpel, 2017), we set L OE to the cross-entropy from f (x ) to the uniform distribution (Lee et al., 2018). When the original objective L is density estimation and labels are not available, we set L OE to a margin ranking loss on the log probabilities f (x ) and f (x).

[EXPERIMENTS]
We evaluate OOD detectors with and without OE on a wide range of datasets. Each evaluation consists of an in-distribution dataset D in used to train an initial model, a dataset of anomalous examples D OE out , and a baseline detector to which we apply OE. We describe the datasets in Section 4.2. The OOD detectors and L OE losses are described on a case-by-case basis.
In the first experiment, we show that OE can help detectors generalize to new text and image anomalies. This is all accomplished without assuming access to the test distribution during training or tuning, unlike much previous work. In the confidence branch experiment, we show that OE is flexible and complements a binary anomaly detector. Then we demonstrate that using synthetic outliers does not work as well as using real and diverse data; previously it was assumed that we need synthetic data or carefully selected close-to-distribution data, but real and diverse data is enough. We conclude with experiments in density estimation. In these experiments we find that a cutting-edge density estimator unexpectedly assigns higher density to out-of-distribution samples than in-distribution samples, and we ameliorate this surprising behavior with Outlier Exposure.  We evaluate out-of-distribution detection methods on their ability to detect OOD points. For this purpose, we treat the OOD examples as the positive class, and we evaluate three metrics: area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPR), and the false positive rate at N % true positive rate (FPRN ). The AUROC and AUPR are holistic metrics that summarize the performance of a detection method across multiple thresholds. The AUROC can be thought of as the probability that an anomalous example is given a higher OOD score than a in-distribution example (Davis & Goadrich, 2006). Thus, a higher AUROC is better, and an uninformative detector has an AUROC of 50%. The AUPR is useful when anomalous examples are infrequent (Manning & Schütze, 1999), as it takes the base rate of anomalies into account. During evaluation with these metrics, the base rate of D test out to D test in test examples in all of our experiments is 1:5.
Whereas the previous two metrics represent the detection performance across various thresholds, the FPRN metric represents performance at one strict threshold. By observing performance at a strict threshold, we can make clear comparisons among strong detectors. The FPRN metric (Liu et al., 2018;Kumar et al., 2016;Balntas et al., 2016) is the probability that an in-distribution example (negative) raises a false alarm when N % of anomalous examples (positive) are detected, so a lower FPRN is better. Capturing nearly all anomalies with few false alarms can be of high practical value.

[IN-DISTRIBUTION DATASETS]
SVHN. The SVHN dataset (Netzer et al., 2011) contains 32 × 32 color images of house numbers. There are ten classes comprised of the digits 0-9. The training set has 604, 388 images, and the test set has 26, 032 images. For preprocessing, we rescale the pixels to be in the interval [0, 1]. CIFAR. The two CIFAR (Krizhevsky & Hinton, 2009) datasets contain 32 × 32 natural color images. CIFAR-10 has ten classes while CIFAR-100 has 100. CIFAR-10 and CIFAR-100 classes are disjoint but have similiarities. For example, CIFAR-10 has ""automobiles"" and ""trucks"" but not CIFAR-100's ""pickup truck"" class. Both have 50, 000 training images and 10, 000 test images. For this and the remaining image datasets, each image is standardized channel-wise. Tiny ImageNet. The Tiny ImageNet dataset (Johnson et al.) is a 200-class subset of the ImageNet (Russakovsky et al., 2015) dataset where images are resized and cropped to 64 × 64 resolution. The dataset's images were cropped using bounding box information so that cropped images contain the target, unlike Downsampled ImageNet (Chrabaszcz et al., 2017). The training set has 100, 000 images and the test set has 10, 000 images. Places365. The Places365 training dataset (Zhou et al., 2017)  For classification tasks on 20 Newsgroups, TREC, and SST, we treat each sentence of WikiText-2 as an individual example, and use simple filters to remove low-quality sentences.

[MULTICLASS CLASSIFICATION]
In what follows, we use Outlier Exposure to enhance the performance of existing OOD detection techniques with multiclass classification as the original task. Throughout the following experiments, we let x ∈ X be a classifier's input and y ∈ Y = {1, 2, . . . , k} be a class. We also represent the classifier with the function f : X → R k , such that for any x, 1 T f (x) = 1 and f (x) 0.

[MAXIMUM SOFTMAX PROBABILITY (MSP).]
Consider the maximum softmax probability baseline (Hendrycks & Gimpel, 2017) which gives an input x the OOD score − max c f c (x). Out-ofdistribution samples are drawn from various unseen distributions (Appendix A). For each task, we test with approximately twice the number of D test out distributions compared to most other papers, and we also test on NLP tasks. The quality of the OOD example scores are judged with the metrics described in Section 4.1. For this multiclass setting, we perform Outlier Exposure by fine-tuning a pre-trained classifier f so that its posterior is more uniform on D OE out samples. Specifically, the fine-
tuning objective is E (x,y)∼Din [− log f y (x)] + λE x∼D OE out [H(U; f (x))]
, where H is the cross entropy and U is the uniform distribution over k classes. When there is class imbalance, we could encourage f (x) to match (P (y = 1), . . . , P (y = k)); yet for the datasets we consider, matching U works well enough. Also, note that training from scratch with OE can result in even better performance than fine-tuning (Appendix C). This approach works on different architectures as well (Appendix D).
Unlike Liang et al. (2018); Lee et al. (2018) and like Hendrycks & Gimpel (2017); DeVries & Taylor (2018), we do not tune our hyperparameters for each D test out distribution, so that D test out is kept unknown like with real-world anomalies. Instead, the λ coefficients were determined early in experimentation with validation D val out distributions described in Appendix A. In particular, we use λ = 0.5 for vision experiments and λ = 1.0 for NLP experiments. Like previous OOD detection methods involving network fine-tuning, we chose λ so that impact on classification accuracy is negligible.
For nearly all of the vision experiments, we train Wide Residual Networks (Zagoruyko & Komodakis, 2016) and then fine-tune network copies with OE for 10 epochs. However we use a pretrained ResNet-18 for Places365. For NLP experiments, we train 2-layer GRUs (Cho et al., 2014) for 5 epochs, then fine-tune network copies with OE for 2 epochs. Networks trained on CIFAR-10 or CIFAR-100 are exposed to images from 80 Million Tiny Images, and the Tiny ImageNet and Places365 classifiers are exposed to ImageNet-22K. NLP classifiers are exposed to WikiText-2. Further architectural and training details are in Appendix B. For all tasks, OE improves average performance by a large margin. Averaged results are shown in Tables 1 and 2. Sample ROC curves are shown in Figures 1 and 4. Detailed results on individual D test out datasets are in Table 7 and Table (Hafner et al., 2018). A method with better success is from Lee et al. (2018). They carefully train a GAN to generate synthetic examples near the classifier's decision boundary. The classifier is encouraged to have a low maximum softmax probability on these synthetic examples. For CIFAR classifiers, they mention that a GAN can be a better source of anomalies than datasets such as SVHN. In contrast, we find that the simpler approach of drawing anomalies from a diverse dataset is sufficient for marked improvements in OOD detection.
We train a 40-4 Wide Residual Network using Lee et al. (2018)'s publicly available code, and use the network's maximum softmax probabilities as our baseline. Another classifier trains concurrently with a GAN so that the classifier assigns GAN-generated examples a high OOD score. We want each D test out to be novel. Consequently we use their code's default hyperparameters, and exactly one model encounters all tested D test out distributions. This is unlike their work since, for each D test out distribution, they train and tune a new network. We do not evaluate on Tiny ImageNet, Places365, nor text, since DCGANs cannot stably generate such images and text reliably. Lastly, we take the network trained in tandem with a GAN and fine-tune it with OE. Table 4 shows the large gains from using OE with a real and diverse dataset over using synthetic samples from a GAN.    (Nalisnick et al., 2019). Consequently, density estimates are another means by which to score anomalies (Zong et al., 2018). We show the ability of OE to improve density estimates on low-probability, outlying data.
PixelCNN++. Autoregressive neural density estimators provide a way to parametrize the probability density of image data. Although sampling from these architectures is slow, they allow for evaluating the probability density with a single forward pass through a CNN, making them promising candidates for OOD detection. We use Pix-elCNN++ (Salimans et al., 2017) as a baseline OOD detector, and we train it on CIFAR-10. The OOD score of example x is the bits per pixel (BPP), defined as nll(x)/num_pixels, where nll is the negative log-likelihood. With this loss we fine-tune for 2 epochs using OE, which we find is sufficient for the training loss to converge. Here OE is implemented with a margin loss over the log-likelihood difference between in-distribution and anomalous examples, so that the loss for a sample x in from D in and point x out from D OE out is max{0, num_pixels + nll(x in ) − nll(x out )}.
Results are shown in Table 5. Notice that PixelCNN++ without OE unexpectedly assigns lower BPP from SVHN images than CIFAR-10 images. For all D test out datasets, OE significantly improves results. Language Modeling. We next explore using OE on language models. We use QRNN (Merity et al., 2018a;b) language models as baseline OOD detectors. For the OOD score, we use bits per character (BPC) or bits per word (BPW), defined as nll(x)/sequence_length, where nll(x) is the negative log-likelihood of the sequence x. Outlier Exposure is implemented by adding the cross entropy to the uniform distribution on tokens from sequences in D OE out as an additional loss term. For D in , we convert the language-modeling version of Penn Treebank, split into sequences of length 70 for backpropagation for word-level models, and 150 for character-level models. We do not train or evaluate with preserved hidden states as in BPTT. This is because retaining hidden states would greatly simplify the task of OOD detection. Accordingly, the OOD detection task is to provide a score for 70-or 150-token sequences in the unseen D test out datasets. We train word-level models for 300 epochs, and character-level models for 50 epochs. We then fine-tune using OE on WikiText-2 for 5 epochs. For the character-level language model, we create a character-level version of WikiText-2 by converting words to lowercase and leaving out characters which do not appear in PTB. OOD detection results for the word-level and character-level language models are shown in 
FPR95 ↓ AUROC ↑ AUPR ↑ D in D

[DISCUSSION]
Extensions to Multilabel Classifiers and the Reject Option. Outlier Exposure can work in more classification regimes than just those considered above. For example, a multilabel classifier trained on CIFAR-10 obtains an 88.8% mean AUROC when using the maximum prediction probability as the OOD score. By training with OE to decrease the classifier's output probabilities on OOD samples, the mean AUROC increases to 97.1%. This is slightly less than the AUROC for a multiclass model tuned with OE. An alternative OOD detection formulation is to give classifiers a ""reject class"" (Bartlett & Wegkamp, 2008). Outlier Exposure is also flexible enough to improve performance in this setting, but we find that even with OE, classifiers with the reject option or multilabel outputs are not as competitive as OOD detectors with multiclass outputs.
Flexibility in Choosing D OE out . Early in experimentation, we found that the choice of D OE out is important for generalization to unseen D test out distributions. For example, adding Gaussian noise to samples from D in to create D OE out does not teach the network to generalize to unseen anomaly distributions for complex D in . Similarly, we found in Section 4.3 that synthetic anomalies do not work as well as real data for D OE out . In contrast, our experiments demonstrate that the large datasets of realistic anomalies described in Section 4.2.2 do generalize to unseen D test out distributions. In addition to size and realism, we found diversity of D OE out to be an important factor. Concretely, a CIFAR-100 classifier with CIFAR-10 as D OE out hardly improves over the baseline. A CIFAR-10 classifier exposed to ten CIFAR-100 outlier classes corresponds to an average AUPR of 78.5%. Exposed to 30 such classes, the classifier's average AUPR becomes 85.1%. Next, 50 classes corresponds to 85.3%, and from thereon additional CIFAR-100 classes barely improve performance. This suggests that dataset diversity is important, not just size. In fact, experiments in this paper often used around 1% of the images in the 80 Million Tiny Images dataset since we only briefly fine-tuned the models. We also found that using only 50,000 examples from this dataset led to a negligible degradation in detection performance. Additionally, D OE out datasets with significantly different statistics can perform similarly. For instance, using the Project Gutenberg dataset in lieu of WikiText-2 for D OE out in the SST experiments gives an average AUROC of 90.1% instead of 89.3%.

[CLOSENESS OF D TEST]
out , D OE out , and D test in . Our experiments show several interesting effects of the closeness of the datasets involved. Firstly, we find that D test out and D OE out need not be close for training with OE to improve performance on D test out . In Appendix A, we observe that an OOD detector for SVHN has its performance improve with Outlier Exposure even though (1) D OE out samples are images of natural scenes rather than digits, and (2) D test out includes unnatural examples such as emojis. We observed the same in our preliminary experiments with MNIST; using 80 Million Tiny Images as D OE out , OE increased the AUPR from 94.2% to 97.0%.
Secondly, we find that the closeness of D OE out to D test in can be an important factor in the success of OE. In the NLP experiments, preprocessing D OE out to be closer to D in improves OOD detection performance significantly. Without preprocessing, the network may discover easy-to-learn cues which reveal whether the input is in-or out-of-distribution, so the OE training objective can be optimized in unintended ways. That results in weaker detectors. In a separate experiment, we use Online Hard Example Mining so that difficult outliers have more weight in Outlier Exposure. Although this improves performance on the hardest anomalies, anomalies without plausible local statistics like noise are detected slightly less effectively than before. Thus hard or close-to-distribution examples do not necessarily teach the detector all valuable heuristics for detecting various forms of anomalies. Real-world applications of OE could use the method of Sun et al. (2018)   OE Improves Calibration. When using classifiers for prediction, it is important that confidence estimates given for the predictions do not misrepresent empirical performance. A calibrated classifier gives confidence probabilities that match the empirical frequency of correctness. That is, if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires.
Existing confidence calibration approaches consider the standard setting where data at test-time is always drawn from D in . We extend this setting to include examples from D test out at test-time since systems should provide calibrated probabilities on both in-and out-of-distribution samples. The classifier should have low-confidence predictions on these OOD examples, since they do not have a class. Building on the temperature tuning method of Guo et al. (2017), we demonstrate that OE can improve calibration performance in this realistic setting. Summary results are shown in Figure 3. Detailed results and a description of the metrics are in Appendix G.

[CONCLUSION]
In this paper, we proposed Outlier Exposure, a simple technique that enhances many current OOD detectors across various settings. It uses out-of-distribution samples to teach a network heuristics to detect new, unmodeled, out-of-distribution examples. We showed that this method is broadly applicable in vision and natural language settings, even for large-scale image tasks. OE can improve model calibration and several previous anomaly detection techniques. Further, OE can teach density estimation models to assign more plausible densities to out-of-distribution samples. Finally, Outlier Exposure is computationally inexpensive, and it can be applied with low overhead to existing systems. In summary, Outlier Exposure is an effective and complementary approach for enhancing out-of-distribution detection systems.

[A EXPANDED MULTICLASS RESULTS]
Expanded mutliclass out-of-distribution detection results are in Table 7 and Table 8   Anomalous Data. For each in-distribution dataset D in , we comprehensively evaluate OOD detectors on artificial and real anomalous distributions D test out following Hendrycks & Gimpel (2017). For each learned distribution D in , the number of test distributions that we compare against is approximately double that of most previous works.
Gaussian anomalies have each dimension i.i.d. sampled from an isotropic Gaussian distribution. Rademacher anomalies are images where each dimension is −1 or 1 with equal probability, so each dimension is sampled from a symmetric Rademacher distribution. Bernoulli images have each pixel sampled from a Bernoulli distribution if the input range is [0, 1]. Blobs data consist in algorithmically generated amorphous shapes with definite edges. Icons-50 is a dataset of icons and emojis (Hendrycks & Dietterich, 2019); icons from the ""Number"" class are removed. Textures is a dataset of describable textural images (Cimpoi et al., 2014). Places365 consists in images for scene recognition rather than object recognition (Zhou et al., 2017). LSUN is another scene understanding dataset with fewer classes than Places365 (Yu et al., 2015). ImageNet anomalous examples are taken from the 800 ImageNet-1K classes disjoint from Tiny ImageNet's 200 classes, and when possible each image is cropped with bounding box information as in Tiny ImageNet. For the Places365 experiment, ImageNet is ImageNet-1K with all 1000 classes. With CIFAR-10 as D in , we use also CIFAR-100 as D test out and vice versa; recall that the CIFAR-10 and CIFAR-100 classes do not overlap. Chars74K is a dataset of photographed characters in various styles; digits and letters such as ""O"" and ""l"" were removed since they can look like numbers. Places69 has images from 69 scene categories not found in the Places365 dataset.
SNLI is a dataset of predicates and hypotheses for natural language inference. We use the hypotheses for D OE out . IMDB is a sentiment classification dataset of movie reviews, with similar statistics to those of SST. Multi30K is a dataset of English-German image descriptions, of which we use the English descriptions. WMT16 is the English portion of the test set from WMT16. Yelp is a dataset of restaurant reviews. English Web Treebank (EWT) consists of five individual datasets: Answers (A), Email (E), Newsgroups (N), Reviews (R), and Weblog (W). Each contains examples from the indicated domain.
Validation Data. For each experiment, we create a set of validation distributions D val out . The first anomalies are uniform noise anomalies where each pixel is sampled from U[0, 1] or U[−1, 1] depending on the input space of the classifier. The remaining D val out validation sources are generated by corrupting in-distribution data, so that the data becomes out-of-distribution. One such source of anomalies is created by taking the pixelwise arithmetic mean of a random pair of in-distribution images. Other anomalies are created by taking the geometric mean of a random pair of in-distribution images. Jigsaw anomalies are created by taking an in-distribution example, partitioning the image into 16 equally sized patches, and permuting those patches. Speckle Noised anomalies are created by applying speckle noise to in-distribution images. RGB Ghosted anomalies involves shifting and reordering the color channels of in-distribution images. Inverted images are anomalies which have some or all of their color channels inverted.

[B ARCHITECTURES AND TRAINING DETAILS]
For CIFAR-10, CIFAR-100, and Tiny ImageNet classification experiments, we use a 40-2 Wide Residual Network (Zagoruyko & Komodakis, 2016). The network trains for 100 epochs with a dropout rate of 0.3. The initial learning rate of 0.1 decays following a cosine learning rate schedule . During fine-tuning of the entire network, we again use a cosine learning rate schedule but with an initial learning rate of 0.001. We use standard flipping and data cropping augmentation, Nesterov momentum, and 2 weight decay with a coefficient of 5 × 10 −4 . SVHN architectures are 16-4 Wide ResNets trained for 20 epochs with an initial learning rate of 0.01 and no data augmentation. For Places365, we use a ResNet-18 pre-trained on Places365. In this Places365 experiment, we tune with Outlier Exposure for 5 epochs, use 512 outlier samples per iteration, and start with a learning rate of 0.0001. Outlier Exposure fine-tuning occurs with each epoch being the length of in-distribution dataset epoch, so that Outlier Exposure completes quickly and does involve reading the entire D OE out dataset. 

[D OE WORKS ON OTHER VISION ARCHITECTURES]
Outlier Exposure also improves vision OOD detection performance for more than just Wide ResNets. Table 9 shows that Outlier Exposure also improves vision OOD detection performance for ""All Convolutional Networks"" (Salimans & Kingma, 2016 

[G CONFIDENCE CALIBRATION]
Models integrated into a decision making process should indicate when they are trustworthy, and such models should not have inordinate confidence in their predictions. In an effort to combat a false sense of certainty from overconfident models, we aim to calibrate model confidence. A model is calibrated if its predicted probabilities match empirical frequencies. Thus if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires. Prior research (Guo et al., 2017;Nguyen & O'Connor, 2015;Kuleshov & Liang, 2015) considers calibrating systems where test-time queries are samples from D in , but systems also encounter samples from D test out and should also ascribe low confidence to these samples. Hence, we use OE to control the confidence on these samples.

[G.1 METRICS]
In order to evaluate a multiclass classifier's calibration, we present three metrics. First we establish context. For input example X ∈ X , let Y ∈ Y = {1, 2, . . . , k} be the ground truth class. Let Y be the model's class prediction, and let C be the corresponding model confidence or prediction probability. Denote the set of prediction-label pairs made by the model with S = {( y 1 , c 1 ), ( y 2 , c 2 ), . . . , ( y n , c n )}. 

[RMS AND MAD CALIBRATION ERROR]
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k 2 .
Along similar lines, the MAD Calibration Error-which is an improper scoring rule due to its use of absolute differences rather than squared differences-is estimated with
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k .
Soft F1 Score. If a classifier makes only a few mistakes, then most examples should have high confidence. But if the classifier gives all predictions high confidence, including its mistakes, then the previous metrics will indicate that the model is calibrated on the vast majority of instances, despite having systematic miscalibration. The Soft F1 score (Pastor-Pellicer et al., 2013;Hendrycks & Gimpel, 2017) is suited for measuring the calibration of a system where there is an acute imbalance between mistaken and correct decisions. Since we treat mistakes a positive examples, we can write the model's confidence that the examples are anomalous with c a = (1 − c 1 , 1 − c 2 , . . . , 1 − c n ). To indicate that an example is positive (mistaken), we use the vector m ∈ {0, 1} n such that m i = 1(y i = y i ) for 1 ≤ i ≤ n. Then the Soft F1 score is c T a m 1 T (c a + m)/2 . 

[G.2 SETUP AND RESULTS]
There are many ways to estimate a classifier's confidence. One way is to bind a logistic regression branch onto the network, so that confidence values are in [0, 1]. Other confidence estimates use the model's logits l ∈ R k , such as the estimate σ(max i l i ) ∈ [0, 1], where σ is the logistic sigmoid.
Another common confidence estimate is max i exp (l i )/ k j=1 exp (l j ) . A modification of this estimate is our baseline. Softmax Temperature Tuning. Guo et al. (2017) show that good calibration can be obtained by including a tuned temperature parameter into the softmax: p(y = i | x) = exp(l i /T )/ k j=1 exp(l j /T ). We tune T to maximize log likelihood on a validation set after the network has been trained on the training set.
Results. In this calibration experiment, the baseline is confidence estimation with softmax temperature tuning. Therefore, we train SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet classifiers with 5000, 5000, 5000, and 10000 training examples held out, respectively. A copy of this classifier is fine-tuned with Outlier Exposure. Then we determine the optimal temperatures of the original and OE-fine-tuned classifiers on the held-out examples. To measure calibration, we take equally many examples from a given in-distribution dataset D test in and OOD dataset D test out . Out-of-distribution points are understood to be incorrectly classified since their label is not in the model's output space, so calibrated models should assign these out-of-distribution points low confidence. Results are in Table 12. Outlier Exposure noticeably improves model calibration.

[G.3 POSTERIOR RESCALING]
While temperature tuning improves calibration, the confidence estimate p(y = i | x) cannot be less than 1/k, k the number of classes. For an out-of-distribution example like Gaussian Noise, a good model should have no confidence in its prediction over k classes. One possibility is to add a reject option, or a (k + 1)st class, which we cover in Section 5. A simpler option we found is to perform an affine transformation of p(y = i | x) ∈ [1/k, 1] with the formula ( p(y = i | x) − 1/k)/(1 − 1/k) ∈ [0, 1]. This simple transformation makes it possible for a network to express no confidence on an out-of-distribution input and improves calibration performance. As Table 13 shows, this simple 0-1 posterior rescaling technique consistently improves calibration, and the model fine-tuned with OE using temperature tuning and posterior rescaling achieved large calibration improvements.   In Figure 4, we show additional PR and ROC Curves using the Tiny ImageNet dataset and various anomalous distributions.

[ACKNOWLEDGMENTS]
We thank NVIDIA for donating GPUs used in this research. This research was supported by a grant from the Future of Life Institute.","[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.","{
    ""1"": {
        ""summary"": ""Concerns about the clarity of writing and structure, reference choices, and discrepancies in data presentation."",
        ""verbatim"": ""The structure of the writing does not clearly present the novel aspects of the paper as opposed to the previous works.""
    },
    ""2"": {
        ""summary"": ""Assumption of having access to and predicting future out-of-distribution data, and the relevance to traditional anomaly detection."",
        ""verbatim"": ""The biggest weakness in this paper is the assumption that we have access to out-of-distribution data, and that we will encounter data from that same distribution in the future.""
    },
    ""3"": {
        ""summary"": ""Lack of novelty and originality in the proposed OE loss for classification, insufficient comparative analysis with baseline methods."",
        ""verbatim"": ""OE loss for softmax classifier. For softmax classifier, the OE loss forces the posterior distribution to become uniform distribution on outlier dataset. I think this loss function is very similar to a confidence loss.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Need for clearer guidelines on auxiliary dataset selection"",
        ""verbatim"": ""The paper acknowledges the importance of the choice of the auxiliary dataset for Outlier Exposure, but further clarification or guidelines on how to select the dataset for optimal performance could strengthen the methodology.""
    },
    ""2"": {
        ""summary"": ""Concerns about the lack of hyperparameter tuning for specific anomaly types"",
        ""verbatim"": ""While not tuning hyperparameters for each test distribution is highlighted as a strength, it might raise questions on the possible impact of not tuning parameters for specific types of anomaly test distributions.""
    },
    ""3"": {
        ""summary"": ""Need for more extensive benchmarking against existing methods"",
        ""verbatim"": ""The comparison with existing methods could be further extended to include more diverse benchmarking scenarios or datasets to provide a more comprehensive evaluation of Outlier Exposure.""
    },
    ""4"": {
        ""summary"": ""Questions regarding scalability and computational efficiency"",
        ""verbatim"": ""Whether Outlier Exposure's computational efficiency can be maintained with larger datasets or more complex models could be a point of concern for scalability.""
    }
}",4,"{
    ""A2-B2"": {
        ""rationale"": ""Both points discuss the challenges associated with not tuning parameters for specific data distributions or anomaly types, bringing up concerns about the assumptions and practical repercussions of using Outlier Exposure in different scenarios."",
        ""similarity"": ""7""
    },
    ""A3-B3"": {
        ""rationale"": ""Both points are concerned with the evaluation of the proposed method against existing or baseline methods. Review A discusses the lack of comparative analysis with baseline methods regarding the OE loss for softmax classifier, while Review B talks about the need for more extensive benchmarking against existing methods to strengthen the evaluation."",
        ""similarity"": ""7""
    }
}",2,"{
    ""1"": {
        ""summary"": ""The paper lacks comparison with existing anomaly detection methods, potentially undermining its contribution."",
        ""verbatim"": ""Lack of comparison with existing anomaly detection methods could weaken the paper's contribution and significance in the field.""
    },
    ""2"": {
        ""summary"": ""There's limited discussion on the scalability and computational efficiency of the OE method."",
        ""verbatim"": ""Limited discussion on the scalability of OE method in real-world applications and its computational efficiency could be potential shortcomings.""
    },
    ""3"": {
        ""summary"": ""The paper needs more in-depth analysis of the auxiliary dataset characteristics that improve performance."",
        ""verbatim"": ""While the paper discusses characteristics of the auxiliary dataset that improve performance, a more in-depth analysis and exploration of these characteristics could enhance the paper's robustness.""
    }
}",3,"{
    ""A3-B1"": {
        ""rationale"": ""Both points address concerns about the lack of comparison and analysis with other techniques or methods. A3 highlights the insufficient comparative analysis with baseline methods specifically related to the proposed OE loss, while B1 focuses on the general lack of comparison with existing anomaly detection methods, which still pertains to evaluating the paper's novelty and thoroughness in the context of existing literature."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The paper lacks specific details on the characteristics of the auxiliary dataset that enhance performance, which might be a reason for rejection."",
        ""verbatim"": ""The paper mentions identifying characteristics of the auxiliary dataset that enhance performance but lacks specific details, which could be a potential reason for rejection.""
    },
    ""2"": {
        ""summary"": ""The analysis on the flexibility and robustness of Outlier Exposure is incomplete, potentially weakening the research."",
        ""verbatim"": ""While the paper mentions analyzing the flexibility and robustness of Outlier Exposure, a more thorough examination could strengthen the research.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""Lack of deeper theoretical exploration of the Outlier Exposure method"",
        ""verbatim"": ""Insufficient Theoretical Foundations: While the paper is robust in empirical results, it somewhat lacks a deeper theoretical exploration of why OE works effectively across different settings, which might lead to skepticism about its adaptability.""
    },
    ""2"": {
        ""summary"": ""Potential overfitting to auxiliary outlier data"",
        ""verbatim"": ""Possible Overfitting to Auxiliary Data: There's potential criticism that the model might overfit to the characteristics of the auxiliary outlier data used in training, thereby reducing its generalizability to truly novel outliers.""
    },
    ""3"": {
        ""summary"": ""Unclear impact on computational complexity and model performance"",
        ""verbatim"": ""Unclear Impact on Model Complexity and Performance: The paper does not extensively discuss the impact of incorporating OE on the computational complexity and the run-time performance of the models, which could be a concern for real-world applications.""
    }
}",3,"{
    ""A3-B1"": {
        ""rationale"": ""Both points discuss the need for a more nuanced understanding of the Outlier Exposure (OE) feature, with Review A elaborating on the similarity of the OE loss to a confidence loss, and Review B indicating the lack of theoretical exploration on why OE works. Both reviews suggest that further exploration into the theoretical underpinnings of OE could enhance the paper."",
        ""similarity"": ""6""
    }
}",0,"{
    ""1"": {
        ""summary"": ""The paper inadequately discusses the limitations of the Outlier Exposure approach."",
        ""verbatim"": ""Limited discussion on the limitations of OE""
    },
    ""2"": {
        ""summary"": ""Concerns about the model potentially overfitting to the auxiliary outlier dataset."",
        ""verbatim"": ""Potential overfitting concerns""
    },
    ""3"": {
        ""summary"": ""Concerns about the scalability and computational efficiency of implementing OE across large datasets."",
        ""verbatim"": ""Scalability and computational efficiency concerns""
    }
}",3,"{
    ""A3-B1"": {
        ""rationale"": ""Both Review A3 and Review B1 discuss aspects of the OE (Outlier Exposure) approach. Review A3 questions the novelty of the OE loss for classification, suggesting its similarity to existing methods, while Review B1 notes a limited discussion on the limitations of the OE approach itself. Both points are concerned with the broader conceptual and theoretical frameworks of the OE methodology."",
        ""similarity"": ""6""
    }
}",0,"{
    ""1"": {
        ""summary"": ""Dependency and potential overfitting to the auxiliary outlier dataset"",
        ""verbatim"": ""One conceivable concern that could arise is the dependency and potential overfitting of the anomaly detectors to the auxiliary outlier dataset, which may not guarantee generalization across truly novel outliers.""
    },
    ""2"": {
        ""summary"": ""Lack of theoretical foundation underlying the Outlier Exposure approach"",
        ""verbatim"": ""While the empirical results are promising, the paper might face criticism for a lack of theoretical foundation underlying why Outlier Exposure works, which could be crucial for robust scientific communication.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both concerns focus on the issues related to handling out-of-distribution data. Review A criticizes the assumption of having access and predictable future occurrences of out-of-distribution data while Review B emphasizes the problem of overfitting to the auxiliary outlier dataset, which may compromise the model's ability to generalize to truly novel outliers."",
        ""similarity"": ""7""
    },
    ""A3-B1"": {
        ""rationale"": ""While A3 critiques the novelty and effectiveness of the OE loss in dealing with outliers in classification, B1 addresses broader concerns about the model's dependency on an auxiliary outlier dataset possibly leading to overfitting. Both points converge on the aspect of how the model handles outlier data although from different perspectives"",
        ""similarity"": ""6""
    }
}",1," [Significance and novelty]
The paper by the authors presents Outlier Exposure (OE), an innovative approach to improve anomaly detection in deep learning models, especially relevant in the context of larger and more complex inputs in deep learning. OE leverages diverse datasets of outliers to train anomaly detectors, enabling them to generalize and detect unseen anomalies, enhancing the significance and novelty of the proposed method in the field of deep learning-based anomaly detection (Anomaly Detection with Outlier Exposure).

[Potential reasons for acceptance]
The research finds that OE significantly improves detection performance on natural language processing and small- and large-scale vision tasks, offering a potential reason for acceptance as it addresses the challenge of detecting anomalous inputs in deep learning systems, especially relevant with the usage of larger and more complex data inputs.

[Potential reasons for rejection]
The paper lacks a clear explanation of the selection process for choosing the auxiliary dataset of outliers, which could be a potential reason for rejection if reviewers or readers find it as a critical concern. Also, there is limited justification offered regarding the scalability and applicability of OE to real-world use-cases.

[Suggestions for improvement]
To improve the paper, the authors could elaborate more on the dataset selection process, emphasizing the criteria used for picking outliers from diverse datasets, providing clear details on the representation, size, diversity, and distribution of the auxiliary dataset, data processing, preparation and preprocessing techniques. Additionally, a more comprehensive evaluation of OE on a larger number of use-cases, including both public and private datasets, could be performed for a more robust demonstration of the applicability and scalability of the proposed method. Furthermore, the authors could explore different OE designs and configurations to identify the best performing model and provide clear implementation details of the OE framework to facilitate reproducibility and transparency. Lastly, the authors could discuss any potential limitations, ethical concerns and future research directions of OE for a more roundened consideration of its contribution to the deep learning field.

","[Significance and novelty]
<Technical novelty and significance> The contributions are only marginally significant or novel. Some claims in the paper are not well-supported or require small changes to be supported. Additionally, the technical and empirical novelty and significance of the contributions are rated as marginal.

[Potential reasons for acceptance]
<Strong experimental results> The paper is well-written and the motivation is clear. The results of the extensive experiments on NLP and vision tasks are good, showing improvements in outlier exposure (OE) and comparison with generative models on CIFAR10 dataset.

[Potential reasons for rejection]
<Limited technical contribution> The technical novelty and significance of the contributions are rated as marginal, and some claims in the paper are not well-supported or require small changes to be supported.

[Suggestions for improvement]
<Expand literature review> An expanded literature review of related works with more deep learning and anomaly detection would provide a more thorough discussion of the importance of the work.
<Further justification of novelty> The paper could benefit from further justification of claimed novelties and significance of the contributions.
<Improving writing quality> The quality of writing can be improved, and the references should be organized for each section.","{
    ""1"": {
        ""summary"": ""Lack of clarity in the selection process for outliers"",
        ""verbatim"": ""The paper lacks a clear explanation of the selection process for choosing the auxiliary dataset of outliers""
    },
    ""2"": {
        ""summary"": ""Limited justification on scalability and applicability"",
        ""verbatim"": ""there is limited justification offered regarding the scalability and applicability of OE to real-world use-cases""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Marginal technical novelty and significance"",
        ""verbatim"": ""The technical novelty and significance of the contributions are rated as marginal.""
    },
    ""2"": {
        ""summary"": ""Unsupported or weakly supported claims"",
        ""verbatim"": ""Some claims in the paper are not well-supported or require small changes to be supported.""
    }
}",2,{},0,"{
    ""A3-B1"": {
        ""rationale"": ""Both points address the lack of novelty in the presented work. Review A critiques the proposed OE loss for classification as lacking originality, suggesting it resembles a previously known confidence loss. Review B comments on the marginal technical novelty and significance of the contributions. Both concern the issue of originality and novelty in the scientific contribution of the paper."",
        ""similarity"": ""7""
    }
}",1
Planning for Implicit Coordination using FOND,HJelxWI9w4.json,"[TITLE]
Planning for Implicit Coordination using FOND

[ABSTRACT]
Epistemic Planning can be used to achieve implicit coordination in cooperative multi-agent settings where knowledge and capabilities are distributed between the agents. In these scenarios, agents plan and act on their own without having to agree on a common plan or protocol beforehand. However, epistemic planning is undecidable in general. In this paper, we identify a decidable fragment of epistemic planning that allows for arbitrary initial state uncertainty and nondeterminism, but where actions can never increase the uncertainty of the agents. We show that in this fragment, planning with and without implicit coordination can be reduced to fully observable nondeterministic (FOND) planning and that it shares the same computational complexity. We also provide a small case study, modeling the problem of multi-agent path finding with destination uncertainty in FOND, to show that our compilation approach can be successfully applied in practice.

[CAPTIONS]
Table 1: Figure 1: A MAPF/DU instance.
Table 2: (:objects a1 a2 -agt l m r b -pos) (:init (adj l m) (adj m l) (adj m r) ; ... (ind a1 w1 w2) (ind a1 w2 w1) ; ... (ind a2 w1 w3) (ind a2 w3 w1) ; ... (goal w1 a1 r) (goal w1 a2 l) ; ... (goals for w2, w3, w4) (des w1) (next-choose)) (:goal (forall (?w -wld ?a -agt ?p -pos) (imply (and (des ?w) (goal ?w ?a ?p)) (at ?a ?p))))
Table 3: Case study.

[INTRODUCTION]
Epistemic planning has gained increasing interest in recent years (Baral et al. 2017). One of the main features of epistemic planning is the support of knowledge goals. For example, epistemic planning is well-suited to model problems in which information is to be confidentially passed between agents. The assumption is usually that there exists an explicit or implicit model about the distributed knowledge of the agents, as well as actions which can change the models.
However, recent work has shown that epistemic planning can also be used to achieve implicit coordination in a setting where multiple agents plan and act for themselves towards a cooperative goal (Engesser et al. 2017). The idea is that the explicit modeling of the agents' knowledge can be exploited as a means to enforce coordination via perspective taking. In particular, by putting themselves into the shoes of the others, agents can account for possible contributions of other agents in their own plans. Bolander et al. (2018) showed under which conditions such plans are guaranteed to be successful. This problem of planning for implicit coordination was originally formalized as a variant of contingent planning in the space of epistemic states (i.e., Kripke models), with actions represented by the action models from Dynamic Epis-temic Logic (van Ditmarsch et al. 2007). The formalization is very similar to the one of Bolander and Andersen (2011), which produces action sequences that can be interpreted as centralized plans. Bolander and Andersen have shown that this type of epistemic planning is undecidable in general. However, some decidable fragments have been identified that rely on restricting the structure of action models and the form of allowed preconditions (Aucher and Bolander 2013;Bolander et al. 2015;Charrier et al. 2016). On the practical side, Kominis and Geffner (2015) and Muise et al. (2015) have identified fragments of epistemic planning that can be solved by compilation to classical planning.
In this paper, we define a decidable fragment that captures contingent epistemic planning and that can be compiled to fully-observable nondeterministic (FOND) planning. Our fragment generalizes the fragment of Kominis and Geffner. We then show how our compilation can be extended to capture planning for implicit coordination. The key insight is that we can use nondeterminism to simulate perspective taking and thus account for the imperfect knowledge of the agents.

[THEORETICAL BACKGROUND]
We will first recapitulate the DEL planning framework using the conventions of Bolander and Andersen (2011), but including conditional effects in the style of van Benthem et al. (2006). We will then review strong fully-observable nondeterministic planning (Cimatti et al. 2003;Ghallab et al. 2004) as well as planning for implicit coordination (Engesser et al. 2017;Bolander et al. 2018).

[THE DEL PLANNING FRAMEWORK]
For a fixed set of agents A and a fixed set of atomic propositions P , the epistemic language L KC is given by the BNF
φ ::= p | ¬φ | φ ∧ φ | K i φ | Cφ,
where p ∈ P and i ∈ A.
We read K i φ as ""agent i knows φ"" and Cφ as ""it is common knowledge between all agents that φ"". The additional connectives ∨, ←, →, ↔ can be defined as abbreviations, analogously to their definition in propositional logic.
We evaluate such formulas over epistemic models. An epistemic model is a tuple M = W, (R i ) i∈A , V , where W is a non-empty, finite set of worlds (the domain of M), R i ⊆ W × W is an equivalence relation for each agent i ∈ A (the indistinguishability relation of i), and with V : P → 2 W (the valuation function). We write R * for the transitive closure of i∈A R i . The truth of a formula φ ∈ L KC in a world w of a model M is then given as follows, where the propositional cases are standard and hence left out:
M, w |= p iff w ∈ V (p) M, w |= K i φ iff M, w |= φ for all wR i w M, w |= Cφ iff M, w |= φ for all wR * w
We depict epistemic models as graphs where nodes correspond to the worlds in the model and are additionally labeled with the atomic propositions that are true in that particular world. The indistinguishability relations are given as labeled edges between the worlds. For readability, we will omit reflexive edges as well as edges that are implied by transitivity. Consider the following epistemic model:
M 0 = w 1 : p w 2 :
1, 2
In our example, both agents 1 and 2 do not know whether or not p is true (which is the case in w 1 ) or false (which is the case in w 2 ). Also, it is common knowledge between the two agents that they do not know. We will now define example actions for agent 1, first to sense the value of p and then to announce it to agent 2.
To define actions, we use event models. These can change the facts about the world as well as the knowledge of the agents. Analogous to epistemic models, an event model is a tuple E = E, (Q i ) i∈A , pre, eff , where E is a non-empty, finite set of events (the domain of E) and R i ⊆ W × W is an equivalence relation for each agent i ∈ A (the indistinguishability relation of i). Instead of a valuation function, we have two functions pre : E → L KC and eff : E → (P → L KC ), assigning a precondition and conditional effects to each event.
We depict event models analogously to epistemic models with the difference that nodes now correspond to events, which are additionally labeled with their respective preconditions and effects. Consider the following event model:
E sense = e 1 : p, {p → p} e 2 : ¬p, {p → p} 2
An event model updates an epistemic model by pairing up every world with every applicable event (i.e., of which the precondition is satisfied). Two updated worlds are indistinguishable for an agent if both the original worlds and the events are indistinguishable for that agent. Furthermore, a proposition is true in an updated world if and only if the event's conditional effect concerning that proposition evaluates to true in the original world.
For example, E sense consists of two events with preconditions p and ¬p. For both events, the effect is {p → p} meaning p will be true if p was true before (from now on, we will omit these trivial effects that preserve the value of an atomic proposition in our depiction of event models). Since the events are distinguishable for agent 1, the agent will, after the execution of the action, be able to distinguish worlds in which p is true from worlds in which p is false.
Formally, we define the product update M ⊗ E of model M = W, (R i ) i∈A , V with respect to an event model E = E, (Q i ) i∈A , pre, eff as model W , (R i ) i∈A , V where
• W = {(w, e) ∈ W × E | M, w |= pre(e)},
• R i = {((w, e), (w , e )) ∈ W × W | wR i w , eQ i e },
• V (p) = {(w, e) ∈ W | M, w |= eff(e, p)}.
In particular, if we apply E sense in M 0 , we obtain the following epistemic model:
M 0 ⊗ E sense = (w 1 , e 1 ) : p (w 2 , e 2 ) :
2
As intended, agent 1 knows now whether or not p is true. Note that additionally agent 2 is aware of this. The event model E sense represents semi-private sensing, meaning that even though the result of the sensing will only be known to agent 1, agent 2 will know that the sensing has taken place.
For planning, we usually consider pointed models (M, w), i.e., where one world w from the domain of M is designated as the actual world. In contrast, we model epistemic actions as multi-pointed event models (E, E d ) where E d is a subset of the domain of E. This is necessary, since sometimes we want the events to be deliberately chosen by the acting agents and sometimes by the environment. E.g., our semi-private sensing action should be defined as (E sense , {e 1 , e 2 }). Since both events are designated, it can be applied regardless of whether p is true or false. Applied in (M 0 , w 1 ), the action results in the pointed model (M 0 ⊗ E sense , (w 1 , e 1 )) and applied in (M 0 , w 2 ) it results in (M 0 ⊗ E sense , (w 2 , e 2 )). The similar action (E sense , {e 1 }) is only applicable in the case where p is true. It can, e.g., be used to model the action of a third agent semi-privately informing agent 1 that p is true.
Formally, an epistemic action (E, E d ) is applicable in (M, w) if there is an applicable event e ∈ E d , meaning that M, w |= pre(e). The application of (E, E d ) in (M, w) then nondeterministically leads to a pointed model (M ⊗ E(w, e)) such that M, w |= pre(e).
Note that any epistemic state represented by a pointed model (M, w), has infinitely many epistemically equivalent representations (i.e., other pointed models that satisfy the exact same set of formulas). It is a central theorem of modal logic that finite models are epistemically equivalent if and only if they are bisimilar. In the following, when using pointed models as states in a transition system, we think of them as representatives of their whole equivalence class. I.e., we consider two epistemic states (M, w) and (M , w ) as identical if they are epistemically equivalent. And we say two epistemic states (M, w) and (M , w ) are indistinguishable for an agent i if there is a world w in M that is indistinguishable to w for agent i such that (M, w ) and (M , w ) are identical. An initial epistemic state together with a set of epistemic actions thus induces a nondeterministic transition system where all states are epistemically different from each other.

[FOND PLANNING]
Our definition of FOND planning loosely follows the conventions of Ghallab et al. (2004). In particular, our actions consist of one common precondition and a set of possible effects, from which one will always be chosen nondeterministically. However, since we want to start out with a formalization that is as close as possible to our DEL formalism, we allow arbitrary propositional formulas as action preconditions and goals. We also use conditional effects which we restrict to effect normal form, which is a special case of Rintanens unary conditionality normal form (Rintanen 2003).
We define a FOND planning task as a tuple F, I, γ, Act where F is a set of fluents (atomic propositions), I ⊆ F is the initial state, γ is a propositional goal formula over F and Act is a set of actions. Each action a = pre a , effs a ∈ Act consists of a propositional formula pre a over F (the precondition) and a set effs a (the conditional effects). Each conditional effect e ∈ effs a is of the form f ∈F (χ e f £ f ) ∧ (χ e ¬f £ ¬f ), where χ e f and χ e ¬f are mutually inconsistent propositional formulas over F (i.e., their conjunction is unsatisfiable). They can be interpreted as ""effect e makes f true under the condition χ e f and false under the condition χ e ¬f "". Such a FOND task induces a finite transition system starting with the initial state I and connecting two states S and S via action a iff S |= pre a and there is an effect e ∈ effs a such that the conditional effects in e transform S to S . This gives us a trivial compilation from FOND to DEL. I.e., we compile the initial state into an epistemic state with exactly one world w 0 where V (p) = {w 0 } iff p ∈ I, or ∅ otherwise. And for each action a ∈ Act, we construct an epistemic action with one event for each nondeterministic effect e ∈ effs a , with precondition pre a and effect {f → χ e f ∨ (f ∧ ¬χ e ¬f ) | f ∈ F}. All events are designated and pairwise distinguishable for all agents. The transition system that we get from our compiled DEL state and actions is isomorphic to the FOND transition system and identified states share the same propositional valuation.
One solution to FOND planning tasks are strong plans. These are partial functions π from states to actions which satisfy the following properties (Cimatti et al. 2003):
• For every state s that is reachable via π from I, there is some state s that is reachable from s via π, s.t. s |= γ.
• There are no cycles, i.e. states s and s such that s is reachable via π from s and s is reachable via π from s.
Since the transition system is finite, following a strong policy always leads to a goal state in finitely many steps. It seems reasonable to assume that the concept of strong policies is also useful for contingent planning over epistemic states.

[IMPLICIT COORDINATION IN DEL]
We define an epistemic planning task as a tuple s 0 , A, ω, γ where s 0 is an epistemic state (the initial state), A is a finite set of epistemic actions (the action library), ω : A → A is a function mapping each action to its owner (the owner function), and γ ∈ L KC is the goal formula. E.g., consider the planning task with s 0 = (M 0 , w 1 ), A = {sense, ann p , ann ¬p } with sense = (E sense , {e 1 , e 2 }), ann p = (E annp , e 1 ) and ann ¬p = (E ann¬p , e 1 ). The actions ann p and ann ¬p are public announcement actions for agent 1, announcing that p is true, or respectively false. That is, the event models E ann¬p and E annp are given as follows:
E annp = e 1 : p, ∅ E ann¬p = e 1 : ¬p, ∅
We assume that all actions are owned by agent 1, i.e., ω = {sense → 1, ann p → 1, ann ¬p → 1}. The goal is for agent 2 to know whether or not p is true, i.e., γ = K 2 p ∨ K 2 ¬p.
A strong policy in the sense of Cimatti et al. (2003) would be to just apply the action ann p in s 0 . This is because the action is applicable in (M 0 , w 1 ) and its application would lead to a successor state consisting of only one world (w 1 , e 1 ) in which p (and K 2 p) is true. We argue that from the perspective of the agents (who initially do not know whether p is true or false), this is not a reasonable solution. If we want agent 1 to be able to come up with the plan for himself, we must consider his incomplete knowledge about the situation. Intuitively, a good plan for agent 1 is to first apply the sensing action and then, depending on the sensing result, apply the action ann p or ann q . This plan works for both states (M, w 1 ) and (M, w 2 ), which agent 1 considers possible.
To capture this, we have to require uniform policies. A uniform policy is a partial function π from epistemic states to sets of epistemic actions, satisfying the following constraints:
• Applicability: for each state s, and action a ∈ π(s), the action a has to be applicable in state s. • Uniformity: for each state s, and action a ∈ π(s), and states s that are indistinguishable to s for the owner ω(a) of the action, also a ∈ π(s ). This definition ensures that the agents can always infer from their own knowledge whether or not and how the policy wants them to act. This also implies that an action is only applicable by an agent, if the agent knows that the action is applicable. Note that because of the uniformity constraint, it is necessary to allow policies to assign multiple actions per state. E.g., sometimes we want a policy to assign an action a of agent 1 to some state s and an action b of agent 2 to some state s . Then by uniformity, if there is a state s that is indistinguishable to s for agent 1 and to s for agent 2, we have to assign both a and b to s .
We then say a uniform policy is subjectively strong, if it satisfies the exact properties of strong plans, but based on subjective reachability: A state s is a subjective successor of s given an action a if there is a successor state of s and a that is indistinguishable to s for agent ω(a). I.e., in our example, the subjective successors of (M 0 , w 1 ) and (E sense , {e 1 , e 2 }) are exactly the states (M 0 ⊗ E sense , (w 1 , e 1 )) and (M 0 ⊗ E sense , (w 2 , e 2 )). A state s is then subjectively reachable from s if either s is identical to s or s is subjectively reachable from a subjective successor of s.
In particular, a policy π that is subjectively strong for an epistemic planning task s 0 , A, ω, γ guarantees for each subjectively reachable state s and action a ∈ π(s), that π is also subjectively strong for s, A, ω, γ , as well as for all planning tasks s , A, ω, γ with an initial state s that is indistinguishable to s for ω(a).

[A DECIDABLE FRAGMENT OF DEL PLANNING]
A straight-forward way to obtain a decidable fragment of DEL planning is to ensure that the induced transition system is finite. This can be done by restricting the action set in such a way that the application of a single action can never lead to a state where the number of worlds is greater than in the state in which the action was applied. We achieve this by requiring our event models to be partitioned into disjoint connected components with mutually inconsistent preconditions. This allows us to think of each of the components as a single nondeterministic effect. Consider the following event model:
E pp = e 1 : p, ∅ e 2 : ¬p, ∅ e 3 : p, ∅2
For example, the action (E pp , {e 1 , e 3 }) could model an agent 3 trying to semi-privately announce p to agent 1. However, there is the possibility that the confidentiality of the announcement is compromised and p is thus effectively publicly announced. If we apply this action in
(M 0 , w 1 ), it re- sults either in (M 0 ⊗ E pp , (w 1 , e 1 )) or (M 0 ⊗ E pp , (w 1 , e 3 ))where
M 0 ⊗ E pp = (w 1 , e 1 ) : p (w 2 , e 2 ) : (w 1 , e 3 ) : p 2
Formally, for actions ( E, (Q i ), pre, eff , E d ) from our action set, we require that the domain E can be partitioned into disjoint subsets E 1 , . . . , E k such that (1) for each pair of events e, e ∈ E j from the same component j ∈ {1, . . . , k}, the preconditions pre(e) and pre(e ) are mutually inconsistent, and (2) two events e, e ∈ E are only allowed to be indistinguishable for an arbitrary agent i ∈ A, i.e. eQ i e , if they belong to the same component, i.e if there exists a j ∈ {1, . . . k} such that e, e ∈ E j .
We can see that if we apply such an action to an arbitrary epistemic state, due to condition (1), each world will be paired up by maximally one of the events of each component. Furthermore, due to condition (2), two worlds can only be distinguishable for any agent if the events they were generated with are from the same component. Thus the state resulting from an action application will consist of at most k connected components which consist each of less or equally many worlds than the original state. Since we can throw away all components that do not contain the updated designated world, we obtain a state that can be represented by less than or equally many worlds as the original state.
Our fragment is a generalization of the fragment introduced by Kominis and Geffner (2015), which allows exactly those actions that can be described with only mutually inconsistent preconditions (even between events from different components) and where all actions are thus deterministic.

[COMPILATION TO FOND]
In the following we will show how to generate a FOND planning task F, I, γ * , Act , given an epistemic planning task s 0 , A, ω, γ from our fragment.

[COMPILATION OF EPISTEMIC STATES]
We use the approach of Kominis and Geffner (2015) to represent epistemic states as classical states. The idea is that we generate fluents directly from the worlds and indistinguishability relation of the initial state s 0 , such that we can use them to encode the valuation functions and indistinguishability relations of arbitrary states reachable from s 0 .
Given the initial state s 0 = ( W, (R i ), V , w 0 ), we introduce a fluent p w ∈ F (read: ""p is true in world w"") for each proposition p ∈ P and world w ∈ W . Similarly, we introduce a fluent D {w1,w2} i (read: ""w 1 is distinguishable to w 2 for agent i"") for each agent i ∈ A and worlds w 1 , w 2 ∈ W with w 1 R i w 2 . Finally, for each world w ∈ W , we introduce the fluent w * ∈ F (read: ""w is the designated world"").
A propositional state S ⊆ F then represents an epistemic state ( W,
(R i ), V , w) where (1) w ∈ V (p) iff p w ∈ S, (2) w 1 R i w 2 iff D {w1,w2} i
∈ S, and (3) w = w iff w * ∈ S. For example, if s 0 = (M 0 , w 1 ), we will generate the set of fluents F = {p w1 , p w2 , D
{w1,w2} 1 , D {w1,w2} 2 , w * 1 , w * 2 }.
The initial state s 0 will then be I = {p w1 , w * 1 }.

[COMPILATION OF EPISTEMIC FORMULAS]
To check whether a propositional formula φ is true in world w of an epistemic state that is represented by a classical state S ⊆ F is simple. We replace the occurrences of each proposition p in φ by p w and check the resulting formula in S.
Checking formulas with knowledge operators is slightly more complicated. Kominis and Geffner (2015) use axioms to compile away all knowledge subformulas into derived variables, the values of which can be inferred in polynomial time.
We will simply assume that all of this is given and that we can thus compile each epistemic formula to a formula φ w that evaluates to true in a classical state representing an epistemic state (M, w 0 ) iff M, w |= φ.
For evaluating a formula directly in the designated world of a state (e.g., the goal formula), we use φ * , which we define as ( w∈W w * ) ∧ w∈W (w * → φ w ).

[COMPILATION OF EPISTEMIC ACTIONS]
We now show how an action a = ( E, (Q i ), pre, eff , E d ) that can be partitioned into distinct components E 1 , . . . , E k accordingly to our fragment, can be compiled into a FOND action pre a , effs a .
We know that an action is applicable in a state (M, w) if there is some event e ∈ E d such that M, w |= pre(e). We directly translate this to pre a = e∈E d pre(e) * . We can then translate each of the components of our event model into a different nondeterministic effect, i.e., we get effs a = {eff j | j = 1, ..., n}. These nondeterministic effects can make propositions true or false, as well as make worlds distinguishable or completely inaccessible. We construct each nondeterministic effect eff j as follows:
eff j = eff P + j ∧ eff P − j ∧ eff D+ j ∧ eff × j ∧ eff × × j
First, each fluent p w is made true or false accordingly to the effects of the event e ∈ E j that is applied in w.
eff P + j = w∈W
p∈P (∨ e∈Ej (pre(e) w ∧ eff(e, p) w ) £ p w ) eff P − j = w∈W p∈P (∨ e∈Ej (pre(e) w ∧ ¬ eff(e, p) w ) £ ¬p w )
Two worlds w and w become distinguishable if the events e and e they were updated with are distinguishable:
eff D+ j = w,w ∈W i∈A,w =w     
e,e ∈Ej ¬eQie (pre(e) w ∧ pre(e ) w ) £D
{w,w } i     
If in some world w, none of the events from E j are applicable, the world should not have a successor. We simulate this by making w distinguishable from all other worlds.
eff × j = w,w ∈W w =w ∧ e∈Ej ¬ pre(e) w £ D {w,w } i
If for the designated world w, there is no applicable event in E j , there should not even be a corresponding successor state. We model this by completely removing the designation w * . Thus, while the effect is still applicable, it leads to a state where all formulas φ * evaluate to false and therefore no actions are applicable and the goal is not satisfied.
eff × × j = ∧ e∈Ej ¬ pre(e) w £ ¬w *

[COMPILATION OF POLICIES]
Our compilation guarantees that the nondeterministic outcomes of an action that is applied in a propositional state corresponds exactly to the nondeterministic outcomes of the original epistemic action applied to the original epistemic state. Thus any strong policy for an epistemic planning task automatically corresponds to a strong policy in its FOND compilation. I.e., we can start in the initial state of the FOND compilation and extract a policy by successively applying the actions assigned by the original policy.
For the other direction, we can proceed similarly. However, we have to be careful about the fact that the policy can can contain multiple propositional states representing the same epistemic state. E.g., consider the states {p w1 , w * 1 } and {p w2 , w * 2 }. Having equivalent states in a policy is unproblematic, as long as one is never reachable from the other. To obtain a strong policy for the original problem, we can apply the same policy extraction procedure from above but ignore each state if we have seen an equivalent state before.
If the policy in our FOND compilation contains equivalent states such that one is reachable from the other, the extraction gets more difficult, as we have to take care of not introducing cycles into our policy. Fortunately, it is easy to argue that if there is no strong policy in the FOND compilation which doesn't include equivalent states that are reachable from each other, there will also be no strong policy that includes them. This is because the transition system looks exactly the same from these states and we do not gain anything from getting from one of the states to the other. This means that if there is a strong policy for the FOND compilation of an epistemic planning task, there also has to exist a strong policy that does not contain epistemically equivalent reachable states. Moreover, if the strong policy in the FOND compilation is optimal (i.e., its tree representation has minimal depth), it is clear that the policy cannot contain equivalent states that are reachable from each other. We thus obtain the following theorem.
Theorem 1. Let Π be an epistemic planning task from our fragment. Then there exists a strong policy for Π if and only if there exists a strong policy for the FOND compilation of Π. Any optimal strong policy for Π directly corresponds to an optimal strong policy for its compilation and vice versa.
The following theorem follows, given the EXPTIMEcompleteness of the plan existence problem for strong planning in FOND (Rintanen 2004).
Theorem 2. In our epistemic planning fragment, the problem of deciding whether there exists a strong policy for a given planning task is EXPTIME-complete.

[PLANNING FOR IMPLICIT COORDINATION]
As explained in our section about planning for implicit coordination, strong policies are not suitable if we want the agents to coordinate implicitly. In this section, we show how to use FOND planning to find subjectively strong plans for epistemic planning tasks from our fragment.

[THE COMPILATION]
We use the same compilation of states and formulas as before. However, we slightly modify the compilation of actions. The idea is to split each action from the action set into two: One auxiliary action for choosing the action that we want to apply in a state and one action that actually applies the effects of the previously selected action. In each choice action, we additionally simulate a perspective shift: We change the designated world nondeterministically to any of the worlds that are indistinguishable for the owner agent of the action. Thus, subjective successors of an action in the original problem are now objective successors.
This means that any strong policy in the compilation will correspond to a subjectively strong policy in the original problem. We can extract such a policy by taking all the apply-actions from our policy and and assigning the corresponding actions to the corresponding states in the original planning task.
Theorem 3. In our epistemic planning fragment, the problem of deciding whether there exists a subjectively strong policy is EXPTIME-complete.

[EXAMPLE: MAPF/DU]
We demonstrate our approach by modeling an instance of multi-agent path finding with destination uncertainty. The problem was first described by Bolander et al. (2018) and more thoroughly analyzed by Nebel et al. (2019). It is a generalization of the multi-agent path finding problem, relaxing the assumption that the agents' goals are commonly known. Instead, we assume that there are pairwise disjoint sets of plausible goal candidates for each agent, which are commonly known. Also, each agent can identify its own goal. As final action, each agent is allowed to announce that he has arrived at its true destination. The joint goal for the agents is that each agent is at his own true goal. Nebel et al. showed that the plan existence problem is PSPACE-complete. The naive algorithm they proposed has a runtime complexity of O(n a 2 +a ) where n is the number of graph vertices and a is the number of agents. Figure 1 shows an example of a MAPF/DU instance with two agents. The goal candidates of the square agent are r and b, and the goal candidates of the circle agent are l and m. One subjectively strong policy is for the square agent to first go to b and to let the circle agent move to l, independently of the actual destinations of the agents. Then, the square agent goes to its true destination (which, depending on the designated world, will be either r or b) and announces success there. Afterwards, the circle agent can go to his true destination (which will be either l or m). Note that after the initial movements of the square agent, the policy has to consider all 4 possible goal combinations. This is because the square agent does not know the actual goal of the circle agent and the circle agent will not know the actual goal of the square agent.
We now show how this problem can be modeled in PDDL (McDermott 1998). We will use the types agt for agents, pos for positions, and wld for worlds. We introduce fluents (at ?a ?p) to denote that agent ?a is at position ?p, (adj ?p ?q) to denote that an agent can step from position ?p to position ?q, and (announced ?a) to denote that the agent ?a has already announced success and will not move any longer. Furthermore, we use (goal ?w ?a ?p) to denote that the actual goal of agent ?a in world ?w is position ?p.
To denote indistinguishability of two worlds ?w1 and ?w2 for agent ?a, we use the fluent (ind ?a ?w1 ?w2). We mark the designated world ?w using the fluent (des ?w).
Finally, we use the predicates (next-choose), (next-move ?a ?p1 ?p2) and (next-announce ?a) to enforce the alternation of auxiliary perspectiveshifting actions and actual actions.
We now show how to split up the movement actions into the actions choose-move and move. The action (choose-move ?a ?w ?p ?q) simulates a perspec-tive shift to agent ?a by nondeterministically switching to an arbitrary world that is indistinguishable from the designated world for agent ?a. Furthermore, by setting the fluent (next-move ?a ?p ?q) to true, it enforces a movement action for agent ?a from ?p to ?q in the successor state. (and (not (des ?w)) (des w2))) ; ... Unfortunately, we have to enumerate all possible worlds to simulate the perspective shift. This forces us to include the worlds as constants into the domain definition. It would be more convenient if we had a dedicated construct in PDDL to automatically generate nondeterministic effects, e.g., by explicitly quantifying over objects (in our case, worlds).
The move action, which has to be applied afterwards, performs the actual change of the agent's position. This action also contains the actual precondition for movement actions: the field to move to has to be adjacent and empty. Also, the action prescribes the next action to be again a choose action by setting the fluent next-choose to true. The actions choose-announce and announce can be defined similarly. Announcing works by making all worlds where the agent has a different goal than its current position distinguishable to any other world for all agents. E.g., our example instance from Figure 1 can then be defined using the following initial state and goal descriptions: We tested our MAPF/DU planning domain using the myND planner of Mattmüller et al. (2010), which is to the Experiment time 2 agents, 4 cells, and 4 worlds 0.55s 3 agents, 6 cells, and 8 worlds 11.5s best of our knowledge the only publicly available FOND planner that supports both strong acyclic plans as well as conditional effects. It also supports axioms, although we did not need them for our example. Table 1 shows the performance of the planner on the example instance from Figure 1 as well as on a slightly bigger version with three agents.

[CONCLUSION]
In our paper, we have shown a decidable fragment of strong epistemic planning that has the same complexity than strong planning in FOND. We have also demonstrated how FOND planning can be used to generate subjectively strong plans. For future work, it is worth noticing that DEL can be used for modeling games. In particular, there is a translation from the game description language GDL-III to DEL (Engesser et al. 2018). There are some very interesting games which fall within our decidable fragment, one of which is Hanabi, which has gained some attention recently (Bard et al. 2019). While using a FOND planner does not seem to be feasible for problems of this size, it will be interesting to investigate how the idea of simulating perspective taking via nondeterminism can be incorporated into techniques such as Monte Carlo tree search or model-based reinforcement learning (e.g., value iteration in fully-observable MDPs).","[TITLE]
Planning for Implicit Coordination using FOND

[ABSTRACT]
Epistemic Planning can be used to achieve implicit coordination in cooperative multi-agent settings where knowledge and capabilities are distributed between the agents. In these scenarios, agents plan and act on their own without having to agree on a common plan or protocol beforehand. However, epistemic planning is undecidable in general. In this paper, we identify a decidable fragment of epistemic planning that allows for arbitrary initial state uncertainty and nondeterminism, but where actions can never increase the uncertainty of the agents. We show that in this fragment, planning with and without implicit coordination can be reduced to fully observable nondeterministic (FOND) planning and that it shares the same computational complexity. We also provide a small case study, modeling the problem of multi-agent path finding with destination uncertainty in FOND, to show that our compilation approach can be successfully applied in practice.","{
    ""1"": {
        ""summary"": ""Lack of additional insights into the characteristics of generated models for better handling by planning engines and addressing challenging aspects."",
        ""verbatim"": ""it would have been nice to provide some additional insights into the characteristics of the generated models. How can they be made easier to handle by planning engines? what are the most challenging aspects? Are there aspects of the language that are forcing the use of some caveats?""
    },
    ""2"": {
        ""summary"": ""Approach does not scale well and lacks evaluation over more instances/domains to validate its effectiveness."",
        ""verbatim"": ""it can be seen that the approach does not scale well. On the other hand, more instances/domains could shed more light into what we can expect from the approach.""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Complexity and limited scalability"",
        ""verbatim"": ""The complexity of the proposed planning approach and the compilation process may limit the scalability and applicability of the methods to real-world larger-scale problems.""
    },
    ""2"": {
        ""summary"": ""Lack of comprehensive evaluation"",
        ""verbatim"": ""The paper lacks a comprehensive evaluation on a wider range of scenarios or benchmarks to demonstrate the effectiveness and efficiency of the proposed techniques in various settings.""
    },
    ""3"": {
        ""summary"": ""High knowledge barrier for practical implementation"",
        ""verbatim"": ""It might be challenging for practitioners without a strong background in epistemic planning to fully understand and implement the methodologies presented in the paper.""
    }
}",3,"{
    ""A1-B3"": {
        ""rationale"": ""Both points express concerns about the complexity or challenging aspects when implementing or utilizing the proposed models or methodologies. Review A discusses the need to understand characteristics that impact planning engine handling and addresses tricky language-caused caveats, while Review B mentions challenges for practitioners lacking deep knowledge in the field."",
        ""similarity"": ""6""
    },
    ""A2-B1"": {
        ""rationale"": ""Both points critique the scalability of the approach, highlighting it as a limitation. Review A notes that the approach 'does not scale well,' while Review B talks about how 'complexity' impacts 'scalability and applicability to real-world larger-scale problems.'"",
        ""similarity"": ""8""
    },
    ""A2-B2"": {
        ""rationale"": ""Both points address the lack of thorough evaluation over diverse instances or scenarios. Review A states more instances/domains could provide better understanding, whereas Review B points out the absence of comprehensive evaluation across various settings."",
        ""similarity"": ""9""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Scope of Real-World Impact"",
        ""verbatim"": ""The paper may face rejection if it fails to clearly justify the practical significance and real-world impact of achieving implicit coordination in the identified fragment of epistemic planning.""
    },
    ""2"": {
        ""summary"": ""Lack of Empirical Validation"",
        ""verbatim"": ""If the empirical evaluation of the proposed approach is insufficient or lacks comparative analysis with existing methods, it could lead to rejection due to a lack of robust validation.""
    },
    ""3"": {
        ""summary"": ""Clarity and Interpretability Issues"",
        ""verbatim"": ""Inadequate clarity in explaining the reduction of planning with implicit coordination to FOND planning or in presenting the case study may raise concerns about the paper's comprehensibility and rigor, potentially leading to rejection.""
    }
}",3,"{
    ""A2-B2"": {
        ""rationale"": ""Both points highlight issues related to the scalability and thorough evaluation of the approach's effectiveness. Review A mentions the approach does not scale well and could benefit from evaluation over more instances, while Review B specifically refers to the insufficiency and lack of comparative analysis in empirical evaluation."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of depth in the case study"",
        ""verbatim"": ""The paper could be at risk of rejection if the case study presented lacks depth or fails to sufficiently demonstrate the effectiveness of the proposed approach in realistic scenarios.""
    },
    ""2"": {
        ""summary"": ""No comparison with existing methods"",
        ""verbatim"": ""Without a comparison with existing methods or approaches, the novelty and advantages of the proposed decidable fragment may not be adequately highlighted for acceptance.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both points discuss concerns regarding the effectiveness and the depth of investigation into the proposed approach. Review A speaks about the approach not scaling well and benefiting from more instances or domains to determine effectiveness, while Review B worries about the case study lacking depth to sufficiently demonstrate effectiveness."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Limitations in the generalizability of the model due to assumptions about actions not increasing uncertainty."",
        ""verbatim"": ""The assumption that actions cannot increase uncertainty might not be applicable or realistic in all multi-agent scenarios, which could limit the generalizability of the approach.""
    },
    ""2"": {
        ""summary"": ""Insufficient depth in experimental validation to robustly validate the theoretical model."",
        ""verbatim"": ""The case studies, while insightful, may not sufficiently validate the theoretical model against a diverse set of practical scenarios. Deeper and more extensive experimental results could be necessary to establish robustness.""
    },
    ""3"": {
        ""summary"": ""Potential reduction in utility due to over-simplification when reducing epistemic planning to FOND planning."",
        ""verbatim"": ""The reduction to FOND planning simplifies the problem but might also strip away significant characteristics of the epistemic states that could be crucial for certain applications, questioning the utility of such a reduction in some contexts.""
    }
}",3,"{
    ""A2-B2"": {
        ""rationale"": ""Both points mention the limitations in the validation process of the approach. Review A highlights the scaling issues and suggests that evaluating over more instances/domains could improve understanding, while Review B discusses the lack of depth in experimental validation which prevents robust establishment of the theoretical model’s merit."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The case studies included in the paper are insufficiently diverse and comprehensive to fully validate the efficiency and applicability of the approach."",
        ""verbatim"": ""Limited scope of case studies""
    },
    ""2"": {
        ""summary"": ""The paper does not compare its proposed methodological contributions against other existing solutions, hindering evaluation of its relative merit or innovation."",
        ""verbatim"": ""Lack of comparison with other methods""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both points express concerns about the adequacy of the validation of the approach. Review A critiques the scaling and insufficient evaluation over diverse instances, while Review B highlights the limited diversity and comprehensiveness of the case studies used in research. Both points suggest that a broader or more varied dataset could provide a better validation and provide more insights into the effectiveness of the methodology."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""The case study described may be too narrow or specific, impacting the generalizability of the findings."",
        ""verbatim"": ""The case study, while providing practical insights, might be too narrow or specific, which could limit the generalizability of the findings across different or more complex scenarios.""
    },
    ""2"": {
        ""summary"": ""The assumption that actions cannot increase uncertainty may not be valid in dynamic or unpredictable environments, potentially limiting the research's real-world applicability."",
        ""verbatim"": ""The assumption that actions can never increase uncertainty might not hold in more dynamic or less predictable real-world environments, potentially restricting the applicability of the research.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both points discuss concerns about the scalability and general scope of their studies or approaches. Review A mentions the approach does not scale well and suggests evaluating more instances or domains for better insights. Review B notes that the narrow scope of the case study might limit its generalizability. Both points are concerned with how the research might apply to a wider context or more varied scenarios."",
        ""similarity"": ""7""
    }
}",1," [Significance and novelty]

This paper makes a significant contribution to the field of AI planning by introducing a decidable fragment of epistemic planning, a type of planning that allows for implicit coordination in multi-agent systems where agents have distributed knowledge and capabilities. The novelty lies in the identification of a decidable fragment of epistemic planning that guarantees arbitrary initial state uncertainty and nondeterminism, while preventing the uncertainty from increasing for agents. This research is particularly important as it allows for the reduction of planning with implicit coordination to fully observable nondeterministic planning, which shares the same computational complexity.

[Potential reasons for acceptance]
The research presented in the paper is valuable due to its potential impact on the field of AI planning. The identification of a decidable fragment of epistemic planning provides a way for agents to coordinate and plan in a distributed environment without requiring agreement on a common plan or protocol beforehand.

Moreover, the paper provides a small case study that demonstrates the application of the proposed compilation approach in practice. The case study involves multi-agent path finding with destination uncertainty in the fully observable nondeterministic domain. The successful solution to the case study further validates the potential of the proposed approach.

[Potential reasons for rejection]
One potential reason for rejection could be the lack of a clear validation methodology within the case study presented in the paper. The case study only includes a single instance run of the algorithm, which may not provide enough evidence for the generalizability of the proposed approach.

Additionally, the decision procedure for the epistemic planning problem is described in a high-level manner, making it difficult to understand the details of the procedure without additional clarification from the authors.

[Suggestions for improvement]
To improve the paper, the authors may consider providing a more comprehensive validation of the proposed approach through multiple runs of the algorithm on a variety of problem instances. This would provide stronger evidence for the generalizability and scalability of the approach.

Additionally, the authors may provide a more detailed description of the decision procedure for the epistemic planning problem to allow for a better understanding of the underlying methods. This would improve transparency and allow for reproducibility of the research findings.

","[Significance and novelty]
:
<Addressing a significant problem in the field of planning> The paper introduces a novel approach to addressing epistemic planning problems, showcasing planning without shared explicit coordination between multiple agents, which can have general applicability to real-world scenarios involving decentralized control.

[Potential reasons for acceptance]
:
<Thorough and effective presentation> The paper is well-written, clear in its presentation, and provides adequate references to support its statements.
<Thorough literature survey and comparison on current work> The paper provides a thorough literature survey and comparison of the work, particularly in implicit coordination and planning literature.

[Potential reasons for rejection]
:
<Lack of clarity around notions like 'epistemic'> Reviewers had difficulties in understanding the paper due to ambiguous notions and undefined terms throughout, such as the term 'epistemic'.
<Lack of clarity in related work and organization> The related work section was very hard to find, and the organization of the paper could be improved for better understanding.

[Suggestions for improvement]
:
1.<Define terms and notions clearly> The authors should clearly define notions and terms in the introduction to improve the understanding for readers, including notions like 'epistemic'.
2.<Clarify the related work and improve organization> The authors should clarify related work, organize the paper better for improved understanding of the content.","{
    ""1"": {
        ""summary"": ""Lack of a thorough validation methodology in the case study"",
        ""verbatim"": ""One potential reason for rejection could be the lack of a clear validation methodology within the case study presented in the paper.""
    },
    ""2"": {
        ""summary"": ""Limited explanation of decision procedure for epistemic planning"",
        ""verbatim"": ""Additionally, the decision procedure for the epistemic planning problem is described in a high-level manner, making it difficult to understand the details of the procedure without additional clarification from the authors.""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Ambiguity and undefined terms in the paper"",
        ""verbatim"": ""Reviewers had difficulties in understanding the paper due to ambiguous notions and undefined terms throughout, such as the term 'epistemic'.""
    },
    ""2"": {
        ""summary"": ""Poor organization and unclear related work section"",
        ""verbatim"": ""The related work section was very hard to find, and the organization of the paper could be improved for better understanding.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both points discuss the inadequacy in demonstrating or evaluating the effectiveness of the approach or methodology. Review A critiques the method's scalability and lack of testing across different instances or domains, essentially pointing out a missing broader validation. Review B addresses the absence of a clear validation methodology in the case study, which aligns with the concern of proper evaluation of the method overall."",
        ""similarity"": ""7""
    }
}",1,{},0
Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Hyl_vjC5KQ.json,"[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Schematic sketch of our HRL approach. By using the advantage-weighted importance, the problem of finding the modes of the advantage-function can be reduced to that of finding the modes of the density of state action pairs.
Table 2: empirically show that this regularization improves the performance of learning latent discrete representations. When computing MI, we need to compute p(o) and H(o|s, a) given by p(o) = p πAd (s, a)p(o|s, a; η)dads = E (s,a)∼p π Ad (s,a) [p(o|s, a; η)] (8) H(o|s, a) = E (s,a)∼p π Ad (s,a) [p(o|s, a; η) log p(o|s, a; η)] .
Table 3: Figure 2 :2Figure 2: Activation of the four options over time steps on the Walker2d task.
Table 4: The activation of options over time and snapshots of the learned option policies on the Walker2d task are shown in Figure2, which visualizes the result from adInfoHRL with four options. One can see that the option policies are activated in different phases of locomotion. While the option indicated by yellow in Figure2corresponds to the phase for kicking the floor, the option indicated by blue corresponds to the phase when the agent was on the fly. Visualization of the options learned on the HalfCheetah and Ant tasks are shown in Appendix D.
Table 5: Output of the option network in the state-action space on Walker2d. Activation of options in the state space on Walker2d.
Table 6: Figure 3 :3Figure 3: Performance of adInfoHRL. (a)-(d) show comparison with baseline methods. (e) and (f) show the output of the option network and the activation of options on Walker2d, respectively.
Table 7: mutual information (MI) between the latent variable o and the state action pair (s, a) is defined as I (s, a), o = H(o) − H(o|s, a) (20) where H(o) = p(o) log p(o)do and H(o|s, a) = p(o|s, a) log p(o|s, a)do. We make the empirical estimate of MI employed by Gomes et al. (2010); Hu et al. (2017) and modify it to employ the importance weight. The empirical estimate of MI with respect to the density induced by a policy π is given byÎ (s, a; o) = o∈Op (o) logp(o) −Ĥ(o|s, a). (21) We consider the case where we have samples collected by a behavior policy β(s|a) and need to estimate MI with respect to the density induced by policy π. Given a model p(o|s, a; η) parameterized by vector η, p(o) can be rewritten as p(o) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η)dads = E [W (s, a)p(o|s, a; η)] ,
Table 8: whereW (s, a) =W (s,a) N j=1W (sj ,aj ) is the normalized importance weight. Likewise, the conditional entropy with respect to the density induced by a policy π is given by H(o|s, a) = p π (s, a)p(o|s, a; η) log p(o|s, a; η)dsda (24) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η) log p(o|s, a; η)dsda (25) = E [W (s, a)p(o|s, a; η) log p(o|s, a; η)] .
Table 9: o∈O π(o|s)π(a|s, o)Q π (s, a)da = o∈O π(o|s) π(a|s, o)Q π (s, a)da (28) Since option policies are deterministic given by µ o θ (s), the state-value function is given by V (s) = o∈O π(o|s)Q π (s, µ o θ (s))da. (29)
Table 10: Activation of options in the state space on HalfCheetah-v1.
Table 11: Figure 4 :4Figure 4: Distribution of options on the HalfCheetah-v1 task using adInfoHRL with two options. The dimensionality is reduced by t-SNE for visualization.
Table 12: Figure 5 :5Figure 5: Activation of options over time steps on the HalfCheetah-v1 task using adInfoHRL with two options.
Table 13: The outputs of the option network and the activation of options on Walker2d are shown in Fig-
Table 14: Hyperparameters of adInfoHRL used in the experiment.
Table 15: Hyperparameters of PPO used in the experiment. We tuned hyperparameters for our tasks, which are defferent from the default parameters in OpenAI baselines(Dhariwal et al., 2017).

[INTRODUCTION]
Reinforcement learning (RL) has been successfully applied to a variety of tasks, including board games , robotic manipulation tasks (Levine et al., 2016), and video games (Mnih et al., 2015). Hierarchical reinforcement learning (HRL) is a type of RL that leverages the hierarchical structure of a given task by learning a hierarchical policy (Sutton et al., 1999;Dietterich, 2000). Past studies in this field have shown that HRL can solve challenging tasks in the video game domain (Vezhnevets et al., 2017;Bacon et al., 2017) and robotic manipulation (Daniel et al., 2016;Osa et al., 2018b). In HRL, lower-level policies, which are often referred to as option policies, learn different behavior/control patterns, and the upper-level policy, which is often referred to as the gating policy, learns to select option policies. Recent studies have developed HRL methods using deep learning (Goodfellow et al., 2016) and have shown that HRL can yield impressive performance for complex tasks (Bacon et al., 2017;Frans et al., 2018;Vezhnevets et al., 2017;Haarnoja et al., 2018a). However, identifying the hierarchical policy structure that yields efficient learning is not a trivial task, since the problem involves learning a sufficient variety of types of behavior to solve a given task.
In this study, we present an HRL method via the mutual information (MI) maximization with advantage-weighted importance, which we refer to as adInfoHRL. We formulate the problem of learning a latent variable in a hierarchical policy as one of learning discrete and interpretable repre-sentations of states and actions. Ideally, each option policy should be located at separate modes of the advantage function. To estimate the latent variable that corresponds to modes of the advantage function, we introduce advantage-weighted importance weights. Our approach can be considered to divide the state-action space based on an information maximization criterion, and it learns option policies corresponding to each region of the state-action space. We derive adInfoHRL as an HRL method based on deterministic option policies that are trained based on an extension of the deterministic policy gradient (Silver et al., 2014;Fujimoto et al., 2018). The contributions of this paper are twofold:
1. We propose the learning of a latent variable of a hierarchical policy as a discrete and hidden representation of the state-action space. To learn option policies that correspond to the modes of the advantage function, we introduce advantage-weighted importance. 2. We propose an HRL method, where the option policies are optimized based on the deterministic policy gradient and the gating policy selects the option that maximizes the expected return. The experimental results show that our proposed method adInfoHRL can learn a diversity of options on continuous control tasks. Moreover, our approach can improve the performance of TD3 on such tasks as the Walker2d and Ant tasks in OpenAI Gym with MuJoco simulator.

[BACKGROUND]
In this section, we formulate the problem of HRL in this paper and describe methods related to our proposal.

[HIERARCHICAL REINFORCEMENT LEARNING]
We consider tasks that can be modeled as a Markov decision process (MDP), consisting of a state space S, an action space A, a reward function r : S × A → R, an initial state distribution ρ(s 0 ), and a transition probability p(s t+1 |s t , a t ) that defines the probability of transitioning from state s t and action a t at time t to next state s t+1 . The return is defined as R t =
T i=t γ i−t r(s i , a i ), where γ is a discount factor, and policy π(a|s) is defined as the density of action a given state s. Let d π (s) = T t=0 γ t p(s t = s) denote the discounted visitation frequency induced by the policy π. The goal of reinforcement learning is to learn a policy that maximizes the expected return J(π) = E s0,a0,... [R 0 ] where s 0 ∼ ρ(s 0 ), a ∼ π and s t+1 ∼ p(s t+1 |s t , a t ). By defining the Q-function as Q π (s, a) = E s0,a0,... [R t |s t = s, a t = a], the objective function of reinforcement learning can be rewritten as follows:
J(π) = d π (s)π(a|s)Q π (s, a)dads.(1)
Herein, we consider hierarchical policy π(a|s) = o∈O π(o|s)π(a|s, o), where o is the latent variable and O is the set of possible values of o. Many existing HRL methods employ a policy structure of this form (Frans et al., 2018;Vezhnevets et al., 2017;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016). In general, latent variable o can be discrete (Frans et al., 2018;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016;Osa & Sugiyama, 2018) or continuous (Vezhnevets et al., 2017). π(o|s) is often referred to as a gating policy (Daniel et al., 2016;Osa & Sugiyama, 2018), policy over options (Bacon et al., 2017), or manager (Vezhnevets et al., 2017). Likewise, π(a|s, o) is often referred to as an option policy (Osa & Sugiyama, 2018), sub-policy (Daniel et al., 2016), or worker (Vezhnevets et al., 2017). In HRL, the objective function is given by
J(π) = d π (s) o∈O π(o|s)π(a|s, o)Q π (s, a)dads.(2)
As discussed in the literature on inverse RL (Ziebart, 2010), multiple policies can yield equivalent expected returns. This indicates that there exist multiple solutions to latent variable o that maximizes the expected return. To obtain the preferable solution for o, we need to impose additional constraints in HRL. Although prior work has employed regularizers (Bacon et al., 2017) and constraints (Daniel et al., 2016) to obtain various option policies, the method of learning a good latent variable o that improves sample-efficiency of the learning process remains unclear. In this study we propose the learning of the latent variable by maximizing MI between latent variables and state-action pairs.

[DETERMINISTIC POLICY GRADIENT]
The deterministic policy gradient (DPG) algorithm was developed for learning a monolithic deterministic policy µ θ (s) : S → A by Silver et al. (2014). In off-policy RL, the objective is to maximize the expectation of the return, averaged over the state distribution induced by a behavior policy β(a|s):
J(π) = d β (s)π(a|s)Q π s, a)dads.(3)
When a policy is deterministic, the objective becomes J(π) = d β (s)Q π s, µ θ (s) ds. Silver et al. (2014) have shown that the gradient of a deterministic policy is given by
∇ θ E s∼d β (s) [Q π (s, a)] = E s∼d β (s) ∇ θ µ θ (s)∇ a Q π s, a | a=µ θ (s) .(4)
The DPG algorithm has been extended to the deep deterministic policy gradient (DDPG) for continuous control problems that require neural network policies . Twin Delayed Deep Deterministic policy gradient algorithm (TD3) proposed by Fujimoto et al. (2018) is a variant of DDPG that outperforms the state-of-the-art on-policy methods such as TRPO (Schulman et al., 2017a) and PPO (Schulman et al., 2017b) in certain domains. We extend this deterministic policy gradient to learn a hierarchical policy.

[REPRESENTATION LEARNING VIA INFORMATION MAXIMIZATION]
Recent studies such as those by ; Hu et al. (2017); Li et al. (2017) have shown that an interpretable representation can be learned by maximizing MI. Given a dataset X = (x 1 , ..., x n ), regularized information maximization (RIM) proposed by Gomes et al. (2010) involves learning a conditional modelp(y|x; η) with parameter vector η that predicts a label y. The objective of RIM is to minimize
(η) − λI η (x, y),(5)
where (η) is the regularization term, I η (x, y) is MI, and λ is a coefficient. MI can be decomposed as I η (x, y) = H(y) − H(y|x) where H(y) is entropy and H(y|x) the conditional entropy. Increasing H(y) conduces the label to be uniformly distributed, and decreasing H(y|x) conduces to clear cluster assignments. Although RIM was originally developed for unsupervised clustering problems, the concept is applicable to various problems that require learning a hidden discrete representation.
In this study, we formulate the problem of learning the latent variable o of a hierarchical policy as one of learning a latent representation of the state-action space.

[LEARNING OPTIONS VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
In this section, we propose a novel HRL method based on advantage-weighted information maximization. We first introduce the latent representation learning via advantage-weighted information maximization, and we then describe the HRL framework based on deterministic option policies.

[LATENT REPRESENTATION LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
Although prior work has often considered H(o|s) or I(s, o), which results in a division of the state space, we are interested in using I (s, a), o for dividing the state-action space instead. A schematic sketch of our approach is shown in Figure 1. As shown in the left side of Figure 1, the advantage function often has multiple modes. Ideally, each option policies should correspond to separate modes of the advantage function. However, it is non-trivial to find the modes of the advantage function in practice. For this purpose, we reduce the problem of finding modes of the advantage function to that of finding the modes of the probability density of state action pairs.
We consider a policy based on the advantage function of the form  where
π Ad (a|s) = f A π (s, a) Z ,(6)
A π (s, a) = Q π (s, a) − V π (s) is the advantage function, V π (s)
is the state value function, and Z is the partition function. f (•) is a functional, which is a function of a function. f (•) is a monotonically increasing function with respect to the input variable and always satisfies f (•) > 0.
In our implementation we used the exponential function f (•) = exp(•). When following such a policy, an action with the larger advantage is drawn with a higher probability. Under this assumption, finding the modes of the advantage function is equivalent to finding modes of the density induced by π Ad . Thus, finding the modes of the advantage function can be reduced to the problem of clustering samples induced by π Ad .
Following the formulation of RIM introduced in Section 2.3, we formulate the problem of clustering samples induced by π Ad as the learning of discrete representations via MI maximization. For this purpose, we consider a neural network that estimates p(o|s, a; η) parameterized with vector η, which we refer to as the option network. We formulate the learning of the latent variable o as minimizing L option (η) = (η) − λI o, (s, a); η ,
where I(o, (s, a)) =Ĥ(o|s, a; η) −Ĥ(o; η), and (η) is the regularization term. In practice, we need to approximate the advantage function, and we learn the discrete variable o that corresponds to the modes of the current estimate of the advantage function. For regularization, we used a simplified version of virtual adversarial training (VAT) proposed by Miyato et al. (2016). Namely, we set (η) = D KL p(o|s noise , a noise ; η)||p(o|s, a; η) where s noise = s + s , a noise = a + a , s and a denote white noise. This regularization term penalizes dissimilarity between an original state-action pair and a perturbed one, and Hu et al. (2017) 
Thus, the probability density of (s, a) induced by π Ad is necessary for computing MI for our purpose. To estimate the probability density of (s, a) induced by π Ad , we introduce the advantageweighted importance in the next section.

[IMPORTANCE WEIGHTS FOR MUTUAL INFORMATION ESTIMATION]
Although we show that the problem of finding the modes of the advantage function can be reduced to MI maximization with respect to the samples induced by π Ad , samples induced by π Ad are not available in practice. While those induced during the learning process are available, a discrete representation obtained from such samples does not correspond to the modes of the advantage function.
To estimate the density induced by π Ad , we employ an importance sampling approach.
We assume that the change of the state distribution induced by the policy update is sufficiently small, namely, d πAd (s) ≈ d β (s). Then, the importance weight can be approximated as
W (s, a) = p πAd (s, a) p β (s, a) = d πAd (s)π Ad (a|s) d β (s)β(a|s) ≈ π Ad (a|s) β(a|s) = f (A(s, a)) Zβ(a|s) . (10
)
and the normalized importance weight is given gỹ
W (s, a) = W (s, a) N j=1 W (s j , a j ) = f (A(s,a)) Zβ(a|s) N j=1 f (A(sj ,aj )) Zβ(aj |sj ) = f (A(s,a)) β(a|s) N j=1 f (A(sj ,aj )) β(aj |sj ) . (11
)
As the partition function Z is canceled, we do not need to compute Z when computing the importance weight in practice. We call this importance weight W the advantage-weighted importance and employ it to compute the objective function used to estimate the latent variable.
This advantage-weighted importance is used to compute the entropy terms for computing MI in Equation ( 7). The empirical estimate of the entropy H(o) is given bŷ
H(o; η) = − o∈Op (o; η) logp(o; η), wherep(o; η) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η). (12
)
where the samples (s i , a i ) are drawn from p β (s, a) induced by a behavior policy β(a|s). Likewise, the empirical estimate of the conditional entropy H(o|s, a) is given bŷ
H(o|s, a; η) = 1 N N i W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(13)
The derivations of Equations ( 12) and ( 13) are provided in Appendix A. To train the option network, we store the samples collected by the M most recent behavior policies, to which we refer as onpolicy buffer D on . Although the algorithm works with entire samples stored in the replay buffer, we observe that the use of the on-policy buffer for latent representation learning exhibits better performance. For this reason, we decided to use the on-policy buffer in our implementation. Therefore, while the algorithm is off-policy in the sense that the option is learned from samples collected by behavior policies, our implementation is ""semi""on-policy in the sense that we use samples collected by the most recent behavior policies.

[HRL OBJECTIVE WITH DETERMINISTIC OPTION POLICIES]
Instead of stochastic option policies, we consider deterministic option policies and model them using separate neural networks. We denote by π(a|s, o) = µ o θ (s) deterministic option policies parameterized by vector θ. The objective function of off-policy HRL with deterministic option policies can then be obtained by replacing π(a|s) with o∈O π(o|s)π(a|s, o) in Equation (3):
J(w, θ) = d β (s) o∈O π(o|s)Q π s, µ o θ (s); w ds, (14
)
where Q π (s, a; w) is an approximated Q-function parameterized using vector w. This form of the objective function is analogous to Equation (3). Thus, we can extend standard RL techniques to the learning of the gating policy π(o|s) in HRL with deterministic option policies.
In HRL, the goal of the gating policy is to generate a value of o that maximizes the conditional expectation of the return:
Q π Ω (s, o) = E [R|s t = s, o t = o] = π(a|s, o)Q π (s, a)da,(15)
which is often referred to as the option-value function (Sutton et al., 1999). When option policies are stochastic, it is often necessary to approximate the option-value function Q π Ω (s, o) in addition to the action-value function Q π (s, a). However, in our case, the option-value function for deterministic option policies is given by
Q π Ω (s, o) = Q π (s, µ o θ (s)),(16)
Algorithm 1 HRL via Advantage-Weighted Information Maximization (adInfoHRL)
Input: Number of options O, size of on-policy buffer Initialize: Replay buffer D R , on-policy buffer D on , network parameters η, θ, w, θ target , w target repeat for t = 0 to t = T do Draw an option for a given s by following Equation 17: o ∼ π(o|s) Draw an action a ∼ β(a|s, o) = µ o θ (s) + Record a data sample (s, a, r, s ) Aggregate the data in D R and D on if the on-policy buffer is full then Update the option network by minimizing Equation ( 7) for samples in D on Clear the on-policy buffer D on end if Sample a batch D batch ∈ D R Update the Q network parameter w if t mod d then Estimate p(o|s i , a i ) for (s i , a i ) ∈ D batch using the option network Assign samples (s i , a i ) ∈ D batch to the option o * = arg max p(o|s i , a i ) Update the option policy networks µ o θ (s) for o = 1, ..., O with Equation ( 19) Update the target networks:
w target ← τ w +(1−τ )w target , θ target ← τ θ +(1−τ )θ target end if end for until the convergence return θ
which we can estimate using the deterministic option policy µ o θ (s) and the approximated actionvalue function Q π (s, a; w). In this work we employ the softmax gating policy of the form
π(o|s) = exp Q π (s, µ o θ (s); w) o∈O exp Q π s, µ o θ (s); w ,(17)
which encodes the exploration in its form (Daniel et al., 2016). The state value function is given as
V π (s) = o∈O π(o|s)Q π (s, µ o θ (s); w),(18)
which can be computed using Equation (17). We use this state-value function when computing the advantage-weighted importance as A(s, a) = Q(s, a) − V (s). In this study, the Q-function is trained in a manner proposed by Fujimoto et al. (2018). Two neural networks (Q π w1 , Q π w2 ) are trained to estimate the Q-function, and the target value of the Q-function is computed as y i = r i + γ min 1,2 Q(s i , a i ) for sample (s i , a i , a i , r i ) in a batch sampled from a replay buffer, where r i = r(s i , a i ). In this study, the gating policy determines the option once every N time steps, i.e., t = 0, N, 2N, . . .

[NEURAL NETWORKS THAT MODEL Μ O]
θ (a|s) for o = 1, ..., O, which we refer to as option-policy networks, are trained separately for each option. In the learning phase, p(o|s, a) is estimated by the option network. Then, samples are assigned to option o * = arg max o p(o|s, a; η) and are used to update the option-policy network that corresponds to o * . When performing a rollout, o is drawn by following the gating policy in Equation ( 17), and an action is generated by the selected option-policy network.
Differentiating the objective function in Equation ( 14), we obtain the deterministic policy gradient of our option-policy µ o θ (s) given by
∇ θ J(w, θ) = E s∼d β (s)π(o|s) ∇ θ µ o θ (s)∇ a Q π s, a | a=µ o θ (s) . (19
)
The procedure of adInfoHRL is summarized by Algorithm 1. As in TD3 (Fujimoto et al., 2018), we employed the soft update using a target value network and a target policy network. 

[EXPERIMENTS]
We evaluated the proposed algorithm adInfoHRL on the OpenAI Gym platform (Brockman et al., 2016) with the MuJoCo Physics simulator (Todorov et al., 2012). We compared its performance with that of PPO implemented in OpenAI baselines (Dhariwal et al., 2017) and TD3. Henderson et al. (2018) have recently claimed that algorithm performance varies across environment, there is thus no clearly best method for all benchmark environments, and off-policy and on-policy methods have advantages in different problem domains. To analyze the performance of adInfoHRL, we compared it with state-of-the-art algorithms for both on-policy and off-policy methods, although we focused on the comparison with TD3, as our implementation of adInfoHRL is based on it. To determine the effect of learning the latent variable via information maximization, we used the same network architectures for the actor and critic in adInfoHRL and TD3. In addition, to evaluate the benefit of the advantage-weighted importance, we evaluated a variant of adInfoHRL, which does not use the advantage-weighted importance for computing mutual information.We refer to this variant of adInfoHRL as infoHRL. The gating policy updated variable o once every three time steps. We tested the performance of adInfoHRL with two and four options. The averaged return of five trials is reported in Figure 3(a)-(d). AdIfoHRL yields the best performance on Ant 1 and Walker2d, whereas the performance of TD3 and adInfoHRL was comparable on HalfCheetah and Hopper, and PPO outperformed the other methods on Hopper. Henderson et al. (2018) claimed that on-policy methods show their superiority on tasks with unstable dynamics, and our experimental results are in line with such previous studies. AdinfoHRL outperformed infoHRL, which isthe variant of adInfoHRL without the advantage-weighted importance on all the tasks. This result shows that the adavatage-weighted importance enhanced the performance of learning options.
AdInfoHRL exhibited the sample efficiency on Ant and Walker2d in the sense that it required fewer samples than TD3 to achieve comparable performance on those tasks. The concept underlying ad-InfoHRL is to divide the state-action space to deal with the multi-modal advantage function and learn option policies corresponding to separate modes of the advantage function. Therefore, adIn-foHRL shows its superiority on tasks with the multi-modal advantage function and not on tasks with a simple advantage function. Thus, it is natural that the benefit of adInfoHRL is dependent on the characteristics of the task.   

[RELATED WORK AND DISCUSSION]
Past studies have proposed several ways to deal with the latent variable in HRL. The recent work by Smith et al. (2018) proposed inferred option policy gradients (IOPG), which is derived as an extension of policy gradient to the option framework. Nachum et al. (2018) recently proposed off-policy target correction for HRL on goal-oriented tasks, where a higher-level policy instructs a lower-level policy by generating the goal signal instead of an inferred latent variable. A popular approach for learning the latent variable in HRL is the variational approach. The recent work by Haarnoja et al. (2018a) is based on soft actor critic (Haarnoja et al., 2018b), and the latent variable is inferred using the variational approach. The work by Hausman et al. (2018) is also closely related to the variational approach, and they proposed a method for learning a latent variable of a hierarchical policy via a variational bound. On the contrary, our method learns the latent variable by maximizing MI with advantage-weighted importance. Recent studies by Gregor et al. (2016); Florensa et al. (2017); Eysenbach et al. (2018) also considered the MI in their formulation. In these methods, MI between the state and the latent variable is considered so as to obtain diverse behaviors. Our approach is different from the previous studies in the sense that we employ MI between the latent variable and the state-action pairs, which leads to the division of the state-action space instead of considering only the state space. We think that dividing the state-action space is an efficient approach when the advantage function is multi-modal, as depicted in Figure 1. InfoGAIL proposed by Li et al. (2017) learns the interpretable representation of the state-action space via MI maximization. InfoGAIL can be interpreted as a method that divides the state-action space based on the density induced by an expert's policy by maximizing the regularized MI objective. In this sense, it is closely related to our method, although their problem setting is imitation learning (Osa et al., 2018a), which is different from our HRL problem setting.
The use of the importance weight based on the value function has appeared in previous studies (Dayan & Hinton, 1997;Kober & Peters, 2011;Neumann & Peters, 2009;Osa & Sugiyama, 2018). For example, the method proposed by Neumann & Peters (2009) employs the importance weight based on the advantage function for learning a monolithic policy, while our method uses a similar importance weight for learning a latent variable of a hierarchical policy. Although Osa & Sugiyama (2018) proposed to learn a latent variable in HRL with importance sampling, their method is limited to episodic settings where only a single option is used in an episode.
Our method can be interpreted as an approach that divides the state-action space based on the MI criterion. This concept is related to that of Divide and Conquer (DnC) proposed by Ghosh et al. (2018), although DnC clusters the initial states and does not consider switching between option policies during the execution of a single trajectory.
In this study we developed adInfoHRL based on deterministic option policies. However, the concept of dividing the state-action space via advantage-weighted importance can be applied to stochastic policy gradients as well. Further investigation in this direction is necessary in future work.

[CONCLUSIONS]
We proposed a novel HRL method, hierarchical reinforcement learning via advantage-weighted information maximization. In our framework, the latent variable of a hierarchical policy is learned as a discrete latent representation of the state-action space. Our HRL framework is derived by considering deterministic option policies and by leveraging the analogy between the gating policy for HRL and a monolithic policy for the standard RL. The results of the experiments indicate that adInfoHRL can learn diverse options on continuous control tasks. Our results also suggested that our approach can improve the performance of TD3 in certain problem domains.

[A MUTUAL INFORMATION WITH ADVANTAGE-WEIGHTED IMPORTANCE]
The 
where W (s, a) = p π (s,a) p β (s,a) is the importance weight. Therefore, the empirical estimate of p(o) with respect to the density induced by a policy π is given bŷ 
p(o) = 1 N N i=1W (s i , a i )p(o|s i , a i ; η),(23)
Therefore, the empirical estimate of the conditional entropy with respect to the density induced by a policy π is given bŷ
H(o|s, a) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(27)
Thus, the empirical estimates of MI can be computed by Equations ( 21), ( 23) and ( 27).

[B DERIVATION OF THE STATE-VALUE FUNCTION]
In HRL, the value function is given by V (s) =  

[C EXPERIMENTAL DETAILS]
We performed evaluations using benchmark tasks in the OpenAI Gym platform (Brockman et al., 2016) with Mujoco physics simulator (Todorov et al., 2012). Hyperparameters of reinforcement learning methods used in the experiment are shown in Tables 1-3. For exploration, both adInfoHRL and TD3 used the clipped noise drawn from the normal distribution as ∼ clip N (0, σ), −c, c , where σ = 0.2 and c = 0.5. For hyperparameters of PPO, we used the default values in OpenAI baselines (Dhariwal et al., 2017). For the Walker2d, HalfCheetah, and Hopper tasks, we used the Walker2d-v1, HalfCHeetah-v1, and Hopper-v1 in the OpenAI Gym, respectively. For the Ant task, we used the AntEnv implemented in the rllab . When training a policy with AdInfoHRL, infoHRL, and TD3, critics are trained once per time step, and actors are trained once every after two updates of the critics. The source code is available at https://github.com/ TakaOsa/adInfoHRL.
We performed the experiments five times with different seeds, and reported the averaged test return where the test return was computed once every 5000 time steps by executing 10 episodes without exploration. When performing the learned policy without exploration, the option was drawn as
o = max o Q π (s, µ o (s)),(30)
instead of following the stochastic gating policy in Equations (17).   The dimensionality is reduced by t-SNE for visualization.

[D ADDITIONAL INFORMATION ON EXPERIMENTAL RESULTS]
On the HalfCheetah task, adInfoHRL delivered the best performance with two options. The distribution of options on HalfCheetah0v1 after one million steps is shown in Figure 4. Although the state-action space is evenly divided, the options are not evenly activated. This behavior can occur because the state-action space is divided based on the density induced by the behavior policy while the activation of options is determined based on the quality of the option policies in a given state. Moreover, an even division in the action-state space is not necessarily the even division in the state space.
The activation of the options over time is shown in Figure 5. It is clear that one of the option corresponds to the stable running phase and the other corresponds to the phase for recovering from unstable states. Figure 6: Distribution of options on Ant-rllab task using adInfoHRL with four options. The dimensionality is reduced by t-SNE for visualization.
Figure 7: Activation of the options over time steps on Ant-rllab task. Four options are learned.
The distribution of four options on the Ant-rllab task after one million steps is shown in Figure 6. Four options are activated in the different domains of the state space. The activation of the options over time on the Ant-rllab task is shown in Figure 7. While four options are actively used in the beginning of the episode, two (blue and yellow) options are mainly activated during the stable locomotion.
Since the Ant task implemented in rllab is known to be harder than the Ant-v1 implemented in the OpenAI gym, we reported the result of the Ant task in rllab in the main manuscript. Here, we report the result of the Ant-v1 task implemented in the OpenAI gym. On the Ant-v1 task, adInfoHRL yielded the best performance with two options. The performance of adInfoHRL with two options is comparable to that of TD3 on Ant-v1. This result indicates that the Ant-v1 task does not require a hierarchical policy structure, while a hierarchical policy improves the performance of learning on Ant-rllab. The distribution of options on Ant-v1 task after one million steps is shown in Figure 8. The activation of the options over time is shown in Figure 9. It is evident that two option policies on the Ant-v1 task corresponded to different postures of the agent.
A recent study on HRL by Smith et al. (2018) reported the performance of IOPG on Walker2d-v1, Hopper-v1, and HalfCheetah-v1. The study by Haarnoja et al. (2018a) reported the performance of SAC-LSP on Walker2d-v1, Hopper-v1, HalfCheetah-v1, and Ant-rllab. A comparison of performance between our method, IOPG, and SAC-LSP is summarized in Table 4. We report the performance after 1 million steps. It is worth noting that adInfoHRL outperformed IOPG on these tasks in terms of the achieved return, although we are aware that the qualitative performance is also important in HRL. AdInfoHRL outperformed SAC-LSP on Walker2d-v1 and Ant-rllab, and SAC-LSP shows its superiority on HalfCheetah-v1 and Hopper-v1. However, the results of SAC-LSP were obtained by using reward scaling, which was not used in the evaluation of adInfoHRL. Therefore, further experiments are necessary for fair comparison under the same condition.

[ACKNOWLEDGMENTS]
MS was partially supported by KAKENHI 17H00757.","[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.","{
    ""1"": {
        ""summary"": ""Questions the clarity and justification of the theoretical approach and lack of advanced experiments."",
        ""verbatim"": ""it is not very clear to me *why* the mutual information should be optimized with respect to this 'optimal' policy instead of the actual policy one is generating trajectories from. experiments on more challenging problems and a more systematic comparison with similar models would make this a much stronger paper.""
    },
    ""2"": {
        ""summary"": ""Points out several unclear terminologies, unexplained equations, and lack of comparative evaluation with other HRL approaches."",
        ""verbatim"": ""Several key terms are used in ways that differ from the rest of the literature. Equation 10 comes out of nowhere. It is also odd that no other HRL approaches are evaluated against.""
    },
    ""3"": {
        ""summary"": ""Highlights ad-hoc design choices and a lack of motivation or explanation for specific approaches used."",
        ""verbatim"": ""Many of the design choices appear very ad hoc, and some are outright confusing. why introduce the off-policy bit at all, and list it as a contribution? Please motivate the choices.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""The method's effectiveness is limited to certain task types."",
        ""verbatim"": ""The paper acknowledges that the benefit of adInfoHRL is dependent on the characteristics of tasks with multi-modal advantage functions, which might limit its generalizability across all types of reinforcement learning problems.""
    },
    ""2"": {
        ""summary"": ""The implementation complexities may hinder practical application."",
        ""verbatim"": ""The approach involves several neural network models and hyperparameters, which might pose challenges in terms of implementation and computational requirements.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""The paper lacks concrete comparisons with existing HRL methods."",
        ""verbatim"": ""The paper could benefit from providing more concrete comparisons with existing HRL methods to showcase the superiority of the proposed approach.""
    },
    ""2"": {
        ""summary"": ""The method's computational complexity and scalability are not addressed."",
        ""verbatim"": ""It may be essential to explain the computational complexity and scalability of the method, as these factors are crucial for practical implementation and deployment.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both points criticize the lack of comparative evaluation with existing hierarchical reinforcement learning (HRL) methods. While Review A specifically mentions that no other HRL approaches are evaluated, Review B suggests that the paper would benefit from concrete comparisons with existing HRL methods."",
        ""similarity"": ""8""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Lack of comparison with existing methods"",
        ""verbatim"": ""Lack of comparison with existing methods""
    },
    ""2"": {
        ""summary"": ""Insufficient theoretical analysis"",
        ""verbatim"": ""Insufficient theoretical analysis""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both Review A1 and Review B2 critique the inadequate theoretical justification. Review A1 specifically questions the rationale behind optimizing mutual information for an 'optimal' policy, indicating a weakness in theoretical grounding, similar to the 'Insufficient theoretical analysis' pointed out in Review B2."",
        ""similarity"": ""8""
    },
    ""A2-B1"": {
        ""rationale"": ""Review A2's concern about the lack of comparative evaluation with other hierarchical reinforcement learning (HRL) approaches closely aligns with Review B1's critique on the absence of comparisons with existing methods. Both remarks stress the need for benchmarking against available methods to validate the efficacy and uniqueness of the proposed approach."",
        ""similarity"": ""9""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Complexity and implementation challenges may hinder practical applications."",
        ""verbatim"": ""'Complexity and implementation challenges': The complex nature of the proposed method might pose practical challenges in implementation, particularly in environments different from the tested simulations. Readers and potential implementers may find the approach too theoretical with insufficient guidance on real-world application.""
    },
    ""2"": {
        ""summary"": ""Limited discussion on scalability and robustness reduces the paper's credibility in diverse environments."",
        ""verbatim"": ""'Limited discussion on scalability and robustness': The experiments are limited to specific simulation environments, with less emphasis on scalability to different domains or robustness against varying operational conditions within those environments.""
    },
    ""3"": {
        ""summary"": ""Insufficient comparative analysis may not fully demonstrate the method's superiority."",
        ""verbatim"": ""'Comparison and benchmarking specifics': There is a lack of detailed comparison regarding how the method performs across a wider range of tasks and against more recent or alternative advanced methods, which might not convincingly argue the superiority of the proposed method.""
    }
}",3,"{
    ""A2-B3"": {
        ""rationale"": ""Both points criticize the insufficiency of comparative evaluation with other approaches. Review A specifically identifies the lack of comparative evaluation with other HRL (Hierarchical Reinforcement Learning) approaches, while Review B highlights a general lack of detailed comparative analysis across a wider range of tasks and advanced methods. Both express a shared concern over the paper's analytical depth in benchmarking against existing alternatives."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Insufficient comparative analysis with other HRL methods"",
        ""verbatim"": ""Lack of comparative analysis""
    },
    ""2"": {
        ""summary"": ""Limited general applicability due to specific focus on continuous control tasks"",
        ""verbatim"": ""Specificity of applications""
    },
    ""3"": {
        ""summary"": ""Potential reproducibility issues due to inadequate details on implementation specifics"",
        ""verbatim"": ""Potential reproducibility issues""
    }
}",3,"{
    ""A2-B1"": {
        ""rationale"": ""Both Review A2 and Review B1 emphasize the absence of comparative evaluation with other hierarchical reinforcement learning (HRL) approaches. Review A2 mentions that no other HRL approaches are evaluated against, while Review B1 directly points out the lack of comparative analysis with other HRL methods. The concerns are essentially about the comparison with peer methods in HRL, highlighting a significant oversight in the manuscript."",
        ""similarity"": ""9""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Concerns about scalability in complex environments"",
        ""verbatim"": ""The paper might face scalability issues, especially in environments with much larger and more complex hierarchical structures. The scalability and computational efficiency in higher dimensional settings are not clearly addressed.""
    },
    ""2"": {
        ""summary"": ""Limited validation of the proposed method"",
        ""verbatim"": ""While experimental results are promising, the scope of validation might be seen as limited. Expanding the experimentation to include a wider range of tasks and conditions could be necessary to fully validate the claims.""
    }
}",2,"{
    ""A1-B2"": {
        ""rationale"": ""Both points address uncertainties in validation of the proposed methodologies under more complex or advanced settings. Review A questions the choice of optimizing mutual information with a designated 'optimal' policy and suggests better validation through more complex experiments and comparisons. Similarly, Review B points to limited scope of validation and suggests expanding experimentation."",
        ""similarity"": ""7""
    }
}",1,"[Significance and novelty]
1. The paper introduces a novel hierarchical reinforcement learning (HRL) method that learns a latent variable of a hierarchical policy using mutual information maximization, which is an innovative approach in the field of HRL.
2. The use of advantage-weighted importance sampling to learn option policies that correspond to modes of the advantage function is a new application of importance sampling in HRL.
3. The gating policy that learns to select option policies based on an option-value function and optimizes these option policies using the deterministic policy gradient method is a novel way of defining hierarchical policies in HRL.

[Potential reasons for acceptance]
1. The proposed HRL method offers a novel and innovative approach to learn a hierarchical policy structure using mutual information maximization, which has the potential to significantly enhance the performance of RL in real-world tasks with a hierarchical structure.
2. The introduction of advantage-weighted importance sampling for learning option policies that correspond to modes of the advantage function demonstrates a sound theoretical understanding of HRL and provides a practical method for learning option policies.
3. The gating policy and its use of the deterministic policy gradient method for optimization contribute to the overall framework's potential for enhancing the performance of RL, thus providing a strong theoretical foundation and practical applicability.

[Potential reasons for rejection]
1. The paper does not provide a thorough comparison with existing HRL methods, which may lack sufficient evidence of the superiority of the proposed approach over similar methods in the literature.
2. The analysis of convergence properties and experimental evaluation of the proposed framework could benefit from further exploration and discussion in the paper.
3. The paper lacks robustness analysis and practical implementation considerations for real-world applications, which may hinder its relevance and applicability to real-world problems.

[Suggestions for improvement]
1. The authors could consider a more comprehensive comparison with other state-of-the-art HRL methods to provide a clearer perspective on the contributions and potential advantages of their approach.
2. The authors could provide a more detailed analysis of the convergence properties of the proposed method and perform a more exhaustive set of experiments to validate its applicability to a wider range of tasks and environments.
3. The authors could discuss practical implementation considerations, such as scaling and computational cost, to prepare the reader for potential challenges and future work related to the framework.","[Significance and novelty]
<Hierarchical policy representation learning> The proposed method of hierarchical policy representation learning and advantage-weighted importance sampling demonstrates novelty and significance in addressing the problem of learning hierarchical policies through mutual information maximization and deterministic policy gradient method.
<Combining options and actor-critic methods> The combination of options learning and actor-critic methods at the option level in the proposed algorithm demonstrates a new approach in reinforcement learning research.

[Potential reasons for acceptance]
<Innovative approach> The innovative approach in learning hierarchical policies using mutual information maximization and deterministic policy gradient method makes the paper a potential reason for acceptance.
<Improved results on complex benchmarking domain> The improved results on a complex benchmarking domain, namely Mujoco provide support for the paper's potential acceptance.

[Potential reasons for rejection]
<Clarity and presentation> The paper's lack of clarity in presenting the method, as suggested by multiple reviewers, could be a potential reason for rejection.
<Missing baseline in results> The absence of a baseline in the results presented in Figures 3 and 4 is a potential reason for rejection as indicated by reviewers.

[Suggestions for improvement]
<Improve clarity in presenting the method> The paper could benefit from improving the clarity in presenting the method as suggested by multiple reviewers, particularly in the explanation of the algorithm and the figures.
<Inclusion of baseline in results> Incorporating a baseline in the presented results, as suggested by reviewers, could strengthen the paper.","{
    ""1"": {
        ""summary"": ""Lack of comprehensive comparison with existing HRL methods."",
        ""verbatim"": ""The paper does not provide a thorough comparison with existing HRL methods, which may lack sufficient evidence of the superiority of the proposed approach over similar methods in the literature.""
    },
    ""2"": {
        ""summary"": ""Insufficient analysis of convergence properties and experimental evaluation."",
        ""verbatim"": ""The analysis of convergence properties and experimental evaluation of the proposed framework could benefit from further exploration and discussion in the paper.""
    },
    ""3"": {
        ""summary"": ""Absence of robustness analysis and practical implementation considerations."",
        ""verbatim"": ""The paper lacks robustness analysis and practical implementation considerations for real-world applications, which may hinder its relevance and applicability to real-world problems.""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Lack of clarity in presentation"",
        ""verbatim"": ""The paper's lack of clarity in presenting the method, as suggested by multiple reviewers, could be a potential reason for rejection.""
    },
    ""2"": {
        ""summary"": ""Missing baseline in results"",
        ""verbatim"": ""The absence of a baseline in the results presented in Figures 3 and 4 is a potential reason for rejection as indicated by reviewers.""
    }
}",2,"{
    ""A2-B1"": {
        ""rationale"": ""Both points highlight the lack of comparative evaluation with other HRL (Hierarchical Reinforcement Learning) approaches. Review A criticizes the absence of systematic comparison with similar models, while Review B points out the missing thorough comparison with existing HRL methods, specifically questioning the evidence of superiority."",
        ""similarity"": ""8""
    }
}",1,"{
    ""A1-B1"": {
        ""rationale"": ""Both Review A1 and Review B1 express concerns about the paper's lack of clarity. A1 focuses specifically on the unclear rationale for optimizing mutual information under a proposed policy, while B1 notes the overall lack of clarity in presenting the method. Both points emphasize the ambiguity in the presentation of the study."",
        ""similarity"": ""7""
    },
    ""A2-B1"": {
        ""rationale"": ""Review A2 and Review B1 similarly criticize the unclear presentation, but in slightly different contexts. A2 points out unclear terminology and unexplained equations, contributing to a comprehensive lack of clarity and comparative evaluation, which aligns with B1's general observation of clarity issues in the paper's presentation."",
        ""similarity"": ""7""
    }
}",2
An Adversarial Learning Framework for a Persona-based Multi-turn Dialogue Model,rkeYUsRqKQ.json,"[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).

[CAPTIONS]
Table 1: phredGAN vs.Li et al. (2016b) on BBT Friends TV Transcripts.
Table 2: phredGAN vs. Li et al. (2016b) on UDC.
Table 3: hredGAN vs phredGAN vs Li et al. (2016b) on TV Transcripts
Table 4: hredGAN vs phredGAN vsLi et al. (2016b) on UDC Of course I love you."" to the dialogue context, ""Do you love me?"" despite the fact that some of the responders sometimes have unfriendly relationship with the addressee. Many of the novel situations explored by phredGAN are unachievable with the Speaker-Addressee model due to lack of informative responses. For example, by conditioning as Sheldon from The Big Bang Theory and asking ""Do you like me?"", our model responds with annoyance if conditioned as Penny (""No, you don't understand. You're an idiot""), brevity with Leonard (""Yes?"") and sarcasm with Raj (""Well , you know , we could be a little more than my friend's friends."") The wide range of responses indicate our model's ability to construct distinct attribute embeddings for each character even from a limited dataset. The other interesting responses in table 3 indicate phredGAN 's ability to infer not only the context of the conversation but important character information about the addressee.
Table 5: Sample of PHRED outputs on UDC and TV Series Howard Okay, you have to understand something, we're in a hospital right now. Response 0 Penny Oh , I ' m sorry . I was just trying to be a girl .Context 0 Sheldon Did I? Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah Response 0 Leonard I ' m sorry . I just don ' t want to be a man who ' s going to be my girlfriend .Context 0 Leonard She's gotten really hooked on Age of Conan, she's playing non-stop.
Table 6: Ranked phredGAN a outputs according to adversarial word-level discrimination score Source Speaker D adv (G(.)) Utterance Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah cd , it should be in the repos , it's a good place to get the source of the kernel Response 1 helper 0.1984 use the UNK package , it should work . . .

[INTRODUCTION]
Recent advances in machine learning especially with deep neural networks has lead to tremendous progress in natural language processing and dialogue modeling research (Sutskever et al., 2014;Vinyals & Le, 2015;Serban et al., 2016). Nevertheless, developing a good conversation model capable of fluent interaction between a human and a machine is still in its infancy stage. Most existing work relies on limited dialogue history to produce response with the assumption that the model parameters will capture all the modalities within a dataset. However, this is not true as dialogue corpora tend to be strongly multi-modal and practical neural network models find it difficult to disambiguate characteristics such as speaker personality, location and sub-topic in the data.
Most work in this domain has primarily focused on optimizing dialogue consistency. For example, Serban et al. (Serban et al., 2016;2017b;a) and Xing et al. (2017) introduced a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not have any guarantee on the generator output since the output conditional probability is not calibrated. Olabiyi et al. (2018) tackles these problems by training a modified HRED generator alongside an adversarial discriminator in order to increase diversity and provide a strong and calibrated guarantee to the generator's output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modality within a dataset and fails to generate persona specific responses in datasets with multiple modalities.
On the other hand, there has been some recent work on introducing persona into dialogue models. For example, Li et al. (2016b) integrates attribute embeddings into a single turn (Seq2Seq) generative dialogue model. In this work, Li et al. consider persona models one with Speaker-only representation and the other with Speaker and Addressee representations (Speaker-Addressee model), both of which capture certain speaker identity and interactions. Nguyen et al. (2018) continue along the Figure 1: The PHRED generator with local attention -The attributes C, allows the generator to condition its response on the utterance attributes such as speaker identity, subtopics and so on. same line of thought by considering a Seq2Seq dialogue model with Responder-only representation. In both of these cases, the attribute representation is learned during the system training. Zhang et al. (2018) proposed a slightly different approach. Here, the attributes are a set of sentences describing the profile of the speaker. In this case, the attributes representation is not learned. The system however learns how to attend to different parts of the attributes during training. Still, the above persona-based models have limited dialogue history (single turn); suffer from exposure bias worsening the trade-off between personalization and conversation quality and cannot generate multiple responses given a dialogue context. This is evident in the relatively short and generic responses produced by these systems, even though they generally capture the persona of the speaker.
In order to overcome these limitations, we propose two variants of an adversarially trained persona conversational generative system, phredGAN , namely phredGAN a and phredGAN d . Both systems aim to maintain the response quality of hredGAN and still capture speaker and other attribute modalities within the conversation. In fact, both systems use the same generator architecture (PHRED generator), i.e., an hredGAN generator (Olabiyi et al., 2018) with additional utterance attribute representation at its encoder and decoder inputs as depicted in Figure 1. Conditioning on external attributes can be seen as another input modality as is the utterance into the underlying system. The attribute representation is an embedding that is learned together with the rest of model parameters similar to Li et al. (2016b). Injecting attributes into a multi-turn dialogue system allows the model to generate responses conditioned on particular attribute(s) across conversation turns. Since the attributes are discrete, it also allows for exploring different what-if scenarios of model responses. The difference between the two systems is in the discriminator architecture based on how the attribute is treated.
We train and sample both variants of phredGAN similar to the procedure for hredGAN (Olabiyi et al., 2018). To demonstrate model capability, we train on a customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poser and answerer, and transcripts from a multi-modal TV series The Big Bang Theory and Friends with quantitative and qualitative analysis. We examine the trade-offs between using either system in bi-modal or multi-modal datasets, and demonstrate system superiority over state-of-the-art persona conversational models in terms of dialogue response quality and quantitatively with perplexity, BLEU, ROUGE and distinct n-gram scores.

[MODEL ARCHITECTURE]
In this section, we briefly introduce the state-of-the-art hredGAN model and subsequently show how we derive the two persona versions by combining it with the distributed representation of the dialogue speaker and utterance attributes, or with an attribute discrimination layer at the end of the model pipeline.
Figure 2: The phredGAN d dual discriminator -Left: D adv is a word-level discriminator used by both phredGAN a and phredGAN d to judge normal dialogue coherency as in hredGAN . Right: D att , an utterance-level attribute discriminator is used only in phredGAN d to predict the likelihood a given utterance was generated from a particular attribute.

[HREDGAN : ADVERSARIAL LEARNING FRAMEWORK]
Problem Formulation: The hredGAN (Olabiyi et al., 2018) formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances,
X i = X 1 , X 2 , • • • , X i , where each utterance X i = X 1 i , X 2 i , • • • , X Mi i
contains a variable-length sequence of M i word tokens such that X i j ∈ V for vocabulary V , the dialogue model produces an output
Y i = Y 1 i , Y 2 i , • • • , Y Ti i
, where T i is the number of generated tokens. The framework uses conditional GAN structure to learn a mapping from an observed dialogue history to a sequence of output tokens. The generator, G, is trained to produce sequences that cannot be distinguished from the ground truth by an adversarially trained discriminator, D akin to a two-player min-max optimization problem. The generator is also trained to minimize the cross-entropy loss L M LE (G) between the ground truth X i+1 , and the generator output Y i . The following objective summarizes both goals:
G * , D * = arg min G max D λ G L cGAN (G, D) + λ M L M LE (G) .
(
)1
where λ G and λ M are training hyperparamters and L cGAN (G, D) and L M LE (G) are defined in Eqs.
(5) and ( 7) of Olabiyi et al. (2018) respectively. Please note that the generator G and discriminator D share the same encoder and embedding representation of the word tokens.

[PHREDGAN : PERSONA ADVERSARIAL LEARNING FRAMEWORK]
The proposed architecture of phredGAN is very similar to that of hredGAN (Olabiyi et al., 2018). The only difference is that the dialogue history is now
X i = (X 1 , C 1 ), (X 2 , C 2 ), • • • , (X i , C i )
where C i is additional input that represents the speaker and/or utterance attributes. Please note that C i can either be a sequence of tokens or single token such that C i j ∈ V c for vocabulary V c. Also, at the ith turn, C i and C i+1 are the source/input attribute and target/output attribute to the generator respectively. The embedding for attribute tokens is also learned similar to that of word tokens.
Both versions of phredGAN shares the same generator architecture (PHRED) but different discriminators. Below is the highlight of how they are derived from the hredGAN architecture.
Encoder: The context RNN, cRN N takes the source attribute C i as an additional input by concatenating its representation with the output of eRN N as in Figure 1. If the attribute C i is a sequence of tokens, then an attention (using the output of eRN N ) over the source attribute representations is concatenated with the output of eRN N . This output is used by the generator to create a context state for a turn i.
Generator: The generator decoder RNN, dRN N takes the target attribute C i+1 as an additional input as in Fig. 1. If the attribute C i+1 is a sequence of tokens, then an attention (using the output of dRN N ) over the attribute representations is concatenated with the rest of the decoder inputs. This forces the generator to draw a connection between the generated responses and the utterance attributes such as speaker identity.
Noise Injection: As in Olabiyi et al. (2018), we also explore different noise injection methods.
Objective: For phredGAN , the optimization objective in eq. ( 1) can be updated as:
G * , D * adv , D * att = arg min G max D adv λ G adv L adv cGAN (G, D adv ) (2) + min Datt λ Gatt L att c (G, D att ) + λ M L M LE (G) .
where L adv cGAN (G, D adv ) and L att c (G, D att ) are the traditional adversarial and attribute prediction loss respectively and dependent on the architectural variation. It is worth to point out that while the former is adversarial, the later is collaborative in nature. The MLE loss is common and can be expressed as:
L M LE (G) = E Xi+1 [−log P G X i+1 |X i , C i+1 , Z i ].
(3) where Z i the noise sample and depends on the choice of either utterance-level or word-level noise input into the generator (Olabiyi et al., 2018).

[PHREDGAN A : ATTRIBUTES AS A DISCRIMINATOR INPUT]
phredGAN a shares the same discriminator architecture as the hredGAN but with additional input, C i+1 . Since it does not use attribute prediction, λ Gatt = 0.
The adversarial loss, L adv cGAN (G, D) can then be expressed as:
L adv cGAN (G, D adv ) = E X i ,Ci+1,Xi+1 [log D adv (X i , C i+1 , X i+1 )] (4) + E X i ,Ci+1,Zi [1 − log D adv (X i , C i+1 , G(X i , C i+1 , Z i ))]
The addition of speaker or utterance attributes allows the dialogue model to exhibit personality traits given consistent responses across style, gender, location, and so on.

[PHREDGAN D : ATTRIBUTES AS A DISCRIMINATOR TARGET]
phredGAN d does not take the attribute representation at its input but rather use the attributes as the target of an additional discriminator D att . The adversarial and the attribute prediction losses can be respectively expressed as:
L adv cGAN (G, D adv ) = E X i ,Xi+1 [log D adv (X i , X i+1 )] (5) + E X i ,Zi [1 − log D adv (X i , G(X i , C i+1 , Z i ))] L att c (G, D att ) = E Ci+1 [− log D att (C i+1 |X i , X i+1 )] (6) + E Ci+1 [− log D att (C i+1 |X i , G(X i , C i+1 , Z i ))]
Attribute Discriminator: In addition to the existing word-level adversarial discriminator D adv from hredGAN , we add an attribute discriminator, D att , that discriminates on an utterance level to capture attribute modalities since attributes are assigned at utterance level. The discriminator uses a unidirectional RNN (D attRN N ) that maps the input utterance to the particular attribute(s) that generated it. The attributes can be seen as hidden states that inform or shape the generator outputs. The attribute discriminator can be expressed as:
D att (C i+1 |X i , χ) = D attRN N (h i , E(χ))(7)
where E(.) is the word embedding lookup (Olabiyi et al., 2018), χ = X i+1 for groundtruth and χ = Y i for the generator output.

[MODEL TRAINING]
We train both the generator and the discriminator (with shared encoder) of both variants of phredGAN using the training procedure in Algorithm 1 (Olabiyi et al., 2018). For both variants, λ G adv = λ M = 1, and for phredGAN a and phredGAN d , λ Gatt = 0 and λ Gatt = 1 respectively. Since the encoder, word embedding and attribute embedding are shared, we are able to train the system end-to-end with back-propagation.
Encoder: The encoder RNN, eRN N , is bidirectional while cRRN is unidirectional. All RNN units are 3-layer GRU cell with hidden state size of 512. We use word vocabulary size, V = 50, 000 with word embedding size of 512. The number of attributes, V c is dataset dependent but we use an attribute embedding size of 512. In this study, we only use one attribute per utterance so that is no need to use attention to combine the attribute embeddings.
Generator: The generator decoder RNN, dRN N is also a 3-layer GRU cell with hidden state size of 512. The aRN N outputs are connected to the dRN N input using an additive attention mechanism (Bahdanau et al., 2015).
Adversarial Discriminator: The word-level discriminator RNN, D RN N is a bidirectional RNN, each 3-layer GRU cell with hidden state size of 512. The output of both the forward and the backward cells for each word are concatenated and passed to a fully-connected layer with binary output.
The output is the probability that the word is from the ground truth given the past and future words of the sequence, and in the case of phredGAN a , the responding speaker's embedding.
Attribute Discriminator: The attribute discriminator RNN, D attRN N is a unidirectional RNN with a 3-layer GRU cell, each of hidden state size 512. A softmax layer is then applied to project the final hidden state to a prespecified number of attributes, V c . The output is the probability distribution over the attributes.
Others: All parameters are initialized with Xavier uniform random initialization (Glorot & Bengio, 2010). Due to the large word vocabulary size, we use sampled softmax loss (Jean et al., 2015) for MLE loss to expedite the training process. However, we use full softmax for model evaluation. For both systems, parameters updates are conditioned on the word-level discriminator accuracy performance as in Olabiyi et al. (2018) with acc D th adv = 0.99 and acc G th = 0.75. The model is trained end-to-end using the stochastic gradient descent algorithm. Finally, the model is implemented, trained, and evaluated using the TensorFlow deep learning framework.

[MODEL INFERENCE]
We use an inference strategy similar to the approach in Olabiyi et al. (2018).
For the modified noise sample, we perform a linear search for α with sample size L = 1 based on the average word-level discriminator loss, −logD adv (G(.)) (Olabiyi et al., 2018) using trained models run in autoregressive mode to reflect performance in actual deployment. The optimum α value is then used for all inferences and evaluations. During inference, we condition the dialogue response generation on the encoder outputs, noise samples, word embedding and the attribute embedding of the intended responder. With multiple noise samples, L = 64, we rank the generator outputs by the discriminator which is also conditioned on encoder outputs, and the intended responder's attribute embedding. The final response is the response ranked highest by the discriminator. For phredGAN d , we average the confidences produced by D adv and D att .

[EXPERIMENTS AND RESULTS]
In this section, we explore the performance of PHRED, phredGAN a and phredGAN d on two conversational datasets and compare its performance to non-adversarial persona Seq2seq models Li et al. (2016b) as well as to the adversarial hredGAN (Olabiyi et al., 2018) with no explicit persona.

[DATASETS]
TV Series Transcripts dataset (Serban et al., 2016). We train all models on transcripts from the two popular TV drama series, Big Bang Theory and Friends. Following a similar preprocessing setup in Li et al. (2016b), we collect utterances from the top 12 speakers from both series to construct a corpus of 5,008 lines of multi-turn dialogue. We split the corpus into training, development, and test set with a 94%, 3%, and 3% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset.
Due to the small size of the combined transcripts dataset, we first train our model on the larger Movie Triplets Corpus (MTC) by Banchs (2012) which consists of 240,000 dialogue triples. We pre-train  our model on this dataset to initialize our model parameters to avoid overfitting on a relatively small persona TV series dataset. After pre-training on MTC, we reinitialize the attribute embeddings in the generator from a uniform distribution following a Xavier initialization (Glorot & Bengio, 2010) for training on the combined person TV series dataset.
Ubuntu Dialogue Corpus (UDC) dataset (Serban et al., 2017b). We train our model on 1.85 million conversations of multi-turn dialogue from the Ubuntu community hub, with an average of 5 utterances per conversation. We assign two types of speaker IDs to utterances in this dataset: questioner and helper. We follow a similar training, development, and test split as the UDC dataset in Olabiyi et al. (2018), with 90%, 5%, and 5% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset While the overwhelming majority of utterances in UDC follow two speaker types, the dataset does include utterances that do not classify under either a questioner or helper speaker type. In order to remain consistent, we assume that there are only two speaker types within this dataset and that the first utterance of every dialogue is from a questioner. This simplifying assumption does introduce a degree of noise into each persona model's ability to construct attribute embeddings. However, our experiment results demonstrate that both phredGAN a and phredGAN d is still able to differentiate between the larger two speaker types in the dataset.

[EVALUATION METRICS]
We use similar evaluation metrics as in Olabiyi et al. (2018) including perplexity, BLEU (Papineni et al., 2002), ROUGE (Lin, 2014), distinct n-gram (Li et al., 2016a) and normalized average sequence length (NASL) scores. For human evaluation, we follow a similar setup as Li et al. (2016a), employing crowd-sourced judges to evaluate a random selection of 200 samples. We present both the multi-turn context and the generated responses from the models to 3 judges and asked them to rank the general response quality in terms of relevance, informativeness, and persona. For N models, the model with the lowest quality is assigned a score 0 and the highest is assigned a score N-1. Ties are not allowed. The scores are normalized between 0 and 1 and averaged over the total number of samples and judges. 

[BASELINE]
We compare the non-adversarial persona HRED model, PHRED with the adversarially trained ones, i.e. hredGAN , phredGAN a and phredGAN d , to demonstrate the impact of adversarial training. Please note that no noise was added to the PHRED model.
We also compare the persona models to Li et al.'s work (Li et al., 2016b) which uses a Seq2Seq framework in conjunction with learnable persona embeddings. Their work explores two persona models in order to incorporate vector representations of speaker interaction and speaker attributes into the decoder of their Seq2Seq model i.e., Speaker model (SM) and Speaker-Addressee model (SAM). All reported results are based on our implementation of their models in Li et al. (2016b).

[HYPERPARAMETER SEARCH]
For both phredGAN a and phredGAN d , we determine the noise injection method and the optimum noise variance α that allows for the best performance on both datasets. We find that phredGAN d performs optimally with word-level noise injection on both Ubuntu and TV transcripts, while phredGAN a performs the best with utterance-level noise injection on TV transcripts and word-level injection on UDC. For all phredGAN models, we perform a linear search for optimal noise variance values between 1 and 30 at an increment of 1, with a sample size of L = 1. For phredGAN d , we obtain an optimal α of 4 and 6 for the UDC and TV Transcripts respectively. For phredGAN a , we obtain an optimal value of 2 and 5 for the combined TV series dataset and the much larger UDC respectively.

[RESULTS]
We will now present our assessment of performance comparisons of phredGAN against the baselines, PHRED, hredGAN and Li et al.'s persona Seq2Seq models.

[QUANTITATIVE ANALYSIS]
We first report the performance on TV series transcripts in table 1. The performance of both SM and SAM models in Li et al. (2016b) compared to the hredGAN shows a strong baseline and indicates that the effect of persona is more important than that of multi-turn and adversarial training for datasets with weak multiple persona. However, once the persona information is added to the hredGAN , the resulting phredGAN shows a significant improvement over the SM and SAM baselines with phredGAN a performing best. We also observe that PHRED performs worse than the baseline S(A)M models on a number of metrics but we attribute this to the effect of persona on a limited dataset that results into less informative responses. This behavior was also reported in Li et al. (2016b) where the persona models produce less informative responses than the non-personal Seq2seq models but it seems to be even worse in multi-turn context. However, unlike the Speaker-Addressee and PHRED models that suffer from lower response quality due to persona conditioning, we note that conditioning the generator and discriminator of phredGAN on speaker embeddings does not compromise the systems ability to produce diverse responses. This problem might have been alleviated by the adversarial training that encourages the generator model to produce longer, more informative, and diverse responses that have high persona relevance even with a limited dataset.
We also compare the models performances on the UDC. The evaluation result is summarized in table 2. While the deleterious effect of persona conditioning on response diversity is still worse with PHRED than with S(A)M models, we note that hredGAN performs much better than the S(A)M models. This is because, the external persona only provides just a little more information than is already available from the UDC utterances. We also note an improvement of phredGAN variants over the hredGAN in a variety of evaluation metrics including perplexity, ROUGE with the exception of distinct n-grams. This is expected as phredGAN should be generally less diverse than hredGAN since the number of distinct data distribution modes is more for phredGAN dataset due to the persona attributes. However, this leads to better response quality with persona, something not achievable with hredGAN . Also, the much better ROUGE(F1) score indicates that phredGAN is able to strike a better balance between diversity and precision while still capturing the characteristics of the speaker attribute modality in the UDC dataset. Within the phredGAN variants, phredGAN d seems to perform better. This is not surprising as speaker classification is much easier on UDC than on TV series. The attribute discriminator, D att is able to provide more informative feedback on UDC than on TV series where it is more difficult to accurately predict the speaker. Therefore, we recommend phredGAN a for datasets with weak attribute distinction and phredGAN d for strong attribute distinction.

[QUALITATIVE ANALYSIS]
In addition to the quantitative analysis above, we report the results of the human evaluation in the last column of tables 1 and 2 for the TV Series and UDC datasets respectively. The human evaluation scores largely agrees with the automatic evaluations on the TV Series with phredGAN a clearly giving the best performance. However, on the UDC, both hredGAN and phredGAN d performs similarly which indicates that there is a trade off between diversity and persona by each model. We believe this is due to the strong persona information that already exists in the UDC utterances.
An additional qualitative assessment of these results are in Table 3 with responses from several characters in the TV series dataset and the two characters in UDC.
We see that for TV drama series, phredGAN responses are comparatively more informative than that of the Speaker-Addressee model of Li et al. (2016b). For example, all the characters in the TV series respond the same to the dialogue context. Similar behavior is reported in Li et al. (2016b) where for the Speaker-Addressee model, nearly all the characters in the TV series respond with We also see similar results with our model's output on UDC in table 4. We demonstrate that by conditioning as either a helper or questioner from the UDC dataset, phredGAN models are able to respond differently to input utterances as well as stay close to the context of the conversation.

[CONCLUSION AND FUTURE WORK]
In this paper, we improve upon state-of-the-art persona-based response generation models by exploring two persona conversational models: phredGAN a which passes the attribute representation as an additional input into a traditional adversarial discriminator, and phredGAN d a dual discriminator system which in addition to the adversarial discriminator from hredGAN , collaboratively predicts the attribute(s) that are intrinsic to the input utterance. Both systems demonstrate quantitative improvements upon state-of-the-art persona conversational systems such as the work from Li et al. (2016b) with respect to both quantitative automatic and qualitative human measures.
Our analysis also demonstrates how both variants of phredGAN perform differently on datasets with weak and strong modality. One of our future direction is to take advantage of phredGAN d 's ability to predict utterance attribute such as speaker identity from just the utterance. We believe its performance can be improved even with weak modality by further conditioning adversarial updates on both the attribute and adversarial discriminator accuracies. Overall, this paper demonstrates clear benefits from adversarial training of persona generative dialogue system and leaves the door open for more interesting work to be accomplished in this domain.  

[APPENDIX ALGORITHM 1 ADVERSARIAL LEARNING OF PHREDGAN]
Require: A generator G with parameters θ G . Require: An adversarial discriminator D adv with parameters θ D adv . Require: An attribute discriminator Datt with parameters θ D att . Require: Training hyperparameters, isT arget, λ G att , λ G adv , and λ M .
for number of training iterations do Initialize cRN N to zero state, h0 Sample a mini-batch of conversations, X = {Xi, Ci} N i=1 , X i = (X1, C1), (X2, C2), • • • , (Xi, Ci) with N utterances. Each utterance mini batch i contains Mi word tokens.
Compute the generator output similar to Eq. (11) in Olabiyi et al. (2018).
Sample a corresponding mini batch of utterance Yi.
else Update phredGANa's θ D adv with gradient of the discriminator loss.
Update θ G with attribute, adversarial and MLE losses.

[RESULTS -DISCRIMINATOR]
After training both phredGAN models on the TV series and UDC datasets, we ran inference on some example dialogue contexts. The responses and their discriminator scores from phredGAN s are listed in Tables 6, and 7. The tables shows that phredGAN (i) can handle multi-turn dialogue context with utterances and corresponding persona attributes; (ii) generates responses conditioned on a persona attribute; (iii) generates multiple responses per dialogue context and score their human likelihood by the discriminator; and (iv) in case of phredGAN d , can predict the attribute such as speaker identity that might have produced the utterance. We observe that the discriminator score(s) is/are generally reasonable with longer, more informative and more persona-related responses receiving higher scores. It worth to note that this behavior, although similar to the behavior of a human judge is learned without supervision. More so, we observe that phredGAN responses retain contextual consistency sometimes referencing background information that is inherent in the conversation between two speakers. For example, in the second sample of the TV series in Table 6, phredGAN a generator, conditioned on Leonard refers to Sheldon by name who is the second interlocutor. Also, in the third sample, phredGAN a , conditioned on Raj refers to Penny when responding to Leonard who happens to be Penny's boy friend. We see similar persona-based response generation for the UDC dataset with distinct communication style between the asker and the helper. For example, in Table 7, when the asker could not hear some music, phredGAN d , conditioned on helper suggested the asker might not be using the right driver. For the purpose of completion, we also show some samples from PHRED generator on both UDC and TV series dataset in Table 5.","[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).","{
    ""1"": {
        ""summary"": ""The paper's novelty is limited and contributions are incremental, relying heavily on previous works."",
        ""verbatim"": ""the novelty of this paper is limited, and the contribution is incremental.""
    },
    ""2"": {
        ""summary"": ""The proposed model's extension on previous work seems minimal, making it less novel."",
        ""verbatim"": ""The idea of learning speaker-specific attribute embeddings is very similar to the Speaker Model proposed by Li et al.(2016) and the proposed system only makes minor changes to hredGAN.""
    },
    ""3"": {
        ""summary"": ""Presentation and explanation of the model and its components are unclear and challenging to follow."",
        ""verbatim"": ""The writing of this paper is a little hard to follow.""
    },
    ""4"": {
        ""summary"": ""The proposed model's evaluation is weak and not sufficiently rigorous compared to existing methods."",
        ""verbatim"": ""The evaluation is not strong enough to demonstrate the benefit of the proposed model.""
    },
    ""5"": {
        ""summary"": ""There is a lack of robust comparison and analysis to justify the paper's contributions and improvements."",
        ""verbatim"": ""The paper lacks more in-depth analysis, comparison and evaluation of the model.""
    }
}",5,"{
    ""1"": {
        ""summary"": ""The paper lacks clear explanation or visualization of the model architecture."",
        ""verbatim"": ""The paper could benefit from a clearer explanation or visualization of the model architecture, particularly the encoder and generator components, to aid in better understanding of the proposed phredGAN framework.""
    },
    ""2"": {
        ""summary"": ""Insufficient specific examples or comparisons in some sections."",
        ""verbatim"": ""While the results are promising, some sections lack specific examples or comparisons, making it challenging to fully grasp the impact of the proposed models in certain contexts.""
    },
    ""3"": {
        ""summary"": ""Limited discussion on the model's limitations."",
        ""verbatim"": ""The discussion on the limitations of the models could be expanded to provide a more in-depth analysis of potential weaknesses or areas for future research.""
    },
    ""4"": {
        ""summary"": ""Need for additional experiments to demonstrate the model's adaptability and robustness."",
        ""verbatim"": ""It would be helpful to include additional experiments or scenarios to showcase the adaptability and robustness of the phredGAN system in handling diverse attributes and persona-specific responses more effectively.""
    }
}",4,"{
    ""A3-B1"": {
        ""rationale"": ""Both review A3 and review B1 highlight difficulties in understanding and following the paper's presentation of the model. A3 directly mentions the writing is hard to follow, while B1 focuses specifically on the need for clearer explanation or visualization of the model architecture to aid comprehension."",
        ""similarity"": ""7""
    },
    ""A5-B2"": {
        ""rationale"": ""Review A5 addresses the paper's lack of in-depth analysis and comparison, which directly corresponds to B2's point on lacking specific examples and comparisons in some sections. Both points critique the paper for not providing enough detailed evaluation in establishing the proposed model's context or effectiveness."",
        ""similarity"": ""8""
    }
}",2,"{
    ""1"": {
        ""summary"": ""Lack of detailed implementation details of phredGAN and modifications to the hredGAN architecture."",
        ""verbatim"": ""The paper could be rejected if it lacks a detailed explanation of the implementation details of phredGAN and lacks clarity on the modifications made to the hredGAN architecture.""
    },
    ""2"": {
        ""summary"": ""Insufficient justification of dataset selection and limited comparisons to other models."",
        ""verbatim"": ""If the paper does not thoroughly justify the choice of datasets or fails to compare phredGAN with a broader set of baseline models, it may face rejection.""
    },
    ""3"": {
        ""summary"": ""Results are inconclusive or do not clearly show the superiority of phredGAN."",
        ""verbatim"": ""If the results presented are inconclusive or do not clearly demonstrate the superiority of phredGAN over existing models, the paper may not be accepted.""
    }
}",3,"{
    ""A3-B1"": {
        ""rationale"": ""Both points address the clarity and descriptive detail of the proposed model and its modifications. Review A mentions the general presentation and explanation of the model are unclear, making it hard to follow. Review B focuses more specifically on the lack of details regarding the implementation of phredGAN and the modifications to the hredGAN architecture."",
        ""similarity"": ""7""
    },
    ""A4-B3"": {
        ""rationale"": ""These points both focus on the evaluation of the proposed model but from slightly different angles. Review A suggests the evaluation of the model is not rigorous or strong enough to demonstrate benefits, whereas Review B points out the results are inconclusive or don't demonstrate superiority over existing models."",
        ""similarity"": ""8""
    },
    ""A5-B2"": {
        ""rationale"": ""Review A's concern about the lack of in-depth analysis and robust comparison closely aligns with Review B's concern about insufficient justification of dataset selection and limited comparisons to other models. Both reviews stress the need for more comprehensive evaluation and comparative analysis."",
        ""similarity"": ""8""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Lack of clarity in the comparison between different approaches in the conditional discriminator"",
        ""verbatim"": ""The paper might face rejection if the comparison between the two approaches for the conditional discriminator, phredGAN a and phredGAN d, is not well-structured or explained clearly.""
    },
    ""2"": {
        ""summary"": ""Inadequate discussion on trade-offs observed with different datasets"",
        ""verbatim"": ""Further elaboration on the trade-offs observed while using phredGAN on datasets with varying attribute modalities could be necessary to avoid potential rejection due to incomplete analysis.""
    }
}",2,"{
    ""A5-B1"": {
        ""rationale"": ""Both points address the lack of substantial comparative analysis in the paper. Review A emphasizes the dire need for a deeper comparison and evaluation to justify the model's contributions, while Review B focuses on the specific need for a clearer comparison between two versions of a model, hinting at a similar underlying issue of inadequacy in robust comparative analysis."",
        ""similarity"": ""7""
    }
}",1,"{
    ""1"": {
        ""summary"": ""Complexity and computational demands may limit practical applicability"",
        ""verbatim"": ""The dual discriminator approach, while innovative, adds to the computational overhead. This might limit the practical applicability of the model in resource-constrained environments.""
    },
    ""2"": {
        ""summary"": ""Concerns about the model's ability to generalize to other datasets"",
        ""verbatim"": ""Given the extensive training on specific datasets with strong persona attributes, there could be concerns about the model's ability to generalize across other less structured or differently characterized dialog datasets.""
    },
    ""3"": {
        ""summary"": ""Limited exploration of noise impact on model performance"",
        ""verbatim"": ""The paper discusses different noise injection methods but does not extensively explore their impact on the model's performance across different settings, which could be seen as a gap in the experimental analysis.""
    }
}",3,{},0,"{
    ""1"": {
        ""summary"": ""Concern about the complexity and scalability of the proposed models"",
        ""verbatim"": ""While the proposed models are intellectually appealing, the increased complexity might raise concerns regarding their scalability and practical deployment, especially in real-time applications.""
    },
    ""2"": {
        ""summary"": ""Assumptions in attribute modality differentiation may limit generalizability"",
        ""verbatim"": ""The differentiation between 'many but weak' versus 'few but strong' attribute modalities is a crucial aspect of the system's design. However, the assumptions underlying these categorizations may not hold across other datasets or real-world scenarios, potentially limiting the generalizability of the findings.""
    }
}",2,{},0,"{
    ""1"": {
        ""summary"": ""Potential Overfitting on Specific Datasets"",
        ""verbatim"": ""There might be a concern regarding how well the proposed models generalize across other types of datasets not covered in the experiments.""
    },
    ""2"": {
        ""summary"": ""Complexity of Implementation"",
        ""verbatim"": ""The increased complexity of the phredGAN models compared to traditional Seq2Seq systems might pose a barrier to adoption and could be a potential reason for rejection if the benefits do not convincingly outweigh the costs.""
    }
}",2,{},0,"[Significance and novelty]

1. Extension of persona-based Seq2Seq model to multi-turn dialogues: The authors introduce a novel approach to extend the persona-based Seq2Seq neural network conversation model to multi-turn dialogue scenarios by modifying the hredGAN architecture, making it a significant contribution to the field [2].
2. Capturing utterance attributes: The authors' proposed system, phredGAN, has a persona-based HRED generator (PHRED) and a conditional discriminator, allowing the model to capture crucial utterance attributes, such as speaker identity, dialogue topic, and speaker sentiments [1].
3. Two approaches for conditional discriminator: The proposed system offers two approaches to accomplish the conditional discriminator, phredGAN a and phredGAN d, providing a more flexible solution to handle varying datasets with different attribute modalities [3].

[Potential reasons for acceptance]
1. Superior performance: The authors provide quantitative measures and crowd-sourced human evaluation to demonstrate the superior performance of their phredGAN system over the person SeqSeq model [4]. They show improvement in various aspects, including conversational fluency, persona consistency, and attribute prediction accuracy.
2. Comprehensive evaluation: The paper's evaluation covers various aspects using a variety of quantitative measures and crowd-sourced human evaluation, providing a robust and in-depth assessment of the proposed system's capabilities [5].
3. Exploration of trade-offs: The authors examine the trade-offs between the two variants of phredGAN on datasets with diverse attribute modalities, shedding light on important considerations for using these models in practice [6].

[Potential reasons for rejection]
1. Lack of theoretical grounding: While the paper demonstrates the practical benefits of the proposed models, it does not provide a solid theoretical grounding to explain the underlying mechanism, making the model hard to reproduce and understand [7].
2. Experiment setup limitations: The paper's evaluation is limited to specific datasets, and the findings may not generalize to other conversational datasets or scenarios, which may impact the broader applicability of the proposed models [8].
3. Limited analysis of attribute importance: The paper focuses on a few specific attributes (speaker identity, dialogue topic, and speaker sentiments) but does not discuss other important attributes or their relative importance, which could affect the overall performance in various conversational scenarios [9].

[Suggestions for improvement]
1. Provide theoretical foundations: The authors could expand their work by providing a more solid theoretical grounding to explain the mechanisms of their models, allowing for a better understanding of their inner workings and potential improvements [10].
2. Evaluate on a broader range of datasets: The authors could experiment with additional conversational datasets, testing the robustness and generalizability of their proposed models in various conversational contexts [11].
3. Analyze the importance of attributes: The authors should explore the importance of different attributes in multi-turn dialogue scenarios, allowing for a more comprehensive understanding of the various factors affecting conversational performance [12].
4. Compare against other state-of-the-art models: It would be beneficial to compare the proposed models against other state-of-the-art dialogue models to provide a more nuanced understanding of their strengths and limitations [13].","[Significance and novelty]
<Discrete attribute representation> The paper proposes a novel extension of the HRED GAN framework to handle discrete attributes, such as speaker identity and sentiment, which is a significant and somewhat new contribution in the field of dialogue modeling.
<Experimentation with different attributes> The paper explores different attributes, such as topic and sentiment, and evaluates the model's performance on both strong and weak attribute modalities, which is an interesting aspect of the research.

[Potential reasons for acceptance]
<Effective demonstration of proposed model> The authors effectively demonstrate the proposed model's performance by showcasing improvements on datasets with different strength of attributes and by conducting human evaluations.
<Extensive empirical analysis> The paper's extensive experiments and human evaluation provide enough evidence to justify the effectiveness of the proposed model.

[Potential reasons for rejection]
<Inadequate technical novelty> The technical novelty of the proposed approach is limited, as the core of the idea comes from existing works, and significant increment in terms of technical contributions is not well justified.
<Clarity and language issues> The paper suffers from poor writing, lack of clarity in notations, and numerous typos, which affects the overall readability and presentation of the research.

[Suggestions for improvement]
<Clarify technical novelty> The authors should better justify the technical novelty of their approach, especially in comparison to existing works, to clearly demonstrate the contributions of their research.
<Address language and clarity issues> To enhance the paper's readability, authors should address the numerous typos, improve notations, and provide clearer explanations in key areas.

","{
    ""1"": {
        ""summary"": ""The paper lacks theoretical grounding for the proposed models, which affects understandability and reproducibility."",
        ""verbatim"": ""Lack of theoretical grounding: While the paper demonstrates the practical benefits of the proposed models, it does not provide a solid theoretical grounding to explain the underlying mechanism, making the model hard to reproduce and understand [7].""
    },
    ""2"": {
        ""summary"": ""The evaluation is limited to specific datasets, thereby questioning the generalizability of the findings to other conversational datasets or scenarios."",
        ""verbatim"": ""Experiment setup limitations: The paper's evaluation is limited to specific datasets, and the findings may not generalize to other conversational datasets or scenarios, which may impact the broader applicability of the proposed models [8].""
    },
    ""3"": {
        ""summary"": ""The analysis of attribute importance is limited, potentially affecting the model's performance in varied conversational scenarios."",
        ""verbatim"": ""Limited analysis of attribute importance: The paper focuses on a few specific attributes (speaker identity, dialogue topic, and speaker sentiments) but does not discuss other important attributes or their relative importance, which could affect the overall performance in various conversational scenarios [9].""
    }
}",3,"{
    ""1"": {
        ""summary"": ""Limited technical novelty"",
        ""verbatim"": ""The technical novelty of the proposed approach is limited, as the core of the idea comes from existing works, and significant increment in terms of technical contributions is not well justified.""
    },
    ""2"": {
        ""summary"": ""Poor writing and lack of clarity"",
        ""verbatim"": ""The paper suffers from poor writing, lack of clarity in notations, and numerous typos, which affects the overall readability and presentation of the research.""
    }
}",2,"{
    ""A4-B2"": {
        ""rationale"": ""Both points criticize the evaluation of the proposed model. Review A discusses the evaluation's weakness in demonstrating the model's benefits, while Review B points out the limitation of the evaluation being specific to certain datasets, questioning its generalizability. These concerns both address the shortcomings in how the model's effectiveness is validated."",
        ""similarity"": ""7""
    }
}",1,"{
    ""A1-B1"": {
        ""rationale"": ""Both points from Review A and Review B comment on the limited novelty of the paper due to its heavy reliance on previous works. The main argument in both points is that the contributions made by the paper are not sufficiently novel or significant, with Review B explicitly mentioning that the 'significant increment in terms of technical contributions is not well justified,' which aligns closely with Review A's note on the contributions being 'incremental.'"",
        ""similarity"": ""9""
    },
    ""A3-B2"": {
        ""rationale"": ""Both reviews highlight issues with the writing and presentation of the paper. Review A focuses on the difficulty in understanding the model and its components due to clarity issues, while Review B stresses poor writing, lack of clarity, and typos affecting readability and presentation. Both points reflect concerns about how the obscurity of presentation affects the reader's comprehension and assessment of the research."",
        ""similarity"": ""7""
    }
}",2