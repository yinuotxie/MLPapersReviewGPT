title,json_file,review,full_input,abstract_input
A Walk with SGD: How SGD Explores Regions of Deep Network Loss?,B1l6e3RcF7.json,"I like the idea of trying to qualitatively illustrate the behavior of SGD when optimizing parameters of complex models, such as Deep and Conv Nets, but I think that the contribution is not very substantial. The connection between SGD and diffusion has been pointed out in previous papers, as acknowledged by the Authors. The study of the effect of batch size is interesting, but again somewhat derived from previous works. 

It would helpful to illustrate the difference between ""crossing"" and ""moving over"" a barrier with a simple figure. 

The experimental validation is interesting, although I think it is limited and perhaps the conclusions that can be drawn from it are not so surprising. I believe it would have been interesting to study other important factors that affect the behavior of SGD, such as learning rate and type of momentum. For example, a larger learning rate might allow for more crossing of barriers. Also, different SGD algorithms (ADAGRAD, ADAM, etc...) would behave considerably differently I expect. At the moment these important factors are overlooked. 

It is not clear to me why we would want to avoid larger batch sizes. A larger batch size allows for a lower variance of stochastic gradients, and therefore faster convergence. I think this point requires elaboration, because this forms the motivation behind theoretically grounded and successful SGD works, such as SAGA and the like. I agree that a smaller batch-size is preferable at the beginning of the optimization, but again this is a well known fact (again, see SAGA) and it is for computational reasons mostly (being far away from the (local) mode, a noisy gradient is enough to move in the right direction - no need to spend computations to use an accurate gradient). There is no guarantee that the local optimum close to initialization is a bad local optimum in general, so I don't think that using a large batch size at the beginning is a bad idea for this reason - again it is just computational. 

Another thing missing I think is the discussion around why it is potentially a good thing to cross the barrier, either at the beginning of the exploration or towards convergence to a local optimum. At the moment, the paper seems to report the behavior of SGD without key insights on the importance of crossing or avoiding crossing barriers.

As a concluding remark - there has been a lot of work on the connections between diffusions and MCMC algorithms (see e.g., the Metropolis Adjusted Langevin Algorithm - MALA) and a lot of the considerations made in the paper are somewhat known. That is, random walk/diffusion type MCMC (and even gradient-based MCMC like Hybrid Monte Carlo) struggle a lot in non-convex problems and they hardly move across modes of a posterior distribution (equivalent to crossing barriers of potential). So I'm not at all surprised that SGD does not cross barriers during optimization and I would challenge the statement in the introduction saying ""Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process.""","[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.

[CAPTIONS]
Table 1: Figure 1 :Figure 2 :12Figure1: Plots for VGG-11 architecture trained using SGD on CIFAR-10. Each plot contains the training loss for 40 iterations of training at various epochs. Between the training loss at every consecutive iteration (vertical gray lines), we uniformly sample 10 points between the parameters before and after the training update and calculate the loss at these points. Thus we take a slice of the loss surface between two iterations. These loss values are plotted between every consecutive training loss value from training updates. We find that the loss interpolation between consecutive iterations have a minimum in between in all cases showing barriers are not being crossed. For epochs 25 and 100 this is not clearly visible, but we quantitatively record it and discuss it later. The dashed orange line (only shown in the epoch 1 plot) connects the minimum of the loss interpolation between consecutive iterations and is shown to highlight that the valley floor has ups and downs along the path of SGD (which can be seen for all epochs).
Table 2: Figure 3 :3Figure 3: Plots for MLP architecture trained using SGD on MNIST. All the descriptions are same as described in figure 1.
Table 3: Figure 4 :4Figure 4: Numbers of barriers found during training loss interpolation for every epoch (450 iterations) for VGG-11 on CIFAR-10.We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
Table 4: Figure 5 :5Figure5: Plots for the alignments between mini-batch gradient and hessian with VGG-11 architecture trained using SGD on CIFAR-10 at the end of Epoch 5 and Epoch 10. Alignments are calculated for mini-batch size 100,1000,10000 and 45000 (dataset size).
Table 5: Figure 7 :7Figure 7: Plots of the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100 for different batch sizes.
Table 6: Figure 8 :8Figure 8: Plots for Resnet-56 Epoch 1 trained using full batch Gradient Descent (GD) on CIFAR-10.
Table 7: Figure 9 :9Figure 9: Plots for Resnet-56 Epoch 1 trained using SGD on CIFAR-10.
Table 8: Figure 10 :10Figure 10: Plots for Resnet-56 Epoch 2 trained using SGD on CIFAR-10.
Table 9: Figure 11 :11Figure 11: Plots for Resnet-56 Epoch 25 trained using SGD on CIFAR-10.
Table 10: Figure 12 :12Figure 12: Plots for Resnet-56 Epoch 100 trained using SGD on CIFAR-10.
Table 11: Figure 13 :13Figure 13: Plots for VGG-11 Epoch 2 trained using SGD on CIFAR-10.
Table 12: Figure 14 :14Figure 14: Plots for VGG-11 Epoch 25 trained using SGD on CIFAR-10.
Table 13: Figure 15 :15Figure 15: Plots for VGG-11 Epoch 100 trained using SGD on CIFAR-10.
Table 14: Figure 16 :16Figure 16: Plots for MLP Epoch 1 trained using full batch Gradient Descent (GD) on MNIST.
Table 15: Figure 17 :17Figure 17: Plots for MLP Epoch 1 trained using SGD on MNIST.
Table 16: Figure 18 :18Figure 18: Plots for MLP Epoch 2 trained using SGD on MNIST.
Table 17: Figure 19 :19Figure 19: Plots for VGG-11 Epoch 1 trained using full batch Gradient Descent (GD) on Tiny-ImageNet.
Table 18: Figure 20 :20Figure 20: Plots for VGG-11 Epoch 1 trained using SGD on Tiny-ImageNet.
Table 19: Figure 21 :21Figure 21: Plots for VGG-11 Epoch 1 trained using learning rate 0.3 batch size 100 on CIFAR-10.
Table 20: 

[INTRODUCTION]
The non-convexity of the deep neural network (DNN) loss surface makes the behavior of optimization algorithms less intuitive compared to the convex setting. Moreover, optimization in DNNs is no longer about finding any minimum, but rather about finding ones that generalizes well (Keskar et al., 2016). Since deep networks are initialized randomly, finding such minima will require exploration of different regions of the loss surface. This intuition has been formalized in recent papers that study stochastic gradient descent (SGD) as a diffusion process (Hoffer et al., 2017;Smith & Le, 2017;Jastrzebski et al., 2017;Chaudhari & Soatto, 2017). Briefly, these papers show that SGD simulates a discrete approximation of stochastic differential equation (SDE), and hence performs a random walk on the potential induced by the DNN loss surface.
In this work, we complement the diffusion perspective of SGD with a qualitative view of how SGD explores different regions of the non-convex loss landscape of deep neural networks through empirical evidence. Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process. We show in this work that SGD rarely crosses any barriers along its path during the course of training. By this observation, we do not claim that SGD does not simulate diffusion. Through experimental deductions, we show an alternate mechanism that SGD seems to dominantly use to explore different regions of the non-convex loss landscape.
Further, it is known that larger batch-sizes slow down the diffusion process (Hoffer et al., 2017). We show the qualitative reason behind this slow down to be an oscillation behavior of SGD which prevents it from moving far away from initialization. This behavior is a result of the mini-batch gradients becoming increasingly aligned with the top eigenvectors of the Hessian for larger batch-sizes. This behavior is known to slow down convergence in optimization theory (for instance consider the motivation behind momentum (Polyak, 1964;Sutskever et al., 2013)). We discuss how it also slows down explorations in the non-convex setting of deep network loss surface.
Experiments are conducted on multiple data sets, architectures and hyper-parameter settings. The findings mentioned above hold true on all of them.

[SETUP]
We now describe the details of how we study the existence of barriers along the optimization path of SGD. The main tool we use for studying the DNN loss surface along SGD's path is to interpolate the loss surface between parameters before and after each training update. We note that this strategy of interpolating the loss surface between parameters was introduced by Goodfellow et al. (2014). In their paper, the interpolation is conducted between initial and final (after training) parameter values for analysis purposes. In contrast, we compute interpolations before and after each training update because this interpolation precisely tells us whether or not SGD crosses a barrier during an update step. We say a barrier is crossed when we see a point in the parameter space interpolated between the parameters just before and after an update step, such that the loss at the barrier point is higher than the loss at both the other points.
Consider that the parameters θ of a neural network are initialized to a value θ 0 . When using an optimization method to update these parameters, the t th update step takes the parameter from θ t to θ t+1 using estimated gradient g t as,
θ t+1 = θ t − ηg t (1)
where η is the learning rate. Notice the t th update step implies the t th epoch only in the case when using the full batch gradient descent (GD). In the case of stochastic gradient descent, one iteration is an update from gradient computed from a mini-batch. We then interpolate the DNN loss between the convex combination of θ t and θ t+1 by considering parameter vectors θ α t = (1 − α)θ t + αθ t+1 , where α ∈ [0, 1] is chosen such that we obtain 10 samples uniformly placed between these two parameter points. We note that even though the updates are performed using mini-batches for SGD, the training loss values we compute for the interpolation use the full dataset to visualize the actual loss landscape.

[BARRIERS AND EXPLORATION DURING SGD TRAINING]
For this section, we perform experiments on MNIST (Lecun & Cortes) and CIFAR-10 (Krizhevsky, 2009) datasets, and use multi-layer perceptrons (MLP), VGG-11 (Simonyan & Zisserman, 2014) and Resnet-56 (He et al., 2016) architectures with various batch sizes and learning rates. We discuss our observations for VGG-11 architecture on CIFAR-10 dataset (figure 1) as a reference but the same conclusions hold for experiments on MLP trained on MNIST (figure 3) and Resnet-56 trained on CIFAR-10 (figure 2).
We train VGG-11 on CIFAR-10 with a batch size of 100 and fixed learning rate of 0.1. We report the visualization of loss interpolation between consecutive iterations for 40 iterations from epochs 1, 2, 25 and 100 for visual clarity. The interpolation is shown in figure 1. To be clear, the x-axis is calibrated by the number of iterations, and there are 10 interpolated loss values between each consecutive iteration (vertical gray lines) in the training loss plots. In these plots, we find two interesting behavior of SGD.
First, we find that the interpolated loss between every consecutive update from SGD optimization update appears to be a quadratic-like structure with a minimum in between. Note that while this is not visible for epochs 25 and 100, we later show quantitative measurements that ensures this claim. This plot thus shows that in the iterations plotted, SGD rarely crosses barriers.
Second, we observe how the minimum of each interpolation evolves as training progresses. This is highlighted in figure 1 (a) with a dashed orange line. We find that this minimum has ups and downs along the path of SGD for all our interpolation plots. To draw deductions from this observation, consider a simple example that helps us understand this scenario concretely. Let parameter points θ A , θ B and θ C be a result of three consecutive SGD updates with loss values A , B and C (using full training set). Note that since these are only three points, they exist in a two dimensional subspace and the loss value can be imagined along the third dimension. Then corresponding to the behavior in the plot, there is a parameter point θ AB between θ A and θ B on the line connecting these two points, which has a loss value AB < A , B . Similarly there is a point θ BC between θ B and θ C on the line connecting these two points, which has a loss value BC < B , C . Given this construction, for any configuration of θ A , θ B and θ C on the two dimensional plane, it is easy to see that if AB < BC , any path from θ AB to θ BC will have loss values that must increase at some point. Hence, what this construction essentially represents (as we refer to it), is a situation where SGD has moved over a barrier. Therefore, the ups and downs of the minimum between loss interpolations in figure 1 (a,b,c,d) represents SGD moving over barriers. In this way we find that when running SGD on the loss surface of deep networks, instead of crossing barriers, a more dominant way SGD performs exploration is by moving over them.  on CIFAR-10 and MLP on MNIST. We say a barrier is crossed during an update step if there exists a point interpolated between the parameters before and after an update which has a loss value higher than the loss at either points. For most parts of the training, we find that SGD does not cross any significant number of barriers.
The same qualitative analysis for SGD with different hyper-parameters are also shown in section 1 in appendix. The observations we described here remain consistent for all these experiments.
So far we showed qualitative visualizations to make the claim that SGD rarely crosses barriers. In order to show that the claim extends to the rest of the training instead of only a few iterations we showed above, we now quantitatively measure how many barriers are crossed for the entire epoch in different phase of training. This result is shown in table 1 for VGG-11 and Resnet-56 trained on CIFAR-10 (trained for 100 epochs) and an MLP trained on MNIST (trained for 40 epochs). We note that each case, an epoch consists of more than 450 iterations. As we see, a negligible number of barriers are crossed for most parts of the training compared to the number of iterations performed during each epoch. For concreteness, we further compute the number of barriers crossed for the first 40 epochs for VGG-11 on CIFAR-10 as shown in Figure 4 and reach the same conclusion. We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
L(θt)+L(θt+1)−2L(θ min t ) 2
) are substantially smaller compared with the value of loss at the corresponding iterations (not mentioned here), meaning they are not significant barriers.

[THE EFFECT OF BATCH-SIZE ON EXPLORATION]
4.1 ANALYSIS Hoffer et al. (2017) discuss that SGD training with different batch-sizes leads to different diffusion rates (very large batch sizes being slower). Further, when training for the same number of epochs, a larger batch training performs less number of iterations. Combining these two observations, they reach the conclusion that large batch training makes the diffusion process slow. As empirical evidence, they show that the distance of parameters from initialization evolves logarithmically in the number of iterations.
We now present a complementary optimization perspective to their observation. To continue, we introduce the following notations. Let p i (θ) denote the predicted probability output (of the correct class in the classification setting for instance) of a DNN parameterized by θ for the i th data sample (in total N samples). Then the negative log likelihood loss for the i th sample is given by
L i (θ) = − log(p i (θ)). The gradient g B (θ) from mini-batch SGD at a parameter value θ is ex- pressed as, g B (θ) = 1 B i∈B ∂Li(θ)
∂θ ,ḡ(θ) denotes the expected gradient using all training samples, B is the mini-batch size (and we have also overloaded it to mean the mini-batch set) and C(θ) is the gradient covariance matrix at θ. Then the relation between the Hessian H(θ) and the dataset gradient covariance C(θ) for negative log likelihood loss is described by the Gauss-Newton decomposition as follows,
H(θ) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂w 2 (2)
where H(θ) is the Hessian of the loss. The derivation can be found in section B of the appendix.
To continue with our argument, we note that it has been discussed by Shwartz-Ziv & Tishby (2017) that early on during training, the mean gradient over the training set is larger in magnitude compared to the variance in gradients. The above argument essentially says that the scale of mean gradient g(θ) is larger compared with the scale of C(θ). Ignoring the second order term in the Gauss-Newton decomposition above, we see that the mean gradient must be aligned with the top eigenvectors of the Hessian since the scale of gradient covariance is much smaller early on during training. Finally, we note that using large batch-sizes makes the mini-batch gradient closer to the mean gradient by reducing the scale of mini-batch gradient covariance as shown by Hoffer et al. (2017),
cov(g B (θ), g B (θ)) = 1 B − 1 N C(θ)(3)
The two arguments together imply that gradients from larger batch-sizes are likely to be more aligned with the high curvature directions of the loss surface especially early on during training.
In convex optimization theory, when gradients point along the top eigenvectors of the Hessian (also referred to as the sharp directions of the loss surface), optimization exhibits under-damped convergence, meaning it oscillates along the sharp directions in the case when the learning rate is smaller than a certain threshold. Applying this idea to non-convex loss landscapes, a large alignment between the mini-batch gradient and the sharp directions should also lead to oscillations. At this point, we depart from the conclusions of the convex setting and recall our observation in the previous section that the interpolation between consecutive iterations has a quadratic like shape and SGD moves over barriers for the deep network loss surface. We thus hypothesize that a lower alignment between mini-batch gradients and the sharp directions of the loss surface makes SGD exploration faster by exhibiting less oscillation, and vice-versa.

[EMPIRICAL VERIFICATION]
Based on the theoretical analysis above, we first conduct experiments to empirically verify that the alignment of mini-batch gradient g B (θ) and hessian H(θ) increases when we increase mini-batch size. To do so, we calculate the alignment of mini-batch gradient g B (θ) and hessian H(θ) as Figure 6: Plots for the alignments between mini-batch gradient and hessian with Resnet-56 architecture trained using SGD on CIFAR-100 at the end of Epoch 5 and Epoch 10. All the descriptions are same as described in figure 5.
g T B (θ)H(θ)g B (θ) g B (θ) 2 2 .(4)
Figure 5 and figure 6 show the alignments calculated according to Equation 4 on both VGG-11 with CIFAR-10 and Resnet-56 with CIFAR-100 separately at the end of Epoch 5 and Epoch 10. We calculate the alignment for mini-batch size 100, 1000, 10000 and 45000 (which is the dataset size).
For every mini-batch size, we sample 50 different batches and calculate the alignment of the current mini-batch gradient with the hessian and show both the mean and standard deviation of alignments in the plots. From both figure 5 and figure 6, we can see that the alignment between mini-batch gradient and hessian is larger for larger mini-batch size.
Based on the empirical verification between mini-batch gradient and sharp directions above, we now verify our argument whether it leads SGD to oscillate in the proximity of the parameter initialization, thus slowing down exploration. Note the latter has been shown by Hoffer et al. (2017). Therefore, to substantiate our claim, we show the degree of oscillation in SGD increases with large batchsize. Specifically, while training deep networks, we keep track of the cosine of the angle between mini-batch gradients from every two consecutive SGD iterations,
cos(g t−1 , g t ) := g T t−1 g t ( g t−1 2 g t 2 )
.
(5)
Figure 7 shows the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100. Experiments are run with the same learning rate for batch size 500, 5000 and 45000 (dataset size). We can see from the plot that the cosine of the angle between mini-batch gradients from two consecutive iterations remains smaller for larger batch sizes, which indicates that the gradients from two consecutive iterations point more in opposite directions for larger batch sizes. Together with the parameter distance results from Hoffer et al. (2017) that shows that within the same number of iterations, the parameter norm for larger batch sizes is smaller, our experiment verifies that for larger batch sizes, SGD oscillates more in the proximity of the parameter initialization instead of exploring farther away regions. 

[BACKGROUND AND RELATED WORK]
There have been previous work on visualizing the loss surface although from different motivations.
Perhaps Goodfellow et al. ( 2014) is most similar to our work since we use the loss interpolation tool suggested in their paper to perform our analysis. They perform interpolation between the initial and final parameters and based on their finding, draw the conclusion that the loss along the line connecting these two points does not have any barriers. We note that we use their tool for a different purpose and our conclusions are fundamentally different from theirs because we use the observations to investigate whether SGD crosses barriers during optimization over deep networks' loss landscape. Li et al. (2017b) also visualize the loss landscape of different network architectures.
Our work is closely related to a number of recent papers that study SGD as a diffusion process because we present a complementary qualitative view to an aspect of their theory. Hoffer et al. (2017) hypothesize this view based on the evidence that the parameter distance moved by SGD from initialization as a function of the number of iterations resembles a diffusion process. Li et al. (2017a) hypothesize this behavior of SGD and theoretically show that this diffusion process would allow SGD to escape sharp local minima. The authors use this theoretical result to support the findings of Keskar et al. (2016) who find that SGD with small mini-batch size find wider minima. Kushner & Yin (2003); Mandt et al. (2017); Chaudhari & Soatto (2017); Smith & Le (2017); Jastrzebski et al. (2017); Li et al. (2015) study SGD as a discrete approximation of stochastic differential equation under the assumption of a reasonably small learning rate and batch-size (compared with dataset size). Broadly, these papers show that the stochastic fluctuation in the stochastic differential equation simulated by SGD is governed by the ratio of learning rate to batch size. In this paper we study the qualitative roles of batch size without any assumption on how large it is with respect to dataset size.
We note that Zhu et al. (2018) present an analysis of how the structure of gradient covariance matrix can help SGD escape sharp minima more efficiently. Specifically, they show that when the top eigenvectors of the gradient covariance and Hessian are aligned, the escaping efficiency of SGD out of sharp minima is best. We find our view of exploration of different regions by SGD on the other side of the spectrum. While they compare the alignment between the gradient covariance (noise) and Hessian, we talk about the alignment between the mean gradient and the Hessian, which may be seen as complementary views.
There is a long list of work towards understanding the loss surface geometry of DNNs from a theoretical standpoint which is similar in spirit to our analysis of loss surface. Dotsenko (1995); Amit et al. (1985); Choromanska et al. (2015) show that under certain assumptions, the DNN loss landscape is similar to the spherical spin glass model which is well studied in terms of its critical points. Safran & Shamir (2016) show that under certain mild assumptions, the initialization is likely to be such that there exists a continuous monotonically decreasing path from the initial point to the global minimum. Freeman & Bruna (2016) theoretically show that for DNNs with rectified linear units (ReLU), the level sets of the loss surface become more connected as network over-parametrization increases. This has also been justified by Sagun et al. (2017) who show that the Hessian of deep ReLU networks is degenerate when the network is over-parametrized and hence the loss surface is flat along such degenerate directions. Broadly these studies analyze DNN loss surfaces (either theoretically or empirically) in isolation from the optimization dynamics.
In our work we do not study the loss surface in isolation, but rather analyze it through the lens of SGD. In other words, we study the DNN loss surface along the trajectory of SGD, based on which we make deductions about how SGD explores the different regions of the loss surface and the effect of batch-size on this aspect.

[DISCUSSION AND CONCLUSION]
Through qualitative results that showed how SGD interacts with the DNN loss surface, we showed evidence that SGD rarely crosses barriers during training. We presented an alternate mechanism that SGD uses to explore different regions of the deep network loss landscape.
We draw similarities between the optimization trajectory in DNNs that we have empirically found, with those in quadratic loss optimization (see section 5 of LeCun et al. (1998)). Based on our empirical evidence, we found that the loss interpolation between parameters from consecutive updates is a quadratic-like shape. This is reminiscent of optimization in a quadratic loss setting with a non-isotropic positive semi-definite Hessian, where the optimal learning rate η causes underdamping without divergence along eigenvectors of the Hessian which have eigenvalues λ i such that λ −1 i < η < 2λ −1 i . In the second part of our analysis, we investigated the role of batch-size in exploration, for different regions during SGD optimization of a DNN loss surface. We presented an argument showing mini-batch gradients from larger batch-sizes should align more with the high curvature directions of the loss surface, especially early during training when the scale of mean gradients dominates over gradient covariance. Additionally, we present a complementary view of the exploration aspect of SGD that stems from its diffusion perspective, and show that the alignment of the mini-batch gradient with the sharp directions of the Hessian leads to oscillations preventing SGD from exploring regions far from the initialized parameters.
Finally, much of what we have discussed is based on the loss landscape of specific datasets and architectures along with network parameterization choices like rectified linear activation units (Re-LUs) and batch normalization Ioffe & Szegedy (2015). These conclusions may differ depending on these choices. In these cases analysis similar to ours can be performed to see if similar dynamics hold or not. Studying these dynamics may provide more practical guidelines for setting optimization hyperparameters.

[APPENDIX]
A OPTIMIZATION TRAJECTORY This is a continuation of section 3.1 in the main text. Here we show further experiments on other datasets, architectures and hyper-parameter settings. The analysis of GD training for Resnet-56 on CIFAR-10, MLP on MNIST and VGG-11 on tiny ImageNet are shown in figures 8, 16 and 19 respectively. Similarly, the analysis of SGD training for Resnet-56 on CIFAR-10 dataset with batch size of 100 and learning rate 0.1 for epochs 1, 2, 25 and 100 are shown in figures 9, 10, 11 and 12 respectively. The analysis of SGD training for VGG-11 on CIFAR-10 with the batch size of 100 and learning rate 0.1 on epochs 2, 25,100 are shown in figures 13, 14 and 15. The analysis of SGD training for MLP on MNIST for epochs 1 and 2 are shown in figures 17 and 18. The analysis of SGD training for VGG-11 on tiny ImageNet for epochs 1 is shown in figure 20. We also conducted the same experiment and analysis on various batch sizes and learning rates for every architecture. Results of VGG-11 can be found in figures 21, 22, 23 and 24. Results of Resnet-56 can be found in figures 25, 26, 27 and 28. The observations and rules we discovered and described in section 3 are all consistent for all these experiments. Specifically, for the interpolation of SGD for VGG-11 on tiny ImageNet, the valley-like trajectory is weird-looking but even so, according to our quantitative evaluation there is no barrier between any two consecutive iterations.

[B IMPORTANCE OF SGD NOISE STRUCTURE]
Here we derive in detail the relation between the Hessian and gradient covariance for the negative log likelihood loss L i (θ) = − log(p i (θ)). Note we use the fact that for this particular loss function,
∂Li(θ) ∂pi(θ) = − 1 pi(θ) , and ∂ 2 Li(θ) ∂pi(θ) 2 = 1 p 2 i (θ) , which yields ∂ 2 Li(θ) ∂pi(θ) 2 = ∂Li(θ) ∂pi(θ) 2 . H(θ) = 1 N N i=1 ∂ 2 L i (θ) ∂θ 2 (6) = 1 N N i=1 ∂ ∂θ ∂L i (θ) ∂p i (θ) • ∂p i (θ) ∂θ (7) = 1 N N i=1 ∂ 2 L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (8) = 1 N N i=1 ∂L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (9) = 1 N N i=1 ∂L i (θ) ∂θ ∂L i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (10) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (11
)
whereḡ(θ) = 1 N N i=1 ∂Li(θ)
∂θ .

[C DISCUSSION]
In the main text, we talk about converge in the quadratic setting depending on the value of learning rate relative to the largest eigenvalue of the Hessian. The convergence in this setting has been visualized in ??.              Figure 22: Plots for VGG-11 Epoch 1 trained using learning rate 0.2 batch size 100 on CIFAR-10.
Figure 23: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 500 on CIFAR-10.
Figure 24: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 1000 on CIFAR-10.
Figure 25: Plots for Resnet-56 Epoch 1 trained using learning rate 0.7 batch size 100 on CIFAR-10.
Figure 26: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 100 on CIFAR-10.
Figure 27: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 500 on CIFAR-10.
Figure 28: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 1000 on CIFAR-10.","[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration."
A Walk with SGD: How SGD Explores Regions of Deep Network Loss?,B1l6e3RcF7.json,"This paper explores the idea that mini-batch SGD rarely *crosses* barriers during DNN optimization, but rather uses a 'seemingly' or 'alternate' mechanism, as the authors somewhat mysteriously call it on the first page. In the second part of the paper,  they also investigate why the loss surface is explored more slowly when the batch size increases. 

I found both parts of the paper reasonably interesting but not too surprising. My main concern is that both parts are , in themselves, not strong enough to warrant publication at ICLR, and the connection between them is rather weak. The authors write 'to complement this finding'  to connect the first to the second investigation, but that's not connecting them very closely is it?
I think it would be better to work out both insights in more detail and publish them in separate papers. 
Especially the second insight should be explored more thoroughly. For example, the authors write 'in convex optimization theory, when gradients point along the sharp directions of the loss surface, optimization exhibits under-damped convergence'. This is repeated later in different wordings.  But no reference to this result (I presume it's a mathematical theorem?) is given, neither here nor later when it is said again. The link from the convex to the nonvex DNN case could also be established more convincingly. Everything became quite (too) heuristic at some point...

A few small remarks (which did not influence my judgement):
- while in general (with the exception of the too-fast move from convex to nonconvex that I just explained) the paper is written quite clearly, the prose could be made significantly tighter. For example, the definition of what 'crossing a barrier' means is given three times (!) in the paper (two times in a figure, once in section 2). BTW, isn't it better to say 'moving *around* barriers' rather than 'over' barriers? You now use 'over' but still sounds very similar to just 'crossing'. 
- plural nouns are often combined with singular vers ('measurements that ensures'). This happens not just once but all the time...

PROS:
- two nice little ideas; esp. the first one is well-explained
- easy to read
CONS:
- ideas are not very surprising; and just tested on a few data sets; things could be more  robust. 
- second idea not fully convincingly explained
- (most important): the two ideas are not closely connected, making this a somewhat strange paper. ","[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.

[CAPTIONS]
Table 1: Figure 1 :Figure 2 :12Figure1: Plots for VGG-11 architecture trained using SGD on CIFAR-10. Each plot contains the training loss for 40 iterations of training at various epochs. Between the training loss at every consecutive iteration (vertical gray lines), we uniformly sample 10 points between the parameters before and after the training update and calculate the loss at these points. Thus we take a slice of the loss surface between two iterations. These loss values are plotted between every consecutive training loss value from training updates. We find that the loss interpolation between consecutive iterations have a minimum in between in all cases showing barriers are not being crossed. For epochs 25 and 100 this is not clearly visible, but we quantitatively record it and discuss it later. The dashed orange line (only shown in the epoch 1 plot) connects the minimum of the loss interpolation between consecutive iterations and is shown to highlight that the valley floor has ups and downs along the path of SGD (which can be seen for all epochs).
Table 2: Figure 3 :3Figure 3: Plots for MLP architecture trained using SGD on MNIST. All the descriptions are same as described in figure 1.
Table 3: Figure 4 :4Figure 4: Numbers of barriers found during training loss interpolation for every epoch (450 iterations) for VGG-11 on CIFAR-10.We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
Table 4: Figure 5 :5Figure5: Plots for the alignments between mini-batch gradient and hessian with VGG-11 architecture trained using SGD on CIFAR-10 at the end of Epoch 5 and Epoch 10. Alignments are calculated for mini-batch size 100,1000,10000 and 45000 (dataset size).
Table 5: Figure 7 :7Figure 7: Plots of the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100 for different batch sizes.
Table 6: Figure 8 :8Figure 8: Plots for Resnet-56 Epoch 1 trained using full batch Gradient Descent (GD) on CIFAR-10.
Table 7: Figure 9 :9Figure 9: Plots for Resnet-56 Epoch 1 trained using SGD on CIFAR-10.
Table 8: Figure 10 :10Figure 10: Plots for Resnet-56 Epoch 2 trained using SGD on CIFAR-10.
Table 9: Figure 11 :11Figure 11: Plots for Resnet-56 Epoch 25 trained using SGD on CIFAR-10.
Table 10: Figure 12 :12Figure 12: Plots for Resnet-56 Epoch 100 trained using SGD on CIFAR-10.
Table 11: Figure 13 :13Figure 13: Plots for VGG-11 Epoch 2 trained using SGD on CIFAR-10.
Table 12: Figure 14 :14Figure 14: Plots for VGG-11 Epoch 25 trained using SGD on CIFAR-10.
Table 13: Figure 15 :15Figure 15: Plots for VGG-11 Epoch 100 trained using SGD on CIFAR-10.
Table 14: Figure 16 :16Figure 16: Plots for MLP Epoch 1 trained using full batch Gradient Descent (GD) on MNIST.
Table 15: Figure 17 :17Figure 17: Plots for MLP Epoch 1 trained using SGD on MNIST.
Table 16: Figure 18 :18Figure 18: Plots for MLP Epoch 2 trained using SGD on MNIST.
Table 17: Figure 19 :19Figure 19: Plots for VGG-11 Epoch 1 trained using full batch Gradient Descent (GD) on Tiny-ImageNet.
Table 18: Figure 20 :20Figure 20: Plots for VGG-11 Epoch 1 trained using SGD on Tiny-ImageNet.
Table 19: Figure 21 :21Figure 21: Plots for VGG-11 Epoch 1 trained using learning rate 0.3 batch size 100 on CIFAR-10.
Table 20: 

[INTRODUCTION]
The non-convexity of the deep neural network (DNN) loss surface makes the behavior of optimization algorithms less intuitive compared to the convex setting. Moreover, optimization in DNNs is no longer about finding any minimum, but rather about finding ones that generalizes well (Keskar et al., 2016). Since deep networks are initialized randomly, finding such minima will require exploration of different regions of the loss surface. This intuition has been formalized in recent papers that study stochastic gradient descent (SGD) as a diffusion process (Hoffer et al., 2017;Smith & Le, 2017;Jastrzebski et al., 2017;Chaudhari & Soatto, 2017). Briefly, these papers show that SGD simulates a discrete approximation of stochastic differential equation (SDE), and hence performs a random walk on the potential induced by the DNN loss surface.
In this work, we complement the diffusion perspective of SGD with a qualitative view of how SGD explores different regions of the non-convex loss landscape of deep neural networks through empirical evidence. Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process. We show in this work that SGD rarely crosses any barriers along its path during the course of training. By this observation, we do not claim that SGD does not simulate diffusion. Through experimental deductions, we show an alternate mechanism that SGD seems to dominantly use to explore different regions of the non-convex loss landscape.
Further, it is known that larger batch-sizes slow down the diffusion process (Hoffer et al., 2017). We show the qualitative reason behind this slow down to be an oscillation behavior of SGD which prevents it from moving far away from initialization. This behavior is a result of the mini-batch gradients becoming increasingly aligned with the top eigenvectors of the Hessian for larger batch-sizes. This behavior is known to slow down convergence in optimization theory (for instance consider the motivation behind momentum (Polyak, 1964;Sutskever et al., 2013)). We discuss how it also slows down explorations in the non-convex setting of deep network loss surface.
Experiments are conducted on multiple data sets, architectures and hyper-parameter settings. The findings mentioned above hold true on all of them.

[SETUP]
We now describe the details of how we study the existence of barriers along the optimization path of SGD. The main tool we use for studying the DNN loss surface along SGD's path is to interpolate the loss surface between parameters before and after each training update. We note that this strategy of interpolating the loss surface between parameters was introduced by Goodfellow et al. (2014). In their paper, the interpolation is conducted between initial and final (after training) parameter values for analysis purposes. In contrast, we compute interpolations before and after each training update because this interpolation precisely tells us whether or not SGD crosses a barrier during an update step. We say a barrier is crossed when we see a point in the parameter space interpolated between the parameters just before and after an update step, such that the loss at the barrier point is higher than the loss at both the other points.
Consider that the parameters θ of a neural network are initialized to a value θ 0 . When using an optimization method to update these parameters, the t th update step takes the parameter from θ t to θ t+1 using estimated gradient g t as,
θ t+1 = θ t − ηg t (1)
where η is the learning rate. Notice the t th update step implies the t th epoch only in the case when using the full batch gradient descent (GD). In the case of stochastic gradient descent, one iteration is an update from gradient computed from a mini-batch. We then interpolate the DNN loss between the convex combination of θ t and θ t+1 by considering parameter vectors θ α t = (1 − α)θ t + αθ t+1 , where α ∈ [0, 1] is chosen such that we obtain 10 samples uniformly placed between these two parameter points. We note that even though the updates are performed using mini-batches for SGD, the training loss values we compute for the interpolation use the full dataset to visualize the actual loss landscape.

[BARRIERS AND EXPLORATION DURING SGD TRAINING]
For this section, we perform experiments on MNIST (Lecun & Cortes) and CIFAR-10 (Krizhevsky, 2009) datasets, and use multi-layer perceptrons (MLP), VGG-11 (Simonyan & Zisserman, 2014) and Resnet-56 (He et al., 2016) architectures with various batch sizes and learning rates. We discuss our observations for VGG-11 architecture on CIFAR-10 dataset (figure 1) as a reference but the same conclusions hold for experiments on MLP trained on MNIST (figure 3) and Resnet-56 trained on CIFAR-10 (figure 2).
We train VGG-11 on CIFAR-10 with a batch size of 100 and fixed learning rate of 0.1. We report the visualization of loss interpolation between consecutive iterations for 40 iterations from epochs 1, 2, 25 and 100 for visual clarity. The interpolation is shown in figure 1. To be clear, the x-axis is calibrated by the number of iterations, and there are 10 interpolated loss values between each consecutive iteration (vertical gray lines) in the training loss plots. In these plots, we find two interesting behavior of SGD.
First, we find that the interpolated loss between every consecutive update from SGD optimization update appears to be a quadratic-like structure with a minimum in between. Note that while this is not visible for epochs 25 and 100, we later show quantitative measurements that ensures this claim. This plot thus shows that in the iterations plotted, SGD rarely crosses barriers.
Second, we observe how the minimum of each interpolation evolves as training progresses. This is highlighted in figure 1 (a) with a dashed orange line. We find that this minimum has ups and downs along the path of SGD for all our interpolation plots. To draw deductions from this observation, consider a simple example that helps us understand this scenario concretely. Let parameter points θ A , θ B and θ C be a result of three consecutive SGD updates with loss values A , B and C (using full training set). Note that since these are only three points, they exist in a two dimensional subspace and the loss value can be imagined along the third dimension. Then corresponding to the behavior in the plot, there is a parameter point θ AB between θ A and θ B on the line connecting these two points, which has a loss value AB < A , B . Similarly there is a point θ BC between θ B and θ C on the line connecting these two points, which has a loss value BC < B , C . Given this construction, for any configuration of θ A , θ B and θ C on the two dimensional plane, it is easy to see that if AB < BC , any path from θ AB to θ BC will have loss values that must increase at some point. Hence, what this construction essentially represents (as we refer to it), is a situation where SGD has moved over a barrier. Therefore, the ups and downs of the minimum between loss interpolations in figure 1 (a,b,c,d) represents SGD moving over barriers. In this way we find that when running SGD on the loss surface of deep networks, instead of crossing barriers, a more dominant way SGD performs exploration is by moving over them.  on CIFAR-10 and MLP on MNIST. We say a barrier is crossed during an update step if there exists a point interpolated between the parameters before and after an update which has a loss value higher than the loss at either points. For most parts of the training, we find that SGD does not cross any significant number of barriers.
The same qualitative analysis for SGD with different hyper-parameters are also shown in section 1 in appendix. The observations we described here remain consistent for all these experiments.
So far we showed qualitative visualizations to make the claim that SGD rarely crosses barriers. In order to show that the claim extends to the rest of the training instead of only a few iterations we showed above, we now quantitatively measure how many barriers are crossed for the entire epoch in different phase of training. This result is shown in table 1 for VGG-11 and Resnet-56 trained on CIFAR-10 (trained for 100 epochs) and an MLP trained on MNIST (trained for 40 epochs). We note that each case, an epoch consists of more than 450 iterations. As we see, a negligible number of barriers are crossed for most parts of the training compared to the number of iterations performed during each epoch. For concreteness, we further compute the number of barriers crossed for the first 40 epochs for VGG-11 on CIFAR-10 as shown in Figure 4 and reach the same conclusion. We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
L(θt)+L(θt+1)−2L(θ min t ) 2
) are substantially smaller compared with the value of loss at the corresponding iterations (not mentioned here), meaning they are not significant barriers.

[THE EFFECT OF BATCH-SIZE ON EXPLORATION]
4.1 ANALYSIS Hoffer et al. (2017) discuss that SGD training with different batch-sizes leads to different diffusion rates (very large batch sizes being slower). Further, when training for the same number of epochs, a larger batch training performs less number of iterations. Combining these two observations, they reach the conclusion that large batch training makes the diffusion process slow. As empirical evidence, they show that the distance of parameters from initialization evolves logarithmically in the number of iterations.
We now present a complementary optimization perspective to their observation. To continue, we introduce the following notations. Let p i (θ) denote the predicted probability output (of the correct class in the classification setting for instance) of a DNN parameterized by θ for the i th data sample (in total N samples). Then the negative log likelihood loss for the i th sample is given by
L i (θ) = − log(p i (θ)). The gradient g B (θ) from mini-batch SGD at a parameter value θ is ex- pressed as, g B (θ) = 1 B i∈B ∂Li(θ)
∂θ ,ḡ(θ) denotes the expected gradient using all training samples, B is the mini-batch size (and we have also overloaded it to mean the mini-batch set) and C(θ) is the gradient covariance matrix at θ. Then the relation between the Hessian H(θ) and the dataset gradient covariance C(θ) for negative log likelihood loss is described by the Gauss-Newton decomposition as follows,
H(θ) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂w 2 (2)
where H(θ) is the Hessian of the loss. The derivation can be found in section B of the appendix.
To continue with our argument, we note that it has been discussed by Shwartz-Ziv & Tishby (2017) that early on during training, the mean gradient over the training set is larger in magnitude compared to the variance in gradients. The above argument essentially says that the scale of mean gradient g(θ) is larger compared with the scale of C(θ). Ignoring the second order term in the Gauss-Newton decomposition above, we see that the mean gradient must be aligned with the top eigenvectors of the Hessian since the scale of gradient covariance is much smaller early on during training. Finally, we note that using large batch-sizes makes the mini-batch gradient closer to the mean gradient by reducing the scale of mini-batch gradient covariance as shown by Hoffer et al. (2017),
cov(g B (θ), g B (θ)) = 1 B − 1 N C(θ)(3)
The two arguments together imply that gradients from larger batch-sizes are likely to be more aligned with the high curvature directions of the loss surface especially early on during training.
In convex optimization theory, when gradients point along the top eigenvectors of the Hessian (also referred to as the sharp directions of the loss surface), optimization exhibits under-damped convergence, meaning it oscillates along the sharp directions in the case when the learning rate is smaller than a certain threshold. Applying this idea to non-convex loss landscapes, a large alignment between the mini-batch gradient and the sharp directions should also lead to oscillations. At this point, we depart from the conclusions of the convex setting and recall our observation in the previous section that the interpolation between consecutive iterations has a quadratic like shape and SGD moves over barriers for the deep network loss surface. We thus hypothesize that a lower alignment between mini-batch gradients and the sharp directions of the loss surface makes SGD exploration faster by exhibiting less oscillation, and vice-versa.

[EMPIRICAL VERIFICATION]
Based on the theoretical analysis above, we first conduct experiments to empirically verify that the alignment of mini-batch gradient g B (θ) and hessian H(θ) increases when we increase mini-batch size. To do so, we calculate the alignment of mini-batch gradient g B (θ) and hessian H(θ) as Figure 6: Plots for the alignments between mini-batch gradient and hessian with Resnet-56 architecture trained using SGD on CIFAR-100 at the end of Epoch 5 and Epoch 10. All the descriptions are same as described in figure 5.
g T B (θ)H(θ)g B (θ) g B (θ) 2 2 .(4)
Figure 5 and figure 6 show the alignments calculated according to Equation 4 on both VGG-11 with CIFAR-10 and Resnet-56 with CIFAR-100 separately at the end of Epoch 5 and Epoch 10. We calculate the alignment for mini-batch size 100, 1000, 10000 and 45000 (which is the dataset size).
For every mini-batch size, we sample 50 different batches and calculate the alignment of the current mini-batch gradient with the hessian and show both the mean and standard deviation of alignments in the plots. From both figure 5 and figure 6, we can see that the alignment between mini-batch gradient and hessian is larger for larger mini-batch size.
Based on the empirical verification between mini-batch gradient and sharp directions above, we now verify our argument whether it leads SGD to oscillate in the proximity of the parameter initialization, thus slowing down exploration. Note the latter has been shown by Hoffer et al. (2017). Therefore, to substantiate our claim, we show the degree of oscillation in SGD increases with large batchsize. Specifically, while training deep networks, we keep track of the cosine of the angle between mini-batch gradients from every two consecutive SGD iterations,
cos(g t−1 , g t ) := g T t−1 g t ( g t−1 2 g t 2 )
.
(5)
Figure 7 shows the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100. Experiments are run with the same learning rate for batch size 500, 5000 and 45000 (dataset size). We can see from the plot that the cosine of the angle between mini-batch gradients from two consecutive iterations remains smaller for larger batch sizes, which indicates that the gradients from two consecutive iterations point more in opposite directions for larger batch sizes. Together with the parameter distance results from Hoffer et al. (2017) that shows that within the same number of iterations, the parameter norm for larger batch sizes is smaller, our experiment verifies that for larger batch sizes, SGD oscillates more in the proximity of the parameter initialization instead of exploring farther away regions. 

[BACKGROUND AND RELATED WORK]
There have been previous work on visualizing the loss surface although from different motivations.
Perhaps Goodfellow et al. ( 2014) is most similar to our work since we use the loss interpolation tool suggested in their paper to perform our analysis. They perform interpolation between the initial and final parameters and based on their finding, draw the conclusion that the loss along the line connecting these two points does not have any barriers. We note that we use their tool for a different purpose and our conclusions are fundamentally different from theirs because we use the observations to investigate whether SGD crosses barriers during optimization over deep networks' loss landscape. Li et al. (2017b) also visualize the loss landscape of different network architectures.
Our work is closely related to a number of recent papers that study SGD as a diffusion process because we present a complementary qualitative view to an aspect of their theory. Hoffer et al. (2017) hypothesize this view based on the evidence that the parameter distance moved by SGD from initialization as a function of the number of iterations resembles a diffusion process. Li et al. (2017a) hypothesize this behavior of SGD and theoretically show that this diffusion process would allow SGD to escape sharp local minima. The authors use this theoretical result to support the findings of Keskar et al. (2016) who find that SGD with small mini-batch size find wider minima. Kushner & Yin (2003); Mandt et al. (2017); Chaudhari & Soatto (2017); Smith & Le (2017); Jastrzebski et al. (2017); Li et al. (2015) study SGD as a discrete approximation of stochastic differential equation under the assumption of a reasonably small learning rate and batch-size (compared with dataset size). Broadly, these papers show that the stochastic fluctuation in the stochastic differential equation simulated by SGD is governed by the ratio of learning rate to batch size. In this paper we study the qualitative roles of batch size without any assumption on how large it is with respect to dataset size.
We note that Zhu et al. (2018) present an analysis of how the structure of gradient covariance matrix can help SGD escape sharp minima more efficiently. Specifically, they show that when the top eigenvectors of the gradient covariance and Hessian are aligned, the escaping efficiency of SGD out of sharp minima is best. We find our view of exploration of different regions by SGD on the other side of the spectrum. While they compare the alignment between the gradient covariance (noise) and Hessian, we talk about the alignment between the mean gradient and the Hessian, which may be seen as complementary views.
There is a long list of work towards understanding the loss surface geometry of DNNs from a theoretical standpoint which is similar in spirit to our analysis of loss surface. Dotsenko (1995); Amit et al. (1985); Choromanska et al. (2015) show that under certain assumptions, the DNN loss landscape is similar to the spherical spin glass model which is well studied in terms of its critical points. Safran & Shamir (2016) show that under certain mild assumptions, the initialization is likely to be such that there exists a continuous monotonically decreasing path from the initial point to the global minimum. Freeman & Bruna (2016) theoretically show that for DNNs with rectified linear units (ReLU), the level sets of the loss surface become more connected as network over-parametrization increases. This has also been justified by Sagun et al. (2017) who show that the Hessian of deep ReLU networks is degenerate when the network is over-parametrized and hence the loss surface is flat along such degenerate directions. Broadly these studies analyze DNN loss surfaces (either theoretically or empirically) in isolation from the optimization dynamics.
In our work we do not study the loss surface in isolation, but rather analyze it through the lens of SGD. In other words, we study the DNN loss surface along the trajectory of SGD, based on which we make deductions about how SGD explores the different regions of the loss surface and the effect of batch-size on this aspect.

[DISCUSSION AND CONCLUSION]
Through qualitative results that showed how SGD interacts with the DNN loss surface, we showed evidence that SGD rarely crosses barriers during training. We presented an alternate mechanism that SGD uses to explore different regions of the deep network loss landscape.
We draw similarities between the optimization trajectory in DNNs that we have empirically found, with those in quadratic loss optimization (see section 5 of LeCun et al. (1998)). Based on our empirical evidence, we found that the loss interpolation between parameters from consecutive updates is a quadratic-like shape. This is reminiscent of optimization in a quadratic loss setting with a non-isotropic positive semi-definite Hessian, where the optimal learning rate η causes underdamping without divergence along eigenvectors of the Hessian which have eigenvalues λ i such that λ −1 i < η < 2λ −1 i . In the second part of our analysis, we investigated the role of batch-size in exploration, for different regions during SGD optimization of a DNN loss surface. We presented an argument showing mini-batch gradients from larger batch-sizes should align more with the high curvature directions of the loss surface, especially early during training when the scale of mean gradients dominates over gradient covariance. Additionally, we present a complementary view of the exploration aspect of SGD that stems from its diffusion perspective, and show that the alignment of the mini-batch gradient with the sharp directions of the Hessian leads to oscillations preventing SGD from exploring regions far from the initialized parameters.
Finally, much of what we have discussed is based on the loss landscape of specific datasets and architectures along with network parameterization choices like rectified linear activation units (Re-LUs) and batch normalization Ioffe & Szegedy (2015). These conclusions may differ depending on these choices. In these cases analysis similar to ours can be performed to see if similar dynamics hold or not. Studying these dynamics may provide more practical guidelines for setting optimization hyperparameters.

[APPENDIX]
A OPTIMIZATION TRAJECTORY This is a continuation of section 3.1 in the main text. Here we show further experiments on other datasets, architectures and hyper-parameter settings. The analysis of GD training for Resnet-56 on CIFAR-10, MLP on MNIST and VGG-11 on tiny ImageNet are shown in figures 8, 16 and 19 respectively. Similarly, the analysis of SGD training for Resnet-56 on CIFAR-10 dataset with batch size of 100 and learning rate 0.1 for epochs 1, 2, 25 and 100 are shown in figures 9, 10, 11 and 12 respectively. The analysis of SGD training for VGG-11 on CIFAR-10 with the batch size of 100 and learning rate 0.1 on epochs 2, 25,100 are shown in figures 13, 14 and 15. The analysis of SGD training for MLP on MNIST for epochs 1 and 2 are shown in figures 17 and 18. The analysis of SGD training for VGG-11 on tiny ImageNet for epochs 1 is shown in figure 20. We also conducted the same experiment and analysis on various batch sizes and learning rates for every architecture. Results of VGG-11 can be found in figures 21, 22, 23 and 24. Results of Resnet-56 can be found in figures 25, 26, 27 and 28. The observations and rules we discovered and described in section 3 are all consistent for all these experiments. Specifically, for the interpolation of SGD for VGG-11 on tiny ImageNet, the valley-like trajectory is weird-looking but even so, according to our quantitative evaluation there is no barrier between any two consecutive iterations.

[B IMPORTANCE OF SGD NOISE STRUCTURE]
Here we derive in detail the relation between the Hessian and gradient covariance for the negative log likelihood loss L i (θ) = − log(p i (θ)). Note we use the fact that for this particular loss function,
∂Li(θ) ∂pi(θ) = − 1 pi(θ) , and ∂ 2 Li(θ) ∂pi(θ) 2 = 1 p 2 i (θ) , which yields ∂ 2 Li(θ) ∂pi(θ) 2 = ∂Li(θ) ∂pi(θ) 2 . H(θ) = 1 N N i=1 ∂ 2 L i (θ) ∂θ 2 (6) = 1 N N i=1 ∂ ∂θ ∂L i (θ) ∂p i (θ) • ∂p i (θ) ∂θ (7) = 1 N N i=1 ∂ 2 L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (8) = 1 N N i=1 ∂L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (9) = 1 N N i=1 ∂L i (θ) ∂θ ∂L i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (10) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (11
)
whereḡ(θ) = 1 N N i=1 ∂Li(θ)
∂θ .

[C DISCUSSION]
In the main text, we talk about converge in the quadratic setting depending on the value of learning rate relative to the largest eigenvalue of the Hessian. The convergence in this setting has been visualized in ??.              Figure 22: Plots for VGG-11 Epoch 1 trained using learning rate 0.2 batch size 100 on CIFAR-10.
Figure 23: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 500 on CIFAR-10.
Figure 24: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 1000 on CIFAR-10.
Figure 25: Plots for Resnet-56 Epoch 1 trained using learning rate 0.7 batch size 100 on CIFAR-10.
Figure 26: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 100 on CIFAR-10.
Figure 27: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 500 on CIFAR-10.
Figure 28: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 1000 on CIFAR-10.","[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration."
A Walk with SGD: How SGD Explores Regions of Deep Network Loss?,B1l6e3RcF7.json,"The subject of how a given algorithm explores the landscape is still a poorly understood area in training neural networks. There is a large body of recent work that attempts to shed light on this puzzle, and each one tries to claim their share in the furthering of the understanding of the relationship between the geometry of the landscape and the dynamics that one chooses in optimization. The present paper is a fine addition to the literature with interesting observations and novel questions, however, it falls short in many core areas: An apparent work in progress that has a great potential. 

It is safe to say that ""A walk with SGD"" has an important single focus in mind: Does the SGD cross over barriers in the weight space of the underlying neural network? This question, at its heart, is intimately linked with the many properties that are attributed to the modern algorithm of the choice and the way it navigates a given non-convex landscape. The paper claims to provide an almost negative answer to the question and thereby busting several myths that are attributed to the ""trick"" part of SGD algorithm. As good as it sounds, unfortunately, the paper falls short of providing a convincing evidence (be it theoretical or empirical), and the way it tries to frame itself unique and different in relation to related works only indicate a lack of deep understanding of the existing literature. Therefore, I think there are several ways the paper should be improved before it is ready.

A major question (that I hope will easily be addressed) is on the definition of the barrier itself. According to the text, a barrier is defined judging by the minima of two 1-dimensional segments that connect weights connecting three consecutive steps: if the minimum of the line segment defined by the latter step is larger than the former, then it declared that a barrier is crossed. In a low dimensional world, this makes total sense, however, I fail to understand what kind of barrier it implies on the geometry of the landscape: Can the 1-dimensional lines be on the sides of a valley? Can one find *another* 1-dimensional projection for which the inequality is broken? How do such dependencies change the understanding of the problem? And if one is indeed only interested in the flat line segments (since SGD is making discrete steps), then one can, in principle, observe barrier crossing in a convex problem, as well? Is there an argument for otherwise? Or if it is a notion that applies equally well in a convex case then how should we really think about the barrier crossing? On the opposite point of view, can one not imagine a barrier crossing that doesn't appear in this triangular inequality above?

The paper is full of empirical evidence that is guided by a simple observable that is very intuitive, however, it lacks a comprehensive discussion on the new quantity they propose that I consider a major flaw, but that I think (hope) that the authors can fix very easily. Some minor points that would improve the readability and clarity for the reader:
- The figures are not very reader-friendly, this can be improved by better using the whitespaces in the paper but it can also be improved by finding further observables that would summarize the observations instead of showing individual consecutive line interpolations.
- What are the values of the y-axis in Figure 5 and 6? Are they the top eigenvalues of the Hessian?
- In the models that are compared in Figure 7, what are their generalization properties (early stopping and otherwise)?
- The interpretation at the end of p. 6 may be a good motivation for the reader if it had been introduced earlier for that section.

Finally, Section 5 reads very strangely, I have hard times understanding why certain phrases exist the way they are in this part. Here are further notes on Section 5:
- Why the whole first paragraph focused on hot the current paper is different than Goodfellow et al (2014) (it is obvious that it is for a different purpose), or why do we read the sentence ""Li et al (2017b) also visualize...."" What way do they visualize, is it the only paper that does visualization, what's the relation with the current paper and barrier crossing? 
- For the second paragraph, I can suggest another paper, https://arxiv.org/abs/1803.06969, that was at ICML which also looks at the diffusion process through the parameter distance at different times which is similar to Hoffer et al. which also claims no barrier crossing similar to the present paper. 
- However, my main issue is the exact connection between diffusion and no barrier crossing and it's connection to SGD preferring wide local minima instead of a narrow one. The second paragraph of the conclusion touches upon this subject. But it is not entirely clear how they are linked (except for the brittle SDE approximation at Li et al (see https://arxiv.org/abs/1810.00004)). Overall, the paper would benefit a lot from the discussion on why it is preferable to have SGD choose one basin over another in the beginning, as it is, it looks like the paper has another agenda behind the scenes.
- In the fourth paragraph of the conclusion, the paper refers to three papers that link DNN to spin glasses, in two of the (older) references the networks are far from what we have today, and the third one is far from ""showing"" anything between DNN and spin glass. In any case, what's the link between the aspects studied there with the present paper?
- Finally, the paper claims at the last few sentences that the works referred a little bit earlier look at the loss surface ""in isolation from the optimization dynamics"", however, many of those works cited have their empirical observations much like the current paper, and clearly they all ""study the DNN loss surface along the trajectory of SGD"" necessarily as it is the way to find local minima, saddle points, paths, curvature etc... The present paper is already very interesting and full of novel insight, I fail to see the value of struggling to stand out like this.

Overall, I think the paper is a very interesting step forward in understanding SGD dynamics on the DNN landscape. And, even though it has many shortcomings as it currently stands, I think it has a lot of room to improve.

","[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration.

[CAPTIONS]
Table 1: Figure 1 :Figure 2 :12Figure1: Plots for VGG-11 architecture trained using SGD on CIFAR-10. Each plot contains the training loss for 40 iterations of training at various epochs. Between the training loss at every consecutive iteration (vertical gray lines), we uniformly sample 10 points between the parameters before and after the training update and calculate the loss at these points. Thus we take a slice of the loss surface between two iterations. These loss values are plotted between every consecutive training loss value from training updates. We find that the loss interpolation between consecutive iterations have a minimum in between in all cases showing barriers are not being crossed. For epochs 25 and 100 this is not clearly visible, but we quantitatively record it and discuss it later. The dashed orange line (only shown in the epoch 1 plot) connects the minimum of the loss interpolation between consecutive iterations and is shown to highlight that the valley floor has ups and downs along the path of SGD (which can be seen for all epochs).
Table 2: Figure 3 :3Figure 3: Plots for MLP architecture trained using SGD on MNIST. All the descriptions are same as described in figure 1.
Table 3: Figure 4 :4Figure 4: Numbers of barriers found during training loss interpolation for every epoch (450 iterations) for VGG-11 on CIFAR-10.We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
Table 4: Figure 5 :5Figure5: Plots for the alignments between mini-batch gradient and hessian with VGG-11 architecture trained using SGD on CIFAR-10 at the end of Epoch 5 and Epoch 10. Alignments are calculated for mini-batch size 100,1000,10000 and 45000 (dataset size).
Table 5: Figure 7 :7Figure 7: Plots of the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100 for different batch sizes.
Table 6: Figure 8 :8Figure 8: Plots for Resnet-56 Epoch 1 trained using full batch Gradient Descent (GD) on CIFAR-10.
Table 7: Figure 9 :9Figure 9: Plots for Resnet-56 Epoch 1 trained using SGD on CIFAR-10.
Table 8: Figure 10 :10Figure 10: Plots for Resnet-56 Epoch 2 trained using SGD on CIFAR-10.
Table 9: Figure 11 :11Figure 11: Plots for Resnet-56 Epoch 25 trained using SGD on CIFAR-10.
Table 10: Figure 12 :12Figure 12: Plots for Resnet-56 Epoch 100 trained using SGD on CIFAR-10.
Table 11: Figure 13 :13Figure 13: Plots for VGG-11 Epoch 2 trained using SGD on CIFAR-10.
Table 12: Figure 14 :14Figure 14: Plots for VGG-11 Epoch 25 trained using SGD on CIFAR-10.
Table 13: Figure 15 :15Figure 15: Plots for VGG-11 Epoch 100 trained using SGD on CIFAR-10.
Table 14: Figure 16 :16Figure 16: Plots for MLP Epoch 1 trained using full batch Gradient Descent (GD) on MNIST.
Table 15: Figure 17 :17Figure 17: Plots for MLP Epoch 1 trained using SGD on MNIST.
Table 16: Figure 18 :18Figure 18: Plots for MLP Epoch 2 trained using SGD on MNIST.
Table 17: Figure 19 :19Figure 19: Plots for VGG-11 Epoch 1 trained using full batch Gradient Descent (GD) on Tiny-ImageNet.
Table 18: Figure 20 :20Figure 20: Plots for VGG-11 Epoch 1 trained using SGD on Tiny-ImageNet.
Table 19: Figure 21 :21Figure 21: Plots for VGG-11 Epoch 1 trained using learning rate 0.3 batch size 100 on CIFAR-10.
Table 20: 

[INTRODUCTION]
The non-convexity of the deep neural network (DNN) loss surface makes the behavior of optimization algorithms less intuitive compared to the convex setting. Moreover, optimization in DNNs is no longer about finding any minimum, but rather about finding ones that generalizes well (Keskar et al., 2016). Since deep networks are initialized randomly, finding such minima will require exploration of different regions of the loss surface. This intuition has been formalized in recent papers that study stochastic gradient descent (SGD) as a diffusion process (Hoffer et al., 2017;Smith & Le, 2017;Jastrzebski et al., 2017;Chaudhari & Soatto, 2017). Briefly, these papers show that SGD simulates a discrete approximation of stochastic differential equation (SDE), and hence performs a random walk on the potential induced by the DNN loss surface.
In this work, we complement the diffusion perspective of SGD with a qualitative view of how SGD explores different regions of the non-convex loss landscape of deep neural networks through empirical evidence. Intuitively, when performing random walk on a potential, one would expect barriers being crossed quite often during the process. We show in this work that SGD rarely crosses any barriers along its path during the course of training. By this observation, we do not claim that SGD does not simulate diffusion. Through experimental deductions, we show an alternate mechanism that SGD seems to dominantly use to explore different regions of the non-convex loss landscape.
Further, it is known that larger batch-sizes slow down the diffusion process (Hoffer et al., 2017). We show the qualitative reason behind this slow down to be an oscillation behavior of SGD which prevents it from moving far away from initialization. This behavior is a result of the mini-batch gradients becoming increasingly aligned with the top eigenvectors of the Hessian for larger batch-sizes. This behavior is known to slow down convergence in optimization theory (for instance consider the motivation behind momentum (Polyak, 1964;Sutskever et al., 2013)). We discuss how it also slows down explorations in the non-convex setting of deep network loss surface.
Experiments are conducted on multiple data sets, architectures and hyper-parameter settings. The findings mentioned above hold true on all of them.

[SETUP]
We now describe the details of how we study the existence of barriers along the optimization path of SGD. The main tool we use for studying the DNN loss surface along SGD's path is to interpolate the loss surface between parameters before and after each training update. We note that this strategy of interpolating the loss surface between parameters was introduced by Goodfellow et al. (2014). In their paper, the interpolation is conducted between initial and final (after training) parameter values for analysis purposes. In contrast, we compute interpolations before and after each training update because this interpolation precisely tells us whether or not SGD crosses a barrier during an update step. We say a barrier is crossed when we see a point in the parameter space interpolated between the parameters just before and after an update step, such that the loss at the barrier point is higher than the loss at both the other points.
Consider that the parameters θ of a neural network are initialized to a value θ 0 . When using an optimization method to update these parameters, the t th update step takes the parameter from θ t to θ t+1 using estimated gradient g t as,
θ t+1 = θ t − ηg t (1)
where η is the learning rate. Notice the t th update step implies the t th epoch only in the case when using the full batch gradient descent (GD). In the case of stochastic gradient descent, one iteration is an update from gradient computed from a mini-batch. We then interpolate the DNN loss between the convex combination of θ t and θ t+1 by considering parameter vectors θ α t = (1 − α)θ t + αθ t+1 , where α ∈ [0, 1] is chosen such that we obtain 10 samples uniformly placed between these two parameter points. We note that even though the updates are performed using mini-batches for SGD, the training loss values we compute for the interpolation use the full dataset to visualize the actual loss landscape.

[BARRIERS AND EXPLORATION DURING SGD TRAINING]
For this section, we perform experiments on MNIST (Lecun & Cortes) and CIFAR-10 (Krizhevsky, 2009) datasets, and use multi-layer perceptrons (MLP), VGG-11 (Simonyan & Zisserman, 2014) and Resnet-56 (He et al., 2016) architectures with various batch sizes and learning rates. We discuss our observations for VGG-11 architecture on CIFAR-10 dataset (figure 1) as a reference but the same conclusions hold for experiments on MLP trained on MNIST (figure 3) and Resnet-56 trained on CIFAR-10 (figure 2).
We train VGG-11 on CIFAR-10 with a batch size of 100 and fixed learning rate of 0.1. We report the visualization of loss interpolation between consecutive iterations for 40 iterations from epochs 1, 2, 25 and 100 for visual clarity. The interpolation is shown in figure 1. To be clear, the x-axis is calibrated by the number of iterations, and there are 10 interpolated loss values between each consecutive iteration (vertical gray lines) in the training loss plots. In these plots, we find two interesting behavior of SGD.
First, we find that the interpolated loss between every consecutive update from SGD optimization update appears to be a quadratic-like structure with a minimum in between. Note that while this is not visible for epochs 25 and 100, we later show quantitative measurements that ensures this claim. This plot thus shows that in the iterations plotted, SGD rarely crosses barriers.
Second, we observe how the minimum of each interpolation evolves as training progresses. This is highlighted in figure 1 (a) with a dashed orange line. We find that this minimum has ups and downs along the path of SGD for all our interpolation plots. To draw deductions from this observation, consider a simple example that helps us understand this scenario concretely. Let parameter points θ A , θ B and θ C be a result of three consecutive SGD updates with loss values A , B and C (using full training set). Note that since these are only three points, they exist in a two dimensional subspace and the loss value can be imagined along the third dimension. Then corresponding to the behavior in the plot, there is a parameter point θ AB between θ A and θ B on the line connecting these two points, which has a loss value AB < A , B . Similarly there is a point θ BC between θ B and θ C on the line connecting these two points, which has a loss value BC < B , C . Given this construction, for any configuration of θ A , θ B and θ C on the two dimensional plane, it is easy to see that if AB < BC , any path from θ AB to θ BC will have loss values that must increase at some point. Hence, what this construction essentially represents (as we refer to it), is a situation where SGD has moved over a barrier. Therefore, the ups and downs of the minimum between loss interpolations in figure 1 (a,b,c,d) represents SGD moving over barriers. In this way we find that when running SGD on the loss surface of deep networks, instead of crossing barriers, a more dominant way SGD performs exploration is by moving over them.  on CIFAR-10 and MLP on MNIST. We say a barrier is crossed during an update step if there exists a point interpolated between the parameters before and after an update which has a loss value higher than the loss at either points. For most parts of the training, we find that SGD does not cross any significant number of barriers.
The same qualitative analysis for SGD with different hyper-parameters are also shown in section 1 in appendix. The observations we described here remain consistent for all these experiments.
So far we showed qualitative visualizations to make the claim that SGD rarely crosses barriers. In order to show that the claim extends to the rest of the training instead of only a few iterations we showed above, we now quantitatively measure how many barriers are crossed for the entire epoch in different phase of training. This result is shown in table 1 for VGG-11 and Resnet-56 trained on CIFAR-10 (trained for 100 epochs) and an MLP trained on MNIST (trained for 40 epochs). We note that each case, an epoch consists of more than 450 iterations. As we see, a negligible number of barriers are crossed for most parts of the training compared to the number of iterations performed during each epoch. For concreteness, we further compute the number of barriers crossed for the first 40 epochs for VGG-11 on CIFAR-10 as shown in Figure 4 and reach the same conclusion. We say a barrier exists during a training update step if there exists a point between the parameters before and after an update which has a loss value higher than the loss at either points. Note that even for these barriers, their heights (defined by
L(θt)+L(θt+1)−2L(θ min t ) 2
) are substantially smaller compared with the value of loss at the corresponding iterations (not mentioned here), meaning they are not significant barriers.

[THE EFFECT OF BATCH-SIZE ON EXPLORATION]
4.1 ANALYSIS Hoffer et al. (2017) discuss that SGD training with different batch-sizes leads to different diffusion rates (very large batch sizes being slower). Further, when training for the same number of epochs, a larger batch training performs less number of iterations. Combining these two observations, they reach the conclusion that large batch training makes the diffusion process slow. As empirical evidence, they show that the distance of parameters from initialization evolves logarithmically in the number of iterations.
We now present a complementary optimization perspective to their observation. To continue, we introduce the following notations. Let p i (θ) denote the predicted probability output (of the correct class in the classification setting for instance) of a DNN parameterized by θ for the i th data sample (in total N samples). Then the negative log likelihood loss for the i th sample is given by
L i (θ) = − log(p i (θ)). The gradient g B (θ) from mini-batch SGD at a parameter value θ is ex- pressed as, g B (θ) = 1 B i∈B ∂Li(θ)
∂θ ,ḡ(θ) denotes the expected gradient using all training samples, B is the mini-batch size (and we have also overloaded it to mean the mini-batch set) and C(θ) is the gradient covariance matrix at θ. Then the relation between the Hessian H(θ) and the dataset gradient covariance C(θ) for negative log likelihood loss is described by the Gauss-Newton decomposition as follows,
H(θ) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂w 2 (2)
where H(θ) is the Hessian of the loss. The derivation can be found in section B of the appendix.
To continue with our argument, we note that it has been discussed by Shwartz-Ziv & Tishby (2017) that early on during training, the mean gradient over the training set is larger in magnitude compared to the variance in gradients. The above argument essentially says that the scale of mean gradient g(θ) is larger compared with the scale of C(θ). Ignoring the second order term in the Gauss-Newton decomposition above, we see that the mean gradient must be aligned with the top eigenvectors of the Hessian since the scale of gradient covariance is much smaller early on during training. Finally, we note that using large batch-sizes makes the mini-batch gradient closer to the mean gradient by reducing the scale of mini-batch gradient covariance as shown by Hoffer et al. (2017),
cov(g B (θ), g B (θ)) = 1 B − 1 N C(θ)(3)
The two arguments together imply that gradients from larger batch-sizes are likely to be more aligned with the high curvature directions of the loss surface especially early on during training.
In convex optimization theory, when gradients point along the top eigenvectors of the Hessian (also referred to as the sharp directions of the loss surface), optimization exhibits under-damped convergence, meaning it oscillates along the sharp directions in the case when the learning rate is smaller than a certain threshold. Applying this idea to non-convex loss landscapes, a large alignment between the mini-batch gradient and the sharp directions should also lead to oscillations. At this point, we depart from the conclusions of the convex setting and recall our observation in the previous section that the interpolation between consecutive iterations has a quadratic like shape and SGD moves over barriers for the deep network loss surface. We thus hypothesize that a lower alignment between mini-batch gradients and the sharp directions of the loss surface makes SGD exploration faster by exhibiting less oscillation, and vice-versa.

[EMPIRICAL VERIFICATION]
Based on the theoretical analysis above, we first conduct experiments to empirically verify that the alignment of mini-batch gradient g B (θ) and hessian H(θ) increases when we increase mini-batch size. To do so, we calculate the alignment of mini-batch gradient g B (θ) and hessian H(θ) as Figure 6: Plots for the alignments between mini-batch gradient and hessian with Resnet-56 architecture trained using SGD on CIFAR-100 at the end of Epoch 5 and Epoch 10. All the descriptions are same as described in figure 5.
g T B (θ)H(θ)g B (θ) g B (θ) 2 2 .(4)
Figure 5 and figure 6 show the alignments calculated according to Equation 4 on both VGG-11 with CIFAR-10 and Resnet-56 with CIFAR-100 separately at the end of Epoch 5 and Epoch 10. We calculate the alignment for mini-batch size 100, 1000, 10000 and 45000 (which is the dataset size).
For every mini-batch size, we sample 50 different batches and calculate the alignment of the current mini-batch gradient with the hessian and show both the mean and standard deviation of alignments in the plots. From both figure 5 and figure 6, we can see that the alignment between mini-batch gradient and hessian is larger for larger mini-batch size.
Based on the empirical verification between mini-batch gradient and sharp directions above, we now verify our argument whether it leads SGD to oscillate in the proximity of the parameter initialization, thus slowing down exploration. Note the latter has been shown by Hoffer et al. (2017). Therefore, to substantiate our claim, we show the degree of oscillation in SGD increases with large batchsize. Specifically, while training deep networks, we keep track of the cosine of the angle between mini-batch gradients from every two consecutive SGD iterations,
cos(g t−1 , g t ) := g T t−1 g t ( g t−1 2 g t 2 )
.
(5)
Figure 7 shows the consine calculated according to Equation 5 for Resnet-56 on CIFAR-10 and WResnet on CIFAR-100. Experiments are run with the same learning rate for batch size 500, 5000 and 45000 (dataset size). We can see from the plot that the cosine of the angle between mini-batch gradients from two consecutive iterations remains smaller for larger batch sizes, which indicates that the gradients from two consecutive iterations point more in opposite directions for larger batch sizes. Together with the parameter distance results from Hoffer et al. (2017) that shows that within the same number of iterations, the parameter norm for larger batch sizes is smaller, our experiment verifies that for larger batch sizes, SGD oscillates more in the proximity of the parameter initialization instead of exploring farther away regions. 

[BACKGROUND AND RELATED WORK]
There have been previous work on visualizing the loss surface although from different motivations.
Perhaps Goodfellow et al. ( 2014) is most similar to our work since we use the loss interpolation tool suggested in their paper to perform our analysis. They perform interpolation between the initial and final parameters and based on their finding, draw the conclusion that the loss along the line connecting these two points does not have any barriers. We note that we use their tool for a different purpose and our conclusions are fundamentally different from theirs because we use the observations to investigate whether SGD crosses barriers during optimization over deep networks' loss landscape. Li et al. (2017b) also visualize the loss landscape of different network architectures.
Our work is closely related to a number of recent papers that study SGD as a diffusion process because we present a complementary qualitative view to an aspect of their theory. Hoffer et al. (2017) hypothesize this view based on the evidence that the parameter distance moved by SGD from initialization as a function of the number of iterations resembles a diffusion process. Li et al. (2017a) hypothesize this behavior of SGD and theoretically show that this diffusion process would allow SGD to escape sharp local minima. The authors use this theoretical result to support the findings of Keskar et al. (2016) who find that SGD with small mini-batch size find wider minima. Kushner & Yin (2003); Mandt et al. (2017); Chaudhari & Soatto (2017); Smith & Le (2017); Jastrzebski et al. (2017); Li et al. (2015) study SGD as a discrete approximation of stochastic differential equation under the assumption of a reasonably small learning rate and batch-size (compared with dataset size). Broadly, these papers show that the stochastic fluctuation in the stochastic differential equation simulated by SGD is governed by the ratio of learning rate to batch size. In this paper we study the qualitative roles of batch size without any assumption on how large it is with respect to dataset size.
We note that Zhu et al. (2018) present an analysis of how the structure of gradient covariance matrix can help SGD escape sharp minima more efficiently. Specifically, they show that when the top eigenvectors of the gradient covariance and Hessian are aligned, the escaping efficiency of SGD out of sharp minima is best. We find our view of exploration of different regions by SGD on the other side of the spectrum. While they compare the alignment between the gradient covariance (noise) and Hessian, we talk about the alignment between the mean gradient and the Hessian, which may be seen as complementary views.
There is a long list of work towards understanding the loss surface geometry of DNNs from a theoretical standpoint which is similar in spirit to our analysis of loss surface. Dotsenko (1995); Amit et al. (1985); Choromanska et al. (2015) show that under certain assumptions, the DNN loss landscape is similar to the spherical spin glass model which is well studied in terms of its critical points. Safran & Shamir (2016) show that under certain mild assumptions, the initialization is likely to be such that there exists a continuous monotonically decreasing path from the initial point to the global minimum. Freeman & Bruna (2016) theoretically show that for DNNs with rectified linear units (ReLU), the level sets of the loss surface become more connected as network over-parametrization increases. This has also been justified by Sagun et al. (2017) who show that the Hessian of deep ReLU networks is degenerate when the network is over-parametrized and hence the loss surface is flat along such degenerate directions. Broadly these studies analyze DNN loss surfaces (either theoretically or empirically) in isolation from the optimization dynamics.
In our work we do not study the loss surface in isolation, but rather analyze it through the lens of SGD. In other words, we study the DNN loss surface along the trajectory of SGD, based on which we make deductions about how SGD explores the different regions of the loss surface and the effect of batch-size on this aspect.

[DISCUSSION AND CONCLUSION]
Through qualitative results that showed how SGD interacts with the DNN loss surface, we showed evidence that SGD rarely crosses barriers during training. We presented an alternate mechanism that SGD uses to explore different regions of the deep network loss landscape.
We draw similarities between the optimization trajectory in DNNs that we have empirically found, with those in quadratic loss optimization (see section 5 of LeCun et al. (1998)). Based on our empirical evidence, we found that the loss interpolation between parameters from consecutive updates is a quadratic-like shape. This is reminiscent of optimization in a quadratic loss setting with a non-isotropic positive semi-definite Hessian, where the optimal learning rate η causes underdamping without divergence along eigenvectors of the Hessian which have eigenvalues λ i such that λ −1 i < η < 2λ −1 i . In the second part of our analysis, we investigated the role of batch-size in exploration, for different regions during SGD optimization of a DNN loss surface. We presented an argument showing mini-batch gradients from larger batch-sizes should align more with the high curvature directions of the loss surface, especially early during training when the scale of mean gradients dominates over gradient covariance. Additionally, we present a complementary view of the exploration aspect of SGD that stems from its diffusion perspective, and show that the alignment of the mini-batch gradient with the sharp directions of the Hessian leads to oscillations preventing SGD from exploring regions far from the initialized parameters.
Finally, much of what we have discussed is based on the loss landscape of specific datasets and architectures along with network parameterization choices like rectified linear activation units (Re-LUs) and batch normalization Ioffe & Szegedy (2015). These conclusions may differ depending on these choices. In these cases analysis similar to ours can be performed to see if similar dynamics hold or not. Studying these dynamics may provide more practical guidelines for setting optimization hyperparameters.

[APPENDIX]
A OPTIMIZATION TRAJECTORY This is a continuation of section 3.1 in the main text. Here we show further experiments on other datasets, architectures and hyper-parameter settings. The analysis of GD training for Resnet-56 on CIFAR-10, MLP on MNIST and VGG-11 on tiny ImageNet are shown in figures 8, 16 and 19 respectively. Similarly, the analysis of SGD training for Resnet-56 on CIFAR-10 dataset with batch size of 100 and learning rate 0.1 for epochs 1, 2, 25 and 100 are shown in figures 9, 10, 11 and 12 respectively. The analysis of SGD training for VGG-11 on CIFAR-10 with the batch size of 100 and learning rate 0.1 on epochs 2, 25,100 are shown in figures 13, 14 and 15. The analysis of SGD training for MLP on MNIST for epochs 1 and 2 are shown in figures 17 and 18. The analysis of SGD training for VGG-11 on tiny ImageNet for epochs 1 is shown in figure 20. We also conducted the same experiment and analysis on various batch sizes and learning rates for every architecture. Results of VGG-11 can be found in figures 21, 22, 23 and 24. Results of Resnet-56 can be found in figures 25, 26, 27 and 28. The observations and rules we discovered and described in section 3 are all consistent for all these experiments. Specifically, for the interpolation of SGD for VGG-11 on tiny ImageNet, the valley-like trajectory is weird-looking but even so, according to our quantitative evaluation there is no barrier between any two consecutive iterations.

[B IMPORTANCE OF SGD NOISE STRUCTURE]
Here we derive in detail the relation between the Hessian and gradient covariance for the negative log likelihood loss L i (θ) = − log(p i (θ)). Note we use the fact that for this particular loss function,
∂Li(θ) ∂pi(θ) = − 1 pi(θ) , and ∂ 2 Li(θ) ∂pi(θ) 2 = 1 p 2 i (θ) , which yields ∂ 2 Li(θ) ∂pi(θ) 2 = ∂Li(θ) ∂pi(θ) 2 . H(θ) = 1 N N i=1 ∂ 2 L i (θ) ∂θ 2 (6) = 1 N N i=1 ∂ ∂θ ∂L i (θ) ∂p i (θ) • ∂p i (θ) ∂θ (7) = 1 N N i=1 ∂ 2 L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (8) = 1 N N i=1 ∂L i (θ) ∂p i (θ) 2 • ∂p i (θ) ∂θ ∂p i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (9) = 1 N N i=1 ∂L i (θ) ∂θ ∂L i (θ) ∂θ T + ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (10) = C(θ) +ḡ(θ)ḡ(θ) T + 1 N N i=1 ∂L i (θ) ∂p i (θ) • ∂ 2 p i (θ) ∂θ 2 (11
)
whereḡ(θ) = 1 N N i=1 ∂Li(θ)
∂θ .

[C DISCUSSION]
In the main text, we talk about converge in the quadratic setting depending on the value of learning rate relative to the largest eigenvalue of the Hessian. The convergence in this setting has been visualized in ??.              Figure 22: Plots for VGG-11 Epoch 1 trained using learning rate 0.2 batch size 100 on CIFAR-10.
Figure 23: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 500 on CIFAR-10.
Figure 24: Plots for VGG-11 Epoch 1 trained using learning rate 0.1 batch size 1000 on CIFAR-10.
Figure 25: Plots for Resnet-56 Epoch 1 trained using learning rate 0.7 batch size 100 on CIFAR-10.
Figure 26: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 100 on CIFAR-10.
Figure 27: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 500 on CIFAR-10.
Figure 28: Plots for Resnet-56 Epoch 1 trained using learning rate 1 batch size 1000 on CIFAR-10.","[TITLE]
A WALK WITH SGD: HOW SGD EXPLORES REGIONS OF DEEP NETWORK LOSS?

[ABSTRACT]
The non-convex nature of the loss landscape of deep neural networks (DNN) lends them the intuition that over the course of training, stochastic optimization algorithms explore different regions of the loss surface by entering and escaping many local minima due to the noise induced by mini-batches. But is this really the case? This question couples the geometry of the DNN loss landscape with how stochastic optimization algorithms like SGD interact with it during training. Answering this question may help us qualitatively understand the dynamics of deep neural network optimization. We show evidence through qualitative and quantitative experiments that mini-batch SGD rarely crosses barriers during DNN optimization. As we show, the mini-batch induced noise helps SGD explore different regions of the loss surface using a seemingly different mechanism. To complement this finding, we also investigate the qualitative reason behind the slowing down of this exploration when using larger batch-sizes. We show this happens because gradients from larger batch-sizes align more with the top eigenvectors of the Hessian, which makes SGD oscillate in the proximity of the parameter initialization, thus preventing exploration."
Neural representation and generation for RNA secondary structures,snOgiCYZgJ7.json,"Summary: This paper proposes 3 deep generative models based on VAEs (with different encoding schemes for RNA secondary structure) for the generation of RNA secondary structures. They test each model on 3 benchmark tasks: unsupervised generation, semi-supervised learning and targeted generation.  This paper has many interesting contributions — a comparison of VAE models that use different RNA secondary structure encoding schemes, including traditional dot-bracket notation and a more complex hierarchical encoding, and they also introduce various decoding schemes to encourage valid secondary structures. 

This is an important problem for drug discovery and basic biology, but the evaluation is very tricky because there are not that many solved RNA structures (unlike proteins). The tasks proposed in this paper are not comprehensive enough to entice a (comp) biologist to be convinced one way or another, but it does provide an introduction to the problem for the ML field. 

Comments:
* While the trends in their results are clear, it’s difficult to know which values of Diversity, FE DEV, and Validity are good enough to say that they have made significant progress relative to the field. The main issue is that all model comparisons are only with their own VAEs. It would be nice to show how their VAEs stack up against other established methods. For unsupervised generation, perhaps Infernal (Nawrocki and Eddy. Bioinformatics, 2017), which uses a stochastic context-free grammar built up from sequences belonging to a single RNA family, but could still form the basis for a comparison. Other models include CONTRAfold (Do et al. Bioinformatics, 2006) and many other thermodynamic models (Lorenz et al. Algorithms for Mol Biol, 2011). These other approaches may lack the scalability of a VAE, but an effort should be made for at least a limited comparison.
* It’s difficult to assess the quality of the generated secondary structures shown in various plots throughout the paper and appendices. This paper should plot a side by side comparison with the ground state structure that they are comparing the FE DEV scores against to see how well the structure corresponds. For instance, the generated (but invalid) structure may visually look similar as they walk through latent space, but the key mismatches can fundamentally change the “ground truth"" structure of the RNA (which is not shown in their generated structure). The ground truth structure is given by RNAfold, which itself is not perfect.  
* Evaluation is very tricky and it’s unclear how to navigate this.  Careful wording can help differentiate true objectives (functional RNA) from difficult evaluations (FE DEV scores). If minimizing FE DEV is the objective, then I worry that models will simply become function approximators for RNAfold. 
* A good comparison (though limited) would be to compare the predicted structures with solved structures deposited in the PDB. Another comparison could be to sample about the posterior near well known non-coding RNAs, such as tRNA or 5S ribosomal RNA. Many more consensus RNA structures can be found in RFAM (Kalvari et al. NAR, 2018).  
* On a side note, the comparison on RNAcompete-S in Table S3 shows that a sequence model is comparable to models that incorporate RNA secondary structures — there is only a small gain by including secondary structures. This is not that surprising as sequence-based models have demonstrated learning secondary structures when trained only on sequences (for the RNAcompete dataset — not RNAcompete-S yet).
* One lingering question I had is, is the latent space of the VAE meaningful? For instance, does it capture well-established non-coding RNA families? 

","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Hierarchical encoding. Panel (A)shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
Table 2: (A), to determine if the decoder should expand to a new tree node or backtrack to its parent node, based on MLP topo (hĜ t,i ); • tree node prediction in Figure 2 (B), on condition that a new tree node is needed due to a possible topological expansion. This procedure determines the label of the new tree node from the set of {S, H, I, M }, based on MLP node (hĜ t,i ); • nucleotide segment decoding in Figure 2 (C), using a single-layered LSTM, whose initial hidden state is MLP dec
Table 3: Figure 3 :3Figure 3: RNAs generated with structural constraints from HierVAE on a random axis in the latent space (step size: 1e-4), for short (top), medium-length (middle), and long (bottom) RNAs. The Free energy (FE) and its deviation (DEV) from the MFE are given for each structure. which means only the canonical base-pairs and Wobble base-pairs are allowed, i.e. [A-U], [G-C] and [G-U];(2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3; (3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
Table 4: Figure S1 :S1FigureS1: A nested RNA secondary structure can be represented by: (A) dotbracket annotation, where base-pairs corresponding to matching parentheses, or (B) a molecular planar graph with two types of edges, corresponding to consecutive nucleotides (backbone) and basepairing interactions, or (C) a junction tree where node are labeled as stems (S), hairpins (H), internal loops (I), or multiloops (M), and edges correspond to the connections between these elements. All three forms are equivalent.
Table 5: Figure S2: This figure contains information of the unlabeled RNA dataset. (A) The number of hypernodes appears to grow linearly with the length of RNA, and (B) the junction tree height also grows as the length increases but on a more moderate scale. (C) and (D) have shown bar-plots of the number of hypernodes and tree height, indicating that the junction tree of RNA can take on significant depth hence contributing to the diversity and complexity of RNA secondary structures represented in this dataset.
Table 6: Figure S4 :S4Figure S4: Neighborhood visualization of tRNA-Cys 6 which is marked by the red bounding box in the center and the walk in the latent space takes place on two random orthogonal axes. Note that actual secondary structure of tRNA-Cys plotted in the figure is different compared to the one deposited online due to the prediction of RNAfold.
Table 7: We evaluate RNAs sampled from the posterior distribution: q(z|x), with a held-out test set of 20,000 RNAs. Each molecule is encoded and decoded 5 times. We also evaluate samples from the prior distribution: N (0, I) subject to the transformation of a latent CNF, where we sample 10,000 encodings and each encoding is decoded 10 times. Normed refers to length normalized FE DEV.
Table 8: Training semi-supervised HierVAE on labeled RNAcompete-S dataset. A test split is used to evaluate the accuracy of embedding classifiers and RNAs decoded from the posterior distribution under two settings: constrained and stochastic (C& S), unconstrained and deterministic (NC&D). RECON ACC refers to reconstruction accuracy which measures the percentage of RNA molecules decoded exactly as the input.
Table 9: 
Table 10: Algorithm 1: DFS decode RNA secondary structure1 Given: z T , z G , M TI, M SI a 2 Initialize: stack ← [ ] 3 function decode(z T , z G ) 4 root ← sample(MLP node (z T )) ;
Table 11: Hyperparameters for training VAE and full classifier models. Note that hidden units refer to the dimensionality of encoders and decoders from LSTMVAE, GraphVAE as well as HierVAE models. Dropout is applied to the embedding MLP classifier in case of training semi-supervised VAEs, which contains one hidden layer. Performance of simple MLP classifiers on top of fixed latent embeddings from VAE models, which have been pretrained on the unlabeled RNA dataset as originally shown in Table1.

[INTRODUCTION]
There is an increasing interest in developing deep generative models for biochemical data, especially in the context of generating drug-like molecules. Learning generative models of biochemical molecules can facilitate the development and discovery of novel treatments for various diseases, reducing the lead time for discovering promising new therapies and potentially translating in reduced costs for drug development (Stokes et al., 2020). Indeed, the study of generative models for molecules has become a rich and active subfield within machine learning, with standard benchmarks (Sterling & Irwin, 2015), a set of well-known baseline approaches (Gómez-Bombarelli et al., 2018;Kusner et al., 2017;Jin et al., 2018), and high-profile cases of real-world impact 1 .
Prior work in this space has focused primarily on the generation of small molecules (with less than 100 atoms), leaving the development of generative models for larger and more complicated biologics and biosimilar drugs (e.g., RNA and protein peptides) an open area for research. Developing generative models for larger biochemicals is critical in order to expand the frontiers of automated treatment design. More generally, developing effective representation learning for such complex biochemicals will allow machine learning systems to integrate knowledge and interactions involving these biologically-rich structures.
In this work, we take a first step towards the development of deep generative models for complex biomolecules, focusing on the representation and generation of RNA structures. RNA plays a crucial role in protein transcription and various regulatory processes within cells which can be influenced by its structure (Crick, 1970;Stefl et al., 2005), and RNA-based therapies are an increasingly active area of research (Pardi et al., 2018;Schlake et al., 2012), making it a natural focus for the development of deep generative models. The key challenge in generating RNA molecules-compared to the generation of small molecules-is that RNA involves a hierarchical, multi-scale structure, including a primary sequential structure based on the sequence of nucleic acids as well as more complex secondary and tertiary structures based on the way that the RNA strand folds onto itself. An effective generative model for RNA must be able to generate sequences that give rise to these more complex emergent structures.
There have been prior works on optimizing or designing RNA sequences-using reinforcement learning or blackbox optimization-to generate particular RNA secondary structures (Runge et al., 2019;Churkin et al., 2017). However, these prior works generally focus on optimizing sequences to conform to a specific secondary structure. In contrast, our goal is to define a generative model, which can facilitate the sampling and generation of diverse RNA molecules with meaningful secondary structures, while also providing a novel avenue for targeted RNA design via search over a tractable latent space.
Key contributions. We propose a series of benchmark tasks and deep generative models for the task of RNA generation, with the goal of facilitating future work on this important and challenging problem. We propose three interrelated benchmark tasks for RNA representation and generation:
1. Unsupervised generation: Generating stable, valid, and diverse RNAs that exhibit complex secondary structures. 2. Semi-supervised learning: Learning latent representations of RNA structure that correlate with known RNA functional properties. 3. Targeted generation: Generating RNAs that exhibit particular functional properties.
These three tasks build upon each other, with the first task only requiring the generation of stable and valid molecules, while the latter two tasks involve representing and generating RNAs that exhibit particular properties. In addition to proposing these novel benchmarks for the field, we introduce and evaluate three generative models for RNA. All three models build upon variational autoencoders (VAEs) (Kingma & Welling, 2014) augmented with normalizing flows (Rezende & Mohamed, 2015;Kingma et al., 2016), and they differ in how they represent the RNA structure. To help readers better understand RNA structures and properties, a self-contained explanation is provided in appendix B.
The simplest model (termed LSTMVAE) learns using a string-based representation of RNA structure. The second model (termed GraphVAE) leverages a graph-based representation and graph neural network (GNN) encoder approach (Gilmer et al., 2017). Finally, the most sophisticated model (termed HierVAE) introduces and leverages a novel hierarchical decomposition of the RNA structure. Extensive experiments on our newly proposed benchmarks highlight how the hierarchical approach allows more effective representation and generation of complex RNA structures, while also highlighting important challenges for future work in the area.

[TASK DESCRIPTION]
Given a dataset of RNA molecules, i.e. sequences of nucleotides and corresponding secondary structures, our goals are to: (a) learn to generate structurally stable, diverse, and valid RNA molecules that reflect the distribution in this training dataset; (b) learn latent representations that reflect the functional properties of RNA. A key factor in both these representation and generation processes is that we seek to jointly represent and generate both the primary sequence structure as well as the secondary structure conformation. Together, these two goals lay the foundations for generating novel RNAs that satisfy certain functional properties. To meet these goals, we create two types of benchmark datasets, each one focusing on one aspect of the above mentioned goals:
Unlabeled and variable-length RNA. The first dataset contains unlabeled RNA with moderate and highly-variable length (32-512 nts), obtained from the human transcriptome (Aken et al., 2016) and through which we focus on the generation aspect of structured RNA and evaluate the validity, stability and diversity of generated RNA molecules. In particular, our goal with this dataset is to jointly generate RNA sequences and secondary structures that are biochemically feasible (i.e., valid), have low free energy (i.e., stable), and are distinct from the training data (i.e., diverse). We will give an extended assessment of the generation aspect under different circumstances, e.g., when constraining the generation procedures with explicit rules.
Labeled RNA. The second dataset is pulled and processed from a previous study on in vitro RNAprotein interaction, which features labeled RNAs with shorter and uniform length (40 nts) (Cook et al., 2017). With this dataset, our objective is slightly expanded (to include obj. a), so that the latent space is adequately organized and reflective of the interaction with proteins. Therefore, key assessment for the latent space includes AUROC for the classification of protein binding, which is crucial for the design of desired novel RNA molecules.
Essentially, this creates slight variations in the task formulation, with the first dataset suited to unsupervised learning of a generative model, while the second datasets involves additional supervision (e.g., for a semi-supervised model or targeted generation). Our specific modeling choices, to be introduced in section 3, are invariant to different task formulations, and flexible enough to handle different representations of RNA secondary structures. We refer readers to appendix C for detailed explanation for the dataset and evaluation metrics on the generated molecules and latent embeddings.

[METHODS]
In this section, we introduce three different generative models for RNA. All three models are based upon the variational autoencoder (VAE) framework, involving three key components:
1. A probabilistic encoder network q φ (z|x), which generates a distribution over latent states given an input representation of an RNA. We experiment with three different types of input encodings for RNA sequence and secondary structures (see Figure S1: a dot-bracket annotated string, a graph with adjacency matrix representing base-pairings, and a graph augmented with a hierarchical junction tree annotation for the secondary structure.
2. A probabilistic decoder network p θ (x|z), which defines a joint distribution over RNA sequences and secondary structures, conditioned on a latent input. As with the encoder network, we design architectures based on a linearized string decoding and a graph-based hierarchical junction-tree decoding approach.
3. A parameterized prior p ψ (z), which defines a prior distribution over latent states and is learned based on a continuous normalizing flow (CNF) (Chen et al., 2018). shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
For all the approaches we propose, the model is optimized via stochastic gradient descent to minimize the evidence lower bound (ELBO):
L = −E q φ (z|x) [p θ (x|z)] + β KL(q φ (z|x)|p ψ (z))
where β is a term to allow KL-annealing over the strength of the prior regularization.
In the following sections, we explain our three different instantiations of the encoder (section 3.1), decoder (section 3.2), as well as our procedures to structurally constrain the decoding process using domain knowledge (section 3.3) and our procedures to avoid posterior collapse (section 3.4).

[ENCODING RNA SECONDARY STRUCTURES]
The input to the encoder is a structured RNA molecule, with its sequence given by an ordered array of nucleotides x 1 . . . x L , with x i ∈ {A, C, G, U }, where L is the length of the sequence, and its secondary structure, either represented as (1) a dot-bracket string S =ẋ 1 . . .ẋ L withẋ i ∈ {., (, )};
(2) or as a graph G with two types of edges -covalent bonds along the RNA backbone, and hydrogen bonds between the base-pairs 2 . We use x uv to denote edge features between nucleotides u and v;
(3) or as a hypergraph T -a depth-first ordered array of subgraphsĜ 1 . . .Ĝ D with L(Ĝ i ) ∈ {S, H, I, M } indicating the subgraph label, and I(Ĝ i ) = {j|j ∈ {1 . . . L}} indicating the assignment of nucleotides to each subgraph.
Encoding RNA secondary structure as sequence. First, we obtain a joint encoding over the nucleotide and the dot-bracket annotation, using the joint sequence-structure vocabulary {A, C, G, U } × {., (, )}. Then, these one-hot encodings are processed by a stacked bidirectional LSTM (Hochreiter & Schmidhuber, 1997), followed by a multi-head self-attention module (Vaswani et al., 2017) to weigh different positions along the RNA backbone. A global max-pooling is used to aggregate the information into h S , and then we obtain mean µ S and log variance log σ S from h S through linear transformations, and draw latent encoding z S from N (µ S , σ S ) using the reparameterization trick (Kingma & Welling, 2014).
Learning graph representation of RNA secondary structure. To encode the graph view G of an RNA secondary structure, we pass rounds of neural messages along the RNA structure, which falls into the framework of Message Passing Neural Network (MPNN) as originally discussed in Gilmer et al. (2017) and similarly motivated by Jin et al. (2018).
For much longer RNAs, it is conceptually beneficial to pass more rounds of messages so that a nucleotide may receive information on its broader structural context. However, this may introduce undesired effects such as training instability and over-smoothing issues. Therefor , we combine our MPNN network with gating mechanism, which is collectively referred as the G-MPNN:
v t−1 uv = σ(W g local [x u || x uv ] + W g msg w∈N (u) v t−1 wu ) (1) v t uv = GRU(v t−1 uv , v t−1 uv )(2)
where [. . . || . . . ] denotes concatenation, σ denotes the activation function and GRU indicates the gated recurrent unit (Cho et al., 2014). Then, after T iterations of message passing, the final nucleotide level embedding is given by:
h u = σ(W g emb [x u || v∈N (u) v T vu ]
). Before pooling the nucleotide level embeddings into the graph level, we pass h 1 . . . h L through a single bidirectional LSTM layer, obtainingĥ 1 . . .ĥ L at each step, and h g = max({ĥ i |i ∈ 1...L}). The latent encoding z G is similarly obtained from h G using the reparameterization trick.
Hierarchical encoding of the RNA hypergraph. To encode the junction tree T of RNA, we employ a type of GRU specifically suited to tree-like structures, which has previously been applied in works such as GGNN (Li et al., 2016) and JTVAE (Jin et al., 2018). We refer to this tree encoding network as T-GRU, and the format of its input is shown in Figure 1.
One major distinction between our RNA junction tree and the one used for chemical compounds (Jin et al., 2018) is that an RNA subgraph assumes more variable nucleotide composition such that it is impossible to enumerate based on the observed data. Therefore, we need to dynamically compute the features for each node in an RNA junction tree based on its contained nucleotides, in a hierarchical manner to leverage the nucleotide level embeddings learnt by G-MPNN.
Considering a subgraphĜ i in the junction tree T , we initialize its node feature with:
xĜ i = [L(Ĝ i ) || max u∈I(Ĝi) h u ].
Notably, max u∈Ĝi h u is a max-pooling over all nucleotides assigned toĜ i , and nucleotide embedding h u comes from G-MPNN. To compute and pass neural messages between adjacent subgraphs in the RNA junction tree T , we use the T-GRU network in Eq.3
v tĜ i,Ĝj = T-GRU(xĜ i , {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}) (3) hĜ i = σ(W t emb [xĜ i || Ĝ ∈N (Ĝi) hĜ]) (4)
with details of T-GRU provided in the appendix D, and compute the embeddings for subgraphs with Eq. 4. Further, we obtain a depth-first traversal of the subgraph embeddings hĜ 1 . . . hĜ D which is also the order for hierarchical decoding to be discussed later. This ordered array of embeddings is processed by another bi-directional LSTM , and the final tree level representation h T is again given by the max-pooling over the bi-LSTM outputs. Likewise, latent encoding z T is obtained from h T .

[RNA MOLECULAR GENERATION]
Decoding linearized sequence and structure. In this setting, the decoder simply autoregressively decodes a token at each step, from the joint sequence-structure vocabulary mentioned before in section 3.1, plus one additional symbol to signal the end of decoding. To simplify the design choice, we use a single-layered forward-directional LSTM, and its hidden state is initialized with the latent encoding z, which can be either z S , z G or z T .
Figure 2: Hierarchical decoding of a structured RNA, involving three types of predictions, that are on the topological level, node level, and nucleotide level. These three types of prediction are interleaved into the procedures of decoding the junction tree structure of RNA and the nucleotide segments.
Hierarchically decoding hypergraph and nucleotide segments. The input to this more sophisticated hierarchical decoder are latent encodings z G which contains order and basic connectivity information of the nucleotides, and z T which contains higher order information about the arrangements of nucleotide branches and their interactions. We give a concise description of the decoding procedures here, along with a detailed algorithm in appendix E. On a high level, we hierarchically decode the tree structure in a depth-first manner, and autoregressively generate a nucleotide segment for each visited tree branch. For these purposes, we interleave three types of prediction (Figure 2).
Denote the current tree node at decode step t and at the i-th visit asĜ t,i , whose features include (1) its node label L(Ĝ t,i ) and, (2) a summary over the already existing i − 1 nucleotide segments max{h l,j u | u ∈Ĝ t,i and l < t and j < i}, with l denoting the nucleotide is decoded at step l, and j indicating the nucleotide belongs to the j-th branch (this feature is simply zeros when i = 1). Then, its local feature xĜ t,i is defined as the concatenation of (1) and (2).
We make use of a notion called node state: hĜ t,i , which is obtained by: hĜ t,i = T-GRU(xĜ t,i , {vĜ ,Ĝ t,i |Ĝ ∈ N (Ĝ t,i )}). Note its similarity to Eq. 3, and hĜ t,i is used to make:
• topological prediction in Figure 2 
([hĜ t,i || z T || z G ])
. The start token is the last nucleotide from the last segment.
Our hierarchical decoder starts off by predicting the label of the root node using z T , followed by topological prediction on the root node and decoding the first nucleotide segment. The algorithm terminates upon revisiting the root node, topologically predicted to backtrack and finishing the last segment of the root node. The decoded junction tree naturally represents an RNA secondary structure that can be easily transformed to the dot-bracket annotation, and the RNA sequence is simply recovered by connecting nucleotide segments along the depth-first traversal of the tree nodes.

[STRUCTURALLY CONSTRAINED DECODING]
To better regulate the decoding process so that generated RNAs have valid secondary structures, a set of constraints can be added to the decoding procedures at the inference stage. Essentially, a valid RNA secondary structure needs to observe the following rules: (1) base-pairing complementarity, (2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3;
(3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
We will translate the above rules into specific and applicable constraints, depending on specific decoders. For the sake of space, we only give a broad remark and leave more details in the appendix.
Linearized decoding constraints. Since the linearized decoder simply proceeds in an autoregressive fashion, constraints can be easily enforced in a way that at each step, a nucleotide with an appropriate structural annotation is sampled by making use of masks and re-normalizing the probabilities. Likewise, a stop token can only sampled when all opening nucleotides have been closed. More details to follow in appendix F.
Hierarchical decoding constraints. The specific set of constraints for hierarchical decoding is discussed in appendix G. Overall, considering the different natures of the three associated types of prediction, each one should require a set of different strategies, which are once again applicable by adding proper masks before sampling. As shown in the algorithm in appendix E, the set of constraints are applied to line 13, 24 and 14 with marked asterisk.

[AVOIDING POSTERIOR COLLAPSE]
As discussed in a line of previous works, VAEs with strong autoregressive decoders are susceptible to posterior collapse, an issue where the decoder simply ignores the latent encoding of the encoder (He et al., 2019). Therefore, to avoid posterior collapsing, we make use of a carefully chosen KL annealing schedule during training to help the encoder adapt its information content in the latent encoding and in coordination with the decoder. This schedule is detailed in section 4. We also learn a parameterized prior as suggested in Chen et al. (2017), but using a CNF instead, following a similar implementation to Yang et al. (2019), with details given in appendix H.
Our KL annealing schedule is chosen based on empirical observations, as to our knowledge, there has yet to exist any principled methods of selecting such schedule. We have used diagnostic metrics such as mutual information (He et al., 2019) and active units (Burda et al., 2016) along with a validation set to select a proper KL annealing schedule which is to be described later in section 4

[RESULTS]
We consider three modes of evaluation: (1) unsupervised RNA generation; (2) generation using semi-supervised VAE models and (3) targeted RNA design from an organized latent space. Results are presented below, and relevant hyperparameters can be found in Table S1.
Unsupervised RNA generation. Here, we evaluate generated RNAs from models trained on the unlabeled RNA dataset for 20 epochs using a KL annealing schedule including 5 epochs of warm-up, followed by gradually increasing the KL annealing term to 3e-3 (for LSTMVAE and GraphVAE), or 2e-3 (for HierVAE). The KL annealing schedule was chosen using a validation set of 1,280 RNAs.
Table 1 compares the generation capability of different models, from the posterior as well as the prior distribution, and in scenarios such as applying structural constraints to the decoding process or not.
It clearly shows that our most advanced model, HierVAE which employs a hierarchical view of the structure in its encoding/decoding aspects, achieves the best performance across different evaluation regimes, generating valid and stable RNAs even when the decoding processed is unconstrained. It is also observed that despite having structural constraints, the validity of our generated RNAs are always slightly below 100%. This can be explained by the threshold hyperparameter which sets the maximum number of steps for topological prediction as well as the maximal length of each nucleotide segment, as shown in Algorithm 1 in appendix E.
To further demonstrate the benefits of model training from structural constraints, we sample RNAs from the prior of an untrained HierVAE model. With structural constraints, the validity amounts to 66.34% with an extremely high free energy deviation of 22.613. Without structural constraints, the validity translates to a mere 9.37% and the model can only decode short single stranded RNAs as it lacks the knowledge of constructing more complex structures. This comparison illustrates that model training is essential for obtaining stable RNA folding.
The junction tree hierarchy of RNAs developed in our work shares certain modelling similarities with the probabilistic context free grammar (Dowell & Eddy, 2004) used by covariance models (CM) (Eddy & Durbin, 1994). Infernal (Nawrocki & Eddy, 2013) is one of the representative works based on CM, which is capable of sampling RNA secondary structures from a CM built around a consensus secondary structure for a conserved RNA family. However, due to the lack of homologous sequences in our dataset, Infernal is seriously limited and can only sample single stranded RNAs.
Figure 3 illustrate RNA structures generated using HierVAE from a randomly chosen short path through the latent space. Notably, latent encoding provided by HierVAE translates smoothly in the RNA structure domain: nearby points in the latent space result in highly similar, yet different, structures. The generated structures are particularly stable for short and medium-size RNAs, and slightly less so for longer RNAs with highly complex structures. A side-by-side comparison between generated RNA secondary structures and MFE structures in Figure S3 shows that generated structures can evolve smoothly in the latent space along with their corresponding MFE structures. We also visualize neighborhoods of a Cysteine-carrying transfer RNA and a 5S ribosomal RNA in figure S4 and S5.
Supervised RNA generation. We then evaluate our generative approaches in a semi-supervised setting using seven RBP binding data sets from RNAcompete-S. First, we compare the efficacy of different representational choices while excluding the generative components, i.e. we jointly train VAE encoders followed by simple MLP classifiers on top of the latent encodings for binary classification on RBP binding.  2. Since our strategy for targeted RNA design makes use of seed molecules in the latent space, we mainly sample RNAs from the posterior distribution of these semi-supervised VAE models. Therefore, we select a KL annealing schedule that tends to retain more information in the latent encodings, i.e. setting maximum β to 5e-4 and training 10 epochs.
Results are promising in that classification AUROC measured by the held-out test set is comparable to the fully supervised classification models in Table S3, and much better compared to models only using fixed and pretrained VAE embeddings as shown in Table S2. Also, RNA structures generated from the posterior distribution, even under the setting of unconstrained and deterministic decoding, have high success rates, very stable conformation and good reconstruction accuracy. Targeted RNA design. We next studied the task of designing RNAs with high RBP binding affinity. Starting from the latent encodings of 10,000 randomly chosen RNA molecules that have negative labels in each RNAcompete-S test set, and use activation maximization to gradually alter the latent encodings so that the predicted binding probability from the embedding classifiers increases. These embedding classifiers have been trained jointly with the VAE models with accuracy reported earlier (Table 2). Then, we use separately trained full classifiers (also earlier shown in Table S3) as proxy of oracles for evaluating the ""ground truth"" probability of RBP binding. Table 3, report the success rate (fraction of RNAs whose ""ground truth"" RBP binding probability was improved), along with the average improvement in binding probabilities. An example of a trajectory of optimized RNAs is shown in Fig. S6.

[RELATED WORK]
Over the years, the field of computational drug discovery has witnessed the emergence of graphcentric approaches. One of the earliest method, proposed in Gómez-Bombarelli et al. (2018), is defined on the linearized format of molecular structures and represents a family of methods that rely on sequential models to represent and generate SMILES strings of chemical compounds. Later methods have sought to construct more chemical priors into the model, via (1) leveraging graph based representation and generation techniques, (2) enforcing direct chemical constraints to the decoding process, (3) considering a multi-scale view of the molecular structures, or (4) using reinforcement learning to integrate more training signal of the molecular structure and function. As a result, greater success has been achieved by models such as Kusner et al. (2017); ; Jin et al. (2018); You et al. (2018) at generating and searching valid and more useful chemical compounds.
Graph representation learning is at the heart of these more recent approaches, to help understand the rules governing the formation of these molecular structures, as well as the correspondence between structures and functions. Duvenaud et al. (2015) were among the first to apply GNN to learn molecular fingerprints, and the general neural message passing framework for molecules is proposed in Gilmer et al. (2017), which demonstrate the power of MPNN across various molecular benchmarking tasks. These prior works on molecular MPNN, together with other GNN architectures developed in other areas, such as considering relational edges (Schlichtkrull et al., 2018) and attention (Velickovic et al., 2018), have laid the foundation for the success of these deep generative models.
Despite the fact that RNA molecules can adopt complex structures, dedicated graph representation learning techniques have been scarce, with some recent works beginning to leverage graph related learning techniques to predict RNA folding (Chen et al., 2020;Singh et al., 2019) and to represent RNA molecular structures (Yan et al., 2020;Oliver et al., 2020). Prior to our work, the design of RNA has mostly focused on the inverse design problem, which is to conditionally generate an RNA sequence whose MFE secondary structure corresponds to an input secondary structure. Therefore, the line of prior works have predominantly relied on sequential techniques, with some representative methods based on reinforcement learning (Runge et al., 2019), or more classically framed as a combinatorial optimization problem and solved with sampling based techniques (Churkin et al., 2017). These prior works are mainly concerned with querying from an energy model with fixed thermodynamic parameters and fixed dynamics of RNA folding, which is in itself limited compared to learning based approaches (Chen et al., 2020;Singh et al., 2019), and are unable to model a joint distribution over RNA sequences and possible folds.

[CONCLUSION AND FUTURE WORKS]
In this work we propose the first graph-based deep generative approach for jointly embedding and generating RNA sequence and structure, along with a series of benchmarking tasks. Our presented work has demonstrated impressive performance at generating diverse, valid and stable RNA secondary structures with useful properties.
For future works, there are several important directions to consider. First, it would be beneficial to obtain non-coding RNA families from the RFAM database (Kalvari et al., 2017) which would help our models learn more biologically-meaningful representation indicative of RNA homology and functions, in addition to the evolutionarily conserved RNA structural motifs that would enable the generation of more stable RNA secondary structures. In that context, a detailed comparison to Infernal and other probabilistic context-free grammar models would be meaningful.
On the methodological aspect, in light of the recent advances in protein sequences pretraining across a large evolutionary-scale (Rives et al., 2019;Elnaggar et al., 2020), our models for RNAs may similarly benefit by such a procedure with the data collected from RFAM. After the pretraining step, reinforcement learning can be used to finetune the generative component of our model with customizable rewards defined jointly on RNA structural validity, folding stability and functions such as binding to certain proteins.
On the evaluation side, it would be of great interest to analyze our models for any potential RNA tertiary structural motifs and to compare them with those deposited in the CaRNAval (Reinharz et al., 2018) or RNA 3D motifs database (Parlea et al., 2016). Our models would also need modifications to allow non-canonical interactions and pseudoknots, which are common in RNA tertiary structures.
All in all, the representation, generation and design of structured RNA molecules represent a rich, promising, and challenging area for future research in computational biology and drug discovery, and an opportunity to develop fundamentally new machine learning approaches.

[A ACKNOWLEDGEMENTS]
We would like to thank all members of the Hamilton lab, Blanchette lab, and the four anonymous reviewers for their insightful suggestions. This work was funded by a Genome Quebec/Canada grant to MB and by the Institut de Valorisation des Données (IAVDO) PhD excellence scholarship to ZY. WLH is supported by a Canada CIFAR AI Chair. We also thank Compute Canada for providing the computational resources.
B BACKGROUND: RNA STRUCTURE AND KEY PROPERTIES , hence stabilizing the molecule 3 . The set of pairs of interacting nucleotides in an RNA forms its so-called RNA secondary structure. In computational analyses of RNA, it is standard to assume that a secondary structure is nested: if [i, j] and [k, l] form base pairs with i < k, then either l < j (nesting) or k > j (non-overlapping). This enables simple string or planar graph representations (Figure S1 a, b).
The nested structure assumption means that secondary structures can be modelled by a probabilistic context free grammar (Dowell & Eddy, 2004), or by the closely related junction tree structure (Figure S1 c) (Sarrazin-Gendron et al., 2020), where each hypernode corresponds to a particular secondary substructure element: (1) stem: consecutive stacked base-pairs locally forming a double-stranded structure;
(2) hairpin loop : unpaired regions closed by a base-pair;
(3) internal loop: unpaired regions located between two stems; (4) multiloop: unpaired regions at the junction of at least three stems. Edges link elements that are adjacent in the structure.
Validity and stability of RNA folding. The notion of free energy of RNA secondary structures can be used to characterize the stability of a particular conformation. Given an RNA sequence, there are combinatorially many valid RNA secondary structures which all need to obey a set of constraints (summarized in section 3.3). However, some structures are more stable than the others by having lower free energy. Therefore, these structures are more likely to exist (hence more useful) in reality due to the stochastic nature of RNA folding. The free energy of an RNA secondary structure can be estimated by an energy-based model with thermodynamic parameters obtained from experiments (Mathews et al., 2004), wherein the minimum free energy (MFE) structure can be predicted, up to a reasonable approximation (Lorenz et al., 2011). 4 

[C DATASET AND METRICS]
The unlabeled dataset is obtained from the complete human transcriptome which is downloaded from the Ensembl database (Aken et al. (2016); version GRCh38). We slice the transcripts into snippets with length randomly drawn between 32 and 512 nts, and use RNAfold to obtain the MFE structures. We randomly split the dataset into a training set that contains 1,149,859 RNAs, and 20,000 held-out RNAs for evaluating decoding from the posterior distribution. More information on the structural diversity and complexity of this dataset is shown in Figure S2, which should present significant challenges for our algorithms.
The labeled dataset is pulled from a previous study on sequence and structural binding preference of RNA binding proteins (RBP), using an in vitro selection protocol called RNAcompete-S (Cook et al., 2017) which generates synthesized RNA sequences bound or unbound to a given RBP. RNAs in this experiment are of uniform length, i.e. 40 nts, and offer a rich abundance of RNA secondary structures compared to its predecessor protocols such as RNAcompete (Ray et al., 2009;2013). Since no benchmark has been ever established since its publication, we randomly sample 500,000 positive sequences bound to an RBP, and the same amount of negative sequences from the pool of unbound sequences, to curate a dataset for each of the seven RBPs investigated in the paper. Then, 80% of all RNAs are randomly selected to the train split, and the rest goes to the test split.
Our evaluation scheme for the generated RNA secondary structures includes the following metrics:
• validity: percentage of generated RNA secondary structures that conform to the structural constraints specified in section 3.3. • free energy deviation (FE DEV): difference of free energy between the generated RNA secondary structure and the MFE structure of the corresponding sequence, which quantifies the gap of both structures from an energy perspective. A lower FE DEV should indicate higher stability of generated RNAs. • free energy deviation normalized by length (Normed FE DEV): FE DEV divided by the length of generated RNA, which distributes the contribution of total FE DEV to each base. • 5-mer sequence diversity: entropy of the normalized counts of 5-mer substrings, which directly measures the diversity of RNA sequences, and indirectly for RNA secondary structures when this metric is combined with FE DEV, since monolithic structures of diverse sequences would lead to high FE DEV.

[D TREE ENCODING GRU]
Following Eq.3, T-GRU computes a new message v tĜ i,Ĝj fromĜ i andĜ j , based on the features in G i denoted by xĜ i , as well as neural messages from neighboring subgraphs toĜ i , i.e. {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}. The internal structure of T-GRU is equivalent to the tree encoder employed in Jin et al. (2018), which is essentially a neural analogue of the belief propagation algorithm on junction trees. Nevertheless, we write down the message passing formulas of T-GRU here:
sĜ i,Ĝj = Ĝ k ∈N (Ĝi) v t−1 G k ,Ĝi(S1)
zĜ i,Ĝj = σ(W z [xĜ i || sĜ i,Ĝj ] + b z ) (S2) rĜ k ,Ĝi = σ(W r [xĜ i || v t−1 G k ,Ĝi ] + b r ) (S3) vĜ i,Ĝj = Tanh(W [xĜ i || Ĝ k ∈N (Ĝi) rĜ k ,Ĝi • v t−1 G k ,Ĝi ]) (S4) v tĜ i,Ĝj = (1 − zĜ i,Ĝj ) sĜ i,Ĝj + zĜ i,Ĝj vĜ i,Ĝj(S5)
E ALGORITHM FOR HIERARCHICALLY DECODING STRUCTURED RNA a M TI refers to the threshold which set the maximum allowed number of topological prediction steps; M SI is another threshold to limit the length of each decoded nucleotide segment.

[H DETAILS FOR PARAMETERIZING PRIOR DISTRIBUTION USING NORMALIZING FLOW]
A normalizing flow involves a series of bijective transformation with tractable Jacobian logdeterminant, to map an observed datapoint x ∼ p θ (x) from a complex distribution to a simpler one, such as the standard normal distribution.
Considering the simplified case where we have a single bijective function f θ : Z → X to map some simple latent variables z to observed datapoint x, then, using the change of variable theorem, the likelihood of the observed datapoint can be evaluated as:
p θ (x) = p z (f −1 θ (x))|det ∂f −1 θ (x) ∂x | (S6)
where p z (.) denotes some simple base distribution, e.g. N (0; I). Then, it becomes clear the efficiency of this scheme heavily relies on the efficiency of inverting the forward mapping f θ as well as computing its Jacobian log-determinant.
In this project, we use a type of continuous normalizing flow (CNF) which simplifies the above mentioned computation (Chen et al., 2018). Consider a time continuous dynamics f ψ (z(t), t) of some intermediate data representation z(t), and again z(t 0 ) ∼ p z (.), the transformation of variable, along with its inverse mapping, can be expressed as:
z z(t 1 ) = z(t 0 ) + t1 t0 f ψ (z(t), t)dt (S7) z(t 0 ) = z(t 1 ) + t0 t1 f ψ (z(t), t)dt (S8)
and the change of probability density can be expressed as:
log p ψ (z) = log p z (z(t 0 )) − t1 t0 tr( ∂f θ ∂z(t) )dt (S9)
Note that the invertibility issue is no longer a concern under some mild constraints (Chen et al., 2018). Also, Eq. S9 only involves a more light-weight trace operation on the Jacobian rather than evaluating its log-determinant.
Therefore, we learn a parameterized prior using a CNF, and observe the decomposition of the KL term in the VAE objective:
KL(q φ (z|x)|p ψ (z)) = −E z∼q φ (z|x) [p ψ (z)] − H[q φ (z|x)](S10)
Therefore, during training our CNF parameterized with ψ works on the transformation of complex latent encodings z ∼ q φ (z|x) to some simple z(t 0 ) ∼ N (0; I), with an exact likelihood described by Eq. S9 and integrated into Eq. S10 for the complete training objective. During inference, we simply sample z t0 ∼ N (0; I), and use our CNF to reversely transform it to z ∼ p ψ (.) which should be closer to the approximate posterior. J HYPERPARAMETERS  The optimization takes place in the latent space of HierVAE, starting from the initial encoding of a random RNA molecule in the test set, and at each step altering the latent encoding by using activation maximization on the embedding classifier. The trajectory of generated RNAs is shown in the order of left to right and top to bottom, and the field PRED indicates that the probability of binding, as predicted by another external full classifier on the decoded molecular structure, is overall increasing as the decoded RNA structures smoothly evolving.

[F DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO LINEARIZED DECODING PROCEDURES]
When decoding from the joint vocabulary of sequence and dot-bracket structure ({A, C, G, U } × {., (, )}), whenever a nucleotide nuc i with a left bracket is sampled at step i, we append them to a stack, i.e. {(nuc i0 , i 0 ) . . . (nuc i , i)}. Then, at decode step j,
• if |i − j| ≤ 3, a proper mask will be added to the categorical logits of the vocabulary, to avoid sampling any nucleotides with right brackets, which means only an unpaired nucleotide or one that comes with a left bracket can be sampled; • if |i − j| > 3, a mask will be applied to make sure that only a nucleotide complementary to nuc i can be sampled with the right bracket. Sampling nucleotides with other forms of structures are allowed.
As soon as a nucleotide with a closing right bracket is sampled, we pop out (nuc i , i) from the stack. The special symbol for stop decoding can only be sampled when the stack has become empty.

[G DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO HIERARCHICAL DECODING PROCEDURES]
Additional constraints to be enforced during the hierarchical decoding process to ensure the validity of the decoded RNA secondary structure. Recall in section 3.2 that three types of predictions are involved with the hierarchical decoding, therefore, each type is associated with its own set of rules.
All set of rules can be observed by adding proper masks to the categorical logits before sampling, which are detailed below.
Constraints for making topological prediction, when the current node is
• stem node, then the algorithm always expands to a new node upon its first visit, or backtracks to its parent node upon re-visit; • hairpin node, then the algorithm always backtracks; • internal loop, then the algorithm acts similarly as for stem node; • multi-loop, then the algorithm always expands upon first visit and the next re-visit. Further re-visits to the same multi-loop node are not regulated.
Constraints for predicting new tree node, when the current node is • stem node, then its child node when exists can be either a hairpin loop, an internal loop, or a multi-loop; • hairpin node, internal loop or multi-loop, then its child node must be a stem node.
Constraints for decoding nucleotide segment. Due to the property of non-empty intersection between adjacent subgraphs, the start token for decoding a segment at the current node, is always the last nucleotide decoded at the last node. Therefore, without explicitly mentioning, the algorithm needs to decode at least one new nucleotide at each segment. When the current node is • stem node, and if it is upon its first visit (i.e. decoding the first segment of a stem), then there is no for constraints. Otherwise, upon its re-visit, the algorithm needs to decode exactly the complementary bases and in the reverse order, according to the first decoded segment; • hairpin node, then the decoder needs to decode at least four nucleotides before seeing the stop symbol, unless the hairpin is also the root node. • internal loop node, and if it is upon its first, then constraint is not necessary. Otherwise, upon its revisit, the algorithm needs to decode at least one unpaired nucleotide on condition that the first decoded internal loop segment does not contain any unpaired nucleotides; • multi-loop node, then there is no need for constraints. HuR 0.880 ± 0.000 0.880 ± 0.000 0.880 ± 0.000 0.888 ± 0.002 PTB 0.900 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 QKI 0.820 ± 0.000 0.830 ± 0.000 0.825 ± 0.002 0.830 ± 0.000 Vts1 0.900 ± 0.000 0.908 ± 0.002 0.637 ± 0.079 0.910 ± 0.000 RBMY 0.905 ± 0.002 0.880 ± 0.003 0.802 ± 0.055 0.870 ± 0.002 SF2 0.890 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 SLBP 0.777 ± 0.002 0.790 ± 0.000 0.797 ± 0.002 0.797 ± 0.002","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules."
Neural representation and generation for RNA secondary structures,snOgiCYZgJ7.json,"Brief summary: The authors describe a generative model constrained to model the primary and secondary structure of RNA.

Pros:

- I applaud the authors for working with RNA data. This is less characterization of modeling RNA data.

- I think the constraints for the RNA generation structure are reasonable, and both the manual and ML constrained generative process are well thought out.

- I think the comparison between constrained and unconstrained generation of molecules was well done.

Cons:

- This paper falls into the common pitfall of not controlling for homologous sequences between the training and test set. For each sequence in the training set, what is the sequence with the largest sequence ID in the test set by alignment?

- It would be great to have a null distribution to characterize your model to. What is FE DEV, Normed, and Diversity for randomized, but kmer controlled, RNA sequences?

- The definition of ""valid structures"" is poorly defined. Could generated sequences still fold if optimized in a traditional biophysical simulation?

- I think a comparison with an algorithm like INFERNAL (http://eddylab.org/infernal/Userguide.pdf) is necessary. This is the most simple context-free grammar generative model.

Neutral:

- It is difficult to assess the range of ""good"" values in Table 1. To aid in interpretability to the reader, I would advise more explicitly denoting whether diversity, FE DEV, and Normed should be metrics that go up or down with improved performance.

- I think it would be wise to train this sort of model on more data, such as RFAM. This is an already curated database of RNAs by sequence family from a large number organisms, much more than just human sequences.

- Is there any way to evaluate valid tertiary contacts? These would be sequence motifs that are preserved across organisms with constrained sequence and function that may not necessarily follow Watson-Crick basepairing rules.

- It is slightly concerning to me that in Table 1, ""Validity"" is the same for LSTMVAE and GraphVAE in *Constrained & Posterior Decoding* and ""Diversity"" is the same for GraphVAE and HierVAE *Constrained & Prior Decoding*. Why is this the case?

- Page 2, typo: “benchmhark datasets”
","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Hierarchical encoding. Panel (A)shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
Table 2: (A), to determine if the decoder should expand to a new tree node or backtrack to its parent node, based on MLP topo (hĜ t,i ); • tree node prediction in Figure 2 (B), on condition that a new tree node is needed due to a possible topological expansion. This procedure determines the label of the new tree node from the set of {S, H, I, M }, based on MLP node (hĜ t,i ); • nucleotide segment decoding in Figure 2 (C), using a single-layered LSTM, whose initial hidden state is MLP dec
Table 3: Figure 3 :3Figure 3: RNAs generated with structural constraints from HierVAE on a random axis in the latent space (step size: 1e-4), for short (top), medium-length (middle), and long (bottom) RNAs. The Free energy (FE) and its deviation (DEV) from the MFE are given for each structure. which means only the canonical base-pairs and Wobble base-pairs are allowed, i.e. [A-U], [G-C] and [G-U];(2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3; (3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
Table 4: Figure S1 :S1FigureS1: A nested RNA secondary structure can be represented by: (A) dotbracket annotation, where base-pairs corresponding to matching parentheses, or (B) a molecular planar graph with two types of edges, corresponding to consecutive nucleotides (backbone) and basepairing interactions, or (C) a junction tree where node are labeled as stems (S), hairpins (H), internal loops (I), or multiloops (M), and edges correspond to the connections between these elements. All three forms are equivalent.
Table 5: Figure S2: This figure contains information of the unlabeled RNA dataset. (A) The number of hypernodes appears to grow linearly with the length of RNA, and (B) the junction tree height also grows as the length increases but on a more moderate scale. (C) and (D) have shown bar-plots of the number of hypernodes and tree height, indicating that the junction tree of RNA can take on significant depth hence contributing to the diversity and complexity of RNA secondary structures represented in this dataset.
Table 6: Figure S4 :S4Figure S4: Neighborhood visualization of tRNA-Cys 6 which is marked by the red bounding box in the center and the walk in the latent space takes place on two random orthogonal axes. Note that actual secondary structure of tRNA-Cys plotted in the figure is different compared to the one deposited online due to the prediction of RNAfold.
Table 7: We evaluate RNAs sampled from the posterior distribution: q(z|x), with a held-out test set of 20,000 RNAs. Each molecule is encoded and decoded 5 times. We also evaluate samples from the prior distribution: N (0, I) subject to the transformation of a latent CNF, where we sample 10,000 encodings and each encoding is decoded 10 times. Normed refers to length normalized FE DEV.
Table 8: Training semi-supervised HierVAE on labeled RNAcompete-S dataset. A test split is used to evaluate the accuracy of embedding classifiers and RNAs decoded from the posterior distribution under two settings: constrained and stochastic (C& S), unconstrained and deterministic (NC&D). RECON ACC refers to reconstruction accuracy which measures the percentage of RNA molecules decoded exactly as the input.
Table 9: 
Table 10: Algorithm 1: DFS decode RNA secondary structure1 Given: z T , z G , M TI, M SI a 2 Initialize: stack ← [ ] 3 function decode(z T , z G ) 4 root ← sample(MLP node (z T )) ;
Table 11: Hyperparameters for training VAE and full classifier models. Note that hidden units refer to the dimensionality of encoders and decoders from LSTMVAE, GraphVAE as well as HierVAE models. Dropout is applied to the embedding MLP classifier in case of training semi-supervised VAEs, which contains one hidden layer. Performance of simple MLP classifiers on top of fixed latent embeddings from VAE models, which have been pretrained on the unlabeled RNA dataset as originally shown in Table1.

[INTRODUCTION]
There is an increasing interest in developing deep generative models for biochemical data, especially in the context of generating drug-like molecules. Learning generative models of biochemical molecules can facilitate the development and discovery of novel treatments for various diseases, reducing the lead time for discovering promising new therapies and potentially translating in reduced costs for drug development (Stokes et al., 2020). Indeed, the study of generative models for molecules has become a rich and active subfield within machine learning, with standard benchmarks (Sterling & Irwin, 2015), a set of well-known baseline approaches (Gómez-Bombarelli et al., 2018;Kusner et al., 2017;Jin et al., 2018), and high-profile cases of real-world impact 1 .
Prior work in this space has focused primarily on the generation of small molecules (with less than 100 atoms), leaving the development of generative models for larger and more complicated biologics and biosimilar drugs (e.g., RNA and protein peptides) an open area for research. Developing generative models for larger biochemicals is critical in order to expand the frontiers of automated treatment design. More generally, developing effective representation learning for such complex biochemicals will allow machine learning systems to integrate knowledge and interactions involving these biologically-rich structures.
In this work, we take a first step towards the development of deep generative models for complex biomolecules, focusing on the representation and generation of RNA structures. RNA plays a crucial role in protein transcription and various regulatory processes within cells which can be influenced by its structure (Crick, 1970;Stefl et al., 2005), and RNA-based therapies are an increasingly active area of research (Pardi et al., 2018;Schlake et al., 2012), making it a natural focus for the development of deep generative models. The key challenge in generating RNA molecules-compared to the generation of small molecules-is that RNA involves a hierarchical, multi-scale structure, including a primary sequential structure based on the sequence of nucleic acids as well as more complex secondary and tertiary structures based on the way that the RNA strand folds onto itself. An effective generative model for RNA must be able to generate sequences that give rise to these more complex emergent structures.
There have been prior works on optimizing or designing RNA sequences-using reinforcement learning or blackbox optimization-to generate particular RNA secondary structures (Runge et al., 2019;Churkin et al., 2017). However, these prior works generally focus on optimizing sequences to conform to a specific secondary structure. In contrast, our goal is to define a generative model, which can facilitate the sampling and generation of diverse RNA molecules with meaningful secondary structures, while also providing a novel avenue for targeted RNA design via search over a tractable latent space.
Key contributions. We propose a series of benchmark tasks and deep generative models for the task of RNA generation, with the goal of facilitating future work on this important and challenging problem. We propose three interrelated benchmark tasks for RNA representation and generation:
1. Unsupervised generation: Generating stable, valid, and diverse RNAs that exhibit complex secondary structures. 2. Semi-supervised learning: Learning latent representations of RNA structure that correlate with known RNA functional properties. 3. Targeted generation: Generating RNAs that exhibit particular functional properties.
These three tasks build upon each other, with the first task only requiring the generation of stable and valid molecules, while the latter two tasks involve representing and generating RNAs that exhibit particular properties. In addition to proposing these novel benchmarks for the field, we introduce and evaluate three generative models for RNA. All three models build upon variational autoencoders (VAEs) (Kingma & Welling, 2014) augmented with normalizing flows (Rezende & Mohamed, 2015;Kingma et al., 2016), and they differ in how they represent the RNA structure. To help readers better understand RNA structures and properties, a self-contained explanation is provided in appendix B.
The simplest model (termed LSTMVAE) learns using a string-based representation of RNA structure. The second model (termed GraphVAE) leverages a graph-based representation and graph neural network (GNN) encoder approach (Gilmer et al., 2017). Finally, the most sophisticated model (termed HierVAE) introduces and leverages a novel hierarchical decomposition of the RNA structure. Extensive experiments on our newly proposed benchmarks highlight how the hierarchical approach allows more effective representation and generation of complex RNA structures, while also highlighting important challenges for future work in the area.

[TASK DESCRIPTION]
Given a dataset of RNA molecules, i.e. sequences of nucleotides and corresponding secondary structures, our goals are to: (a) learn to generate structurally stable, diverse, and valid RNA molecules that reflect the distribution in this training dataset; (b) learn latent representations that reflect the functional properties of RNA. A key factor in both these representation and generation processes is that we seek to jointly represent and generate both the primary sequence structure as well as the secondary structure conformation. Together, these two goals lay the foundations for generating novel RNAs that satisfy certain functional properties. To meet these goals, we create two types of benchmark datasets, each one focusing on one aspect of the above mentioned goals:
Unlabeled and variable-length RNA. The first dataset contains unlabeled RNA with moderate and highly-variable length (32-512 nts), obtained from the human transcriptome (Aken et al., 2016) and through which we focus on the generation aspect of structured RNA and evaluate the validity, stability and diversity of generated RNA molecules. In particular, our goal with this dataset is to jointly generate RNA sequences and secondary structures that are biochemically feasible (i.e., valid), have low free energy (i.e., stable), and are distinct from the training data (i.e., diverse). We will give an extended assessment of the generation aspect under different circumstances, e.g., when constraining the generation procedures with explicit rules.
Labeled RNA. The second dataset is pulled and processed from a previous study on in vitro RNAprotein interaction, which features labeled RNAs with shorter and uniform length (40 nts) (Cook et al., 2017). With this dataset, our objective is slightly expanded (to include obj. a), so that the latent space is adequately organized and reflective of the interaction with proteins. Therefore, key assessment for the latent space includes AUROC for the classification of protein binding, which is crucial for the design of desired novel RNA molecules.
Essentially, this creates slight variations in the task formulation, with the first dataset suited to unsupervised learning of a generative model, while the second datasets involves additional supervision (e.g., for a semi-supervised model or targeted generation). Our specific modeling choices, to be introduced in section 3, are invariant to different task formulations, and flexible enough to handle different representations of RNA secondary structures. We refer readers to appendix C for detailed explanation for the dataset and evaluation metrics on the generated molecules and latent embeddings.

[METHODS]
In this section, we introduce three different generative models for RNA. All three models are based upon the variational autoencoder (VAE) framework, involving three key components:
1. A probabilistic encoder network q φ (z|x), which generates a distribution over latent states given an input representation of an RNA. We experiment with three different types of input encodings for RNA sequence and secondary structures (see Figure S1: a dot-bracket annotated string, a graph with adjacency matrix representing base-pairings, and a graph augmented with a hierarchical junction tree annotation for the secondary structure.
2. A probabilistic decoder network p θ (x|z), which defines a joint distribution over RNA sequences and secondary structures, conditioned on a latent input. As with the encoder network, we design architectures based on a linearized string decoding and a graph-based hierarchical junction-tree decoding approach.
3. A parameterized prior p ψ (z), which defines a prior distribution over latent states and is learned based on a continuous normalizing flow (CNF) (Chen et al., 2018). shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
For all the approaches we propose, the model is optimized via stochastic gradient descent to minimize the evidence lower bound (ELBO):
L = −E q φ (z|x) [p θ (x|z)] + β KL(q φ (z|x)|p ψ (z))
where β is a term to allow KL-annealing over the strength of the prior regularization.
In the following sections, we explain our three different instantiations of the encoder (section 3.1), decoder (section 3.2), as well as our procedures to structurally constrain the decoding process using domain knowledge (section 3.3) and our procedures to avoid posterior collapse (section 3.4).

[ENCODING RNA SECONDARY STRUCTURES]
The input to the encoder is a structured RNA molecule, with its sequence given by an ordered array of nucleotides x 1 . . . x L , with x i ∈ {A, C, G, U }, where L is the length of the sequence, and its secondary structure, either represented as (1) a dot-bracket string S =ẋ 1 . . .ẋ L withẋ i ∈ {., (, )};
(2) or as a graph G with two types of edges -covalent bonds along the RNA backbone, and hydrogen bonds between the base-pairs 2 . We use x uv to denote edge features between nucleotides u and v;
(3) or as a hypergraph T -a depth-first ordered array of subgraphsĜ 1 . . .Ĝ D with L(Ĝ i ) ∈ {S, H, I, M } indicating the subgraph label, and I(Ĝ i ) = {j|j ∈ {1 . . . L}} indicating the assignment of nucleotides to each subgraph.
Encoding RNA secondary structure as sequence. First, we obtain a joint encoding over the nucleotide and the dot-bracket annotation, using the joint sequence-structure vocabulary {A, C, G, U } × {., (, )}. Then, these one-hot encodings are processed by a stacked bidirectional LSTM (Hochreiter & Schmidhuber, 1997), followed by a multi-head self-attention module (Vaswani et al., 2017) to weigh different positions along the RNA backbone. A global max-pooling is used to aggregate the information into h S , and then we obtain mean µ S and log variance log σ S from h S through linear transformations, and draw latent encoding z S from N (µ S , σ S ) using the reparameterization trick (Kingma & Welling, 2014).
Learning graph representation of RNA secondary structure. To encode the graph view G of an RNA secondary structure, we pass rounds of neural messages along the RNA structure, which falls into the framework of Message Passing Neural Network (MPNN) as originally discussed in Gilmer et al. (2017) and similarly motivated by Jin et al. (2018).
For much longer RNAs, it is conceptually beneficial to pass more rounds of messages so that a nucleotide may receive information on its broader structural context. However, this may introduce undesired effects such as training instability and over-smoothing issues. Therefor , we combine our MPNN network with gating mechanism, which is collectively referred as the G-MPNN:
v t−1 uv = σ(W g local [x u || x uv ] + W g msg w∈N (u) v t−1 wu ) (1) v t uv = GRU(v t−1 uv , v t−1 uv )(2)
where [. . . || . . . ] denotes concatenation, σ denotes the activation function and GRU indicates the gated recurrent unit (Cho et al., 2014). Then, after T iterations of message passing, the final nucleotide level embedding is given by:
h u = σ(W g emb [x u || v∈N (u) v T vu ]
). Before pooling the nucleotide level embeddings into the graph level, we pass h 1 . . . h L through a single bidirectional LSTM layer, obtainingĥ 1 . . .ĥ L at each step, and h g = max({ĥ i |i ∈ 1...L}). The latent encoding z G is similarly obtained from h G using the reparameterization trick.
Hierarchical encoding of the RNA hypergraph. To encode the junction tree T of RNA, we employ a type of GRU specifically suited to tree-like structures, which has previously been applied in works such as GGNN (Li et al., 2016) and JTVAE (Jin et al., 2018). We refer to this tree encoding network as T-GRU, and the format of its input is shown in Figure 1.
One major distinction between our RNA junction tree and the one used for chemical compounds (Jin et al., 2018) is that an RNA subgraph assumes more variable nucleotide composition such that it is impossible to enumerate based on the observed data. Therefore, we need to dynamically compute the features for each node in an RNA junction tree based on its contained nucleotides, in a hierarchical manner to leverage the nucleotide level embeddings learnt by G-MPNN.
Considering a subgraphĜ i in the junction tree T , we initialize its node feature with:
xĜ i = [L(Ĝ i ) || max u∈I(Ĝi) h u ].
Notably, max u∈Ĝi h u is a max-pooling over all nucleotides assigned toĜ i , and nucleotide embedding h u comes from G-MPNN. To compute and pass neural messages between adjacent subgraphs in the RNA junction tree T , we use the T-GRU network in Eq.3
v tĜ i,Ĝj = T-GRU(xĜ i , {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}) (3) hĜ i = σ(W t emb [xĜ i || Ĝ ∈N (Ĝi) hĜ]) (4)
with details of T-GRU provided in the appendix D, and compute the embeddings for subgraphs with Eq. 4. Further, we obtain a depth-first traversal of the subgraph embeddings hĜ 1 . . . hĜ D which is also the order for hierarchical decoding to be discussed later. This ordered array of embeddings is processed by another bi-directional LSTM , and the final tree level representation h T is again given by the max-pooling over the bi-LSTM outputs. Likewise, latent encoding z T is obtained from h T .

[RNA MOLECULAR GENERATION]
Decoding linearized sequence and structure. In this setting, the decoder simply autoregressively decodes a token at each step, from the joint sequence-structure vocabulary mentioned before in section 3.1, plus one additional symbol to signal the end of decoding. To simplify the design choice, we use a single-layered forward-directional LSTM, and its hidden state is initialized with the latent encoding z, which can be either z S , z G or z T .
Figure 2: Hierarchical decoding of a structured RNA, involving three types of predictions, that are on the topological level, node level, and nucleotide level. These three types of prediction are interleaved into the procedures of decoding the junction tree structure of RNA and the nucleotide segments.
Hierarchically decoding hypergraph and nucleotide segments. The input to this more sophisticated hierarchical decoder are latent encodings z G which contains order and basic connectivity information of the nucleotides, and z T which contains higher order information about the arrangements of nucleotide branches and their interactions. We give a concise description of the decoding procedures here, along with a detailed algorithm in appendix E. On a high level, we hierarchically decode the tree structure in a depth-first manner, and autoregressively generate a nucleotide segment for each visited tree branch. For these purposes, we interleave three types of prediction (Figure 2).
Denote the current tree node at decode step t and at the i-th visit asĜ t,i , whose features include (1) its node label L(Ĝ t,i ) and, (2) a summary over the already existing i − 1 nucleotide segments max{h l,j u | u ∈Ĝ t,i and l < t and j < i}, with l denoting the nucleotide is decoded at step l, and j indicating the nucleotide belongs to the j-th branch (this feature is simply zeros when i = 1). Then, its local feature xĜ t,i is defined as the concatenation of (1) and (2).
We make use of a notion called node state: hĜ t,i , which is obtained by: hĜ t,i = T-GRU(xĜ t,i , {vĜ ,Ĝ t,i |Ĝ ∈ N (Ĝ t,i )}). Note its similarity to Eq. 3, and hĜ t,i is used to make:
• topological prediction in Figure 2 
([hĜ t,i || z T || z G ])
. The start token is the last nucleotide from the last segment.
Our hierarchical decoder starts off by predicting the label of the root node using z T , followed by topological prediction on the root node and decoding the first nucleotide segment. The algorithm terminates upon revisiting the root node, topologically predicted to backtrack and finishing the last segment of the root node. The decoded junction tree naturally represents an RNA secondary structure that can be easily transformed to the dot-bracket annotation, and the RNA sequence is simply recovered by connecting nucleotide segments along the depth-first traversal of the tree nodes.

[STRUCTURALLY CONSTRAINED DECODING]
To better regulate the decoding process so that generated RNAs have valid secondary structures, a set of constraints can be added to the decoding procedures at the inference stage. Essentially, a valid RNA secondary structure needs to observe the following rules: (1) base-pairing complementarity, (2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3;
(3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
We will translate the above rules into specific and applicable constraints, depending on specific decoders. For the sake of space, we only give a broad remark and leave more details in the appendix.
Linearized decoding constraints. Since the linearized decoder simply proceeds in an autoregressive fashion, constraints can be easily enforced in a way that at each step, a nucleotide with an appropriate structural annotation is sampled by making use of masks and re-normalizing the probabilities. Likewise, a stop token can only sampled when all opening nucleotides have been closed. More details to follow in appendix F.
Hierarchical decoding constraints. The specific set of constraints for hierarchical decoding is discussed in appendix G. Overall, considering the different natures of the three associated types of prediction, each one should require a set of different strategies, which are once again applicable by adding proper masks before sampling. As shown in the algorithm in appendix E, the set of constraints are applied to line 13, 24 and 14 with marked asterisk.

[AVOIDING POSTERIOR COLLAPSE]
As discussed in a line of previous works, VAEs with strong autoregressive decoders are susceptible to posterior collapse, an issue where the decoder simply ignores the latent encoding of the encoder (He et al., 2019). Therefore, to avoid posterior collapsing, we make use of a carefully chosen KL annealing schedule during training to help the encoder adapt its information content in the latent encoding and in coordination with the decoder. This schedule is detailed in section 4. We also learn a parameterized prior as suggested in Chen et al. (2017), but using a CNF instead, following a similar implementation to Yang et al. (2019), with details given in appendix H.
Our KL annealing schedule is chosen based on empirical observations, as to our knowledge, there has yet to exist any principled methods of selecting such schedule. We have used diagnostic metrics such as mutual information (He et al., 2019) and active units (Burda et al., 2016) along with a validation set to select a proper KL annealing schedule which is to be described later in section 4

[RESULTS]
We consider three modes of evaluation: (1) unsupervised RNA generation; (2) generation using semi-supervised VAE models and (3) targeted RNA design from an organized latent space. Results are presented below, and relevant hyperparameters can be found in Table S1.
Unsupervised RNA generation. Here, we evaluate generated RNAs from models trained on the unlabeled RNA dataset for 20 epochs using a KL annealing schedule including 5 epochs of warm-up, followed by gradually increasing the KL annealing term to 3e-3 (for LSTMVAE and GraphVAE), or 2e-3 (for HierVAE). The KL annealing schedule was chosen using a validation set of 1,280 RNAs.
Table 1 compares the generation capability of different models, from the posterior as well as the prior distribution, and in scenarios such as applying structural constraints to the decoding process or not.
It clearly shows that our most advanced model, HierVAE which employs a hierarchical view of the structure in its encoding/decoding aspects, achieves the best performance across different evaluation regimes, generating valid and stable RNAs even when the decoding processed is unconstrained. It is also observed that despite having structural constraints, the validity of our generated RNAs are always slightly below 100%. This can be explained by the threshold hyperparameter which sets the maximum number of steps for topological prediction as well as the maximal length of each nucleotide segment, as shown in Algorithm 1 in appendix E.
To further demonstrate the benefits of model training from structural constraints, we sample RNAs from the prior of an untrained HierVAE model. With structural constraints, the validity amounts to 66.34% with an extremely high free energy deviation of 22.613. Without structural constraints, the validity translates to a mere 9.37% and the model can only decode short single stranded RNAs as it lacks the knowledge of constructing more complex structures. This comparison illustrates that model training is essential for obtaining stable RNA folding.
The junction tree hierarchy of RNAs developed in our work shares certain modelling similarities with the probabilistic context free grammar (Dowell & Eddy, 2004) used by covariance models (CM) (Eddy & Durbin, 1994). Infernal (Nawrocki & Eddy, 2013) is one of the representative works based on CM, which is capable of sampling RNA secondary structures from a CM built around a consensus secondary structure for a conserved RNA family. However, due to the lack of homologous sequences in our dataset, Infernal is seriously limited and can only sample single stranded RNAs.
Figure 3 illustrate RNA structures generated using HierVAE from a randomly chosen short path through the latent space. Notably, latent encoding provided by HierVAE translates smoothly in the RNA structure domain: nearby points in the latent space result in highly similar, yet different, structures. The generated structures are particularly stable for short and medium-size RNAs, and slightly less so for longer RNAs with highly complex structures. A side-by-side comparison between generated RNA secondary structures and MFE structures in Figure S3 shows that generated structures can evolve smoothly in the latent space along with their corresponding MFE structures. We also visualize neighborhoods of a Cysteine-carrying transfer RNA and a 5S ribosomal RNA in figure S4 and S5.
Supervised RNA generation. We then evaluate our generative approaches in a semi-supervised setting using seven RBP binding data sets from RNAcompete-S. First, we compare the efficacy of different representational choices while excluding the generative components, i.e. we jointly train VAE encoders followed by simple MLP classifiers on top of the latent encodings for binary classification on RBP binding.  2. Since our strategy for targeted RNA design makes use of seed molecules in the latent space, we mainly sample RNAs from the posterior distribution of these semi-supervised VAE models. Therefore, we select a KL annealing schedule that tends to retain more information in the latent encodings, i.e. setting maximum β to 5e-4 and training 10 epochs.
Results are promising in that classification AUROC measured by the held-out test set is comparable to the fully supervised classification models in Table S3, and much better compared to models only using fixed and pretrained VAE embeddings as shown in Table S2. Also, RNA structures generated from the posterior distribution, even under the setting of unconstrained and deterministic decoding, have high success rates, very stable conformation and good reconstruction accuracy. Targeted RNA design. We next studied the task of designing RNAs with high RBP binding affinity. Starting from the latent encodings of 10,000 randomly chosen RNA molecules that have negative labels in each RNAcompete-S test set, and use activation maximization to gradually alter the latent encodings so that the predicted binding probability from the embedding classifiers increases. These embedding classifiers have been trained jointly with the VAE models with accuracy reported earlier (Table 2). Then, we use separately trained full classifiers (also earlier shown in Table S3) as proxy of oracles for evaluating the ""ground truth"" probability of RBP binding. Table 3, report the success rate (fraction of RNAs whose ""ground truth"" RBP binding probability was improved), along with the average improvement in binding probabilities. An example of a trajectory of optimized RNAs is shown in Fig. S6.

[RELATED WORK]
Over the years, the field of computational drug discovery has witnessed the emergence of graphcentric approaches. One of the earliest method, proposed in Gómez-Bombarelli et al. (2018), is defined on the linearized format of molecular structures and represents a family of methods that rely on sequential models to represent and generate SMILES strings of chemical compounds. Later methods have sought to construct more chemical priors into the model, via (1) leveraging graph based representation and generation techniques, (2) enforcing direct chemical constraints to the decoding process, (3) considering a multi-scale view of the molecular structures, or (4) using reinforcement learning to integrate more training signal of the molecular structure and function. As a result, greater success has been achieved by models such as Kusner et al. (2017); ; Jin et al. (2018); You et al. (2018) at generating and searching valid and more useful chemical compounds.
Graph representation learning is at the heart of these more recent approaches, to help understand the rules governing the formation of these molecular structures, as well as the correspondence between structures and functions. Duvenaud et al. (2015) were among the first to apply GNN to learn molecular fingerprints, and the general neural message passing framework for molecules is proposed in Gilmer et al. (2017), which demonstrate the power of MPNN across various molecular benchmarking tasks. These prior works on molecular MPNN, together with other GNN architectures developed in other areas, such as considering relational edges (Schlichtkrull et al., 2018) and attention (Velickovic et al., 2018), have laid the foundation for the success of these deep generative models.
Despite the fact that RNA molecules can adopt complex structures, dedicated graph representation learning techniques have been scarce, with some recent works beginning to leverage graph related learning techniques to predict RNA folding (Chen et al., 2020;Singh et al., 2019) and to represent RNA molecular structures (Yan et al., 2020;Oliver et al., 2020). Prior to our work, the design of RNA has mostly focused on the inverse design problem, which is to conditionally generate an RNA sequence whose MFE secondary structure corresponds to an input secondary structure. Therefore, the line of prior works have predominantly relied on sequential techniques, with some representative methods based on reinforcement learning (Runge et al., 2019), or more classically framed as a combinatorial optimization problem and solved with sampling based techniques (Churkin et al., 2017). These prior works are mainly concerned with querying from an energy model with fixed thermodynamic parameters and fixed dynamics of RNA folding, which is in itself limited compared to learning based approaches (Chen et al., 2020;Singh et al., 2019), and are unable to model a joint distribution over RNA sequences and possible folds.

[CONCLUSION AND FUTURE WORKS]
In this work we propose the first graph-based deep generative approach for jointly embedding and generating RNA sequence and structure, along with a series of benchmarking tasks. Our presented work has demonstrated impressive performance at generating diverse, valid and stable RNA secondary structures with useful properties.
For future works, there are several important directions to consider. First, it would be beneficial to obtain non-coding RNA families from the RFAM database (Kalvari et al., 2017) which would help our models learn more biologically-meaningful representation indicative of RNA homology and functions, in addition to the evolutionarily conserved RNA structural motifs that would enable the generation of more stable RNA secondary structures. In that context, a detailed comparison to Infernal and other probabilistic context-free grammar models would be meaningful.
On the methodological aspect, in light of the recent advances in protein sequences pretraining across a large evolutionary-scale (Rives et al., 2019;Elnaggar et al., 2020), our models for RNAs may similarly benefit by such a procedure with the data collected from RFAM. After the pretraining step, reinforcement learning can be used to finetune the generative component of our model with customizable rewards defined jointly on RNA structural validity, folding stability and functions such as binding to certain proteins.
On the evaluation side, it would be of great interest to analyze our models for any potential RNA tertiary structural motifs and to compare them with those deposited in the CaRNAval (Reinharz et al., 2018) or RNA 3D motifs database (Parlea et al., 2016). Our models would also need modifications to allow non-canonical interactions and pseudoknots, which are common in RNA tertiary structures.
All in all, the representation, generation and design of structured RNA molecules represent a rich, promising, and challenging area for future research in computational biology and drug discovery, and an opportunity to develop fundamentally new machine learning approaches.

[A ACKNOWLEDGEMENTS]
We would like to thank all members of the Hamilton lab, Blanchette lab, and the four anonymous reviewers for their insightful suggestions. This work was funded by a Genome Quebec/Canada grant to MB and by the Institut de Valorisation des Données (IAVDO) PhD excellence scholarship to ZY. WLH is supported by a Canada CIFAR AI Chair. We also thank Compute Canada for providing the computational resources.
B BACKGROUND: RNA STRUCTURE AND KEY PROPERTIES , hence stabilizing the molecule 3 . The set of pairs of interacting nucleotides in an RNA forms its so-called RNA secondary structure. In computational analyses of RNA, it is standard to assume that a secondary structure is nested: if [i, j] and [k, l] form base pairs with i < k, then either l < j (nesting) or k > j (non-overlapping). This enables simple string or planar graph representations (Figure S1 a, b).
The nested structure assumption means that secondary structures can be modelled by a probabilistic context free grammar (Dowell & Eddy, 2004), or by the closely related junction tree structure (Figure S1 c) (Sarrazin-Gendron et al., 2020), where each hypernode corresponds to a particular secondary substructure element: (1) stem: consecutive stacked base-pairs locally forming a double-stranded structure;
(2) hairpin loop : unpaired regions closed by a base-pair;
(3) internal loop: unpaired regions located between two stems; (4) multiloop: unpaired regions at the junction of at least three stems. Edges link elements that are adjacent in the structure.
Validity and stability of RNA folding. The notion of free energy of RNA secondary structures can be used to characterize the stability of a particular conformation. Given an RNA sequence, there are combinatorially many valid RNA secondary structures which all need to obey a set of constraints (summarized in section 3.3). However, some structures are more stable than the others by having lower free energy. Therefore, these structures are more likely to exist (hence more useful) in reality due to the stochastic nature of RNA folding. The free energy of an RNA secondary structure can be estimated by an energy-based model with thermodynamic parameters obtained from experiments (Mathews et al., 2004), wherein the minimum free energy (MFE) structure can be predicted, up to a reasonable approximation (Lorenz et al., 2011). 4 

[C DATASET AND METRICS]
The unlabeled dataset is obtained from the complete human transcriptome which is downloaded from the Ensembl database (Aken et al. (2016); version GRCh38). We slice the transcripts into snippets with length randomly drawn between 32 and 512 nts, and use RNAfold to obtain the MFE structures. We randomly split the dataset into a training set that contains 1,149,859 RNAs, and 20,000 held-out RNAs for evaluating decoding from the posterior distribution. More information on the structural diversity and complexity of this dataset is shown in Figure S2, which should present significant challenges for our algorithms.
The labeled dataset is pulled from a previous study on sequence and structural binding preference of RNA binding proteins (RBP), using an in vitro selection protocol called RNAcompete-S (Cook et al., 2017) which generates synthesized RNA sequences bound or unbound to a given RBP. RNAs in this experiment are of uniform length, i.e. 40 nts, and offer a rich abundance of RNA secondary structures compared to its predecessor protocols such as RNAcompete (Ray et al., 2009;2013). Since no benchmark has been ever established since its publication, we randomly sample 500,000 positive sequences bound to an RBP, and the same amount of negative sequences from the pool of unbound sequences, to curate a dataset for each of the seven RBPs investigated in the paper. Then, 80% of all RNAs are randomly selected to the train split, and the rest goes to the test split.
Our evaluation scheme for the generated RNA secondary structures includes the following metrics:
• validity: percentage of generated RNA secondary structures that conform to the structural constraints specified in section 3.3. • free energy deviation (FE DEV): difference of free energy between the generated RNA secondary structure and the MFE structure of the corresponding sequence, which quantifies the gap of both structures from an energy perspective. A lower FE DEV should indicate higher stability of generated RNAs. • free energy deviation normalized by length (Normed FE DEV): FE DEV divided by the length of generated RNA, which distributes the contribution of total FE DEV to each base. • 5-mer sequence diversity: entropy of the normalized counts of 5-mer substrings, which directly measures the diversity of RNA sequences, and indirectly for RNA secondary structures when this metric is combined with FE DEV, since monolithic structures of diverse sequences would lead to high FE DEV.

[D TREE ENCODING GRU]
Following Eq.3, T-GRU computes a new message v tĜ i,Ĝj fromĜ i andĜ j , based on the features in G i denoted by xĜ i , as well as neural messages from neighboring subgraphs toĜ i , i.e. {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}. The internal structure of T-GRU is equivalent to the tree encoder employed in Jin et al. (2018), which is essentially a neural analogue of the belief propagation algorithm on junction trees. Nevertheless, we write down the message passing formulas of T-GRU here:
sĜ i,Ĝj = Ĝ k ∈N (Ĝi) v t−1 G k ,Ĝi(S1)
zĜ i,Ĝj = σ(W z [xĜ i || sĜ i,Ĝj ] + b z ) (S2) rĜ k ,Ĝi = σ(W r [xĜ i || v t−1 G k ,Ĝi ] + b r ) (S3) vĜ i,Ĝj = Tanh(W [xĜ i || Ĝ k ∈N (Ĝi) rĜ k ,Ĝi • v t−1 G k ,Ĝi ]) (S4) v tĜ i,Ĝj = (1 − zĜ i,Ĝj ) sĜ i,Ĝj + zĜ i,Ĝj vĜ i,Ĝj(S5)
E ALGORITHM FOR HIERARCHICALLY DECODING STRUCTURED RNA a M TI refers to the threshold which set the maximum allowed number of topological prediction steps; M SI is another threshold to limit the length of each decoded nucleotide segment.

[H DETAILS FOR PARAMETERIZING PRIOR DISTRIBUTION USING NORMALIZING FLOW]
A normalizing flow involves a series of bijective transformation with tractable Jacobian logdeterminant, to map an observed datapoint x ∼ p θ (x) from a complex distribution to a simpler one, such as the standard normal distribution.
Considering the simplified case where we have a single bijective function f θ : Z → X to map some simple latent variables z to observed datapoint x, then, using the change of variable theorem, the likelihood of the observed datapoint can be evaluated as:
p θ (x) = p z (f −1 θ (x))|det ∂f −1 θ (x) ∂x | (S6)
where p z (.) denotes some simple base distribution, e.g. N (0; I). Then, it becomes clear the efficiency of this scheme heavily relies on the efficiency of inverting the forward mapping f θ as well as computing its Jacobian log-determinant.
In this project, we use a type of continuous normalizing flow (CNF) which simplifies the above mentioned computation (Chen et al., 2018). Consider a time continuous dynamics f ψ (z(t), t) of some intermediate data representation z(t), and again z(t 0 ) ∼ p z (.), the transformation of variable, along with its inverse mapping, can be expressed as:
z z(t 1 ) = z(t 0 ) + t1 t0 f ψ (z(t), t)dt (S7) z(t 0 ) = z(t 1 ) + t0 t1 f ψ (z(t), t)dt (S8)
and the change of probability density can be expressed as:
log p ψ (z) = log p z (z(t 0 )) − t1 t0 tr( ∂f θ ∂z(t) )dt (S9)
Note that the invertibility issue is no longer a concern under some mild constraints (Chen et al., 2018). Also, Eq. S9 only involves a more light-weight trace operation on the Jacobian rather than evaluating its log-determinant.
Therefore, we learn a parameterized prior using a CNF, and observe the decomposition of the KL term in the VAE objective:
KL(q φ (z|x)|p ψ (z)) = −E z∼q φ (z|x) [p ψ (z)] − H[q φ (z|x)](S10)
Therefore, during training our CNF parameterized with ψ works on the transformation of complex latent encodings z ∼ q φ (z|x) to some simple z(t 0 ) ∼ N (0; I), with an exact likelihood described by Eq. S9 and integrated into Eq. S10 for the complete training objective. During inference, we simply sample z t0 ∼ N (0; I), and use our CNF to reversely transform it to z ∼ p ψ (.) which should be closer to the approximate posterior. J HYPERPARAMETERS  The optimization takes place in the latent space of HierVAE, starting from the initial encoding of a random RNA molecule in the test set, and at each step altering the latent encoding by using activation maximization on the embedding classifier. The trajectory of generated RNAs is shown in the order of left to right and top to bottom, and the field PRED indicates that the probability of binding, as predicted by another external full classifier on the decoded molecular structure, is overall increasing as the decoded RNA structures smoothly evolving.

[F DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO LINEARIZED DECODING PROCEDURES]
When decoding from the joint vocabulary of sequence and dot-bracket structure ({A, C, G, U } × {., (, )}), whenever a nucleotide nuc i with a left bracket is sampled at step i, we append them to a stack, i.e. {(nuc i0 , i 0 ) . . . (nuc i , i)}. Then, at decode step j,
• if |i − j| ≤ 3, a proper mask will be added to the categorical logits of the vocabulary, to avoid sampling any nucleotides with right brackets, which means only an unpaired nucleotide or one that comes with a left bracket can be sampled; • if |i − j| > 3, a mask will be applied to make sure that only a nucleotide complementary to nuc i can be sampled with the right bracket. Sampling nucleotides with other forms of structures are allowed.
As soon as a nucleotide with a closing right bracket is sampled, we pop out (nuc i , i) from the stack. The special symbol for stop decoding can only be sampled when the stack has become empty.

[G DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO HIERARCHICAL DECODING PROCEDURES]
Additional constraints to be enforced during the hierarchical decoding process to ensure the validity of the decoded RNA secondary structure. Recall in section 3.2 that three types of predictions are involved with the hierarchical decoding, therefore, each type is associated with its own set of rules.
All set of rules can be observed by adding proper masks to the categorical logits before sampling, which are detailed below.
Constraints for making topological prediction, when the current node is
• stem node, then the algorithm always expands to a new node upon its first visit, or backtracks to its parent node upon re-visit; • hairpin node, then the algorithm always backtracks; • internal loop, then the algorithm acts similarly as for stem node; • multi-loop, then the algorithm always expands upon first visit and the next re-visit. Further re-visits to the same multi-loop node are not regulated.
Constraints for predicting new tree node, when the current node is • stem node, then its child node when exists can be either a hairpin loop, an internal loop, or a multi-loop; • hairpin node, internal loop or multi-loop, then its child node must be a stem node.
Constraints for decoding nucleotide segment. Due to the property of non-empty intersection between adjacent subgraphs, the start token for decoding a segment at the current node, is always the last nucleotide decoded at the last node. Therefore, without explicitly mentioning, the algorithm needs to decode at least one new nucleotide at each segment. When the current node is • stem node, and if it is upon its first visit (i.e. decoding the first segment of a stem), then there is no for constraints. Otherwise, upon its re-visit, the algorithm needs to decode exactly the complementary bases and in the reverse order, according to the first decoded segment; • hairpin node, then the decoder needs to decode at least four nucleotides before seeing the stop symbol, unless the hairpin is also the root node. • internal loop node, and if it is upon its first, then constraint is not necessary. Otherwise, upon its revisit, the algorithm needs to decode at least one unpaired nucleotide on condition that the first decoded internal loop segment does not contain any unpaired nucleotides; • multi-loop node, then there is no need for constraints. HuR 0.880 ± 0.000 0.880 ± 0.000 0.880 ± 0.000 0.888 ± 0.002 PTB 0.900 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 QKI 0.820 ± 0.000 0.830 ± 0.000 0.825 ± 0.002 0.830 ± 0.000 Vts1 0.900 ± 0.000 0.908 ± 0.002 0.637 ± 0.079 0.910 ± 0.000 RBMY 0.905 ± 0.002 0.880 ± 0.003 0.802 ± 0.055 0.870 ± 0.002 SF2 0.890 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 SLBP 0.777 ± 0.002 0.790 ± 0.000 0.797 ± 0.002 0.797 ± 0.002","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules."
Neural representation and generation for RNA secondary structures,snOgiCYZgJ7.json,"##########################################################################

Summary:

 
This paper sheds light into an impactful problem of neural representation and generation for RNA secondary structures. Authors presented benchmark tasks in unsupervised, semi-supervised and targeted generation setting and presented deep generative models to solve this tasks. They presented three different generative models using variational auto encoders based on sequence, graph and hierarchical representation. 


##########################################################################

Pros:
 
1. The paper deals with a very important problem of neural representation and generation for RNA secondary structures. I think this area needs a lot more work to help solving real-world biological problems.

 
2. As far as I can tell their method is novel in terms of idea. Generation of RNAs are not trivial. It requires extra attention to detail in terms of checking for validity and stability in folding. There are also diverse families of RNA which are very different from each other. In this work, the validity and stability constraints are implemented inside a depth first search traversal of building the RNA. Without these constraints its much harder to learn the structures of the RNA from embedding only as it is evident from the result on the unsupervised setting as well. In supervised setting, the authors showed their models efficacy in RBP datasets. Related works also contains recent works in RNA.
 
3. Overall the paper is well written. I liked the illustrations for explaining the methods. Result section is also well structured. It clearly shows the effectiveness of the generative model used.
 
##########################################################################

Cons: 

 
1. Is it possible to use this generation technique to use in solving problems like RNA folding or targeted RNA design? Specially the authors mentioned RL based method from (Runge et al., 2019). Is it possible to use this generative model and combine it with the existing RL techniques for RNA design or ML techniques for folding to show improvement in their result? That would definitely increase the impact of this work.

2. Is it also feasible to learn the structural constraints from the data? Can we get rid of the dependency on constraints with more automated learning?

3. I am also interested to know if we can compare it against any discriminative learning based baseline for any task. For example a recent work from (Yan et. al 2020) for RNA protein interaction.

##########################################################################

Questions during rebuttal period: 

 
Please address and clarify the cons above 

 
#########################################################################

","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Hierarchical encoding. Panel (A)shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
Table 2: (A), to determine if the decoder should expand to a new tree node or backtrack to its parent node, based on MLP topo (hĜ t,i ); • tree node prediction in Figure 2 (B), on condition that a new tree node is needed due to a possible topological expansion. This procedure determines the label of the new tree node from the set of {S, H, I, M }, based on MLP node (hĜ t,i ); • nucleotide segment decoding in Figure 2 (C), using a single-layered LSTM, whose initial hidden state is MLP dec
Table 3: Figure 3 :3Figure 3: RNAs generated with structural constraints from HierVAE on a random axis in the latent space (step size: 1e-4), for short (top), medium-length (middle), and long (bottom) RNAs. The Free energy (FE) and its deviation (DEV) from the MFE are given for each structure. which means only the canonical base-pairs and Wobble base-pairs are allowed, i.e. [A-U], [G-C] and [G-U];(2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3; (3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
Table 4: Figure S1 :S1FigureS1: A nested RNA secondary structure can be represented by: (A) dotbracket annotation, where base-pairs corresponding to matching parentheses, or (B) a molecular planar graph with two types of edges, corresponding to consecutive nucleotides (backbone) and basepairing interactions, or (C) a junction tree where node are labeled as stems (S), hairpins (H), internal loops (I), or multiloops (M), and edges correspond to the connections between these elements. All three forms are equivalent.
Table 5: Figure S2: This figure contains information of the unlabeled RNA dataset. (A) The number of hypernodes appears to grow linearly with the length of RNA, and (B) the junction tree height also grows as the length increases but on a more moderate scale. (C) and (D) have shown bar-plots of the number of hypernodes and tree height, indicating that the junction tree of RNA can take on significant depth hence contributing to the diversity and complexity of RNA secondary structures represented in this dataset.
Table 6: Figure S4 :S4Figure S4: Neighborhood visualization of tRNA-Cys 6 which is marked by the red bounding box in the center and the walk in the latent space takes place on two random orthogonal axes. Note that actual secondary structure of tRNA-Cys plotted in the figure is different compared to the one deposited online due to the prediction of RNAfold.
Table 7: We evaluate RNAs sampled from the posterior distribution: q(z|x), with a held-out test set of 20,000 RNAs. Each molecule is encoded and decoded 5 times. We also evaluate samples from the prior distribution: N (0, I) subject to the transformation of a latent CNF, where we sample 10,000 encodings and each encoding is decoded 10 times. Normed refers to length normalized FE DEV.
Table 8: Training semi-supervised HierVAE on labeled RNAcompete-S dataset. A test split is used to evaluate the accuracy of embedding classifiers and RNAs decoded from the posterior distribution under two settings: constrained and stochastic (C& S), unconstrained and deterministic (NC&D). RECON ACC refers to reconstruction accuracy which measures the percentage of RNA molecules decoded exactly as the input.
Table 9: 
Table 10: Algorithm 1: DFS decode RNA secondary structure1 Given: z T , z G , M TI, M SI a 2 Initialize: stack ← [ ] 3 function decode(z T , z G ) 4 root ← sample(MLP node (z T )) ;
Table 11: Hyperparameters for training VAE and full classifier models. Note that hidden units refer to the dimensionality of encoders and decoders from LSTMVAE, GraphVAE as well as HierVAE models. Dropout is applied to the embedding MLP classifier in case of training semi-supervised VAEs, which contains one hidden layer. Performance of simple MLP classifiers on top of fixed latent embeddings from VAE models, which have been pretrained on the unlabeled RNA dataset as originally shown in Table1.

[INTRODUCTION]
There is an increasing interest in developing deep generative models for biochemical data, especially in the context of generating drug-like molecules. Learning generative models of biochemical molecules can facilitate the development and discovery of novel treatments for various diseases, reducing the lead time for discovering promising new therapies and potentially translating in reduced costs for drug development (Stokes et al., 2020). Indeed, the study of generative models for molecules has become a rich and active subfield within machine learning, with standard benchmarks (Sterling & Irwin, 2015), a set of well-known baseline approaches (Gómez-Bombarelli et al., 2018;Kusner et al., 2017;Jin et al., 2018), and high-profile cases of real-world impact 1 .
Prior work in this space has focused primarily on the generation of small molecules (with less than 100 atoms), leaving the development of generative models for larger and more complicated biologics and biosimilar drugs (e.g., RNA and protein peptides) an open area for research. Developing generative models for larger biochemicals is critical in order to expand the frontiers of automated treatment design. More generally, developing effective representation learning for such complex biochemicals will allow machine learning systems to integrate knowledge and interactions involving these biologically-rich structures.
In this work, we take a first step towards the development of deep generative models for complex biomolecules, focusing on the representation and generation of RNA structures. RNA plays a crucial role in protein transcription and various regulatory processes within cells which can be influenced by its structure (Crick, 1970;Stefl et al., 2005), and RNA-based therapies are an increasingly active area of research (Pardi et al., 2018;Schlake et al., 2012), making it a natural focus for the development of deep generative models. The key challenge in generating RNA molecules-compared to the generation of small molecules-is that RNA involves a hierarchical, multi-scale structure, including a primary sequential structure based on the sequence of nucleic acids as well as more complex secondary and tertiary structures based on the way that the RNA strand folds onto itself. An effective generative model for RNA must be able to generate sequences that give rise to these more complex emergent structures.
There have been prior works on optimizing or designing RNA sequences-using reinforcement learning or blackbox optimization-to generate particular RNA secondary structures (Runge et al., 2019;Churkin et al., 2017). However, these prior works generally focus on optimizing sequences to conform to a specific secondary structure. In contrast, our goal is to define a generative model, which can facilitate the sampling and generation of diverse RNA molecules with meaningful secondary structures, while also providing a novel avenue for targeted RNA design via search over a tractable latent space.
Key contributions. We propose a series of benchmark tasks and deep generative models for the task of RNA generation, with the goal of facilitating future work on this important and challenging problem. We propose three interrelated benchmark tasks for RNA representation and generation:
1. Unsupervised generation: Generating stable, valid, and diverse RNAs that exhibit complex secondary structures. 2. Semi-supervised learning: Learning latent representations of RNA structure that correlate with known RNA functional properties. 3. Targeted generation: Generating RNAs that exhibit particular functional properties.
These three tasks build upon each other, with the first task only requiring the generation of stable and valid molecules, while the latter two tasks involve representing and generating RNAs that exhibit particular properties. In addition to proposing these novel benchmarks for the field, we introduce and evaluate three generative models for RNA. All three models build upon variational autoencoders (VAEs) (Kingma & Welling, 2014) augmented with normalizing flows (Rezende & Mohamed, 2015;Kingma et al., 2016), and they differ in how they represent the RNA structure. To help readers better understand RNA structures and properties, a self-contained explanation is provided in appendix B.
The simplest model (termed LSTMVAE) learns using a string-based representation of RNA structure. The second model (termed GraphVAE) leverages a graph-based representation and graph neural network (GNN) encoder approach (Gilmer et al., 2017). Finally, the most sophisticated model (termed HierVAE) introduces and leverages a novel hierarchical decomposition of the RNA structure. Extensive experiments on our newly proposed benchmarks highlight how the hierarchical approach allows more effective representation and generation of complex RNA structures, while also highlighting important challenges for future work in the area.

[TASK DESCRIPTION]
Given a dataset of RNA molecules, i.e. sequences of nucleotides and corresponding secondary structures, our goals are to: (a) learn to generate structurally stable, diverse, and valid RNA molecules that reflect the distribution in this training dataset; (b) learn latent representations that reflect the functional properties of RNA. A key factor in both these representation and generation processes is that we seek to jointly represent and generate both the primary sequence structure as well as the secondary structure conformation. Together, these two goals lay the foundations for generating novel RNAs that satisfy certain functional properties. To meet these goals, we create two types of benchmark datasets, each one focusing on one aspect of the above mentioned goals:
Unlabeled and variable-length RNA. The first dataset contains unlabeled RNA with moderate and highly-variable length (32-512 nts), obtained from the human transcriptome (Aken et al., 2016) and through which we focus on the generation aspect of structured RNA and evaluate the validity, stability and diversity of generated RNA molecules. In particular, our goal with this dataset is to jointly generate RNA sequences and secondary structures that are biochemically feasible (i.e., valid), have low free energy (i.e., stable), and are distinct from the training data (i.e., diverse). We will give an extended assessment of the generation aspect under different circumstances, e.g., when constraining the generation procedures with explicit rules.
Labeled RNA. The second dataset is pulled and processed from a previous study on in vitro RNAprotein interaction, which features labeled RNAs with shorter and uniform length (40 nts) (Cook et al., 2017). With this dataset, our objective is slightly expanded (to include obj. a), so that the latent space is adequately organized and reflective of the interaction with proteins. Therefore, key assessment for the latent space includes AUROC for the classification of protein binding, which is crucial for the design of desired novel RNA molecules.
Essentially, this creates slight variations in the task formulation, with the first dataset suited to unsupervised learning of a generative model, while the second datasets involves additional supervision (e.g., for a semi-supervised model or targeted generation). Our specific modeling choices, to be introduced in section 3, are invariant to different task formulations, and flexible enough to handle different representations of RNA secondary structures. We refer readers to appendix C for detailed explanation for the dataset and evaluation metrics on the generated molecules and latent embeddings.

[METHODS]
In this section, we introduce three different generative models for RNA. All three models are based upon the variational autoencoder (VAE) framework, involving three key components:
1. A probabilistic encoder network q φ (z|x), which generates a distribution over latent states given an input representation of an RNA. We experiment with three different types of input encodings for RNA sequence and secondary structures (see Figure S1: a dot-bracket annotated string, a graph with adjacency matrix representing base-pairings, and a graph augmented with a hierarchical junction tree annotation for the secondary structure.
2. A probabilistic decoder network p θ (x|z), which defines a joint distribution over RNA sequences and secondary structures, conditioned on a latent input. As with the encoder network, we design architectures based on a linearized string decoding and a graph-based hierarchical junction-tree decoding approach.
3. A parameterized prior p ψ (z), which defines a prior distribution over latent states and is learned based on a continuous normalizing flow (CNF) (Chen et al., 2018). shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
For all the approaches we propose, the model is optimized via stochastic gradient descent to minimize the evidence lower bound (ELBO):
L = −E q φ (z|x) [p θ (x|z)] + β KL(q φ (z|x)|p ψ (z))
where β is a term to allow KL-annealing over the strength of the prior regularization.
In the following sections, we explain our three different instantiations of the encoder (section 3.1), decoder (section 3.2), as well as our procedures to structurally constrain the decoding process using domain knowledge (section 3.3) and our procedures to avoid posterior collapse (section 3.4).

[ENCODING RNA SECONDARY STRUCTURES]
The input to the encoder is a structured RNA molecule, with its sequence given by an ordered array of nucleotides x 1 . . . x L , with x i ∈ {A, C, G, U }, where L is the length of the sequence, and its secondary structure, either represented as (1) a dot-bracket string S =ẋ 1 . . .ẋ L withẋ i ∈ {., (, )};
(2) or as a graph G with two types of edges -covalent bonds along the RNA backbone, and hydrogen bonds between the base-pairs 2 . We use x uv to denote edge features between nucleotides u and v;
(3) or as a hypergraph T -a depth-first ordered array of subgraphsĜ 1 . . .Ĝ D with L(Ĝ i ) ∈ {S, H, I, M } indicating the subgraph label, and I(Ĝ i ) = {j|j ∈ {1 . . . L}} indicating the assignment of nucleotides to each subgraph.
Encoding RNA secondary structure as sequence. First, we obtain a joint encoding over the nucleotide and the dot-bracket annotation, using the joint sequence-structure vocabulary {A, C, G, U } × {., (, )}. Then, these one-hot encodings are processed by a stacked bidirectional LSTM (Hochreiter & Schmidhuber, 1997), followed by a multi-head self-attention module (Vaswani et al., 2017) to weigh different positions along the RNA backbone. A global max-pooling is used to aggregate the information into h S , and then we obtain mean µ S and log variance log σ S from h S through linear transformations, and draw latent encoding z S from N (µ S , σ S ) using the reparameterization trick (Kingma & Welling, 2014).
Learning graph representation of RNA secondary structure. To encode the graph view G of an RNA secondary structure, we pass rounds of neural messages along the RNA structure, which falls into the framework of Message Passing Neural Network (MPNN) as originally discussed in Gilmer et al. (2017) and similarly motivated by Jin et al. (2018).
For much longer RNAs, it is conceptually beneficial to pass more rounds of messages so that a nucleotide may receive information on its broader structural context. However, this may introduce undesired effects such as training instability and over-smoothing issues. Therefor , we combine our MPNN network with gating mechanism, which is collectively referred as the G-MPNN:
v t−1 uv = σ(W g local [x u || x uv ] + W g msg w∈N (u) v t−1 wu ) (1) v t uv = GRU(v t−1 uv , v t−1 uv )(2)
where [. . . || . . . ] denotes concatenation, σ denotes the activation function and GRU indicates the gated recurrent unit (Cho et al., 2014). Then, after T iterations of message passing, the final nucleotide level embedding is given by:
h u = σ(W g emb [x u || v∈N (u) v T vu ]
). Before pooling the nucleotide level embeddings into the graph level, we pass h 1 . . . h L through a single bidirectional LSTM layer, obtainingĥ 1 . . .ĥ L at each step, and h g = max({ĥ i |i ∈ 1...L}). The latent encoding z G is similarly obtained from h G using the reparameterization trick.
Hierarchical encoding of the RNA hypergraph. To encode the junction tree T of RNA, we employ a type of GRU specifically suited to tree-like structures, which has previously been applied in works such as GGNN (Li et al., 2016) and JTVAE (Jin et al., 2018). We refer to this tree encoding network as T-GRU, and the format of its input is shown in Figure 1.
One major distinction between our RNA junction tree and the one used for chemical compounds (Jin et al., 2018) is that an RNA subgraph assumes more variable nucleotide composition such that it is impossible to enumerate based on the observed data. Therefore, we need to dynamically compute the features for each node in an RNA junction tree based on its contained nucleotides, in a hierarchical manner to leverage the nucleotide level embeddings learnt by G-MPNN.
Considering a subgraphĜ i in the junction tree T , we initialize its node feature with:
xĜ i = [L(Ĝ i ) || max u∈I(Ĝi) h u ].
Notably, max u∈Ĝi h u is a max-pooling over all nucleotides assigned toĜ i , and nucleotide embedding h u comes from G-MPNN. To compute and pass neural messages between adjacent subgraphs in the RNA junction tree T , we use the T-GRU network in Eq.3
v tĜ i,Ĝj = T-GRU(xĜ i , {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}) (3) hĜ i = σ(W t emb [xĜ i || Ĝ ∈N (Ĝi) hĜ]) (4)
with details of T-GRU provided in the appendix D, and compute the embeddings for subgraphs with Eq. 4. Further, we obtain a depth-first traversal of the subgraph embeddings hĜ 1 . . . hĜ D which is also the order for hierarchical decoding to be discussed later. This ordered array of embeddings is processed by another bi-directional LSTM , and the final tree level representation h T is again given by the max-pooling over the bi-LSTM outputs. Likewise, latent encoding z T is obtained from h T .

[RNA MOLECULAR GENERATION]
Decoding linearized sequence and structure. In this setting, the decoder simply autoregressively decodes a token at each step, from the joint sequence-structure vocabulary mentioned before in section 3.1, plus one additional symbol to signal the end of decoding. To simplify the design choice, we use a single-layered forward-directional LSTM, and its hidden state is initialized with the latent encoding z, which can be either z S , z G or z T .
Figure 2: Hierarchical decoding of a structured RNA, involving three types of predictions, that are on the topological level, node level, and nucleotide level. These three types of prediction are interleaved into the procedures of decoding the junction tree structure of RNA and the nucleotide segments.
Hierarchically decoding hypergraph and nucleotide segments. The input to this more sophisticated hierarchical decoder are latent encodings z G which contains order and basic connectivity information of the nucleotides, and z T which contains higher order information about the arrangements of nucleotide branches and their interactions. We give a concise description of the decoding procedures here, along with a detailed algorithm in appendix E. On a high level, we hierarchically decode the tree structure in a depth-first manner, and autoregressively generate a nucleotide segment for each visited tree branch. For these purposes, we interleave three types of prediction (Figure 2).
Denote the current tree node at decode step t and at the i-th visit asĜ t,i , whose features include (1) its node label L(Ĝ t,i ) and, (2) a summary over the already existing i − 1 nucleotide segments max{h l,j u | u ∈Ĝ t,i and l < t and j < i}, with l denoting the nucleotide is decoded at step l, and j indicating the nucleotide belongs to the j-th branch (this feature is simply zeros when i = 1). Then, its local feature xĜ t,i is defined as the concatenation of (1) and (2).
We make use of a notion called node state: hĜ t,i , which is obtained by: hĜ t,i = T-GRU(xĜ t,i , {vĜ ,Ĝ t,i |Ĝ ∈ N (Ĝ t,i )}). Note its similarity to Eq. 3, and hĜ t,i is used to make:
• topological prediction in Figure 2 
([hĜ t,i || z T || z G ])
. The start token is the last nucleotide from the last segment.
Our hierarchical decoder starts off by predicting the label of the root node using z T , followed by topological prediction on the root node and decoding the first nucleotide segment. The algorithm terminates upon revisiting the root node, topologically predicted to backtrack and finishing the last segment of the root node. The decoded junction tree naturally represents an RNA secondary structure that can be easily transformed to the dot-bracket annotation, and the RNA sequence is simply recovered by connecting nucleotide segments along the depth-first traversal of the tree nodes.

[STRUCTURALLY CONSTRAINED DECODING]
To better regulate the decoding process so that generated RNAs have valid secondary structures, a set of constraints can be added to the decoding procedures at the inference stage. Essentially, a valid RNA secondary structure needs to observe the following rules: (1) base-pairing complementarity, (2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3;
(3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
We will translate the above rules into specific and applicable constraints, depending on specific decoders. For the sake of space, we only give a broad remark and leave more details in the appendix.
Linearized decoding constraints. Since the linearized decoder simply proceeds in an autoregressive fashion, constraints can be easily enforced in a way that at each step, a nucleotide with an appropriate structural annotation is sampled by making use of masks and re-normalizing the probabilities. Likewise, a stop token can only sampled when all opening nucleotides have been closed. More details to follow in appendix F.
Hierarchical decoding constraints. The specific set of constraints for hierarchical decoding is discussed in appendix G. Overall, considering the different natures of the three associated types of prediction, each one should require a set of different strategies, which are once again applicable by adding proper masks before sampling. As shown in the algorithm in appendix E, the set of constraints are applied to line 13, 24 and 14 with marked asterisk.

[AVOIDING POSTERIOR COLLAPSE]
As discussed in a line of previous works, VAEs with strong autoregressive decoders are susceptible to posterior collapse, an issue where the decoder simply ignores the latent encoding of the encoder (He et al., 2019). Therefore, to avoid posterior collapsing, we make use of a carefully chosen KL annealing schedule during training to help the encoder adapt its information content in the latent encoding and in coordination with the decoder. This schedule is detailed in section 4. We also learn a parameterized prior as suggested in Chen et al. (2017), but using a CNF instead, following a similar implementation to Yang et al. (2019), with details given in appendix H.
Our KL annealing schedule is chosen based on empirical observations, as to our knowledge, there has yet to exist any principled methods of selecting such schedule. We have used diagnostic metrics such as mutual information (He et al., 2019) and active units (Burda et al., 2016) along with a validation set to select a proper KL annealing schedule which is to be described later in section 4

[RESULTS]
We consider three modes of evaluation: (1) unsupervised RNA generation; (2) generation using semi-supervised VAE models and (3) targeted RNA design from an organized latent space. Results are presented below, and relevant hyperparameters can be found in Table S1.
Unsupervised RNA generation. Here, we evaluate generated RNAs from models trained on the unlabeled RNA dataset for 20 epochs using a KL annealing schedule including 5 epochs of warm-up, followed by gradually increasing the KL annealing term to 3e-3 (for LSTMVAE and GraphVAE), or 2e-3 (for HierVAE). The KL annealing schedule was chosen using a validation set of 1,280 RNAs.
Table 1 compares the generation capability of different models, from the posterior as well as the prior distribution, and in scenarios such as applying structural constraints to the decoding process or not.
It clearly shows that our most advanced model, HierVAE which employs a hierarchical view of the structure in its encoding/decoding aspects, achieves the best performance across different evaluation regimes, generating valid and stable RNAs even when the decoding processed is unconstrained. It is also observed that despite having structural constraints, the validity of our generated RNAs are always slightly below 100%. This can be explained by the threshold hyperparameter which sets the maximum number of steps for topological prediction as well as the maximal length of each nucleotide segment, as shown in Algorithm 1 in appendix E.
To further demonstrate the benefits of model training from structural constraints, we sample RNAs from the prior of an untrained HierVAE model. With structural constraints, the validity amounts to 66.34% with an extremely high free energy deviation of 22.613. Without structural constraints, the validity translates to a mere 9.37% and the model can only decode short single stranded RNAs as it lacks the knowledge of constructing more complex structures. This comparison illustrates that model training is essential for obtaining stable RNA folding.
The junction tree hierarchy of RNAs developed in our work shares certain modelling similarities with the probabilistic context free grammar (Dowell & Eddy, 2004) used by covariance models (CM) (Eddy & Durbin, 1994). Infernal (Nawrocki & Eddy, 2013) is one of the representative works based on CM, which is capable of sampling RNA secondary structures from a CM built around a consensus secondary structure for a conserved RNA family. However, due to the lack of homologous sequences in our dataset, Infernal is seriously limited and can only sample single stranded RNAs.
Figure 3 illustrate RNA structures generated using HierVAE from a randomly chosen short path through the latent space. Notably, latent encoding provided by HierVAE translates smoothly in the RNA structure domain: nearby points in the latent space result in highly similar, yet different, structures. The generated structures are particularly stable for short and medium-size RNAs, and slightly less so for longer RNAs with highly complex structures. A side-by-side comparison between generated RNA secondary structures and MFE structures in Figure S3 shows that generated structures can evolve smoothly in the latent space along with their corresponding MFE structures. We also visualize neighborhoods of a Cysteine-carrying transfer RNA and a 5S ribosomal RNA in figure S4 and S5.
Supervised RNA generation. We then evaluate our generative approaches in a semi-supervised setting using seven RBP binding data sets from RNAcompete-S. First, we compare the efficacy of different representational choices while excluding the generative components, i.e. we jointly train VAE encoders followed by simple MLP classifiers on top of the latent encodings for binary classification on RBP binding.  2. Since our strategy for targeted RNA design makes use of seed molecules in the latent space, we mainly sample RNAs from the posterior distribution of these semi-supervised VAE models. Therefore, we select a KL annealing schedule that tends to retain more information in the latent encodings, i.e. setting maximum β to 5e-4 and training 10 epochs.
Results are promising in that classification AUROC measured by the held-out test set is comparable to the fully supervised classification models in Table S3, and much better compared to models only using fixed and pretrained VAE embeddings as shown in Table S2. Also, RNA structures generated from the posterior distribution, even under the setting of unconstrained and deterministic decoding, have high success rates, very stable conformation and good reconstruction accuracy. Targeted RNA design. We next studied the task of designing RNAs with high RBP binding affinity. Starting from the latent encodings of 10,000 randomly chosen RNA molecules that have negative labels in each RNAcompete-S test set, and use activation maximization to gradually alter the latent encodings so that the predicted binding probability from the embedding classifiers increases. These embedding classifiers have been trained jointly with the VAE models with accuracy reported earlier (Table 2). Then, we use separately trained full classifiers (also earlier shown in Table S3) as proxy of oracles for evaluating the ""ground truth"" probability of RBP binding. Table 3, report the success rate (fraction of RNAs whose ""ground truth"" RBP binding probability was improved), along with the average improvement in binding probabilities. An example of a trajectory of optimized RNAs is shown in Fig. S6.

[RELATED WORK]
Over the years, the field of computational drug discovery has witnessed the emergence of graphcentric approaches. One of the earliest method, proposed in Gómez-Bombarelli et al. (2018), is defined on the linearized format of molecular structures and represents a family of methods that rely on sequential models to represent and generate SMILES strings of chemical compounds. Later methods have sought to construct more chemical priors into the model, via (1) leveraging graph based representation and generation techniques, (2) enforcing direct chemical constraints to the decoding process, (3) considering a multi-scale view of the molecular structures, or (4) using reinforcement learning to integrate more training signal of the molecular structure and function. As a result, greater success has been achieved by models such as Kusner et al. (2017); ; Jin et al. (2018); You et al. (2018) at generating and searching valid and more useful chemical compounds.
Graph representation learning is at the heart of these more recent approaches, to help understand the rules governing the formation of these molecular structures, as well as the correspondence between structures and functions. Duvenaud et al. (2015) were among the first to apply GNN to learn molecular fingerprints, and the general neural message passing framework for molecules is proposed in Gilmer et al. (2017), which demonstrate the power of MPNN across various molecular benchmarking tasks. These prior works on molecular MPNN, together with other GNN architectures developed in other areas, such as considering relational edges (Schlichtkrull et al., 2018) and attention (Velickovic et al., 2018), have laid the foundation for the success of these deep generative models.
Despite the fact that RNA molecules can adopt complex structures, dedicated graph representation learning techniques have been scarce, with some recent works beginning to leverage graph related learning techniques to predict RNA folding (Chen et al., 2020;Singh et al., 2019) and to represent RNA molecular structures (Yan et al., 2020;Oliver et al., 2020). Prior to our work, the design of RNA has mostly focused on the inverse design problem, which is to conditionally generate an RNA sequence whose MFE secondary structure corresponds to an input secondary structure. Therefore, the line of prior works have predominantly relied on sequential techniques, with some representative methods based on reinforcement learning (Runge et al., 2019), or more classically framed as a combinatorial optimization problem and solved with sampling based techniques (Churkin et al., 2017). These prior works are mainly concerned with querying from an energy model with fixed thermodynamic parameters and fixed dynamics of RNA folding, which is in itself limited compared to learning based approaches (Chen et al., 2020;Singh et al., 2019), and are unable to model a joint distribution over RNA sequences and possible folds.

[CONCLUSION AND FUTURE WORKS]
In this work we propose the first graph-based deep generative approach for jointly embedding and generating RNA sequence and structure, along with a series of benchmarking tasks. Our presented work has demonstrated impressive performance at generating diverse, valid and stable RNA secondary structures with useful properties.
For future works, there are several important directions to consider. First, it would be beneficial to obtain non-coding RNA families from the RFAM database (Kalvari et al., 2017) which would help our models learn more biologically-meaningful representation indicative of RNA homology and functions, in addition to the evolutionarily conserved RNA structural motifs that would enable the generation of more stable RNA secondary structures. In that context, a detailed comparison to Infernal and other probabilistic context-free grammar models would be meaningful.
On the methodological aspect, in light of the recent advances in protein sequences pretraining across a large evolutionary-scale (Rives et al., 2019;Elnaggar et al., 2020), our models for RNAs may similarly benefit by such a procedure with the data collected from RFAM. After the pretraining step, reinforcement learning can be used to finetune the generative component of our model with customizable rewards defined jointly on RNA structural validity, folding stability and functions such as binding to certain proteins.
On the evaluation side, it would be of great interest to analyze our models for any potential RNA tertiary structural motifs and to compare them with those deposited in the CaRNAval (Reinharz et al., 2018) or RNA 3D motifs database (Parlea et al., 2016). Our models would also need modifications to allow non-canonical interactions and pseudoknots, which are common in RNA tertiary structures.
All in all, the representation, generation and design of structured RNA molecules represent a rich, promising, and challenging area for future research in computational biology and drug discovery, and an opportunity to develop fundamentally new machine learning approaches.

[A ACKNOWLEDGEMENTS]
We would like to thank all members of the Hamilton lab, Blanchette lab, and the four anonymous reviewers for their insightful suggestions. This work was funded by a Genome Quebec/Canada grant to MB and by the Institut de Valorisation des Données (IAVDO) PhD excellence scholarship to ZY. WLH is supported by a Canada CIFAR AI Chair. We also thank Compute Canada for providing the computational resources.
B BACKGROUND: RNA STRUCTURE AND KEY PROPERTIES , hence stabilizing the molecule 3 . The set of pairs of interacting nucleotides in an RNA forms its so-called RNA secondary structure. In computational analyses of RNA, it is standard to assume that a secondary structure is nested: if [i, j] and [k, l] form base pairs with i < k, then either l < j (nesting) or k > j (non-overlapping). This enables simple string or planar graph representations (Figure S1 a, b).
The nested structure assumption means that secondary structures can be modelled by a probabilistic context free grammar (Dowell & Eddy, 2004), or by the closely related junction tree structure (Figure S1 c) (Sarrazin-Gendron et al., 2020), where each hypernode corresponds to a particular secondary substructure element: (1) stem: consecutive stacked base-pairs locally forming a double-stranded structure;
(2) hairpin loop : unpaired regions closed by a base-pair;
(3) internal loop: unpaired regions located between two stems; (4) multiloop: unpaired regions at the junction of at least three stems. Edges link elements that are adjacent in the structure.
Validity and stability of RNA folding. The notion of free energy of RNA secondary structures can be used to characterize the stability of a particular conformation. Given an RNA sequence, there are combinatorially many valid RNA secondary structures which all need to obey a set of constraints (summarized in section 3.3). However, some structures are more stable than the others by having lower free energy. Therefore, these structures are more likely to exist (hence more useful) in reality due to the stochastic nature of RNA folding. The free energy of an RNA secondary structure can be estimated by an energy-based model with thermodynamic parameters obtained from experiments (Mathews et al., 2004), wherein the minimum free energy (MFE) structure can be predicted, up to a reasonable approximation (Lorenz et al., 2011). 4 

[C DATASET AND METRICS]
The unlabeled dataset is obtained from the complete human transcriptome which is downloaded from the Ensembl database (Aken et al. (2016); version GRCh38). We slice the transcripts into snippets with length randomly drawn between 32 and 512 nts, and use RNAfold to obtain the MFE structures. We randomly split the dataset into a training set that contains 1,149,859 RNAs, and 20,000 held-out RNAs for evaluating decoding from the posterior distribution. More information on the structural diversity and complexity of this dataset is shown in Figure S2, which should present significant challenges for our algorithms.
The labeled dataset is pulled from a previous study on sequence and structural binding preference of RNA binding proteins (RBP), using an in vitro selection protocol called RNAcompete-S (Cook et al., 2017) which generates synthesized RNA sequences bound or unbound to a given RBP. RNAs in this experiment are of uniform length, i.e. 40 nts, and offer a rich abundance of RNA secondary structures compared to its predecessor protocols such as RNAcompete (Ray et al., 2009;2013). Since no benchmark has been ever established since its publication, we randomly sample 500,000 positive sequences bound to an RBP, and the same amount of negative sequences from the pool of unbound sequences, to curate a dataset for each of the seven RBPs investigated in the paper. Then, 80% of all RNAs are randomly selected to the train split, and the rest goes to the test split.
Our evaluation scheme for the generated RNA secondary structures includes the following metrics:
• validity: percentage of generated RNA secondary structures that conform to the structural constraints specified in section 3.3. • free energy deviation (FE DEV): difference of free energy between the generated RNA secondary structure and the MFE structure of the corresponding sequence, which quantifies the gap of both structures from an energy perspective. A lower FE DEV should indicate higher stability of generated RNAs. • free energy deviation normalized by length (Normed FE DEV): FE DEV divided by the length of generated RNA, which distributes the contribution of total FE DEV to each base. • 5-mer sequence diversity: entropy of the normalized counts of 5-mer substrings, which directly measures the diversity of RNA sequences, and indirectly for RNA secondary structures when this metric is combined with FE DEV, since monolithic structures of diverse sequences would lead to high FE DEV.

[D TREE ENCODING GRU]
Following Eq.3, T-GRU computes a new message v tĜ i,Ĝj fromĜ i andĜ j , based on the features in G i denoted by xĜ i , as well as neural messages from neighboring subgraphs toĜ i , i.e. {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}. The internal structure of T-GRU is equivalent to the tree encoder employed in Jin et al. (2018), which is essentially a neural analogue of the belief propagation algorithm on junction trees. Nevertheless, we write down the message passing formulas of T-GRU here:
sĜ i,Ĝj = Ĝ k ∈N (Ĝi) v t−1 G k ,Ĝi(S1)
zĜ i,Ĝj = σ(W z [xĜ i || sĜ i,Ĝj ] + b z ) (S2) rĜ k ,Ĝi = σ(W r [xĜ i || v t−1 G k ,Ĝi ] + b r ) (S3) vĜ i,Ĝj = Tanh(W [xĜ i || Ĝ k ∈N (Ĝi) rĜ k ,Ĝi • v t−1 G k ,Ĝi ]) (S4) v tĜ i,Ĝj = (1 − zĜ i,Ĝj ) sĜ i,Ĝj + zĜ i,Ĝj vĜ i,Ĝj(S5)
E ALGORITHM FOR HIERARCHICALLY DECODING STRUCTURED RNA a M TI refers to the threshold which set the maximum allowed number of topological prediction steps; M SI is another threshold to limit the length of each decoded nucleotide segment.

[H DETAILS FOR PARAMETERIZING PRIOR DISTRIBUTION USING NORMALIZING FLOW]
A normalizing flow involves a series of bijective transformation with tractable Jacobian logdeterminant, to map an observed datapoint x ∼ p θ (x) from a complex distribution to a simpler one, such as the standard normal distribution.
Considering the simplified case where we have a single bijective function f θ : Z → X to map some simple latent variables z to observed datapoint x, then, using the change of variable theorem, the likelihood of the observed datapoint can be evaluated as:
p θ (x) = p z (f −1 θ (x))|det ∂f −1 θ (x) ∂x | (S6)
where p z (.) denotes some simple base distribution, e.g. N (0; I). Then, it becomes clear the efficiency of this scheme heavily relies on the efficiency of inverting the forward mapping f θ as well as computing its Jacobian log-determinant.
In this project, we use a type of continuous normalizing flow (CNF) which simplifies the above mentioned computation (Chen et al., 2018). Consider a time continuous dynamics f ψ (z(t), t) of some intermediate data representation z(t), and again z(t 0 ) ∼ p z (.), the transformation of variable, along with its inverse mapping, can be expressed as:
z z(t 1 ) = z(t 0 ) + t1 t0 f ψ (z(t), t)dt (S7) z(t 0 ) = z(t 1 ) + t0 t1 f ψ (z(t), t)dt (S8)
and the change of probability density can be expressed as:
log p ψ (z) = log p z (z(t 0 )) − t1 t0 tr( ∂f θ ∂z(t) )dt (S9)
Note that the invertibility issue is no longer a concern under some mild constraints (Chen et al., 2018). Also, Eq. S9 only involves a more light-weight trace operation on the Jacobian rather than evaluating its log-determinant.
Therefore, we learn a parameterized prior using a CNF, and observe the decomposition of the KL term in the VAE objective:
KL(q φ (z|x)|p ψ (z)) = −E z∼q φ (z|x) [p ψ (z)] − H[q φ (z|x)](S10)
Therefore, during training our CNF parameterized with ψ works on the transformation of complex latent encodings z ∼ q φ (z|x) to some simple z(t 0 ) ∼ N (0; I), with an exact likelihood described by Eq. S9 and integrated into Eq. S10 for the complete training objective. During inference, we simply sample z t0 ∼ N (0; I), and use our CNF to reversely transform it to z ∼ p ψ (.) which should be closer to the approximate posterior. J HYPERPARAMETERS  The optimization takes place in the latent space of HierVAE, starting from the initial encoding of a random RNA molecule in the test set, and at each step altering the latent encoding by using activation maximization on the embedding classifier. The trajectory of generated RNAs is shown in the order of left to right and top to bottom, and the field PRED indicates that the probability of binding, as predicted by another external full classifier on the decoded molecular structure, is overall increasing as the decoded RNA structures smoothly evolving.

[F DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO LINEARIZED DECODING PROCEDURES]
When decoding from the joint vocabulary of sequence and dot-bracket structure ({A, C, G, U } × {., (, )}), whenever a nucleotide nuc i with a left bracket is sampled at step i, we append them to a stack, i.e. {(nuc i0 , i 0 ) . . . (nuc i , i)}. Then, at decode step j,
• if |i − j| ≤ 3, a proper mask will be added to the categorical logits of the vocabulary, to avoid sampling any nucleotides with right brackets, which means only an unpaired nucleotide or one that comes with a left bracket can be sampled; • if |i − j| > 3, a mask will be applied to make sure that only a nucleotide complementary to nuc i can be sampled with the right bracket. Sampling nucleotides with other forms of structures are allowed.
As soon as a nucleotide with a closing right bracket is sampled, we pop out (nuc i , i) from the stack. The special symbol for stop decoding can only be sampled when the stack has become empty.

[G DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO HIERARCHICAL DECODING PROCEDURES]
Additional constraints to be enforced during the hierarchical decoding process to ensure the validity of the decoded RNA secondary structure. Recall in section 3.2 that three types of predictions are involved with the hierarchical decoding, therefore, each type is associated with its own set of rules.
All set of rules can be observed by adding proper masks to the categorical logits before sampling, which are detailed below.
Constraints for making topological prediction, when the current node is
• stem node, then the algorithm always expands to a new node upon its first visit, or backtracks to its parent node upon re-visit; • hairpin node, then the algorithm always backtracks; • internal loop, then the algorithm acts similarly as for stem node; • multi-loop, then the algorithm always expands upon first visit and the next re-visit. Further re-visits to the same multi-loop node are not regulated.
Constraints for predicting new tree node, when the current node is • stem node, then its child node when exists can be either a hairpin loop, an internal loop, or a multi-loop; • hairpin node, internal loop or multi-loop, then its child node must be a stem node.
Constraints for decoding nucleotide segment. Due to the property of non-empty intersection between adjacent subgraphs, the start token for decoding a segment at the current node, is always the last nucleotide decoded at the last node. Therefore, without explicitly mentioning, the algorithm needs to decode at least one new nucleotide at each segment. When the current node is • stem node, and if it is upon its first visit (i.e. decoding the first segment of a stem), then there is no for constraints. Otherwise, upon its re-visit, the algorithm needs to decode exactly the complementary bases and in the reverse order, according to the first decoded segment; • hairpin node, then the decoder needs to decode at least four nucleotides before seeing the stop symbol, unless the hairpin is also the root node. • internal loop node, and if it is upon its first, then constraint is not necessary. Otherwise, upon its revisit, the algorithm needs to decode at least one unpaired nucleotide on condition that the first decoded internal loop segment does not contain any unpaired nucleotides; • multi-loop node, then there is no need for constraints. HuR 0.880 ± 0.000 0.880 ± 0.000 0.880 ± 0.000 0.888 ± 0.002 PTB 0.900 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 QKI 0.820 ± 0.000 0.830 ± 0.000 0.825 ± 0.002 0.830 ± 0.000 Vts1 0.900 ± 0.000 0.908 ± 0.002 0.637 ± 0.079 0.910 ± 0.000 RBMY 0.905 ± 0.002 0.880 ± 0.003 0.802 ± 0.055 0.870 ± 0.002 SF2 0.890 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 SLBP 0.777 ± 0.002 0.790 ± 0.000 0.797 ± 0.002 0.797 ± 0.002","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules."
Neural representation and generation for RNA secondary structures,snOgiCYZgJ7.json,"**Summary**
The work studies of the problem of jointly modelling RNA sequence and secondary structure using the framework of variational autoencoders. Several encoder and decoder architectures with increasing inductive bias for RNA structures are proposed. These are compared on two novel RNA sequence datasets and supervised and unsupervised RNA generation tasks.

**Score justification**
The work presents a comparison of several interesting approaches to the problem of joint modelling of RNA structure and sequence using VAEs. Encoders and decoders that make use of (i) RNA sequence and structure in a string representation; (ii) in a graph representation; and (iii) in a junction tree representation are considered. This works particularly well on the encoder side, where the appropriate deep learning architectures are used; but becomes considerably less elegant on the decoder side, where the authors have to make use of sample masking with complex rules to prevent generation of invalid sequence-structure combinations. Furthermore it is not immediately clear from the empirical evaluation that the added complexity of the HierVAE, or that using VAEs are an optimal choice for the supervised generation task.


**Major comments**
* As noted by the authors, their decoders (linearized sequence and structure, as well as the hierarchical decoder) may generate invalide sequence-structure combinations. To eliminate this possibility the authors restrict the autoregressive decoder from sampling invalid sequence-structures by masking out some samples using a number of heuristic rules. First of all, it is unclear from the text whether this is also done during training and how the probabilities at masked stages are treated (are they re-normalized after the masking is applied?)
* I appreciate that the authors include a comparison between decoding with and without the heuristic masking rules. Could the authors explain why the FE DEV in Table 1 is lower for samples produced without the masking rules (unconstrained generation)? This result seems counter-intuitive.
* From the results in Table 1 it is still difficult to tease apart the relative contributions of (i) model training; (ii) inductive bias of decoder architecture; and (iii) decoding constraints. I would appreciate it if the authors also included decoding results for untrained decoders with and without constraints.
* Comparing the AUC ROC in Table 2 to the results in Tables S2 and S3 it is not obvious what the benefit of using a VAE setup for the supervised problem is. The purely supervised approach is conceptually and technically easier.
* I am a bit skeptical about the results presentable in Table 3 and section ""Targeted RNA design"" - judging improvement of a sequence using a different classifier trained on the same data is not very convincing. Ideally, empirical wet lab validation should be carried out for the improved designs; but absent that it would be great to see a more independent/less correlated evaluation. Perhaps the authors could demonstrate a reduction of edit distance of an improved negative test set sequence to a positive (test set / train set) sequence?
* When generating the dataset for the unsupervised RNA modelling task the authors take human transcripts and draw random short (32 to 512 nucleotide) sequences from them for their dataset. Since the authors introduce a new dataset it would great to see more information about its composition - which kind of RNAs compose the dataset (e.g. mRNA, tRNA, lincRNA, shRNA, rRNA, etc); does this include alternatively spliced RNA (i.e. could there be leakage between the training and holdout sets)? What is the reasoning between random slicing of the RNAs into shorter sequences? Are the smaller RNA slices biologically plausible/relevant?
* Could the authors provide additional justification / ablations of the choice of $\beta \ll 1$? How do the models behave if KL annealing is not used?


**Minor comments**
* The abstract states ""the design of large scale and complex biological structures *requires* dedicated graph-based deep generative modelling techniques"", which I believe is a too strong statement. These techniques work better right now, but does not mean that they are required (and the only appropriate way forward).
* In ""Task Description"" section: ""functional properties *or* RNA"" -> ""functional properties of RNA""
* Is Figure 1 using some domain-specific terminology? It refers to cliques, but the nucleotides put into boxes are not all fully connected.
* Why doesn't constrained generation (Table 2) achieve 100% validity? It seems to me that it should be possible at least for the linearized sequence-structure decoder.
* Apologies if I missed it in the text, but what does ""RECON ACC "" in Table 2 refer to?
* Incorrect opening quotes used for ""ground truth"" in section ""Targeted RNA design""
* In Appendix A there is ""$j > j$""
* In Appendix B: ""in-vitro"" -> ""*in vitro*"" and probably in italic
* Why is FE DEV defined as the absolute value? Should not the MFE structure always have smaller Free Energy than any structure output by the model?","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Hierarchical encoding. Panel (A)shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
Table 2: (A), to determine if the decoder should expand to a new tree node or backtrack to its parent node, based on MLP topo (hĜ t,i ); • tree node prediction in Figure 2 (B), on condition that a new tree node is needed due to a possible topological expansion. This procedure determines the label of the new tree node from the set of {S, H, I, M }, based on MLP node (hĜ t,i ); • nucleotide segment decoding in Figure 2 (C), using a single-layered LSTM, whose initial hidden state is MLP dec
Table 3: Figure 3 :3Figure 3: RNAs generated with structural constraints from HierVAE on a random axis in the latent space (step size: 1e-4), for short (top), medium-length (middle), and long (bottom) RNAs. The Free energy (FE) and its deviation (DEV) from the MFE are given for each structure. which means only the canonical base-pairs and Wobble base-pairs are allowed, i.e. [A-U], [G-C] and [G-U];(2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3; (3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
Table 4: Figure S1 :S1FigureS1: A nested RNA secondary structure can be represented by: (A) dotbracket annotation, where base-pairs corresponding to matching parentheses, or (B) a molecular planar graph with two types of edges, corresponding to consecutive nucleotides (backbone) and basepairing interactions, or (C) a junction tree where node are labeled as stems (S), hairpins (H), internal loops (I), or multiloops (M), and edges correspond to the connections between these elements. All three forms are equivalent.
Table 5: Figure S2: This figure contains information of the unlabeled RNA dataset. (A) The number of hypernodes appears to grow linearly with the length of RNA, and (B) the junction tree height also grows as the length increases but on a more moderate scale. (C) and (D) have shown bar-plots of the number of hypernodes and tree height, indicating that the junction tree of RNA can take on significant depth hence contributing to the diversity and complexity of RNA secondary structures represented in this dataset.
Table 6: Figure S4 :S4Figure S4: Neighborhood visualization of tRNA-Cys 6 which is marked by the red bounding box in the center and the walk in the latent space takes place on two random orthogonal axes. Note that actual secondary structure of tRNA-Cys plotted in the figure is different compared to the one deposited online due to the prediction of RNAfold.
Table 7: We evaluate RNAs sampled from the posterior distribution: q(z|x), with a held-out test set of 20,000 RNAs. Each molecule is encoded and decoded 5 times. We also evaluate samples from the prior distribution: N (0, I) subject to the transformation of a latent CNF, where we sample 10,000 encodings and each encoding is decoded 10 times. Normed refers to length normalized FE DEV.
Table 8: Training semi-supervised HierVAE on labeled RNAcompete-S dataset. A test split is used to evaluate the accuracy of embedding classifiers and RNAs decoded from the posterior distribution under two settings: constrained and stochastic (C& S), unconstrained and deterministic (NC&D). RECON ACC refers to reconstruction accuracy which measures the percentage of RNA molecules decoded exactly as the input.
Table 9: 
Table 10: Algorithm 1: DFS decode RNA secondary structure1 Given: z T , z G , M TI, M SI a 2 Initialize: stack ← [ ] 3 function decode(z T , z G ) 4 root ← sample(MLP node (z T )) ;
Table 11: Hyperparameters for training VAE and full classifier models. Note that hidden units refer to the dimensionality of encoders and decoders from LSTMVAE, GraphVAE as well as HierVAE models. Dropout is applied to the embedding MLP classifier in case of training semi-supervised VAEs, which contains one hidden layer. Performance of simple MLP classifiers on top of fixed latent embeddings from VAE models, which have been pretrained on the unlabeled RNA dataset as originally shown in Table1.

[INTRODUCTION]
There is an increasing interest in developing deep generative models for biochemical data, especially in the context of generating drug-like molecules. Learning generative models of biochemical molecules can facilitate the development and discovery of novel treatments for various diseases, reducing the lead time for discovering promising new therapies and potentially translating in reduced costs for drug development (Stokes et al., 2020). Indeed, the study of generative models for molecules has become a rich and active subfield within machine learning, with standard benchmarks (Sterling & Irwin, 2015), a set of well-known baseline approaches (Gómez-Bombarelli et al., 2018;Kusner et al., 2017;Jin et al., 2018), and high-profile cases of real-world impact 1 .
Prior work in this space has focused primarily on the generation of small molecules (with less than 100 atoms), leaving the development of generative models for larger and more complicated biologics and biosimilar drugs (e.g., RNA and protein peptides) an open area for research. Developing generative models for larger biochemicals is critical in order to expand the frontiers of automated treatment design. More generally, developing effective representation learning for such complex biochemicals will allow machine learning systems to integrate knowledge and interactions involving these biologically-rich structures.
In this work, we take a first step towards the development of deep generative models for complex biomolecules, focusing on the representation and generation of RNA structures. RNA plays a crucial role in protein transcription and various regulatory processes within cells which can be influenced by its structure (Crick, 1970;Stefl et al., 2005), and RNA-based therapies are an increasingly active area of research (Pardi et al., 2018;Schlake et al., 2012), making it a natural focus for the development of deep generative models. The key challenge in generating RNA molecules-compared to the generation of small molecules-is that RNA involves a hierarchical, multi-scale structure, including a primary sequential structure based on the sequence of nucleic acids as well as more complex secondary and tertiary structures based on the way that the RNA strand folds onto itself. An effective generative model for RNA must be able to generate sequences that give rise to these more complex emergent structures.
There have been prior works on optimizing or designing RNA sequences-using reinforcement learning or blackbox optimization-to generate particular RNA secondary structures (Runge et al., 2019;Churkin et al., 2017). However, these prior works generally focus on optimizing sequences to conform to a specific secondary structure. In contrast, our goal is to define a generative model, which can facilitate the sampling and generation of diverse RNA molecules with meaningful secondary structures, while also providing a novel avenue for targeted RNA design via search over a tractable latent space.
Key contributions. We propose a series of benchmark tasks and deep generative models for the task of RNA generation, with the goal of facilitating future work on this important and challenging problem. We propose three interrelated benchmark tasks for RNA representation and generation:
1. Unsupervised generation: Generating stable, valid, and diverse RNAs that exhibit complex secondary structures. 2. Semi-supervised learning: Learning latent representations of RNA structure that correlate with known RNA functional properties. 3. Targeted generation: Generating RNAs that exhibit particular functional properties.
These three tasks build upon each other, with the first task only requiring the generation of stable and valid molecules, while the latter two tasks involve representing and generating RNAs that exhibit particular properties. In addition to proposing these novel benchmarks for the field, we introduce and evaluate three generative models for RNA. All three models build upon variational autoencoders (VAEs) (Kingma & Welling, 2014) augmented with normalizing flows (Rezende & Mohamed, 2015;Kingma et al., 2016), and they differ in how they represent the RNA structure. To help readers better understand RNA structures and properties, a self-contained explanation is provided in appendix B.
The simplest model (termed LSTMVAE) learns using a string-based representation of RNA structure. The second model (termed GraphVAE) leverages a graph-based representation and graph neural network (GNN) encoder approach (Gilmer et al., 2017). Finally, the most sophisticated model (termed HierVAE) introduces and leverages a novel hierarchical decomposition of the RNA structure. Extensive experiments on our newly proposed benchmarks highlight how the hierarchical approach allows more effective representation and generation of complex RNA structures, while also highlighting important challenges for future work in the area.

[TASK DESCRIPTION]
Given a dataset of RNA molecules, i.e. sequences of nucleotides and corresponding secondary structures, our goals are to: (a) learn to generate structurally stable, diverse, and valid RNA molecules that reflect the distribution in this training dataset; (b) learn latent representations that reflect the functional properties of RNA. A key factor in both these representation and generation processes is that we seek to jointly represent and generate both the primary sequence structure as well as the secondary structure conformation. Together, these two goals lay the foundations for generating novel RNAs that satisfy certain functional properties. To meet these goals, we create two types of benchmark datasets, each one focusing on one aspect of the above mentioned goals:
Unlabeled and variable-length RNA. The first dataset contains unlabeled RNA with moderate and highly-variable length (32-512 nts), obtained from the human transcriptome (Aken et al., 2016) and through which we focus on the generation aspect of structured RNA and evaluate the validity, stability and diversity of generated RNA molecules. In particular, our goal with this dataset is to jointly generate RNA sequences and secondary structures that are biochemically feasible (i.e., valid), have low free energy (i.e., stable), and are distinct from the training data (i.e., diverse). We will give an extended assessment of the generation aspect under different circumstances, e.g., when constraining the generation procedures with explicit rules.
Labeled RNA. The second dataset is pulled and processed from a previous study on in vitro RNAprotein interaction, which features labeled RNAs with shorter and uniform length (40 nts) (Cook et al., 2017). With this dataset, our objective is slightly expanded (to include obj. a), so that the latent space is adequately organized and reflective of the interaction with proteins. Therefore, key assessment for the latent space includes AUROC for the classification of protein binding, which is crucial for the design of desired novel RNA molecules.
Essentially, this creates slight variations in the task formulation, with the first dataset suited to unsupervised learning of a generative model, while the second datasets involves additional supervision (e.g., for a semi-supervised model or targeted generation). Our specific modeling choices, to be introduced in section 3, are invariant to different task formulations, and flexible enough to handle different representations of RNA secondary structures. We refer readers to appendix C for detailed explanation for the dataset and evaluation metrics on the generated molecules and latent embeddings.

[METHODS]
In this section, we introduce three different generative models for RNA. All three models are based upon the variational autoencoder (VAE) framework, involving three key components:
1. A probabilistic encoder network q φ (z|x), which generates a distribution over latent states given an input representation of an RNA. We experiment with three different types of input encodings for RNA sequence and secondary structures (see Figure S1: a dot-bracket annotated string, a graph with adjacency matrix representing base-pairings, and a graph augmented with a hierarchical junction tree annotation for the secondary structure.
2. A probabilistic decoder network p θ (x|z), which defines a joint distribution over RNA sequences and secondary structures, conditioned on a latent input. As with the encoder network, we design architectures based on a linearized string decoding and a graph-based hierarchical junction-tree decoding approach.
3. A parameterized prior p ψ (z), which defines a prior distribution over latent states and is learned based on a continuous normalizing flow (CNF) (Chen et al., 2018). shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.
For all the approaches we propose, the model is optimized via stochastic gradient descent to minimize the evidence lower bound (ELBO):
L = −E q φ (z|x) [p θ (x|z)] + β KL(q φ (z|x)|p ψ (z))
where β is a term to allow KL-annealing over the strength of the prior regularization.
In the following sections, we explain our three different instantiations of the encoder (section 3.1), decoder (section 3.2), as well as our procedures to structurally constrain the decoding process using domain knowledge (section 3.3) and our procedures to avoid posterior collapse (section 3.4).

[ENCODING RNA SECONDARY STRUCTURES]
The input to the encoder is a structured RNA molecule, with its sequence given by an ordered array of nucleotides x 1 . . . x L , with x i ∈ {A, C, G, U }, where L is the length of the sequence, and its secondary structure, either represented as (1) a dot-bracket string S =ẋ 1 . . .ẋ L withẋ i ∈ {., (, )};
(2) or as a graph G with two types of edges -covalent bonds along the RNA backbone, and hydrogen bonds between the base-pairs 2 . We use x uv to denote edge features between nucleotides u and v;
(3) or as a hypergraph T -a depth-first ordered array of subgraphsĜ 1 . . .Ĝ D with L(Ĝ i ) ∈ {S, H, I, M } indicating the subgraph label, and I(Ĝ i ) = {j|j ∈ {1 . . . L}} indicating the assignment of nucleotides to each subgraph.
Encoding RNA secondary structure as sequence. First, we obtain a joint encoding over the nucleotide and the dot-bracket annotation, using the joint sequence-structure vocabulary {A, C, G, U } × {., (, )}. Then, these one-hot encodings are processed by a stacked bidirectional LSTM (Hochreiter & Schmidhuber, 1997), followed by a multi-head self-attention module (Vaswani et al., 2017) to weigh different positions along the RNA backbone. A global max-pooling is used to aggregate the information into h S , and then we obtain mean µ S and log variance log σ S from h S through linear transformations, and draw latent encoding z S from N (µ S , σ S ) using the reparameterization trick (Kingma & Welling, 2014).
Learning graph representation of RNA secondary structure. To encode the graph view G of an RNA secondary structure, we pass rounds of neural messages along the RNA structure, which falls into the framework of Message Passing Neural Network (MPNN) as originally discussed in Gilmer et al. (2017) and similarly motivated by Jin et al. (2018).
For much longer RNAs, it is conceptually beneficial to pass more rounds of messages so that a nucleotide may receive information on its broader structural context. However, this may introduce undesired effects such as training instability and over-smoothing issues. Therefor , we combine our MPNN network with gating mechanism, which is collectively referred as the G-MPNN:
v t−1 uv = σ(W g local [x u || x uv ] + W g msg w∈N (u) v t−1 wu ) (1) v t uv = GRU(v t−1 uv , v t−1 uv )(2)
where [. . . || . . . ] denotes concatenation, σ denotes the activation function and GRU indicates the gated recurrent unit (Cho et al., 2014). Then, after T iterations of message passing, the final nucleotide level embedding is given by:
h u = σ(W g emb [x u || v∈N (u) v T vu ]
). Before pooling the nucleotide level embeddings into the graph level, we pass h 1 . . . h L through a single bidirectional LSTM layer, obtainingĥ 1 . . .ĥ L at each step, and h g = max({ĥ i |i ∈ 1...L}). The latent encoding z G is similarly obtained from h G using the reparameterization trick.
Hierarchical encoding of the RNA hypergraph. To encode the junction tree T of RNA, we employ a type of GRU specifically suited to tree-like structures, which has previously been applied in works such as GGNN (Li et al., 2016) and JTVAE (Jin et al., 2018). We refer to this tree encoding network as T-GRU, and the format of its input is shown in Figure 1.
One major distinction between our RNA junction tree and the one used for chemical compounds (Jin et al., 2018) is that an RNA subgraph assumes more variable nucleotide composition such that it is impossible to enumerate based on the observed data. Therefore, we need to dynamically compute the features for each node in an RNA junction tree based on its contained nucleotides, in a hierarchical manner to leverage the nucleotide level embeddings learnt by G-MPNN.
Considering a subgraphĜ i in the junction tree T , we initialize its node feature with:
xĜ i = [L(Ĝ i ) || max u∈I(Ĝi) h u ].
Notably, max u∈Ĝi h u is a max-pooling over all nucleotides assigned toĜ i , and nucleotide embedding h u comes from G-MPNN. To compute and pass neural messages between adjacent subgraphs in the RNA junction tree T , we use the T-GRU network in Eq.3
v tĜ i,Ĝj = T-GRU(xĜ i , {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}) (3) hĜ i = σ(W t emb [xĜ i || Ĝ ∈N (Ĝi) hĜ]) (4)
with details of T-GRU provided in the appendix D, and compute the embeddings for subgraphs with Eq. 4. Further, we obtain a depth-first traversal of the subgraph embeddings hĜ 1 . . . hĜ D which is also the order for hierarchical decoding to be discussed later. This ordered array of embeddings is processed by another bi-directional LSTM , and the final tree level representation h T is again given by the max-pooling over the bi-LSTM outputs. Likewise, latent encoding z T is obtained from h T .

[RNA MOLECULAR GENERATION]
Decoding linearized sequence and structure. In this setting, the decoder simply autoregressively decodes a token at each step, from the joint sequence-structure vocabulary mentioned before in section 3.1, plus one additional symbol to signal the end of decoding. To simplify the design choice, we use a single-layered forward-directional LSTM, and its hidden state is initialized with the latent encoding z, which can be either z S , z G or z T .
Figure 2: Hierarchical decoding of a structured RNA, involving three types of predictions, that are on the topological level, node level, and nucleotide level. These three types of prediction are interleaved into the procedures of decoding the junction tree structure of RNA and the nucleotide segments.
Hierarchically decoding hypergraph and nucleotide segments. The input to this more sophisticated hierarchical decoder are latent encodings z G which contains order and basic connectivity information of the nucleotides, and z T which contains higher order information about the arrangements of nucleotide branches and their interactions. We give a concise description of the decoding procedures here, along with a detailed algorithm in appendix E. On a high level, we hierarchically decode the tree structure in a depth-first manner, and autoregressively generate a nucleotide segment for each visited tree branch. For these purposes, we interleave three types of prediction (Figure 2).
Denote the current tree node at decode step t and at the i-th visit asĜ t,i , whose features include (1) its node label L(Ĝ t,i ) and, (2) a summary over the already existing i − 1 nucleotide segments max{h l,j u | u ∈Ĝ t,i and l < t and j < i}, with l denoting the nucleotide is decoded at step l, and j indicating the nucleotide belongs to the j-th branch (this feature is simply zeros when i = 1). Then, its local feature xĜ t,i is defined as the concatenation of (1) and (2).
We make use of a notion called node state: hĜ t,i , which is obtained by: hĜ t,i = T-GRU(xĜ t,i , {vĜ ,Ĝ t,i |Ĝ ∈ N (Ĝ t,i )}). Note its similarity to Eq. 3, and hĜ t,i is used to make:
• topological prediction in Figure 2 
([hĜ t,i || z T || z G ])
. The start token is the last nucleotide from the last segment.
Our hierarchical decoder starts off by predicting the label of the root node using z T , followed by topological prediction on the root node and decoding the first nucleotide segment. The algorithm terminates upon revisiting the root node, topologically predicted to backtrack and finishing the last segment of the root node. The decoded junction tree naturally represents an RNA secondary structure that can be easily transformed to the dot-bracket annotation, and the RNA sequence is simply recovered by connecting nucleotide segments along the depth-first traversal of the tree nodes.

[STRUCTURALLY CONSTRAINED DECODING]
To better regulate the decoding process so that generated RNAs have valid secondary structures, a set of constraints can be added to the decoding procedures at the inference stage. Essentially, a valid RNA secondary structure needs to observe the following rules: (1) base-pairing complementarity, (2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i − j| > 3;
(3) each nucleotide can only be paired once, and overlapping pairs are disallowed.
We will translate the above rules into specific and applicable constraints, depending on specific decoders. For the sake of space, we only give a broad remark and leave more details in the appendix.
Linearized decoding constraints. Since the linearized decoder simply proceeds in an autoregressive fashion, constraints can be easily enforced in a way that at each step, a nucleotide with an appropriate structural annotation is sampled by making use of masks and re-normalizing the probabilities. Likewise, a stop token can only sampled when all opening nucleotides have been closed. More details to follow in appendix F.
Hierarchical decoding constraints. The specific set of constraints for hierarchical decoding is discussed in appendix G. Overall, considering the different natures of the three associated types of prediction, each one should require a set of different strategies, which are once again applicable by adding proper masks before sampling. As shown in the algorithm in appendix E, the set of constraints are applied to line 13, 24 and 14 with marked asterisk.

[AVOIDING POSTERIOR COLLAPSE]
As discussed in a line of previous works, VAEs with strong autoregressive decoders are susceptible to posterior collapse, an issue where the decoder simply ignores the latent encoding of the encoder (He et al., 2019). Therefore, to avoid posterior collapsing, we make use of a carefully chosen KL annealing schedule during training to help the encoder adapt its information content in the latent encoding and in coordination with the decoder. This schedule is detailed in section 4. We also learn a parameterized prior as suggested in Chen et al. (2017), but using a CNF instead, following a similar implementation to Yang et al. (2019), with details given in appendix H.
Our KL annealing schedule is chosen based on empirical observations, as to our knowledge, there has yet to exist any principled methods of selecting such schedule. We have used diagnostic metrics such as mutual information (He et al., 2019) and active units (Burda et al., 2016) along with a validation set to select a proper KL annealing schedule which is to be described later in section 4

[RESULTS]
We consider three modes of evaluation: (1) unsupervised RNA generation; (2) generation using semi-supervised VAE models and (3) targeted RNA design from an organized latent space. Results are presented below, and relevant hyperparameters can be found in Table S1.
Unsupervised RNA generation. Here, we evaluate generated RNAs from models trained on the unlabeled RNA dataset for 20 epochs using a KL annealing schedule including 5 epochs of warm-up, followed by gradually increasing the KL annealing term to 3e-3 (for LSTMVAE and GraphVAE), or 2e-3 (for HierVAE). The KL annealing schedule was chosen using a validation set of 1,280 RNAs.
Table 1 compares the generation capability of different models, from the posterior as well as the prior distribution, and in scenarios such as applying structural constraints to the decoding process or not.
It clearly shows that our most advanced model, HierVAE which employs a hierarchical view of the structure in its encoding/decoding aspects, achieves the best performance across different evaluation regimes, generating valid and stable RNAs even when the decoding processed is unconstrained. It is also observed that despite having structural constraints, the validity of our generated RNAs are always slightly below 100%. This can be explained by the threshold hyperparameter which sets the maximum number of steps for topological prediction as well as the maximal length of each nucleotide segment, as shown in Algorithm 1 in appendix E.
To further demonstrate the benefits of model training from structural constraints, we sample RNAs from the prior of an untrained HierVAE model. With structural constraints, the validity amounts to 66.34% with an extremely high free energy deviation of 22.613. Without structural constraints, the validity translates to a mere 9.37% and the model can only decode short single stranded RNAs as it lacks the knowledge of constructing more complex structures. This comparison illustrates that model training is essential for obtaining stable RNA folding.
The junction tree hierarchy of RNAs developed in our work shares certain modelling similarities with the probabilistic context free grammar (Dowell & Eddy, 2004) used by covariance models (CM) (Eddy & Durbin, 1994). Infernal (Nawrocki & Eddy, 2013) is one of the representative works based on CM, which is capable of sampling RNA secondary structures from a CM built around a consensus secondary structure for a conserved RNA family. However, due to the lack of homologous sequences in our dataset, Infernal is seriously limited and can only sample single stranded RNAs.
Figure 3 illustrate RNA structures generated using HierVAE from a randomly chosen short path through the latent space. Notably, latent encoding provided by HierVAE translates smoothly in the RNA structure domain: nearby points in the latent space result in highly similar, yet different, structures. The generated structures are particularly stable for short and medium-size RNAs, and slightly less so for longer RNAs with highly complex structures. A side-by-side comparison between generated RNA secondary structures and MFE structures in Figure S3 shows that generated structures can evolve smoothly in the latent space along with their corresponding MFE structures. We also visualize neighborhoods of a Cysteine-carrying transfer RNA and a 5S ribosomal RNA in figure S4 and S5.
Supervised RNA generation. We then evaluate our generative approaches in a semi-supervised setting using seven RBP binding data sets from RNAcompete-S. First, we compare the efficacy of different representational choices while excluding the generative components, i.e. we jointly train VAE encoders followed by simple MLP classifiers on top of the latent encodings for binary classification on RBP binding.  2. Since our strategy for targeted RNA design makes use of seed molecules in the latent space, we mainly sample RNAs from the posterior distribution of these semi-supervised VAE models. Therefore, we select a KL annealing schedule that tends to retain more information in the latent encodings, i.e. setting maximum β to 5e-4 and training 10 epochs.
Results are promising in that classification AUROC measured by the held-out test set is comparable to the fully supervised classification models in Table S3, and much better compared to models only using fixed and pretrained VAE embeddings as shown in Table S2. Also, RNA structures generated from the posterior distribution, even under the setting of unconstrained and deterministic decoding, have high success rates, very stable conformation and good reconstruction accuracy. Targeted RNA design. We next studied the task of designing RNAs with high RBP binding affinity. Starting from the latent encodings of 10,000 randomly chosen RNA molecules that have negative labels in each RNAcompete-S test set, and use activation maximization to gradually alter the latent encodings so that the predicted binding probability from the embedding classifiers increases. These embedding classifiers have been trained jointly with the VAE models with accuracy reported earlier (Table 2). Then, we use separately trained full classifiers (also earlier shown in Table S3) as proxy of oracles for evaluating the ""ground truth"" probability of RBP binding. Table 3, report the success rate (fraction of RNAs whose ""ground truth"" RBP binding probability was improved), along with the average improvement in binding probabilities. An example of a trajectory of optimized RNAs is shown in Fig. S6.

[RELATED WORK]
Over the years, the field of computational drug discovery has witnessed the emergence of graphcentric approaches. One of the earliest method, proposed in Gómez-Bombarelli et al. (2018), is defined on the linearized format of molecular structures and represents a family of methods that rely on sequential models to represent and generate SMILES strings of chemical compounds. Later methods have sought to construct more chemical priors into the model, via (1) leveraging graph based representation and generation techniques, (2) enforcing direct chemical constraints to the decoding process, (3) considering a multi-scale view of the molecular structures, or (4) using reinforcement learning to integrate more training signal of the molecular structure and function. As a result, greater success has been achieved by models such as Kusner et al. (2017); ; Jin et al. (2018); You et al. (2018) at generating and searching valid and more useful chemical compounds.
Graph representation learning is at the heart of these more recent approaches, to help understand the rules governing the formation of these molecular structures, as well as the correspondence between structures and functions. Duvenaud et al. (2015) were among the first to apply GNN to learn molecular fingerprints, and the general neural message passing framework for molecules is proposed in Gilmer et al. (2017), which demonstrate the power of MPNN across various molecular benchmarking tasks. These prior works on molecular MPNN, together with other GNN architectures developed in other areas, such as considering relational edges (Schlichtkrull et al., 2018) and attention (Velickovic et al., 2018), have laid the foundation for the success of these deep generative models.
Despite the fact that RNA molecules can adopt complex structures, dedicated graph representation learning techniques have been scarce, with some recent works beginning to leverage graph related learning techniques to predict RNA folding (Chen et al., 2020;Singh et al., 2019) and to represent RNA molecular structures (Yan et al., 2020;Oliver et al., 2020). Prior to our work, the design of RNA has mostly focused on the inverse design problem, which is to conditionally generate an RNA sequence whose MFE secondary structure corresponds to an input secondary structure. Therefore, the line of prior works have predominantly relied on sequential techniques, with some representative methods based on reinforcement learning (Runge et al., 2019), or more classically framed as a combinatorial optimization problem and solved with sampling based techniques (Churkin et al., 2017). These prior works are mainly concerned with querying from an energy model with fixed thermodynamic parameters and fixed dynamics of RNA folding, which is in itself limited compared to learning based approaches (Chen et al., 2020;Singh et al., 2019), and are unable to model a joint distribution over RNA sequences and possible folds.

[CONCLUSION AND FUTURE WORKS]
In this work we propose the first graph-based deep generative approach for jointly embedding and generating RNA sequence and structure, along with a series of benchmarking tasks. Our presented work has demonstrated impressive performance at generating diverse, valid and stable RNA secondary structures with useful properties.
For future works, there are several important directions to consider. First, it would be beneficial to obtain non-coding RNA families from the RFAM database (Kalvari et al., 2017) which would help our models learn more biologically-meaningful representation indicative of RNA homology and functions, in addition to the evolutionarily conserved RNA structural motifs that would enable the generation of more stable RNA secondary structures. In that context, a detailed comparison to Infernal and other probabilistic context-free grammar models would be meaningful.
On the methodological aspect, in light of the recent advances in protein sequences pretraining across a large evolutionary-scale (Rives et al., 2019;Elnaggar et al., 2020), our models for RNAs may similarly benefit by such a procedure with the data collected from RFAM. After the pretraining step, reinforcement learning can be used to finetune the generative component of our model with customizable rewards defined jointly on RNA structural validity, folding stability and functions such as binding to certain proteins.
On the evaluation side, it would be of great interest to analyze our models for any potential RNA tertiary structural motifs and to compare them with those deposited in the CaRNAval (Reinharz et al., 2018) or RNA 3D motifs database (Parlea et al., 2016). Our models would also need modifications to allow non-canonical interactions and pseudoknots, which are common in RNA tertiary structures.
All in all, the representation, generation and design of structured RNA molecules represent a rich, promising, and challenging area for future research in computational biology and drug discovery, and an opportunity to develop fundamentally new machine learning approaches.

[A ACKNOWLEDGEMENTS]
We would like to thank all members of the Hamilton lab, Blanchette lab, and the four anonymous reviewers for their insightful suggestions. This work was funded by a Genome Quebec/Canada grant to MB and by the Institut de Valorisation des Données (IAVDO) PhD excellence scholarship to ZY. WLH is supported by a Canada CIFAR AI Chair. We also thank Compute Canada for providing the computational resources.
B BACKGROUND: RNA STRUCTURE AND KEY PROPERTIES , hence stabilizing the molecule 3 . The set of pairs of interacting nucleotides in an RNA forms its so-called RNA secondary structure. In computational analyses of RNA, it is standard to assume that a secondary structure is nested: if [i, j] and [k, l] form base pairs with i < k, then either l < j (nesting) or k > j (non-overlapping). This enables simple string or planar graph representations (Figure S1 a, b).
The nested structure assumption means that secondary structures can be modelled by a probabilistic context free grammar (Dowell & Eddy, 2004), or by the closely related junction tree structure (Figure S1 c) (Sarrazin-Gendron et al., 2020), where each hypernode corresponds to a particular secondary substructure element: (1) stem: consecutive stacked base-pairs locally forming a double-stranded structure;
(2) hairpin loop : unpaired regions closed by a base-pair;
(3) internal loop: unpaired regions located between two stems; (4) multiloop: unpaired regions at the junction of at least three stems. Edges link elements that are adjacent in the structure.
Validity and stability of RNA folding. The notion of free energy of RNA secondary structures can be used to characterize the stability of a particular conformation. Given an RNA sequence, there are combinatorially many valid RNA secondary structures which all need to obey a set of constraints (summarized in section 3.3). However, some structures are more stable than the others by having lower free energy. Therefore, these structures are more likely to exist (hence more useful) in reality due to the stochastic nature of RNA folding. The free energy of an RNA secondary structure can be estimated by an energy-based model with thermodynamic parameters obtained from experiments (Mathews et al., 2004), wherein the minimum free energy (MFE) structure can be predicted, up to a reasonable approximation (Lorenz et al., 2011). 4 

[C DATASET AND METRICS]
The unlabeled dataset is obtained from the complete human transcriptome which is downloaded from the Ensembl database (Aken et al. (2016); version GRCh38). We slice the transcripts into snippets with length randomly drawn between 32 and 512 nts, and use RNAfold to obtain the MFE structures. We randomly split the dataset into a training set that contains 1,149,859 RNAs, and 20,000 held-out RNAs for evaluating decoding from the posterior distribution. More information on the structural diversity and complexity of this dataset is shown in Figure S2, which should present significant challenges for our algorithms.
The labeled dataset is pulled from a previous study on sequence and structural binding preference of RNA binding proteins (RBP), using an in vitro selection protocol called RNAcompete-S (Cook et al., 2017) which generates synthesized RNA sequences bound or unbound to a given RBP. RNAs in this experiment are of uniform length, i.e. 40 nts, and offer a rich abundance of RNA secondary structures compared to its predecessor protocols such as RNAcompete (Ray et al., 2009;2013). Since no benchmark has been ever established since its publication, we randomly sample 500,000 positive sequences bound to an RBP, and the same amount of negative sequences from the pool of unbound sequences, to curate a dataset for each of the seven RBPs investigated in the paper. Then, 80% of all RNAs are randomly selected to the train split, and the rest goes to the test split.
Our evaluation scheme for the generated RNA secondary structures includes the following metrics:
• validity: percentage of generated RNA secondary structures that conform to the structural constraints specified in section 3.3. • free energy deviation (FE DEV): difference of free energy between the generated RNA secondary structure and the MFE structure of the corresponding sequence, which quantifies the gap of both structures from an energy perspective. A lower FE DEV should indicate higher stability of generated RNAs. • free energy deviation normalized by length (Normed FE DEV): FE DEV divided by the length of generated RNA, which distributes the contribution of total FE DEV to each base. • 5-mer sequence diversity: entropy of the normalized counts of 5-mer substrings, which directly measures the diversity of RNA sequences, and indirectly for RNA secondary structures when this metric is combined with FE DEV, since monolithic structures of diverse sequences would lead to high FE DEV.

[D TREE ENCODING GRU]
Following Eq.3, T-GRU computes a new message v tĜ i,Ĝj fromĜ i andĜ j , based on the features in G i denoted by xĜ i , as well as neural messages from neighboring subgraphs toĜ i , i.e. {v t−1 G k ,Ĝi |Ĝ k ∈ N (Ĝ i )}. The internal structure of T-GRU is equivalent to the tree encoder employed in Jin et al. (2018), which is essentially a neural analogue of the belief propagation algorithm on junction trees. Nevertheless, we write down the message passing formulas of T-GRU here:
sĜ i,Ĝj = Ĝ k ∈N (Ĝi) v t−1 G k ,Ĝi(S1)
zĜ i,Ĝj = σ(W z [xĜ i || sĜ i,Ĝj ] + b z ) (S2) rĜ k ,Ĝi = σ(W r [xĜ i || v t−1 G k ,Ĝi ] + b r ) (S3) vĜ i,Ĝj = Tanh(W [xĜ i || Ĝ k ∈N (Ĝi) rĜ k ,Ĝi • v t−1 G k ,Ĝi ]) (S4) v tĜ i,Ĝj = (1 − zĜ i,Ĝj ) sĜ i,Ĝj + zĜ i,Ĝj vĜ i,Ĝj(S5)
E ALGORITHM FOR HIERARCHICALLY DECODING STRUCTURED RNA a M TI refers to the threshold which set the maximum allowed number of topological prediction steps; M SI is another threshold to limit the length of each decoded nucleotide segment.

[H DETAILS FOR PARAMETERIZING PRIOR DISTRIBUTION USING NORMALIZING FLOW]
A normalizing flow involves a series of bijective transformation with tractable Jacobian logdeterminant, to map an observed datapoint x ∼ p θ (x) from a complex distribution to a simpler one, such as the standard normal distribution.
Considering the simplified case where we have a single bijective function f θ : Z → X to map some simple latent variables z to observed datapoint x, then, using the change of variable theorem, the likelihood of the observed datapoint can be evaluated as:
p θ (x) = p z (f −1 θ (x))|det ∂f −1 θ (x) ∂x | (S6)
where p z (.) denotes some simple base distribution, e.g. N (0; I). Then, it becomes clear the efficiency of this scheme heavily relies on the efficiency of inverting the forward mapping f θ as well as computing its Jacobian log-determinant.
In this project, we use a type of continuous normalizing flow (CNF) which simplifies the above mentioned computation (Chen et al., 2018). Consider a time continuous dynamics f ψ (z(t), t) of some intermediate data representation z(t), and again z(t 0 ) ∼ p z (.), the transformation of variable, along with its inverse mapping, can be expressed as:
z z(t 1 ) = z(t 0 ) + t1 t0 f ψ (z(t), t)dt (S7) z(t 0 ) = z(t 1 ) + t0 t1 f ψ (z(t), t)dt (S8)
and the change of probability density can be expressed as:
log p ψ (z) = log p z (z(t 0 )) − t1 t0 tr( ∂f θ ∂z(t) )dt (S9)
Note that the invertibility issue is no longer a concern under some mild constraints (Chen et al., 2018). Also, Eq. S9 only involves a more light-weight trace operation on the Jacobian rather than evaluating its log-determinant.
Therefore, we learn a parameterized prior using a CNF, and observe the decomposition of the KL term in the VAE objective:
KL(q φ (z|x)|p ψ (z)) = −E z∼q φ (z|x) [p ψ (z)] − H[q φ (z|x)](S10)
Therefore, during training our CNF parameterized with ψ works on the transformation of complex latent encodings z ∼ q φ (z|x) to some simple z(t 0 ) ∼ N (0; I), with an exact likelihood described by Eq. S9 and integrated into Eq. S10 for the complete training objective. During inference, we simply sample z t0 ∼ N (0; I), and use our CNF to reversely transform it to z ∼ p ψ (.) which should be closer to the approximate posterior. J HYPERPARAMETERS  The optimization takes place in the latent space of HierVAE, starting from the initial encoding of a random RNA molecule in the test set, and at each step altering the latent encoding by using activation maximization on the embedding classifier. The trajectory of generated RNAs is shown in the order of left to right and top to bottom, and the field PRED indicates that the probability of binding, as predicted by another external full classifier on the decoded molecular structure, is overall increasing as the decoded RNA structures smoothly evolving.

[F DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO LINEARIZED DECODING PROCEDURES]
When decoding from the joint vocabulary of sequence and dot-bracket structure ({A, C, G, U } × {., (, )}), whenever a nucleotide nuc i with a left bracket is sampled at step i, we append them to a stack, i.e. {(nuc i0 , i 0 ) . . . (nuc i , i)}. Then, at decode step j,
• if |i − j| ≤ 3, a proper mask will be added to the categorical logits of the vocabulary, to avoid sampling any nucleotides with right brackets, which means only an unpaired nucleotide or one that comes with a left bracket can be sampled; • if |i − j| > 3, a mask will be applied to make sure that only a nucleotide complementary to nuc i can be sampled with the right bracket. Sampling nucleotides with other forms of structures are allowed.
As soon as a nucleotide with a closing right bracket is sampled, we pop out (nuc i , i) from the stack. The special symbol for stop decoding can only be sampled when the stack has become empty.

[G DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO HIERARCHICAL DECODING PROCEDURES]
Additional constraints to be enforced during the hierarchical decoding process to ensure the validity of the decoded RNA secondary structure. Recall in section 3.2 that three types of predictions are involved with the hierarchical decoding, therefore, each type is associated with its own set of rules.
All set of rules can be observed by adding proper masks to the categorical logits before sampling, which are detailed below.
Constraints for making topological prediction, when the current node is
• stem node, then the algorithm always expands to a new node upon its first visit, or backtracks to its parent node upon re-visit; • hairpin node, then the algorithm always backtracks; • internal loop, then the algorithm acts similarly as for stem node; • multi-loop, then the algorithm always expands upon first visit and the next re-visit. Further re-visits to the same multi-loop node are not regulated.
Constraints for predicting new tree node, when the current node is • stem node, then its child node when exists can be either a hairpin loop, an internal loop, or a multi-loop; • hairpin node, internal loop or multi-loop, then its child node must be a stem node.
Constraints for decoding nucleotide segment. Due to the property of non-empty intersection between adjacent subgraphs, the start token for decoding a segment at the current node, is always the last nucleotide decoded at the last node. Therefore, without explicitly mentioning, the algorithm needs to decode at least one new nucleotide at each segment. When the current node is • stem node, and if it is upon its first visit (i.e. decoding the first segment of a stem), then there is no for constraints. Otherwise, upon its re-visit, the algorithm needs to decode exactly the complementary bases and in the reverse order, according to the first decoded segment; • hairpin node, then the decoder needs to decode at least four nucleotides before seeing the stop symbol, unless the hairpin is also the root node. • internal loop node, and if it is upon its first, then constraint is not necessary. Otherwise, upon its revisit, the algorithm needs to decode at least one unpaired nucleotide on condition that the first decoded internal loop segment does not contain any unpaired nucleotides; • multi-loop node, then there is no need for constraints. HuR 0.880 ± 0.000 0.880 ± 0.000 0.880 ± 0.000 0.888 ± 0.002 PTB 0.900 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 0.910 ± 0.000 QKI 0.820 ± 0.000 0.830 ± 0.000 0.825 ± 0.002 0.830 ± 0.000 Vts1 0.900 ± 0.000 0.908 ± 0.002 0.637 ± 0.079 0.910 ± 0.000 RBMY 0.905 ± 0.002 0.880 ± 0.003 0.802 ± 0.055 0.870 ± 0.002 SF2 0.890 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 0.900 ± 0.000 SLBP 0.777 ± 0.002 0.790 ± 0.000 0.797 ± 0.002 0.797 ± 0.002","[TITLE]
NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES

[ABSTRACT]
Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules."
Deep Neural Networks as Gaussian Processes,B1EA-M-0Z.json,"This paper presents a new covariance function for Gaussian processes (GPs) that is equivalent to a Bayesian deep neural network with a Gaussian prior on the weights and an infinite width. As a result, exact Bayesian inference with a deep neural network can be solved with the standard GP machinery.


Pros:

The result highlights an interesting relationship between deep nets and Gaussian processes. (Although I am unsure about how much of the kernel design had already appeared outside of the GP literature.)

The paper is clear and very well written.

The analysis of the phases in the hyperparameter space is interesting and insightful. On the other hand, one of the great assets of GPs is the powerful way to tune their hyperparameters via maximisation of the marginal likelihood but the authors have left this for future work!


Cons:

Although the computational complexity of computing the covariance matrix is given, no actual computational times are reported in the article.

I suggest using the same axis limits for all subplots in Figure 3.","[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency).

[CAPTIONS]
Table 1: Figure1: The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
Table 2: TO DEEP SIGNAL PROPAGATION Several prior works (Poole et al. (2016); Schoenholz et al. (2017); Daniely et al. (2016); Duvenaud et al. (
Table 3: Figure 2 :Figure 3 :23Figure2: Generalization gap for five hidden layer fully-connected networks with variable widths, using ReLU and Tanh nonlinearities on CIFAR-10. Random optimization and initialization hyperparameters were used and results were filtered for networks with 100% classification training accuracy, resulting in a total of 125 Tanh and 55 ReLU networks. The best generalizing networks are consistently the widest.
Table 4: Figure 4 :4Figure 4: The best performing NNGP hyperparameters agree with those predicted by deep signal propagation. Test set accuracy heatmaps for NNGPs evaluated for a grid of σ 2 w and σ 2 b values. The right plot in each subfigure (a), (b) is a theoretical phase diagram for that nonlinearity following the methodology ofSchoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure9.
Table 5: Figure 5 :5Figure 5: Samples from an NNGP prior for 1D functions. Different lines correspond to different draws (arbitrary colors).
Table 6: Figure 7 :7Figure 7: Graphical model for neural network's computation.
Table 7: Figure 10 :10Figure 10: Best performing NNGPs are distributed near the critical line. Weight and bias variance distribution for the 25 best performing runs for NNGP with the given training set size is shown.
Table 8: The NNGP often outperforms finite width networks. Test accuracy on MNIST and CIFAR-10 datasets. The reported NNGP results correspond to the best performing depth, σ 2 w , and σ 2 b values on the validation set. The traditional NN results correspond to the best performing depth, width and optimization hyperparameters. Best models for a given training set size are specified by (depthwidth-σ 2 w -σ 2 b ) for NNs and (depth-σ 2 w -σ 2 b ) for GPs. More results are in Appendix Table2.

[INTRODUCTION]
Deep neural networks have emerged in recent years as flexible parametric models which can fit complex patterns in data. As a contrasting approach, Gaussian processes have long served as a traditional nonparametric tool for modeling. An equivalence between these two approaches was derived in Neal (1994a), for the case of one layer networks in the limit of infinite width. Neal (1994a) further suggested that a similar correspondence might hold for deeper networks.
Consider a deep fully-connected neural network with i.i.d. random parameters. Each scalar output of the network, an affine transformation of the final hidden layer, will be a sum of i.i.d. terms. As we will discuss in detail below, in the limit of infinite width the Central Limit Theorem 1 implies that the function computed by the neural network (NN) is a function drawn from a Gaussian process (GP). In the case of single hidden-layer networks, the form of the kernel of this GP is well known (Neal (1994a); Williams (1997)). This correspondence implies that if we choose the hypothesis space to be the class of infinitely wide neural networks, an i.i.d. prior over weights and biases can be replaced with a corresponding GP prior over functions. As noted by (Williams, 1997), this substitution enables exact Bayesian inference for regression using neural networks. The computation requires building the necessary covariance matrices over the training and test sets and straightforward linear algebra computations.
In light of the resurgence in popularity of neural networks, it is timely to revisit this line of work. We delineate the correspondence between deep and wide neural networks and GPs and utilize it for Bayesian training of neural networks on regression tasks.

[RELATED WORK]
Our work touches on aspects of GPs, Bayesian learning, and compositional kernels. The correspondence between infinite neural networks and GPs was first noted by Neal (1994a;b). Williams (1997) computes analytic GP kernels for single hidden-layer neural networks with error function or Gaussian nonlinearities and noted the use of the GP prior for exact Bayesian inference in regression. Duvenaud et al. (2014) discusses several routes to building deep GPs and observes the degenerate form of kernels that are composed infinitely many times -a point we will return to Section 3.2but they do not derive the form of GP kernels as we do. Hazan & Jaakkola (2015) also discusses constructing kernels equivalent to infinitely wide deep neural networks, but their construction does not go beyond two hidden layers with nonlinearities.
Related work has also appeared outside of the GP context but in compositional kernel constructions. Cho & Saul (2009) derives compositional kernels for polynomial rectified nonlinearities, which includes the Sign and ReLU nonlinearities, and can be used in GPs; our manner of composing kernels matches theirs, though the context is different. Daniely et al. (2016) extends the construction of compositional kernels to neural networks whose underlying directed acyclic graph is of general form. They also prove, utilizing the formalism of dual activations, that compositional kernels originating from fully-connected topologies with the same nonlinearity become degenerate when composed infinitely many times. In a different context than compositional kernels, Poole et al. (2016); Schoenholz et al. (2017) study the same underlying recurrence relation for the specific case of fully-connected networks and bounded nonlinearities. They distinguish regions in hyperparameter space with different fixed points and convergence behavior in the recurrence relations. The focus in these works was to better understand the expressivity and trainability of deep networks.
Drawing inspiration from the multi-layer nature of deep neural networks, there is a line of work considering various approaches to stacking GPs, such as deep GPs (Lawrence & Moore (2007); Damianou & Lawrence (2013); Hensman & Lawrence (2014); Duvenaud et al. (2014); Bui et al. (2016)), which can give rise to a richer class of probabilistic models beyond GPs. This contrasts with our work, where we study GPs that are in direct correspondence with deep, infinitely wide neural networks. Krauth et al. (2016) has recently explored the performance of GP models with deep kernels given in Cho & Saul (2009), implemented with scalable approximations. However, they do not discuss the equivalence between deep neural networks and GPs with compositional kernels, which constitutes a conceptual contribution of our work. Furthermore, we note that the GP kernels in our work are more general than the compositional kernel construction outlined in Cho & Saul (2009) in two respects: (i) we are not limited to rectified polynomials but can deal with general nonlinearities, and (ii) we consider two additional hyperparameters in the kernels, which would correspond to the weight and bias parameter variances in a neural network. Finally, Gal & Ghahramani (2016) connects dropout in deep neural networks with approximate Bayesian inference in deep GPs.
Another series of recent works (Wilson et al. (2016b;a); Al-Shedivat et al. (2017)), termed deep kernel learning, utilize GPs with base kernels which take in features produced by a deep multilayer neural network, and train the resulting model end-to-end. Our work differs from these in that our GP corresponds to a multilayer neural network. Additionally, our GP kernels have many fewer parameters, and these parameters correspond to the hyperparameters of the equivalent neural network.

[SUMMARY OF CONTRIBUTIONS]
We begin by specifying the form of a GP which corresponds to a deep, infinitely wide neural network -hereafter referred to as the Neural Network GP (NNGP) -in terms of a recursive, deterministic computation of the kernel function. The prescription is valid for generic pointwise nonlinearities in fully-connected feedforward networks. We develop a computationally efficient method (Section 2.5) to compute the covariance function corresponding to deep neural networks with fixed hyperparameters.
In this work, as a first proof of concept of our NNGP construction, we focus on exact Bayesian inference for regression tasks, treating classification as regression on class labels. While less principled, least-squares classification performs well (Rifkin et al., 2003) and allows us to compare exact inference via a GP to prediction by a trained neural network on well-studied tasks (MNIST and CIFAR-10 classification). Note that it is possible to extend GPs to softmax classification with cross entropy loss (Williams & Barber (1998); Rasmussen & Williams (2006)), which we aim to investigate in future work.
We conduct experiments making Bayesian predictions on MNIST and CIFAR-10 (Section 3) and compare against NNs trained with standard gradient-based approaches. The experiments explore different hyperparameter settings of the Bayesian training including network depth, nonlinearity, training set size (up to and including the full dataset consisting of tens of thousands of images), and weight and bias variance. Our experiments reveal that the best NNGP performance is consistently competitive against that of NNs trained with gradient-based techniques, and the best NNGP setting, chosen across hyperparameters, often surpasses that of conventional training (Section 3, Table 1). We further observe that, with increasing network width, the performance of neural networks with gradient-based training approaches that of the NNGP computation, and that the GP uncertainty is strongly correlated with prediction error. Furthermore, the performance of the NNGP depends on the structure of the kernel, which can be connected to recent work on signal propagation in networks with random parameters (Schoenholz et al., 2017).

[DEEP, INFINITELY WIDE NEURAL NETWORKS ARE DRAWN FROM GPS]
We begin by specifying the correspondence between GPs and deep, infinitely wide neural networks, which hinges crucially on application of the Central Limit Theorem. We review the single-hidden layer case (Section 2.2) before moving to the multi-layer case (Section 2.3).

[NOTATION]
Consider an L-hidden-layer fully-connected neural network with hidden layers of width N l (for layer l) and pointwise nonlinearities φ. Let x ∈ R din denote the input to the network, and let z L ∈ R dout denote its output. The ith component of the activations in the lth layer, post-nonlinearity and postaffine transformation, are denoted x l i and z l i respectively. We will refer to these as the post-and pre-activations. (We let x 0 i ≡ x i for the input, dropping the Arabic numeral superscript, and instead use a Greek superscript x α to denote a particular input α). Weight and bias parameters for the lth layer have components W l ij , b l i , which are independent and randomly drawn, and we take them all to have zero mean and variances σ 2 w /N l and σ 2 b , respectively. GP(µ, K) denotes a Gaussian process with mean and covariance functions µ(•), K(•, •), respectively.

[REVIEW OF GAUSSIAN PROCESSES AND SINGLE-LAYER NEURAL NETWORKS]
We briefly review the correspondence between single-hidden layer neural networks and GPs (Neal (1994a;b); Williams (1997)). The ith component of the network output, z 1 i , is computed as,
z 1 i (x) = b 1 i + N1 j=1 W 1 ij x 1 j (x), x 1 j (x) = φ b 0 j + din k=1 W 0 jk x k ,(1)
where we have emphasized the dependence on input x. Because the weight and bias parameters are taken to be i.i.d., the post-activations x 1 j , x 1 j are independent for j = j . Moreover, since z 1 i (x) is a sum of i.i.d terms, it follows from the Central Limit Theorem that in the limit of infinite width N 1 → ∞, z 1 i (x) will be Gaussian distributed. Likewise, from the multidimensional Central Limit Theorem, any finite collection of {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have a joint multivariate Gaussian distribution, which is exactly the definition of a Gaussian process. Therefore we conclude that z 1 i ∼ GP(µ 1 , K 1 ), a GP with mean µ 1 and covariance K 1 , which are themselves independent of i.
Because the parameters have zero mean, we have that µ 1 (x) = E z 1 i (x) = 0 and,
K 1 (x, x ) ≡ E z 1 i (x)z 1 i (x ) = σ 2 b + σ 2 w E x 1 i (x)x 1 i (x ) ≡ σ 2 b + σ 2 w C(x, x ),(2)
where we have introduced C(x, x ) as in Neal (1994a); it is obtained by integrating against the distribution of W 0 , b 0 . Note that, as any two z 1 i , z 1 j for i = j are joint Gaussian and have zero covariance, they are guaranteed to be independent despite utilizing the same features produced by the hidden layer.

[GAUSSIAN PROCESSES AND DEEP NEURAL NETWORKS]
The arguments of the previous section can be extended to deeper layers by induction. We proceed by taking the hidden layer widths to be infinite in succession (N 1 → ∞, N 2 → ∞, etc.) as we continue with the induction, to guarantee that the input to the layer under consideration is already governed by a GP. In Appendix C we provide an alternative derivation in terms of Bayesian marginalization over intermediate layers, which does not depend on the order of limits, in the case of a Gaussian prior on the weights. A concurrent work (de G. Matthews et al., 2018) further derives the convergence rate towards a GP if all layers are taken to infinite width simultaneously, but at different rates.
Suppose that z l−1 j is a GP, identical and independent for every j (and hence x l j (x) are independent and identically distributed). After l − 1 steps, the network computes
z l i (x) = b l i + N l j=1 W l ij x l j (x), x l j (x) = φ(z l−1 j (x)).(3)
As before, z l i (x) is a sum of i.i.d. random terms so that, as N l → ∞, any finite collection {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have joint multivariate Gaussian distribution and
z l i ∼ GP(0, K l ). The covariance is K l (x, x ) ≡ E z l i (x)z l i (x ) = σ 2 b + σ 2 w E z l−1 i ∼GP(0,K l−1 ) φ(z l−1 i (x))φ(z l−1 i (x )) . (4)
By induction, the expectation in Equation 4 is over the GP governing z l−1 i , but this is equivalent to integrating against the joint distribution of only z l−1 i (x) and z l−1 i (x ). The latter is described by a zero mean, two-dimensional Gaussian whose covariance matrix has distinct entries K l−1 (x, x ), K l−1 (x, x), and K l−1 (x , x ). As such, these are the only three quantities that appear in the result. We introduce the shorthand
K l (x, x ) = σ 2 b + σ 2 w F φ K l−1 (x, x ), K l−1 (x, x), K l−1 (x , x )(5)
to emphasize the recursive relationship between K l and K l−1 via a deterministic function F whose form depends only on the nonlinearity φ. This gives an iterative series of computations which can be performed to obtain K L for the GP describing the network's final output.
For the base case
K 0 , suppose W 0 ij ∼ N (0, σ 2 w /d in ), b 0 j ∼ N (0, σ 2 b )
; we can utilize the recursion relating K 1 and K 0 , where
K 0 (x, x ) = E z 0 j (x)z 0 j (x ) = σ 2 b + σ 2 w x•x din .(6)
In fact, these recurrence relations have appeared in other contexts. They are exactly the relations derived in the mean field theory of signal propagation in fully-connected random neural networks (Poole et al. (2016);Schoenholz et al. (2017)) and also appear in the literature on compositional kernels (Cho & Saul (2009); Daniely et al. (2016)). For certain activation functions, Equation 5 can be computed analytically (Cho & Saul (2009); Daniely et al. (2016)). In the case of the ReLU nonlinearity, it yields the well-known arccosine kernel (Cho & Saul (2009)) whose form we reproduce in Appendix B. When no analytic form exists, it can instead be efficiently computed numerically, as described in Section 2.5.

[BAYESIAN TRAINING OF NEURAL NETWORKS USING GAUSSIAN PROCESS PRIORS]
Here we provide a short review of how a GP prior over functions can be used to do Bayesian inference; see e.g. (Rasmussen & Williams, 2006) for a comprehensive review of GPs. Given a dataset D = {(x 1 , t 1 ), ..., (x n , t n )} consisting of input-target pairs (x, t), we wish to make a Bayesian prediction at test point x * using a distribution over functions z(x). This distribution is constrained to take values z ≡ (z 1 , ..., z n ) on the training inputs x ≡ (x 1 , ..., x n ) and,
P (z * |D, x * ) = dz P (z * |z, x, x * ) P (z|D) = 1 P (t) dz P (z * , z|x * , x) P (t|z) ,(7)
where t = (t 1 , ..., t n ) T are the targets on the training set, and P (t|z) corresponds to observation noise. We will assume a noise model consisting of a Gaussian with variance σ 2 centered at z.
If the conditions of Section 2.2 or 2.3 apply, our choice of prior over functions implies that z 1 , ..., z n , z * are n + 1 draws from a GP and z * , z|x * , x ∼ N (0, K) is a multivariate Gaussian whose covariance matrix has the form
K = K D,D K T x * ,D K x * ,D K x * ,x * ,
where the block structure corresponds to the division between the training set and the test point. That is, K D,D is an n × n matrix whose (i, j)th element is K(x i , x j ) with x i , x j ∈ D, while e.g. the ith
element of K x * ,D is K(x * , x i ), x i ∈ D.
As is standard in GPs, the integral in Equation 7 can be done exactly, resulting in z * |D, x * ∼ N (μ,K) with
µ = K x * ,D (K D,D + σ 2 I n ) −1 t (8) K = K x * ,x * − K x * ,D (K D,D + σ 2 I n ) −1 K T x * ,D(9)
where I n is the n × n identity. The predicted distribution for z * |D, x * is hence determined from straightforward matrix computations, yet nonetheless corresponds to fully Bayesian training of the deep neural network. The form of the covariance function used is determined by the choice of GP prior, i.e. the neural network model class, which depends on depth, nonlinearity, and weight and bias variances. We henceforth resume placing a superscript L as in K L to emphasize the choice of depth for the compositional kernel.

[EFFICIENT IMPLEMENTATION OF THE GP KERNEL]
Given an L-layer deep neural network with fixed hyperparameters, constructing the covariance matrix K L for the equivalent GP involves computing the Gaussian integral in Equation 4 for all pairs of training-training and training-test points, recursively for all layers. For some nonlinearities, such as ReLU, this integration can be done analytically. However, to compute the kernel corresponding to arbitrary nonlinearities, the integral must be performed numerically.
The most direct implementation of a numerical algorithm for K L would be to compute integrals independently for each pair of datapoints and each layer. This is prohibitively expensive and costs O n 2 g L(n 2 train + n train n test ) , where n 2 g is the sampling density for the pair of Gaussian random variables in the 2D integral and n train , n test are the training and test set sizes, respectively. However, by careful pipelining, and by preprocessing all inputs to have identical norm, we can improve this cost to O n 2 g n v n c + L(n 2 train + n train n test ) , where n v and n c are sampling densities for a variance and correlation grid, as described below. In order to achieve this, we break the process into several steps: 1. Generate: pre-activations u = [−u max , • • • , u max ] consisting of n g elements linearly spaced between −u max and u max ; variances s = [0, • • • , s max ] with n v linearly spaced elements, where s max < u 2 max ; and correlations c = (−1, • • • , 1) with n c linearly spaced elements. Note that we are using fixed, rather than adaptive, sampling grids to allow operations to be parallelized and reused across datapoints and layers.
2. Populate a matrix F containing a lookup table for the function F φ in Equation 5. This involves numerically approximating a Gaussian integral, in terms of the marginal variances s and correlations c. We guarantee that the marginal variance is identical for each datapoint, by preprocessing all datapoints to have identical norm at the input layer, so the number of entries in the lookup table need only be n v n c . These entries are computed as 2 :
F ij = ab φ(u a )φ(u b ) exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b ab exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b .(10)
3. For every pair of datapoints x and x in layer l, compute K l (x, x ) using Equation 5.
Approximate the function F φ K l−1 (x, x ); K l−1 (x, x); K l−1 (x , x ) by bilinear interpolation into the matrix F from Step 2, where we interpolate into s using the value of K l−1 (x, x), and interpolate into c using K l−1 (x, x )/K l−1 (x, x) . Remember that K l−1 (x, x) = K l−1 (x , x ), due to data preprocessing to guarantee constant norm.
4. Repeat the previous step recursively for all layers. Bilinear interpolation has constant cost, so this has cost O L(n 2 train + n train n test ) .
This computational recipe allows us to compute the covariance matrix for the NNGP corresponding to any well-behaved nonlinearity φ. All computational steps above can be implemented using accelerated tensor operations, and computation of K L is typically faster than solving the system of linear equations in Equation 8-9. Figure 6 illustrates the close agreement between the kernel function computed numerically (using this approach) and analytically, for the ReLU nonlinearity. It also illustrates the angular dependence of the kernel and its evolution with increasing depth.
Finally, note that the full computational pipeline is deterministic and differentiable. The shape and properties of a deep network kernel are purely determined by hyperparameters of the deep neural network. Since GPs give exact marginal likelihood estimates, this kernel construction may allow principled hyperparameter selection, or nonlinearity design, e.g. by gradient ascent on the log likelihood w.r.t. the hyperparameters. Although this is not the focus of current work, we hope to return to this topic in follow-up work.
An open source implementation of the algorithm is available at https://github.com/brainresearch/nngp.

[DESCRIPTION]
We compare NNGPs with SGD 3 trained neural networks on the permutation invariant MNIST and CIFAR-10 datasets. The baseline neural network is a fully-connected network with identical width at each hidden layer. Training is on the mean squared error (MSE) loss, chosen so as to allow direct comparison to GP predictions. Formulating classification as regression often leads to good results (Rifkin & Klautau, 2004). Future work may involve evaluating the NNGP on a cross entropy loss using the approach in (Williams & Barber, 1998;Rasmussen & Williams, 2006). Training used the Adam optimizer (Kingma & Ba (2014)) with learning rate and initial weight/bias variances optimized over validation error using the Google Vizier hyperparameter tuner (Golovin et al., 2017). Dropout was not used. In future work, it would be interesting to incorporate dropout into the NNGP covariance matrix using an approach like that in (Schoenholz et al., 2017). For the study, nonlinearities were chosen to be either rectified linear units (ReLU) or hyperbolic tangent (Tanh). Class labels were encoded as a one-hot, zero-mean, regression target (i.e., entries of -0.1 for the incorrect class and 0.9 for the correct class). We constructed the covariance kernel numerically for ReLU and Tanh nonlinearities following the method described in Section 2.5.
Performance: We find that the NNGP often outperforms trained finite width networks. See Table 1 and Figure 1. The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
We additionally find the performance of the best finite-width NNs, trained with a variant of SGD, approaches that of the NNGP with increasing layer width. This is interesting from at least two, potentially related, standpoints. (1) NNs are commonly believed to be powerful because of their ability to do flexible representation learning, while our NNGP uses fixed basis functions; nonetheless, in our experiments we find no salient performance advantage to the former. (2) It hints at a possible relationship between SGD and Bayesian inference in certain regimes -were the neural networks trained in a fully Bayesian fashion, rather than by SGD, the approach to NNGP in the large width limit would be guaranteed. There is recent work suggesting that SGD can implement approximate Bayesian inference (Mandt et al., 2017) under certain assumptions.
The similarity of the performance of the widest NN in Figure 1 with the NNGP suggests that the limit of infinite network width, which is inherent to the GP, is far from being a disadvantage. Indeed, in practice it is found that the best generalizing NNs are in fact the widest. To support this, in Figure 2 we show generalization gap results from an experiment in which we train 180 fully-connected networks with five hidden layers on CIFAR-10 with a range of layer widths. For this experiment, we trained the networks using a standard cross entropy loss rather than MSE, leading to a slight difference in performance.
Uncertainty: One benefit in using a GP is that, due to its Bayesian nature, all predictions have uncertainty estimates (Equation 9). For conventional neural networks, capturing the uncertainty in a model's predictions is challenging (Gal, 2016). In the NNGP, every test point has an explicit estimate of prediction variance associated with it (Equation 9). In our experiments, we observe that the NNGP uncertainty estimate is highly correlated with prediction error (Figure 3).  2014)) have noted the recurrence relations Equation 5 commonly approach a functionally uninteresting fixed point with depth l → ∞, in that K ∞ (x, x ) becomes a constant or piecewise constant map. We now briefly relate our ability to train NNGPs with the convergence of K l (x, x ) to the fixed-point kernel.

[RELATIONSHIP]
We will be particularly interested in contextualizing our results in relation to Poole et al. (2016);Schoenholz et al. (2017) which analyzed the fixed points and the approach to them in detail for bounded nonlinearities. To briefly recapitulate: there are regions of hyperparameter space (called phases) where K ∞ (x, x ) changes only quantitatively with σ 2 w and σ 2 b . However, there are low For the Tanh nonlinearity, there are two distinct phases respectively called the ""ordered"" phase and the ""chaotic"" phase that can be understood as a competition between the weights and the biases of the network. A diagram showing these phases and the boundary between them is shown in Figure 4a. In the ordered phase, the features obtained by propagating an input through the each layer of the recursion become similar for dissimilar inputs. Fundamentally, this occurs because the different inputs share common bias vectors and so all inputs end up just approaching the random bias. In this case the covariance K l (x, x ) → q * for every pair of inputs x, x , where q * is a constant that depends only on σ 2 w and σ 2 b . All inputs have unit correlation asymptotically with depth. By contrast in the chaotic phase the weight variance σ 2 w dominates and similar inputs become dissimilar with depth as they are randomly projected by the weight matrices. In this case, the covariance K l (x, x ) → q * for x = x but q * c * for x = x . Here c * < 1 is the fixed point correlation. In each of these regimes, there is also a finite depth-scale ξ which describes the characteristic number of layers over which the covariance function decays exponentially towards its fixed point form. Exactly at the boundary between these two regimes is a line in (σ 2 w , σ 2 b )-space where the decay K l (x, x ) towards its fixed  2017) that this approach to the fixed-point covariance fundamentally bounded whether or not neural networks could successfully be trained. It was shown that initializing networks on this line allowed for significantly deeper neural networks to be trained.
For ReLU networks a similar picture emerges, however there are some subtleties due to the unbounded nature of the nonlinearity. In this case for all σ 2 w and σ 2 b , K ∞ (x, x ) = q * for all x, x and every point becomes asymptotically correlated. Despite this, there are again two phases: a ""bounded"" phase in which q * is finite (and nonzero) and an unbounded phase in which q * is either infinite or zero. As in the Tanh case there are depth scales that control the rate of convergence to these fixed points and therefore limit the maximum trainable depth. The phase diagram for the ReLU nonlinearity is also shown in Figure 4b.
In a striking analogy with the trainability of neural networks, we observe that the performance of the NNGP appears to closely track the structure from the phase diagram, clearly illustrated in Figure 4. Indeed, we see that as for hyperparameter settings that are far from criticality, the GP is unable to train and we encounter poor test set performance. By contrast, near criticality we observe that our models display high accuracy. Moreover, we find that the accuracy appears to drop more quickly away from the phase boundary with increase in depth L of the GP kernel, K L . To understand this effect we note that information about data will be available to our model only through the difference K L (x, x ) − K ∞ (x, x ). However, as the depth gets larger, this difference becomes increasingly small and at some point can no longer be represented due to numerical precision. At this point our test accuracy begins to quickly degrade to random chance.

[CONCLUSION AND FUTURE DIRECTIONS]
By harnessing the limit of infinite width, we have specified a correspondence between priors on deep neural networks and Gaussian processes whose kernel function is constructed in a compositional, but fully deterministic and differentiable, manner. Use of a GP prior on functions enables exact Bayesian inference for regression from matrix computations, and hence we are able to obtain predictions and uncertainty estimates from deep neural networks without stochastic gradient-based training. The performance is competitive with the best neural networks (within specified class of fully-connected models) trained on the same regression task under similar hyperparameter settings. While we were able to run experiments for somewhat large datasets (sizes of 50k), we intend to look into scalability for larger learning tasks, possibly harnessing recent progress in scalable GPs (Quiñonero-Candela & Rasmussen (2005); Hensman et al. (2013)).  b) is a theoretical phase diagram for that nonlinearity following the methodology of Schoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure 9.
In our experiments, we observed the performance of the optimized neural network appears to approach that of the GP computation with increasing width. Whether gradient-based stochastic optimization implements an approximate Bayesian computation is an interesting question (Mandt et al., 2017). Further investigation is needed to determine if SGD does approximately implement Bayesian inference under the conditions typically employed in practice.
Additionally, the NNGP provides explicit estimates of uncertainty. This may be useful in predicting model failure in critical applications of deep learning, or for active learning tasks where it can be used to identify the best datapoints to hand label.
A DRAWS FROM AN NNGP PRIOR Figure 5 illustrates the nature of the GP prior for the ReLU nonlinearity by depicting samples of 1D functions z(x) drawn from a ReLU GP, GP(0, K L ), with fixed depth L = 10 and (σ 2 w , σ 2 b ) = (1.8, 0.01). Figure 6: The angular structure of the kernel and its evolution with depth. Also illustrated is the good agreement between the kernel computed using the methods of Section 2.5 (blue, starred) and the analytic form of the kernel (red). The depth l in K l runs from l = 0, ..., 9 (flattened curves for increasing l), and (σ 2 w , σ 2 b ) = (1.6, 0.1).
In the main text, we noted that the recurrence relation Equation 5 can be computed analytically for certain nonlinearities. In particular, this was computed in Cho & Saul (2009) for polynomial rectified nonlinearities. For ReLU, the result including the weight and bias variance is
K l (x, x ) = σ 2 b + σ 2 w 2π K l−1 (x, x)K l−1 (x , x ) sin θ l−1 x,x + (π − θ l−1 x,x ) cos θ l−1 x,x , θ l x,x = cos −1 K l (x, x ) K l (x, x)K l (x , x ) . (11
)
To illustrate the angular form of K l (x, x ) and its evolution with l, in Figure 6 we plot K l (θ) for the ReLU nonlinearity, where θ is the angle between x and x with norms such that ||x|| 2 = ||x || 2 = d in . We observe a flattening of the angular structure with increase in depth l, as predicted from the understanding in Section 3.2. Simultaneously, the figure also illustrates the good agreement between the kernel computed using the numerical implementation of Section 2.5 (blue, starred) and the analytic arccosine kernel, Equation 11 (red), for a particular choice of hyperparameters (σ 2 w , σ 2 b ).

[C BAYESIAN MARGINALIZATION OVER INTERMEDIATE LAYERS]
In this section, we present an alternate derivation of the equivalence between infinitely wide deep neural networks and Gaussian process by marginalization over intermediate layers. For this derivation, we take the weight and bias parameters to be drawn from independent Gaussians, with zero mean and appropriately scaled variance.
We are interested in finding the distribution p(z L |x) over network outputs z L ∈ R dout×B , conditioned on network inputs x ∈ R din×B , for input dimensionality d in , output dimensionality d out , and dataset size B. Intervening layers will have width N l , z l ∈ R N l+1 ×B for L > l > 0. We define the second moment matrix (here post-nonlinearity) for each layer l to be
K l ab = 1 din n x na x nb l = 0 1 N l n φ(z l−1 na )φ(z l−1 nb ) l > 0 . (12
)
Our approach is to think of intermediate random variables corresponding to these second moments defined above. By definition, K l only depends on z l−1 . In turn, the pre-activations z l are described by a Gaussian process conditioned on the second moment matrix K l ,
p(z l |K l ) = N vec z l ; 0, G K l ⊗ I =: GP z l ; 0, G K l ,(13)
where
G K l := σ 2 w K l + σ 2 b 11 T .(14)
This correspondence of each layer to a GP, conditioned on the layer's second moment matrix, is exact even for finite width N l because the parameters are drawn from a Gaussian. Altogether, this justifies the graphical model depicted in Figure 7.
We will write p(z L |x) as an integral over all the intervening second moment matrices K l ,
p(z L |x) = p z L , K 0 , K 1 , • • • , K L |x dK 0•••L . (15
)
This joint distribution can be decomposed as
p(z L |x) = p(z L |K L ) L l=1 p(K l |K l−1 ) p(K 0 |x)dK 0•••L .(16)
The directed decomposition in Equation 16 holds because K L is a function only of z L−1 (see Equation 12), z L−1 depends only on K L−1 (see Equation 13), K L−1 is a function only of z L−2 , etc (Figure 7). The sum in Equation 12for l > 0 is a sum over i.i.d. terms. As N l grows large, the Central Limit Theorem applies, and p K l |K l−1 converges to a Gaussian with variance that shrinks as 1 N l . Further, in the infinite width limit it will go to a delta function,
z L K L ... K 1 K 0 z L−1 K L−1 z L−2 x z 0
lim N l →∞ p K l |K l−1 = δ K l − (F • G) K l−1 ,(17)
with F (•) defined as in Equation 5. Similarly, the dependence of K 0 on x can be expressed as a delta function,
p K 0 |x = δ K 0 − 1 d in x T x .(18)
Substituting p(z L |K L ), p K l |K l−1 and p K 0 |x into Equation 16, we get
lim N L →∞,...,N1→∞ p(z L |x) = GP z L ; 0, G K L L l=1 δ K l − (F • G) K l−1 δ K 0 − 1 d in x T x dK 0•••L = GP z L ; 0, G • (F • G) L 1 d in x T x = GP z L ; 0, G • (F • G) L K 0 . (19
)
So, in the limit of infinite width, z L |x is described by a Gaussian process with kernel G • (F • G)
L K 0 .

[D DETAILS OF THE EXPERIMENTS]
We outline details of the experiments for Section 3. For MNIST we use a 50k/10k/10k split of the training/validation/test dataset. For CIFAR-10, we used a 45k/5k/10k split. The validation set was used for choosing the best hyperparameters and evaluation on the test set is reported.
For training neural networks hyperparameters were optimized via random search on average 250 trials for each choice of (n train , depth, width, nonlinearity).
Random search range: Learning rate was sampled within (10 −4 , 0.2) in log-scale, weight decay constant was sampled from (10 −8 , 1.0) in log-scale, σ w ∈ [0.01, 2.5], σ b ∈ [0, 1.5] was uniformly sampled and mini-batch size was chosen equally among [16,32,64,128,256].
For the GP with given depth and nonlinearity, a grid of 30 points evenly spaced from 0.1 to 5.0 (for σ 2 w ) and 30 points evenly spaced from 0 to 2.0 (for σ 2 b ) was evaluated to generate the heatmap. The best GP run was chosen among the 900 evaluations in the σ 2 w -σ 2 b grid. Computation time: We report computation times for NNGP experiments. The grid generation with took 440-460s with 6 CPUs for n g = 501, n v = 501, n c = 500, which was amortized over all the experiments. For full (50k) MNIST, constructing K DD for each layer took 90-140s (depending on CPU generation) running on 64 CPUs. Solving linear equations via Cholesky decomposition took 180-220s for 1000 test points.

[DETAILS OF NNGP IMPLEMENTAION:]
For all the experiments we used pre-computed lookup tables F with n g = 501, n v = 501, n c = 500, and s max = 100. Default value for the target noise σ 2 was set to 10 −10 and was increased by factor of 10 when Cholesky decomposition failed while solving Equation 8 and 9. We refer to Rasmussen & Williams (2006) for standard numerically stable implementation of GP regression.

[E FURTHER RESULTS]
Here we include more results from experiments described in Section 3.
Uncertainty: Relationship between the target MSE and the GP's uncertainty estimate for smaller training set size is shown in Figure 8.
Performance: Performance of grid points of σ 2 w -σ 2 b for varying depth is shown in Figure 9. The best performing NNGP's hyperparameters are distributed near the critical line (Figure 10) where the phase changes as described in Section 3.2. 

[ACKNOWLEDGMENTS]
We thank Ryan Adams, Samy Bengio, and Matt Hoffman for useful discussions and feedback, and Gamaleldin Elsayed and Daniel Levy for helpful comments on the manuscript.","[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency)."
Deep Neural Networks as Gaussian Processes,B1EA-M-0Z.json,"This paper leverages how deep Bayesian NNs, in the limit of infinite width, are Gaussian processes (GPs). After characterizing the kernel function, this allows us to use the GP framework for prediction, model selection, uncertainty estimation, etc.


- Pros of this work

The paper provides a specific method to efficiently compute the covariance matrix of the equivalent GP and shows experimentally on CIFAR and MNIST the benefits of using the this GP as opposed to a finite-width non-Bayesian NN.

The provided phase analysis and its relation to the depth of the network is also very interesting.

Both are useful contributions as long as deep wide Bayesian NNs are concerned. A different question is whether that regime is actually useful.


- Cons of this work

Although this work introduces a new GP covariance function inspired by deep wide NNs, I am unconvinced of the usefulness of this regime for the cases in which deep learning is useful. 

For instance, looking at the experiments, we can see that on MNIST-50k (the one with most data, and therefore, the one that best informs about the ""true"" underlying NN structure) the inferred depth is 1 for the GP and 2 for the NN, i.e., not deep. Similarly for CIFAR, where only up to depth 3 is used. None of these results beat state-of-the-art deep NNs.

Also, the results about the phase structure show how increased depth makes the parameter regime in which these networks work more and more constrained. 

In [1], it is argued that kernel machines with fixed kernels do not learn a hierarchical representation. And such representation is generally regarded as essential for the success of deep learning. 

My impression is that the present line of work will not be relevant for deep learning and will not beat state-of-the-art results because of the lack of a structured prior. In that sense, to me this work is more of a negative result informing that to be successful, deep Bayesian NNs should not be wide and should have more structure to avoid reaching the GP regime.


- Other comments:

In Fig. 5, use a consistent naming for the axes (bias and variances).

In Fig. 1, I didn't find the meaning of the acronym NN with no specified width.

Does the unit norm normalization used to construct the covariance disallow ARD input selection?


[1] Yoshua Bengio, Olivier Delalleau, and Nicolas Le Roux. The Curse of Dimensionality for Local Kernel Machines. 2005.","[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency).

[CAPTIONS]
Table 1: Figure1: The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
Table 2: TO DEEP SIGNAL PROPAGATION Several prior works (Poole et al. (2016); Schoenholz et al. (2017); Daniely et al. (2016); Duvenaud et al. (
Table 3: Figure 2 :Figure 3 :23Figure2: Generalization gap for five hidden layer fully-connected networks with variable widths, using ReLU and Tanh nonlinearities on CIFAR-10. Random optimization and initialization hyperparameters were used and results were filtered for networks with 100% classification training accuracy, resulting in a total of 125 Tanh and 55 ReLU networks. The best generalizing networks are consistently the widest.
Table 4: Figure 4 :4Figure 4: The best performing NNGP hyperparameters agree with those predicted by deep signal propagation. Test set accuracy heatmaps for NNGPs evaluated for a grid of σ 2 w and σ 2 b values. The right plot in each subfigure (a), (b) is a theoretical phase diagram for that nonlinearity following the methodology ofSchoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure9.
Table 5: Figure 5 :5Figure 5: Samples from an NNGP prior for 1D functions. Different lines correspond to different draws (arbitrary colors).
Table 6: Figure 7 :7Figure 7: Graphical model for neural network's computation.
Table 7: Figure 10 :10Figure 10: Best performing NNGPs are distributed near the critical line. Weight and bias variance distribution for the 25 best performing runs for NNGP with the given training set size is shown.
Table 8: The NNGP often outperforms finite width networks. Test accuracy on MNIST and CIFAR-10 datasets. The reported NNGP results correspond to the best performing depth, σ 2 w , and σ 2 b values on the validation set. The traditional NN results correspond to the best performing depth, width and optimization hyperparameters. Best models for a given training set size are specified by (depthwidth-σ 2 w -σ 2 b ) for NNs and (depth-σ 2 w -σ 2 b ) for GPs. More results are in Appendix Table2.

[INTRODUCTION]
Deep neural networks have emerged in recent years as flexible parametric models which can fit complex patterns in data. As a contrasting approach, Gaussian processes have long served as a traditional nonparametric tool for modeling. An equivalence between these two approaches was derived in Neal (1994a), for the case of one layer networks in the limit of infinite width. Neal (1994a) further suggested that a similar correspondence might hold for deeper networks.
Consider a deep fully-connected neural network with i.i.d. random parameters. Each scalar output of the network, an affine transformation of the final hidden layer, will be a sum of i.i.d. terms. As we will discuss in detail below, in the limit of infinite width the Central Limit Theorem 1 implies that the function computed by the neural network (NN) is a function drawn from a Gaussian process (GP). In the case of single hidden-layer networks, the form of the kernel of this GP is well known (Neal (1994a); Williams (1997)). This correspondence implies that if we choose the hypothesis space to be the class of infinitely wide neural networks, an i.i.d. prior over weights and biases can be replaced with a corresponding GP prior over functions. As noted by (Williams, 1997), this substitution enables exact Bayesian inference for regression using neural networks. The computation requires building the necessary covariance matrices over the training and test sets and straightforward linear algebra computations.
In light of the resurgence in popularity of neural networks, it is timely to revisit this line of work. We delineate the correspondence between deep and wide neural networks and GPs and utilize it for Bayesian training of neural networks on regression tasks.

[RELATED WORK]
Our work touches on aspects of GPs, Bayesian learning, and compositional kernels. The correspondence between infinite neural networks and GPs was first noted by Neal (1994a;b). Williams (1997) computes analytic GP kernels for single hidden-layer neural networks with error function or Gaussian nonlinearities and noted the use of the GP prior for exact Bayesian inference in regression. Duvenaud et al. (2014) discusses several routes to building deep GPs and observes the degenerate form of kernels that are composed infinitely many times -a point we will return to Section 3.2but they do not derive the form of GP kernels as we do. Hazan & Jaakkola (2015) also discusses constructing kernels equivalent to infinitely wide deep neural networks, but their construction does not go beyond two hidden layers with nonlinearities.
Related work has also appeared outside of the GP context but in compositional kernel constructions. Cho & Saul (2009) derives compositional kernels for polynomial rectified nonlinearities, which includes the Sign and ReLU nonlinearities, and can be used in GPs; our manner of composing kernels matches theirs, though the context is different. Daniely et al. (2016) extends the construction of compositional kernels to neural networks whose underlying directed acyclic graph is of general form. They also prove, utilizing the formalism of dual activations, that compositional kernels originating from fully-connected topologies with the same nonlinearity become degenerate when composed infinitely many times. In a different context than compositional kernels, Poole et al. (2016); Schoenholz et al. (2017) study the same underlying recurrence relation for the specific case of fully-connected networks and bounded nonlinearities. They distinguish regions in hyperparameter space with different fixed points and convergence behavior in the recurrence relations. The focus in these works was to better understand the expressivity and trainability of deep networks.
Drawing inspiration from the multi-layer nature of deep neural networks, there is a line of work considering various approaches to stacking GPs, such as deep GPs (Lawrence & Moore (2007); Damianou & Lawrence (2013); Hensman & Lawrence (2014); Duvenaud et al. (2014); Bui et al. (2016)), which can give rise to a richer class of probabilistic models beyond GPs. This contrasts with our work, where we study GPs that are in direct correspondence with deep, infinitely wide neural networks. Krauth et al. (2016) has recently explored the performance of GP models with deep kernels given in Cho & Saul (2009), implemented with scalable approximations. However, they do not discuss the equivalence between deep neural networks and GPs with compositional kernels, which constitutes a conceptual contribution of our work. Furthermore, we note that the GP kernels in our work are more general than the compositional kernel construction outlined in Cho & Saul (2009) in two respects: (i) we are not limited to rectified polynomials but can deal with general nonlinearities, and (ii) we consider two additional hyperparameters in the kernels, which would correspond to the weight and bias parameter variances in a neural network. Finally, Gal & Ghahramani (2016) connects dropout in deep neural networks with approximate Bayesian inference in deep GPs.
Another series of recent works (Wilson et al. (2016b;a); Al-Shedivat et al. (2017)), termed deep kernel learning, utilize GPs with base kernels which take in features produced by a deep multilayer neural network, and train the resulting model end-to-end. Our work differs from these in that our GP corresponds to a multilayer neural network. Additionally, our GP kernels have many fewer parameters, and these parameters correspond to the hyperparameters of the equivalent neural network.

[SUMMARY OF CONTRIBUTIONS]
We begin by specifying the form of a GP which corresponds to a deep, infinitely wide neural network -hereafter referred to as the Neural Network GP (NNGP) -in terms of a recursive, deterministic computation of the kernel function. The prescription is valid for generic pointwise nonlinearities in fully-connected feedforward networks. We develop a computationally efficient method (Section 2.5) to compute the covariance function corresponding to deep neural networks with fixed hyperparameters.
In this work, as a first proof of concept of our NNGP construction, we focus on exact Bayesian inference for regression tasks, treating classification as regression on class labels. While less principled, least-squares classification performs well (Rifkin et al., 2003) and allows us to compare exact inference via a GP to prediction by a trained neural network on well-studied tasks (MNIST and CIFAR-10 classification). Note that it is possible to extend GPs to softmax classification with cross entropy loss (Williams & Barber (1998); Rasmussen & Williams (2006)), which we aim to investigate in future work.
We conduct experiments making Bayesian predictions on MNIST and CIFAR-10 (Section 3) and compare against NNs trained with standard gradient-based approaches. The experiments explore different hyperparameter settings of the Bayesian training including network depth, nonlinearity, training set size (up to and including the full dataset consisting of tens of thousands of images), and weight and bias variance. Our experiments reveal that the best NNGP performance is consistently competitive against that of NNs trained with gradient-based techniques, and the best NNGP setting, chosen across hyperparameters, often surpasses that of conventional training (Section 3, Table 1). We further observe that, with increasing network width, the performance of neural networks with gradient-based training approaches that of the NNGP computation, and that the GP uncertainty is strongly correlated with prediction error. Furthermore, the performance of the NNGP depends on the structure of the kernel, which can be connected to recent work on signal propagation in networks with random parameters (Schoenholz et al., 2017).

[DEEP, INFINITELY WIDE NEURAL NETWORKS ARE DRAWN FROM GPS]
We begin by specifying the correspondence between GPs and deep, infinitely wide neural networks, which hinges crucially on application of the Central Limit Theorem. We review the single-hidden layer case (Section 2.2) before moving to the multi-layer case (Section 2.3).

[NOTATION]
Consider an L-hidden-layer fully-connected neural network with hidden layers of width N l (for layer l) and pointwise nonlinearities φ. Let x ∈ R din denote the input to the network, and let z L ∈ R dout denote its output. The ith component of the activations in the lth layer, post-nonlinearity and postaffine transformation, are denoted x l i and z l i respectively. We will refer to these as the post-and pre-activations. (We let x 0 i ≡ x i for the input, dropping the Arabic numeral superscript, and instead use a Greek superscript x α to denote a particular input α). Weight and bias parameters for the lth layer have components W l ij , b l i , which are independent and randomly drawn, and we take them all to have zero mean and variances σ 2 w /N l and σ 2 b , respectively. GP(µ, K) denotes a Gaussian process with mean and covariance functions µ(•), K(•, •), respectively.

[REVIEW OF GAUSSIAN PROCESSES AND SINGLE-LAYER NEURAL NETWORKS]
We briefly review the correspondence between single-hidden layer neural networks and GPs (Neal (1994a;b); Williams (1997)). The ith component of the network output, z 1 i , is computed as,
z 1 i (x) = b 1 i + N1 j=1 W 1 ij x 1 j (x), x 1 j (x) = φ b 0 j + din k=1 W 0 jk x k ,(1)
where we have emphasized the dependence on input x. Because the weight and bias parameters are taken to be i.i.d., the post-activations x 1 j , x 1 j are independent for j = j . Moreover, since z 1 i (x) is a sum of i.i.d terms, it follows from the Central Limit Theorem that in the limit of infinite width N 1 → ∞, z 1 i (x) will be Gaussian distributed. Likewise, from the multidimensional Central Limit Theorem, any finite collection of {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have a joint multivariate Gaussian distribution, which is exactly the definition of a Gaussian process. Therefore we conclude that z 1 i ∼ GP(µ 1 , K 1 ), a GP with mean µ 1 and covariance K 1 , which are themselves independent of i.
Because the parameters have zero mean, we have that µ 1 (x) = E z 1 i (x) = 0 and,
K 1 (x, x ) ≡ E z 1 i (x)z 1 i (x ) = σ 2 b + σ 2 w E x 1 i (x)x 1 i (x ) ≡ σ 2 b + σ 2 w C(x, x ),(2)
where we have introduced C(x, x ) as in Neal (1994a); it is obtained by integrating against the distribution of W 0 , b 0 . Note that, as any two z 1 i , z 1 j for i = j are joint Gaussian and have zero covariance, they are guaranteed to be independent despite utilizing the same features produced by the hidden layer.

[GAUSSIAN PROCESSES AND DEEP NEURAL NETWORKS]
The arguments of the previous section can be extended to deeper layers by induction. We proceed by taking the hidden layer widths to be infinite in succession (N 1 → ∞, N 2 → ∞, etc.) as we continue with the induction, to guarantee that the input to the layer under consideration is already governed by a GP. In Appendix C we provide an alternative derivation in terms of Bayesian marginalization over intermediate layers, which does not depend on the order of limits, in the case of a Gaussian prior on the weights. A concurrent work (de G. Matthews et al., 2018) further derives the convergence rate towards a GP if all layers are taken to infinite width simultaneously, but at different rates.
Suppose that z l−1 j is a GP, identical and independent for every j (and hence x l j (x) are independent and identically distributed). After l − 1 steps, the network computes
z l i (x) = b l i + N l j=1 W l ij x l j (x), x l j (x) = φ(z l−1 j (x)).(3)
As before, z l i (x) is a sum of i.i.d. random terms so that, as N l → ∞, any finite collection {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have joint multivariate Gaussian distribution and
z l i ∼ GP(0, K l ). The covariance is K l (x, x ) ≡ E z l i (x)z l i (x ) = σ 2 b + σ 2 w E z l−1 i ∼GP(0,K l−1 ) φ(z l−1 i (x))φ(z l−1 i (x )) . (4)
By induction, the expectation in Equation 4 is over the GP governing z l−1 i , but this is equivalent to integrating against the joint distribution of only z l−1 i (x) and z l−1 i (x ). The latter is described by a zero mean, two-dimensional Gaussian whose covariance matrix has distinct entries K l−1 (x, x ), K l−1 (x, x), and K l−1 (x , x ). As such, these are the only three quantities that appear in the result. We introduce the shorthand
K l (x, x ) = σ 2 b + σ 2 w F φ K l−1 (x, x ), K l−1 (x, x), K l−1 (x , x )(5)
to emphasize the recursive relationship between K l and K l−1 via a deterministic function F whose form depends only on the nonlinearity φ. This gives an iterative series of computations which can be performed to obtain K L for the GP describing the network's final output.
For the base case
K 0 , suppose W 0 ij ∼ N (0, σ 2 w /d in ), b 0 j ∼ N (0, σ 2 b )
; we can utilize the recursion relating K 1 and K 0 , where
K 0 (x, x ) = E z 0 j (x)z 0 j (x ) = σ 2 b + σ 2 w x•x din .(6)
In fact, these recurrence relations have appeared in other contexts. They are exactly the relations derived in the mean field theory of signal propagation in fully-connected random neural networks (Poole et al. (2016);Schoenholz et al. (2017)) and also appear in the literature on compositional kernels (Cho & Saul (2009); Daniely et al. (2016)). For certain activation functions, Equation 5 can be computed analytically (Cho & Saul (2009); Daniely et al. (2016)). In the case of the ReLU nonlinearity, it yields the well-known arccosine kernel (Cho & Saul (2009)) whose form we reproduce in Appendix B. When no analytic form exists, it can instead be efficiently computed numerically, as described in Section 2.5.

[BAYESIAN TRAINING OF NEURAL NETWORKS USING GAUSSIAN PROCESS PRIORS]
Here we provide a short review of how a GP prior over functions can be used to do Bayesian inference; see e.g. (Rasmussen & Williams, 2006) for a comprehensive review of GPs. Given a dataset D = {(x 1 , t 1 ), ..., (x n , t n )} consisting of input-target pairs (x, t), we wish to make a Bayesian prediction at test point x * using a distribution over functions z(x). This distribution is constrained to take values z ≡ (z 1 , ..., z n ) on the training inputs x ≡ (x 1 , ..., x n ) and,
P (z * |D, x * ) = dz P (z * |z, x, x * ) P (z|D) = 1 P (t) dz P (z * , z|x * , x) P (t|z) ,(7)
where t = (t 1 , ..., t n ) T are the targets on the training set, and P (t|z) corresponds to observation noise. We will assume a noise model consisting of a Gaussian with variance σ 2 centered at z.
If the conditions of Section 2.2 or 2.3 apply, our choice of prior over functions implies that z 1 , ..., z n , z * are n + 1 draws from a GP and z * , z|x * , x ∼ N (0, K) is a multivariate Gaussian whose covariance matrix has the form
K = K D,D K T x * ,D K x * ,D K x * ,x * ,
where the block structure corresponds to the division between the training set and the test point. That is, K D,D is an n × n matrix whose (i, j)th element is K(x i , x j ) with x i , x j ∈ D, while e.g. the ith
element of K x * ,D is K(x * , x i ), x i ∈ D.
As is standard in GPs, the integral in Equation 7 can be done exactly, resulting in z * |D, x * ∼ N (μ,K) with
µ = K x * ,D (K D,D + σ 2 I n ) −1 t (8) K = K x * ,x * − K x * ,D (K D,D + σ 2 I n ) −1 K T x * ,D(9)
where I n is the n × n identity. The predicted distribution for z * |D, x * is hence determined from straightforward matrix computations, yet nonetheless corresponds to fully Bayesian training of the deep neural network. The form of the covariance function used is determined by the choice of GP prior, i.e. the neural network model class, which depends on depth, nonlinearity, and weight and bias variances. We henceforth resume placing a superscript L as in K L to emphasize the choice of depth for the compositional kernel.

[EFFICIENT IMPLEMENTATION OF THE GP KERNEL]
Given an L-layer deep neural network with fixed hyperparameters, constructing the covariance matrix K L for the equivalent GP involves computing the Gaussian integral in Equation 4 for all pairs of training-training and training-test points, recursively for all layers. For some nonlinearities, such as ReLU, this integration can be done analytically. However, to compute the kernel corresponding to arbitrary nonlinearities, the integral must be performed numerically.
The most direct implementation of a numerical algorithm for K L would be to compute integrals independently for each pair of datapoints and each layer. This is prohibitively expensive and costs O n 2 g L(n 2 train + n train n test ) , where n 2 g is the sampling density for the pair of Gaussian random variables in the 2D integral and n train , n test are the training and test set sizes, respectively. However, by careful pipelining, and by preprocessing all inputs to have identical norm, we can improve this cost to O n 2 g n v n c + L(n 2 train + n train n test ) , where n v and n c are sampling densities for a variance and correlation grid, as described below. In order to achieve this, we break the process into several steps: 1. Generate: pre-activations u = [−u max , • • • , u max ] consisting of n g elements linearly spaced between −u max and u max ; variances s = [0, • • • , s max ] with n v linearly spaced elements, where s max < u 2 max ; and correlations c = (−1, • • • , 1) with n c linearly spaced elements. Note that we are using fixed, rather than adaptive, sampling grids to allow operations to be parallelized and reused across datapoints and layers.
2. Populate a matrix F containing a lookup table for the function F φ in Equation 5. This involves numerically approximating a Gaussian integral, in terms of the marginal variances s and correlations c. We guarantee that the marginal variance is identical for each datapoint, by preprocessing all datapoints to have identical norm at the input layer, so the number of entries in the lookup table need only be n v n c . These entries are computed as 2 :
F ij = ab φ(u a )φ(u b ) exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b ab exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b .(10)
3. For every pair of datapoints x and x in layer l, compute K l (x, x ) using Equation 5.
Approximate the function F φ K l−1 (x, x ); K l−1 (x, x); K l−1 (x , x ) by bilinear interpolation into the matrix F from Step 2, where we interpolate into s using the value of K l−1 (x, x), and interpolate into c using K l−1 (x, x )/K l−1 (x, x) . Remember that K l−1 (x, x) = K l−1 (x , x ), due to data preprocessing to guarantee constant norm.
4. Repeat the previous step recursively for all layers. Bilinear interpolation has constant cost, so this has cost O L(n 2 train + n train n test ) .
This computational recipe allows us to compute the covariance matrix for the NNGP corresponding to any well-behaved nonlinearity φ. All computational steps above can be implemented using accelerated tensor operations, and computation of K L is typically faster than solving the system of linear equations in Equation 8-9. Figure 6 illustrates the close agreement between the kernel function computed numerically (using this approach) and analytically, for the ReLU nonlinearity. It also illustrates the angular dependence of the kernel and its evolution with increasing depth.
Finally, note that the full computational pipeline is deterministic and differentiable. The shape and properties of a deep network kernel are purely determined by hyperparameters of the deep neural network. Since GPs give exact marginal likelihood estimates, this kernel construction may allow principled hyperparameter selection, or nonlinearity design, e.g. by gradient ascent on the log likelihood w.r.t. the hyperparameters. Although this is not the focus of current work, we hope to return to this topic in follow-up work.
An open source implementation of the algorithm is available at https://github.com/brainresearch/nngp.

[DESCRIPTION]
We compare NNGPs with SGD 3 trained neural networks on the permutation invariant MNIST and CIFAR-10 datasets. The baseline neural network is a fully-connected network with identical width at each hidden layer. Training is on the mean squared error (MSE) loss, chosen so as to allow direct comparison to GP predictions. Formulating classification as regression often leads to good results (Rifkin & Klautau, 2004). Future work may involve evaluating the NNGP on a cross entropy loss using the approach in (Williams & Barber, 1998;Rasmussen & Williams, 2006). Training used the Adam optimizer (Kingma & Ba (2014)) with learning rate and initial weight/bias variances optimized over validation error using the Google Vizier hyperparameter tuner (Golovin et al., 2017). Dropout was not used. In future work, it would be interesting to incorporate dropout into the NNGP covariance matrix using an approach like that in (Schoenholz et al., 2017). For the study, nonlinearities were chosen to be either rectified linear units (ReLU) or hyperbolic tangent (Tanh). Class labels were encoded as a one-hot, zero-mean, regression target (i.e., entries of -0.1 for the incorrect class and 0.9 for the correct class). We constructed the covariance kernel numerically for ReLU and Tanh nonlinearities following the method described in Section 2.5.
Performance: We find that the NNGP often outperforms trained finite width networks. See Table 1 and Figure 1. The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
We additionally find the performance of the best finite-width NNs, trained with a variant of SGD, approaches that of the NNGP with increasing layer width. This is interesting from at least two, potentially related, standpoints. (1) NNs are commonly believed to be powerful because of their ability to do flexible representation learning, while our NNGP uses fixed basis functions; nonetheless, in our experiments we find no salient performance advantage to the former. (2) It hints at a possible relationship between SGD and Bayesian inference in certain regimes -were the neural networks trained in a fully Bayesian fashion, rather than by SGD, the approach to NNGP in the large width limit would be guaranteed. There is recent work suggesting that SGD can implement approximate Bayesian inference (Mandt et al., 2017) under certain assumptions.
The similarity of the performance of the widest NN in Figure 1 with the NNGP suggests that the limit of infinite network width, which is inherent to the GP, is far from being a disadvantage. Indeed, in practice it is found that the best generalizing NNs are in fact the widest. To support this, in Figure 2 we show generalization gap results from an experiment in which we train 180 fully-connected networks with five hidden layers on CIFAR-10 with a range of layer widths. For this experiment, we trained the networks using a standard cross entropy loss rather than MSE, leading to a slight difference in performance.
Uncertainty: One benefit in using a GP is that, due to its Bayesian nature, all predictions have uncertainty estimates (Equation 9). For conventional neural networks, capturing the uncertainty in a model's predictions is challenging (Gal, 2016). In the NNGP, every test point has an explicit estimate of prediction variance associated with it (Equation 9). In our experiments, we observe that the NNGP uncertainty estimate is highly correlated with prediction error (Figure 3).  2014)) have noted the recurrence relations Equation 5 commonly approach a functionally uninteresting fixed point with depth l → ∞, in that K ∞ (x, x ) becomes a constant or piecewise constant map. We now briefly relate our ability to train NNGPs with the convergence of K l (x, x ) to the fixed-point kernel.

[RELATIONSHIP]
We will be particularly interested in contextualizing our results in relation to Poole et al. (2016);Schoenholz et al. (2017) which analyzed the fixed points and the approach to them in detail for bounded nonlinearities. To briefly recapitulate: there are regions of hyperparameter space (called phases) where K ∞ (x, x ) changes only quantitatively with σ 2 w and σ 2 b . However, there are low For the Tanh nonlinearity, there are two distinct phases respectively called the ""ordered"" phase and the ""chaotic"" phase that can be understood as a competition between the weights and the biases of the network. A diagram showing these phases and the boundary between them is shown in Figure 4a. In the ordered phase, the features obtained by propagating an input through the each layer of the recursion become similar for dissimilar inputs. Fundamentally, this occurs because the different inputs share common bias vectors and so all inputs end up just approaching the random bias. In this case the covariance K l (x, x ) → q * for every pair of inputs x, x , where q * is a constant that depends only on σ 2 w and σ 2 b . All inputs have unit correlation asymptotically with depth. By contrast in the chaotic phase the weight variance σ 2 w dominates and similar inputs become dissimilar with depth as they are randomly projected by the weight matrices. In this case, the covariance K l (x, x ) → q * for x = x but q * c * for x = x . Here c * < 1 is the fixed point correlation. In each of these regimes, there is also a finite depth-scale ξ which describes the characteristic number of layers over which the covariance function decays exponentially towards its fixed point form. Exactly at the boundary between these two regimes is a line in (σ 2 w , σ 2 b )-space where the decay K l (x, x ) towards its fixed  2017) that this approach to the fixed-point covariance fundamentally bounded whether or not neural networks could successfully be trained. It was shown that initializing networks on this line allowed for significantly deeper neural networks to be trained.
For ReLU networks a similar picture emerges, however there are some subtleties due to the unbounded nature of the nonlinearity. In this case for all σ 2 w and σ 2 b , K ∞ (x, x ) = q * for all x, x and every point becomes asymptotically correlated. Despite this, there are again two phases: a ""bounded"" phase in which q * is finite (and nonzero) and an unbounded phase in which q * is either infinite or zero. As in the Tanh case there are depth scales that control the rate of convergence to these fixed points and therefore limit the maximum trainable depth. The phase diagram for the ReLU nonlinearity is also shown in Figure 4b.
In a striking analogy with the trainability of neural networks, we observe that the performance of the NNGP appears to closely track the structure from the phase diagram, clearly illustrated in Figure 4. Indeed, we see that as for hyperparameter settings that are far from criticality, the GP is unable to train and we encounter poor test set performance. By contrast, near criticality we observe that our models display high accuracy. Moreover, we find that the accuracy appears to drop more quickly away from the phase boundary with increase in depth L of the GP kernel, K L . To understand this effect we note that information about data will be available to our model only through the difference K L (x, x ) − K ∞ (x, x ). However, as the depth gets larger, this difference becomes increasingly small and at some point can no longer be represented due to numerical precision. At this point our test accuracy begins to quickly degrade to random chance.

[CONCLUSION AND FUTURE DIRECTIONS]
By harnessing the limit of infinite width, we have specified a correspondence between priors on deep neural networks and Gaussian processes whose kernel function is constructed in a compositional, but fully deterministic and differentiable, manner. Use of a GP prior on functions enables exact Bayesian inference for regression from matrix computations, and hence we are able to obtain predictions and uncertainty estimates from deep neural networks without stochastic gradient-based training. The performance is competitive with the best neural networks (within specified class of fully-connected models) trained on the same regression task under similar hyperparameter settings. While we were able to run experiments for somewhat large datasets (sizes of 50k), we intend to look into scalability for larger learning tasks, possibly harnessing recent progress in scalable GPs (Quiñonero-Candela & Rasmussen (2005); Hensman et al. (2013)).  b) is a theoretical phase diagram for that nonlinearity following the methodology of Schoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure 9.
In our experiments, we observed the performance of the optimized neural network appears to approach that of the GP computation with increasing width. Whether gradient-based stochastic optimization implements an approximate Bayesian computation is an interesting question (Mandt et al., 2017). Further investigation is needed to determine if SGD does approximately implement Bayesian inference under the conditions typically employed in practice.
Additionally, the NNGP provides explicit estimates of uncertainty. This may be useful in predicting model failure in critical applications of deep learning, or for active learning tasks where it can be used to identify the best datapoints to hand label.
A DRAWS FROM AN NNGP PRIOR Figure 5 illustrates the nature of the GP prior for the ReLU nonlinearity by depicting samples of 1D functions z(x) drawn from a ReLU GP, GP(0, K L ), with fixed depth L = 10 and (σ 2 w , σ 2 b ) = (1.8, 0.01). Figure 6: The angular structure of the kernel and its evolution with depth. Also illustrated is the good agreement between the kernel computed using the methods of Section 2.5 (blue, starred) and the analytic form of the kernel (red). The depth l in K l runs from l = 0, ..., 9 (flattened curves for increasing l), and (σ 2 w , σ 2 b ) = (1.6, 0.1).
In the main text, we noted that the recurrence relation Equation 5 can be computed analytically for certain nonlinearities. In particular, this was computed in Cho & Saul (2009) for polynomial rectified nonlinearities. For ReLU, the result including the weight and bias variance is
K l (x, x ) = σ 2 b + σ 2 w 2π K l−1 (x, x)K l−1 (x , x ) sin θ l−1 x,x + (π − θ l−1 x,x ) cos θ l−1 x,x , θ l x,x = cos −1 K l (x, x ) K l (x, x)K l (x , x ) . (11
)
To illustrate the angular form of K l (x, x ) and its evolution with l, in Figure 6 we plot K l (θ) for the ReLU nonlinearity, where θ is the angle between x and x with norms such that ||x|| 2 = ||x || 2 = d in . We observe a flattening of the angular structure with increase in depth l, as predicted from the understanding in Section 3.2. Simultaneously, the figure also illustrates the good agreement between the kernel computed using the numerical implementation of Section 2.5 (blue, starred) and the analytic arccosine kernel, Equation 11 (red), for a particular choice of hyperparameters (σ 2 w , σ 2 b ).

[C BAYESIAN MARGINALIZATION OVER INTERMEDIATE LAYERS]
In this section, we present an alternate derivation of the equivalence between infinitely wide deep neural networks and Gaussian process by marginalization over intermediate layers. For this derivation, we take the weight and bias parameters to be drawn from independent Gaussians, with zero mean and appropriately scaled variance.
We are interested in finding the distribution p(z L |x) over network outputs z L ∈ R dout×B , conditioned on network inputs x ∈ R din×B , for input dimensionality d in , output dimensionality d out , and dataset size B. Intervening layers will have width N l , z l ∈ R N l+1 ×B for L > l > 0. We define the second moment matrix (here post-nonlinearity) for each layer l to be
K l ab = 1 din n x na x nb l = 0 1 N l n φ(z l−1 na )φ(z l−1 nb ) l > 0 . (12
)
Our approach is to think of intermediate random variables corresponding to these second moments defined above. By definition, K l only depends on z l−1 . In turn, the pre-activations z l are described by a Gaussian process conditioned on the second moment matrix K l ,
p(z l |K l ) = N vec z l ; 0, G K l ⊗ I =: GP z l ; 0, G K l ,(13)
where
G K l := σ 2 w K l + σ 2 b 11 T .(14)
This correspondence of each layer to a GP, conditioned on the layer's second moment matrix, is exact even for finite width N l because the parameters are drawn from a Gaussian. Altogether, this justifies the graphical model depicted in Figure 7.
We will write p(z L |x) as an integral over all the intervening second moment matrices K l ,
p(z L |x) = p z L , K 0 , K 1 , • • • , K L |x dK 0•••L . (15
)
This joint distribution can be decomposed as
p(z L |x) = p(z L |K L ) L l=1 p(K l |K l−1 ) p(K 0 |x)dK 0•••L .(16)
The directed decomposition in Equation 16 holds because K L is a function only of z L−1 (see Equation 12), z L−1 depends only on K L−1 (see Equation 13), K L−1 is a function only of z L−2 , etc (Figure 7). The sum in Equation 12for l > 0 is a sum over i.i.d. terms. As N l grows large, the Central Limit Theorem applies, and p K l |K l−1 converges to a Gaussian with variance that shrinks as 1 N l . Further, in the infinite width limit it will go to a delta function,
z L K L ... K 1 K 0 z L−1 K L−1 z L−2 x z 0
lim N l →∞ p K l |K l−1 = δ K l − (F • G) K l−1 ,(17)
with F (•) defined as in Equation 5. Similarly, the dependence of K 0 on x can be expressed as a delta function,
p K 0 |x = δ K 0 − 1 d in x T x .(18)
Substituting p(z L |K L ), p K l |K l−1 and p K 0 |x into Equation 16, we get
lim N L →∞,...,N1→∞ p(z L |x) = GP z L ; 0, G K L L l=1 δ K l − (F • G) K l−1 δ K 0 − 1 d in x T x dK 0•••L = GP z L ; 0, G • (F • G) L 1 d in x T x = GP z L ; 0, G • (F • G) L K 0 . (19
)
So, in the limit of infinite width, z L |x is described by a Gaussian process with kernel G • (F • G)
L K 0 .

[D DETAILS OF THE EXPERIMENTS]
We outline details of the experiments for Section 3. For MNIST we use a 50k/10k/10k split of the training/validation/test dataset. For CIFAR-10, we used a 45k/5k/10k split. The validation set was used for choosing the best hyperparameters and evaluation on the test set is reported.
For training neural networks hyperparameters were optimized via random search on average 250 trials for each choice of (n train , depth, width, nonlinearity).
Random search range: Learning rate was sampled within (10 −4 , 0.2) in log-scale, weight decay constant was sampled from (10 −8 , 1.0) in log-scale, σ w ∈ [0.01, 2.5], σ b ∈ [0, 1.5] was uniformly sampled and mini-batch size was chosen equally among [16,32,64,128,256].
For the GP with given depth and nonlinearity, a grid of 30 points evenly spaced from 0.1 to 5.0 (for σ 2 w ) and 30 points evenly spaced from 0 to 2.0 (for σ 2 b ) was evaluated to generate the heatmap. The best GP run was chosen among the 900 evaluations in the σ 2 w -σ 2 b grid. Computation time: We report computation times for NNGP experiments. The grid generation with took 440-460s with 6 CPUs for n g = 501, n v = 501, n c = 500, which was amortized over all the experiments. For full (50k) MNIST, constructing K DD for each layer took 90-140s (depending on CPU generation) running on 64 CPUs. Solving linear equations via Cholesky decomposition took 180-220s for 1000 test points.

[DETAILS OF NNGP IMPLEMENTAION:]
For all the experiments we used pre-computed lookup tables F with n g = 501, n v = 501, n c = 500, and s max = 100. Default value for the target noise σ 2 was set to 10 −10 and was increased by factor of 10 when Cholesky decomposition failed while solving Equation 8 and 9. We refer to Rasmussen & Williams (2006) for standard numerically stable implementation of GP regression.

[E FURTHER RESULTS]
Here we include more results from experiments described in Section 3.
Uncertainty: Relationship between the target MSE and the GP's uncertainty estimate for smaller training set size is shown in Figure 8.
Performance: Performance of grid points of σ 2 w -σ 2 b for varying depth is shown in Figure 9. The best performing NNGP's hyperparameters are distributed near the critical line (Figure 10) where the phase changes as described in Section 3.2. 

[ACKNOWLEDGMENTS]
We thank Ryan Adams, Samy Bengio, and Matt Hoffman for useful discussions and feedback, and Gamaleldin Elsayed and Daniel Levy for helpful comments on the manuscript.","[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency)."
Deep Neural Networks as Gaussian Processes,B1EA-M-0Z.json,"Neal (1994) showed that a one hidden layer Bayesian neural network, under certain conditions, converges to a Gaussian process as the number of hidden units approaches infinity. Neal (1994) and Williams (1997) derive the resulting kernel functions for such Gaussian processes when the neural networks have certain transfer functions.

Similarly, the authors show an analogous result for deep neural networks with multiple hidden layers and an infinite number of hidden units per layer, and show the form of the resulting kernel functions. For certain transfer functions, the authors perform a numerical integration to compute the resulting kernels. They perform experiments on MNIST and CIFAR-10, doing classification by scaled regression. 

Overall, the work is an interesting read, and a nice follow-up to Neal’s earlier observations about 1 hidden layer neural networks. It combines several insights into a nice narrative about infinite Bayesian deep networks. However, the practical utility, significance, and novelty of this work -- in its current form -- are questionable, and the related work sections, analysis, and experiments should be significantly extended. 


In detail:

(1) This paper misses some obvious connections and references, such as 
* Krauth et. al (2017): “Exploring the capabilities and limitations of Gaussian process models” for recursive kernels with GPs.
* Hazzan & Jakkola (2015): “Steps Toward Deep Kernel Methods from Infinite Neural Networks” for GPs corresponding to NNs with more than one hidden layer.
* The growing body of work on deep kernel learning, which “combines the inductive biases and representation learning abilities of deep neural networks with the non-parametric flexibility of Gaussian processes”. E.g.: (i) “Deep Kernel Learning” (AISTATS 2016); (ii) “Stochastic Variational Deep Kernel Learning” (NIPS 2016); (iii) “Learning Scalable Deep Kernels with Recurrent Structure” (JMLR 2017). 

These works should be discussed in the text.

(2) Moreover, as the authors rightly point out, covariance functions of the form used in (4) have already been proposed. It seems the novelty here is mainly the empirical exploration (will return to this later), and numerical integration for various activation functions. That is perfectly fine -- and this work is still valuable. However, the statement “recently, kernel functions for multi-layer random neural networks have been developed, but only outside of a Bayesian framework” is incorrect. For example, Hazzan & Jakkola (2015) in “Steps Toward Deep Kernel Methods from Infinite Neural Networks” consider GP constructions with more than one hidden layer. Thus the novelty of this aspect of the paper is overstated. 

See also comment [*] later on the presentation. In any case, the derivation for computing the covariance function (4) of a multi-layer network is a very simple reapplication of the procedure in Neal (1994). What is less trivial is estimating (4) for various activations, and that seems to the major methodological contribution. 

Also note that multidimensional CLT here is glossed over. It’s actually really unclear whether the final limit will converge to a multidimensional Gaussian with that kernel without stronger conditions.  This derivation should be treated more thoroughly and carefully.

(3) Most importantly, in this derivation, we see that the kernels lose the interesting representations that come from depth in deep neural networks. Indeed, Neal himself says that in the multi-output settings, all the outputs become uncorrelated. Multi-layer representations are mostly interesting because each layer shares hidden basis functions. Here, the sharing is essentially meaningless, because the variance of the weights in this derivation shrinks to zero. 
In Neal’s case, the method was explored for single output regression, where the fact that we lose this sharing of basis functions may not be so restrictive. However, these assumptions are very constraining for multi-output classification and also interesting multi-output regressions.

[*]: Generally, in reading the abstract and introduction, we get the impression that this work somehow allows us to use really deep and infinitely wide neural networks as Gaussian processes, and even without the pain of training these networks. “Deep neural networks without training deep networks”. This is not an accurate portrayal. The very title “Deep neural networks as Gaussian processes” is misleading, since it’s not really the deep neural networks that we know and love. In fact, you lose valuable structure when you take these limits, and what you get is very different than a standard deep neural network. In this sense, the presentation should be re-worked.

(4) Moreover, neural networks are mostly interesting because they learn the representation. To do something similar with GPs, we would need to learn the kernel. But here, essentially no kernel learning is happening. The kernel is fixed. 

(5) Given the above considerations, there is great importance in understanding the practical utility of the proposed approach through a detailed empirical evaluation. In other words, how structured is this prior and does it really give us some of the interesting properties of deep neural networks, or is it mostly a cute mathematical trick?  

Unfortunately, the empirical evaluation is very preliminary, and provides no reassurance that this approach will have any practical relevance:
(i) Directly performing regression on classification problems is very heuristic and unnecessary.
(ii) Given the loss of dependence between neurons in this approach, it makes sense to first explore this method on single output regression, where we will likely get the best idea of its useful properties and advantages. 
(iii) The results on CIFAR10 are very poor. We don’t need to see SOTA performance to get some useful insights in comparing for example parametric vs non-parametric, but 40% more error than SOTA makes it very hard to say whether any of the observed patterns hold weight for more competitive architectural choices. 

A few more minor comments:
(i) How are you training a GP exactly on 50k training points? Even storing a 50k x 50k matrix requires about 20GB of RAM. Even with the best hardware, computing the marginal likelihood dozens of times to learn hyperparameters would be near impossible. What are the runtimes?
(ii) ""One benefit in using the GP is due to its Bayesian nature, so that predictions have uncertainty estimates (Equation (9)).”  The main benefit of the GP is not the uncertainty in the predictions, but the marginal likelihood which is useful for kernel learning.","[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency).

[CAPTIONS]
Table 1: Figure1: The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
Table 2: TO DEEP SIGNAL PROPAGATION Several prior works (Poole et al. (2016); Schoenholz et al. (2017); Daniely et al. (2016); Duvenaud et al. (
Table 3: Figure 2 :Figure 3 :23Figure2: Generalization gap for five hidden layer fully-connected networks with variable widths, using ReLU and Tanh nonlinearities on CIFAR-10. Random optimization and initialization hyperparameters were used and results were filtered for networks with 100% classification training accuracy, resulting in a total of 125 Tanh and 55 ReLU networks. The best generalizing networks are consistently the widest.
Table 4: Figure 4 :4Figure 4: The best performing NNGP hyperparameters agree with those predicted by deep signal propagation. Test set accuracy heatmaps for NNGPs evaluated for a grid of σ 2 w and σ 2 b values. The right plot in each subfigure (a), (b) is a theoretical phase diagram for that nonlinearity following the methodology ofSchoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure9.
Table 5: Figure 5 :5Figure 5: Samples from an NNGP prior for 1D functions. Different lines correspond to different draws (arbitrary colors).
Table 6: Figure 7 :7Figure 7: Graphical model for neural network's computation.
Table 7: Figure 10 :10Figure 10: Best performing NNGPs are distributed near the critical line. Weight and bias variance distribution for the 25 best performing runs for NNGP with the given training set size is shown.
Table 8: The NNGP often outperforms finite width networks. Test accuracy on MNIST and CIFAR-10 datasets. The reported NNGP results correspond to the best performing depth, σ 2 w , and σ 2 b values on the validation set. The traditional NN results correspond to the best performing depth, width and optimization hyperparameters. Best models for a given training set size are specified by (depthwidth-σ 2 w -σ 2 b ) for NNs and (depth-σ 2 w -σ 2 b ) for GPs. More results are in Appendix Table2.

[INTRODUCTION]
Deep neural networks have emerged in recent years as flexible parametric models which can fit complex patterns in data. As a contrasting approach, Gaussian processes have long served as a traditional nonparametric tool for modeling. An equivalence between these two approaches was derived in Neal (1994a), for the case of one layer networks in the limit of infinite width. Neal (1994a) further suggested that a similar correspondence might hold for deeper networks.
Consider a deep fully-connected neural network with i.i.d. random parameters. Each scalar output of the network, an affine transformation of the final hidden layer, will be a sum of i.i.d. terms. As we will discuss in detail below, in the limit of infinite width the Central Limit Theorem 1 implies that the function computed by the neural network (NN) is a function drawn from a Gaussian process (GP). In the case of single hidden-layer networks, the form of the kernel of this GP is well known (Neal (1994a); Williams (1997)). This correspondence implies that if we choose the hypothesis space to be the class of infinitely wide neural networks, an i.i.d. prior over weights and biases can be replaced with a corresponding GP prior over functions. As noted by (Williams, 1997), this substitution enables exact Bayesian inference for regression using neural networks. The computation requires building the necessary covariance matrices over the training and test sets and straightforward linear algebra computations.
In light of the resurgence in popularity of neural networks, it is timely to revisit this line of work. We delineate the correspondence between deep and wide neural networks and GPs and utilize it for Bayesian training of neural networks on regression tasks.

[RELATED WORK]
Our work touches on aspects of GPs, Bayesian learning, and compositional kernels. The correspondence between infinite neural networks and GPs was first noted by Neal (1994a;b). Williams (1997) computes analytic GP kernels for single hidden-layer neural networks with error function or Gaussian nonlinearities and noted the use of the GP prior for exact Bayesian inference in regression. Duvenaud et al. (2014) discusses several routes to building deep GPs and observes the degenerate form of kernels that are composed infinitely many times -a point we will return to Section 3.2but they do not derive the form of GP kernels as we do. Hazan & Jaakkola (2015) also discusses constructing kernels equivalent to infinitely wide deep neural networks, but their construction does not go beyond two hidden layers with nonlinearities.
Related work has also appeared outside of the GP context but in compositional kernel constructions. Cho & Saul (2009) derives compositional kernels for polynomial rectified nonlinearities, which includes the Sign and ReLU nonlinearities, and can be used in GPs; our manner of composing kernels matches theirs, though the context is different. Daniely et al. (2016) extends the construction of compositional kernels to neural networks whose underlying directed acyclic graph is of general form. They also prove, utilizing the formalism of dual activations, that compositional kernels originating from fully-connected topologies with the same nonlinearity become degenerate when composed infinitely many times. In a different context than compositional kernels, Poole et al. (2016); Schoenholz et al. (2017) study the same underlying recurrence relation for the specific case of fully-connected networks and bounded nonlinearities. They distinguish regions in hyperparameter space with different fixed points and convergence behavior in the recurrence relations. The focus in these works was to better understand the expressivity and trainability of deep networks.
Drawing inspiration from the multi-layer nature of deep neural networks, there is a line of work considering various approaches to stacking GPs, such as deep GPs (Lawrence & Moore (2007); Damianou & Lawrence (2013); Hensman & Lawrence (2014); Duvenaud et al. (2014); Bui et al. (2016)), which can give rise to a richer class of probabilistic models beyond GPs. This contrasts with our work, where we study GPs that are in direct correspondence with deep, infinitely wide neural networks. Krauth et al. (2016) has recently explored the performance of GP models with deep kernels given in Cho & Saul (2009), implemented with scalable approximations. However, they do not discuss the equivalence between deep neural networks and GPs with compositional kernels, which constitutes a conceptual contribution of our work. Furthermore, we note that the GP kernels in our work are more general than the compositional kernel construction outlined in Cho & Saul (2009) in two respects: (i) we are not limited to rectified polynomials but can deal with general nonlinearities, and (ii) we consider two additional hyperparameters in the kernels, which would correspond to the weight and bias parameter variances in a neural network. Finally, Gal & Ghahramani (2016) connects dropout in deep neural networks with approximate Bayesian inference in deep GPs.
Another series of recent works (Wilson et al. (2016b;a); Al-Shedivat et al. (2017)), termed deep kernel learning, utilize GPs with base kernels which take in features produced by a deep multilayer neural network, and train the resulting model end-to-end. Our work differs from these in that our GP corresponds to a multilayer neural network. Additionally, our GP kernels have many fewer parameters, and these parameters correspond to the hyperparameters of the equivalent neural network.

[SUMMARY OF CONTRIBUTIONS]
We begin by specifying the form of a GP which corresponds to a deep, infinitely wide neural network -hereafter referred to as the Neural Network GP (NNGP) -in terms of a recursive, deterministic computation of the kernel function. The prescription is valid for generic pointwise nonlinearities in fully-connected feedforward networks. We develop a computationally efficient method (Section 2.5) to compute the covariance function corresponding to deep neural networks with fixed hyperparameters.
In this work, as a first proof of concept of our NNGP construction, we focus on exact Bayesian inference for regression tasks, treating classification as regression on class labels. While less principled, least-squares classification performs well (Rifkin et al., 2003) and allows us to compare exact inference via a GP to prediction by a trained neural network on well-studied tasks (MNIST and CIFAR-10 classification). Note that it is possible to extend GPs to softmax classification with cross entropy loss (Williams & Barber (1998); Rasmussen & Williams (2006)), which we aim to investigate in future work.
We conduct experiments making Bayesian predictions on MNIST and CIFAR-10 (Section 3) and compare against NNs trained with standard gradient-based approaches. The experiments explore different hyperparameter settings of the Bayesian training including network depth, nonlinearity, training set size (up to and including the full dataset consisting of tens of thousands of images), and weight and bias variance. Our experiments reveal that the best NNGP performance is consistently competitive against that of NNs trained with gradient-based techniques, and the best NNGP setting, chosen across hyperparameters, often surpasses that of conventional training (Section 3, Table 1). We further observe that, with increasing network width, the performance of neural networks with gradient-based training approaches that of the NNGP computation, and that the GP uncertainty is strongly correlated with prediction error. Furthermore, the performance of the NNGP depends on the structure of the kernel, which can be connected to recent work on signal propagation in networks with random parameters (Schoenholz et al., 2017).

[DEEP, INFINITELY WIDE NEURAL NETWORKS ARE DRAWN FROM GPS]
We begin by specifying the correspondence between GPs and deep, infinitely wide neural networks, which hinges crucially on application of the Central Limit Theorem. We review the single-hidden layer case (Section 2.2) before moving to the multi-layer case (Section 2.3).

[NOTATION]
Consider an L-hidden-layer fully-connected neural network with hidden layers of width N l (for layer l) and pointwise nonlinearities φ. Let x ∈ R din denote the input to the network, and let z L ∈ R dout denote its output. The ith component of the activations in the lth layer, post-nonlinearity and postaffine transformation, are denoted x l i and z l i respectively. We will refer to these as the post-and pre-activations. (We let x 0 i ≡ x i for the input, dropping the Arabic numeral superscript, and instead use a Greek superscript x α to denote a particular input α). Weight and bias parameters for the lth layer have components W l ij , b l i , which are independent and randomly drawn, and we take them all to have zero mean and variances σ 2 w /N l and σ 2 b , respectively. GP(µ, K) denotes a Gaussian process with mean and covariance functions µ(•), K(•, •), respectively.

[REVIEW OF GAUSSIAN PROCESSES AND SINGLE-LAYER NEURAL NETWORKS]
We briefly review the correspondence between single-hidden layer neural networks and GPs (Neal (1994a;b); Williams (1997)). The ith component of the network output, z 1 i , is computed as,
z 1 i (x) = b 1 i + N1 j=1 W 1 ij x 1 j (x), x 1 j (x) = φ b 0 j + din k=1 W 0 jk x k ,(1)
where we have emphasized the dependence on input x. Because the weight and bias parameters are taken to be i.i.d., the post-activations x 1 j , x 1 j are independent for j = j . Moreover, since z 1 i (x) is a sum of i.i.d terms, it follows from the Central Limit Theorem that in the limit of infinite width N 1 → ∞, z 1 i (x) will be Gaussian distributed. Likewise, from the multidimensional Central Limit Theorem, any finite collection of {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have a joint multivariate Gaussian distribution, which is exactly the definition of a Gaussian process. Therefore we conclude that z 1 i ∼ GP(µ 1 , K 1 ), a GP with mean µ 1 and covariance K 1 , which are themselves independent of i.
Because the parameters have zero mean, we have that µ 1 (x) = E z 1 i (x) = 0 and,
K 1 (x, x ) ≡ E z 1 i (x)z 1 i (x ) = σ 2 b + σ 2 w E x 1 i (x)x 1 i (x ) ≡ σ 2 b + σ 2 w C(x, x ),(2)
where we have introduced C(x, x ) as in Neal (1994a); it is obtained by integrating against the distribution of W 0 , b 0 . Note that, as any two z 1 i , z 1 j for i = j are joint Gaussian and have zero covariance, they are guaranteed to be independent despite utilizing the same features produced by the hidden layer.

[GAUSSIAN PROCESSES AND DEEP NEURAL NETWORKS]
The arguments of the previous section can be extended to deeper layers by induction. We proceed by taking the hidden layer widths to be infinite in succession (N 1 → ∞, N 2 → ∞, etc.) as we continue with the induction, to guarantee that the input to the layer under consideration is already governed by a GP. In Appendix C we provide an alternative derivation in terms of Bayesian marginalization over intermediate layers, which does not depend on the order of limits, in the case of a Gaussian prior on the weights. A concurrent work (de G. Matthews et al., 2018) further derives the convergence rate towards a GP if all layers are taken to infinite width simultaneously, but at different rates.
Suppose that z l−1 j is a GP, identical and independent for every j (and hence x l j (x) are independent and identically distributed). After l − 1 steps, the network computes
z l i (x) = b l i + N l j=1 W l ij x l j (x), x l j (x) = φ(z l−1 j (x)).(3)
As before, z l i (x) is a sum of i.i.d. random terms so that, as N l → ∞, any finite collection {z 1 i (x α=1 ), ..., z 1 i (x α=k )} will have joint multivariate Gaussian distribution and
z l i ∼ GP(0, K l ). The covariance is K l (x, x ) ≡ E z l i (x)z l i (x ) = σ 2 b + σ 2 w E z l−1 i ∼GP(0,K l−1 ) φ(z l−1 i (x))φ(z l−1 i (x )) . (4)
By induction, the expectation in Equation 4 is over the GP governing z l−1 i , but this is equivalent to integrating against the joint distribution of only z l−1 i (x) and z l−1 i (x ). The latter is described by a zero mean, two-dimensional Gaussian whose covariance matrix has distinct entries K l−1 (x, x ), K l−1 (x, x), and K l−1 (x , x ). As such, these are the only three quantities that appear in the result. We introduce the shorthand
K l (x, x ) = σ 2 b + σ 2 w F φ K l−1 (x, x ), K l−1 (x, x), K l−1 (x , x )(5)
to emphasize the recursive relationship between K l and K l−1 via a deterministic function F whose form depends only on the nonlinearity φ. This gives an iterative series of computations which can be performed to obtain K L for the GP describing the network's final output.
For the base case
K 0 , suppose W 0 ij ∼ N (0, σ 2 w /d in ), b 0 j ∼ N (0, σ 2 b )
; we can utilize the recursion relating K 1 and K 0 , where
K 0 (x, x ) = E z 0 j (x)z 0 j (x ) = σ 2 b + σ 2 w x•x din .(6)
In fact, these recurrence relations have appeared in other contexts. They are exactly the relations derived in the mean field theory of signal propagation in fully-connected random neural networks (Poole et al. (2016);Schoenholz et al. (2017)) and also appear in the literature on compositional kernels (Cho & Saul (2009); Daniely et al. (2016)). For certain activation functions, Equation 5 can be computed analytically (Cho & Saul (2009); Daniely et al. (2016)). In the case of the ReLU nonlinearity, it yields the well-known arccosine kernel (Cho & Saul (2009)) whose form we reproduce in Appendix B. When no analytic form exists, it can instead be efficiently computed numerically, as described in Section 2.5.

[BAYESIAN TRAINING OF NEURAL NETWORKS USING GAUSSIAN PROCESS PRIORS]
Here we provide a short review of how a GP prior over functions can be used to do Bayesian inference; see e.g. (Rasmussen & Williams, 2006) for a comprehensive review of GPs. Given a dataset D = {(x 1 , t 1 ), ..., (x n , t n )} consisting of input-target pairs (x, t), we wish to make a Bayesian prediction at test point x * using a distribution over functions z(x). This distribution is constrained to take values z ≡ (z 1 , ..., z n ) on the training inputs x ≡ (x 1 , ..., x n ) and,
P (z * |D, x * ) = dz P (z * |z, x, x * ) P (z|D) = 1 P (t) dz P (z * , z|x * , x) P (t|z) ,(7)
where t = (t 1 , ..., t n ) T are the targets on the training set, and P (t|z) corresponds to observation noise. We will assume a noise model consisting of a Gaussian with variance σ 2 centered at z.
If the conditions of Section 2.2 or 2.3 apply, our choice of prior over functions implies that z 1 , ..., z n , z * are n + 1 draws from a GP and z * , z|x * , x ∼ N (0, K) is a multivariate Gaussian whose covariance matrix has the form
K = K D,D K T x * ,D K x * ,D K x * ,x * ,
where the block structure corresponds to the division between the training set and the test point. That is, K D,D is an n × n matrix whose (i, j)th element is K(x i , x j ) with x i , x j ∈ D, while e.g. the ith
element of K x * ,D is K(x * , x i ), x i ∈ D.
As is standard in GPs, the integral in Equation 7 can be done exactly, resulting in z * |D, x * ∼ N (μ,K) with
µ = K x * ,D (K D,D + σ 2 I n ) −1 t (8) K = K x * ,x * − K x * ,D (K D,D + σ 2 I n ) −1 K T x * ,D(9)
where I n is the n × n identity. The predicted distribution for z * |D, x * is hence determined from straightforward matrix computations, yet nonetheless corresponds to fully Bayesian training of the deep neural network. The form of the covariance function used is determined by the choice of GP prior, i.e. the neural network model class, which depends on depth, nonlinearity, and weight and bias variances. We henceforth resume placing a superscript L as in K L to emphasize the choice of depth for the compositional kernel.

[EFFICIENT IMPLEMENTATION OF THE GP KERNEL]
Given an L-layer deep neural network with fixed hyperparameters, constructing the covariance matrix K L for the equivalent GP involves computing the Gaussian integral in Equation 4 for all pairs of training-training and training-test points, recursively for all layers. For some nonlinearities, such as ReLU, this integration can be done analytically. However, to compute the kernel corresponding to arbitrary nonlinearities, the integral must be performed numerically.
The most direct implementation of a numerical algorithm for K L would be to compute integrals independently for each pair of datapoints and each layer. This is prohibitively expensive and costs O n 2 g L(n 2 train + n train n test ) , where n 2 g is the sampling density for the pair of Gaussian random variables in the 2D integral and n train , n test are the training and test set sizes, respectively. However, by careful pipelining, and by preprocessing all inputs to have identical norm, we can improve this cost to O n 2 g n v n c + L(n 2 train + n train n test ) , where n v and n c are sampling densities for a variance and correlation grid, as described below. In order to achieve this, we break the process into several steps: 1. Generate: pre-activations u = [−u max , • • • , u max ] consisting of n g elements linearly spaced between −u max and u max ; variances s = [0, • • • , s max ] with n v linearly spaced elements, where s max < u 2 max ; and correlations c = (−1, • • • , 1) with n c linearly spaced elements. Note that we are using fixed, rather than adaptive, sampling grids to allow operations to be parallelized and reused across datapoints and layers.
2. Populate a matrix F containing a lookup table for the function F φ in Equation 5. This involves numerically approximating a Gaussian integral, in terms of the marginal variances s and correlations c. We guarantee that the marginal variance is identical for each datapoint, by preprocessing all datapoints to have identical norm at the input layer, so the number of entries in the lookup table need only be n v n c . These entries are computed as 2 :
F ij = ab φ(u a )φ(u b ) exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b ab exp − 1 2 u a u b T s i s i c j s i c j s i −1 u a u b .(10)
3. For every pair of datapoints x and x in layer l, compute K l (x, x ) using Equation 5.
Approximate the function F φ K l−1 (x, x ); K l−1 (x, x); K l−1 (x , x ) by bilinear interpolation into the matrix F from Step 2, where we interpolate into s using the value of K l−1 (x, x), and interpolate into c using K l−1 (x, x )/K l−1 (x, x) . Remember that K l−1 (x, x) = K l−1 (x , x ), due to data preprocessing to guarantee constant norm.
4. Repeat the previous step recursively for all layers. Bilinear interpolation has constant cost, so this has cost O L(n 2 train + n train n test ) .
This computational recipe allows us to compute the covariance matrix for the NNGP corresponding to any well-behaved nonlinearity φ. All computational steps above can be implemented using accelerated tensor operations, and computation of K L is typically faster than solving the system of linear equations in Equation 8-9. Figure 6 illustrates the close agreement between the kernel function computed numerically (using this approach) and analytically, for the ReLU nonlinearity. It also illustrates the angular dependence of the kernel and its evolution with increasing depth.
Finally, note that the full computational pipeline is deterministic and differentiable. The shape and properties of a deep network kernel are purely determined by hyperparameters of the deep neural network. Since GPs give exact marginal likelihood estimates, this kernel construction may allow principled hyperparameter selection, or nonlinearity design, e.g. by gradient ascent on the log likelihood w.r.t. the hyperparameters. Although this is not the focus of current work, we hope to return to this topic in follow-up work.
An open source implementation of the algorithm is available at https://github.com/brainresearch/nngp.

[DESCRIPTION]
We compare NNGPs with SGD 3 trained neural networks on the permutation invariant MNIST and CIFAR-10 datasets. The baseline neural network is a fully-connected network with identical width at each hidden layer. Training is on the mean squared error (MSE) loss, chosen so as to allow direct comparison to GP predictions. Formulating classification as regression often leads to good results (Rifkin & Klautau, 2004). Future work may involve evaluating the NNGP on a cross entropy loss using the approach in (Williams & Barber, 1998;Rasmussen & Williams, 2006). Training used the Adam optimizer (Kingma & Ba (2014)) with learning rate and initial weight/bias variances optimized over validation error using the Google Vizier hyperparameter tuner (Golovin et al., 2017). Dropout was not used. In future work, it would be interesting to incorporate dropout into the NNGP covariance matrix using an approach like that in (Schoenholz et al., 2017). For the study, nonlinearities were chosen to be either rectified linear units (ReLU) or hyperbolic tangent (Tanh). Class labels were encoded as a one-hot, zero-mean, regression target (i.e., entries of -0.1 for the incorrect class and 0.9 for the correct class). We constructed the covariance kernel numerically for ReLU and Tanh nonlinearities following the method described in Section 2.5.
Performance: We find that the NNGP often outperforms trained finite width networks. See Table 1 and Figure 1. The NNGP often outperforms finite width networks, and neural network performance more closely resembles NNGP performance with increasing width. Test accuracy and mean squared error on MNIST and CIFAR-10 dataset are shown for the best performing NNGP and best performing SGD trained neural networks for given width. 'NN-best' denotes the best performing (on the validation set) neural network across all widths and trials. Often this is the neural network with the largest width.
We additionally find the performance of the best finite-width NNs, trained with a variant of SGD, approaches that of the NNGP with increasing layer width. This is interesting from at least two, potentially related, standpoints. (1) NNs are commonly believed to be powerful because of their ability to do flexible representation learning, while our NNGP uses fixed basis functions; nonetheless, in our experiments we find no salient performance advantage to the former. (2) It hints at a possible relationship between SGD and Bayesian inference in certain regimes -were the neural networks trained in a fully Bayesian fashion, rather than by SGD, the approach to NNGP in the large width limit would be guaranteed. There is recent work suggesting that SGD can implement approximate Bayesian inference (Mandt et al., 2017) under certain assumptions.
The similarity of the performance of the widest NN in Figure 1 with the NNGP suggests that the limit of infinite network width, which is inherent to the GP, is far from being a disadvantage. Indeed, in practice it is found that the best generalizing NNs are in fact the widest. To support this, in Figure 2 we show generalization gap results from an experiment in which we train 180 fully-connected networks with five hidden layers on CIFAR-10 with a range of layer widths. For this experiment, we trained the networks using a standard cross entropy loss rather than MSE, leading to a slight difference in performance.
Uncertainty: One benefit in using a GP is that, due to its Bayesian nature, all predictions have uncertainty estimates (Equation 9). For conventional neural networks, capturing the uncertainty in a model's predictions is challenging (Gal, 2016). In the NNGP, every test point has an explicit estimate of prediction variance associated with it (Equation 9). In our experiments, we observe that the NNGP uncertainty estimate is highly correlated with prediction error (Figure 3).  2014)) have noted the recurrence relations Equation 5 commonly approach a functionally uninteresting fixed point with depth l → ∞, in that K ∞ (x, x ) becomes a constant or piecewise constant map. We now briefly relate our ability to train NNGPs with the convergence of K l (x, x ) to the fixed-point kernel.

[RELATIONSHIP]
We will be particularly interested in contextualizing our results in relation to Poole et al. (2016);Schoenholz et al. (2017) which analyzed the fixed points and the approach to them in detail for bounded nonlinearities. To briefly recapitulate: there are regions of hyperparameter space (called phases) where K ∞ (x, x ) changes only quantitatively with σ 2 w and σ 2 b . However, there are low For the Tanh nonlinearity, there are two distinct phases respectively called the ""ordered"" phase and the ""chaotic"" phase that can be understood as a competition between the weights and the biases of the network. A diagram showing these phases and the boundary between them is shown in Figure 4a. In the ordered phase, the features obtained by propagating an input through the each layer of the recursion become similar for dissimilar inputs. Fundamentally, this occurs because the different inputs share common bias vectors and so all inputs end up just approaching the random bias. In this case the covariance K l (x, x ) → q * for every pair of inputs x, x , where q * is a constant that depends only on σ 2 w and σ 2 b . All inputs have unit correlation asymptotically with depth. By contrast in the chaotic phase the weight variance σ 2 w dominates and similar inputs become dissimilar with depth as they are randomly projected by the weight matrices. In this case, the covariance K l (x, x ) → q * for x = x but q * c * for x = x . Here c * < 1 is the fixed point correlation. In each of these regimes, there is also a finite depth-scale ξ which describes the characteristic number of layers over which the covariance function decays exponentially towards its fixed point form. Exactly at the boundary between these two regimes is a line in (σ 2 w , σ 2 b )-space where the decay K l (x, x ) towards its fixed  2017) that this approach to the fixed-point covariance fundamentally bounded whether or not neural networks could successfully be trained. It was shown that initializing networks on this line allowed for significantly deeper neural networks to be trained.
For ReLU networks a similar picture emerges, however there are some subtleties due to the unbounded nature of the nonlinearity. In this case for all σ 2 w and σ 2 b , K ∞ (x, x ) = q * for all x, x and every point becomes asymptotically correlated. Despite this, there are again two phases: a ""bounded"" phase in which q * is finite (and nonzero) and an unbounded phase in which q * is either infinite or zero. As in the Tanh case there are depth scales that control the rate of convergence to these fixed points and therefore limit the maximum trainable depth. The phase diagram for the ReLU nonlinearity is also shown in Figure 4b.
In a striking analogy with the trainability of neural networks, we observe that the performance of the NNGP appears to closely track the structure from the phase diagram, clearly illustrated in Figure 4. Indeed, we see that as for hyperparameter settings that are far from criticality, the GP is unable to train and we encounter poor test set performance. By contrast, near criticality we observe that our models display high accuracy. Moreover, we find that the accuracy appears to drop more quickly away from the phase boundary with increase in depth L of the GP kernel, K L . To understand this effect we note that information about data will be available to our model only through the difference K L (x, x ) − K ∞ (x, x ). However, as the depth gets larger, this difference becomes increasingly small and at some point can no longer be represented due to numerical precision. At this point our test accuracy begins to quickly degrade to random chance.

[CONCLUSION AND FUTURE DIRECTIONS]
By harnessing the limit of infinite width, we have specified a correspondence between priors on deep neural networks and Gaussian processes whose kernel function is constructed in a compositional, but fully deterministic and differentiable, manner. Use of a GP prior on functions enables exact Bayesian inference for regression from matrix computations, and hence we are able to obtain predictions and uncertainty estimates from deep neural networks without stochastic gradient-based training. The performance is competitive with the best neural networks (within specified class of fully-connected models) trained on the same regression task under similar hyperparameter settings. While we were able to run experiments for somewhat large datasets (sizes of 50k), we intend to look into scalability for larger learning tasks, possibly harnessing recent progress in scalable GPs (Quiñonero-Candela & Rasmussen (2005); Hensman et al. (2013)).  b) is a theoretical phase diagram for that nonlinearity following the methodology of Schoenholz et al. (2017). We observe that the performance of the NNGP is best along the critical line (dotted lines). Additional depths are shown in the Appendix Figure 9.
In our experiments, we observed the performance of the optimized neural network appears to approach that of the GP computation with increasing width. Whether gradient-based stochastic optimization implements an approximate Bayesian computation is an interesting question (Mandt et al., 2017). Further investigation is needed to determine if SGD does approximately implement Bayesian inference under the conditions typically employed in practice.
Additionally, the NNGP provides explicit estimates of uncertainty. This may be useful in predicting model failure in critical applications of deep learning, or for active learning tasks where it can be used to identify the best datapoints to hand label.
A DRAWS FROM AN NNGP PRIOR Figure 5 illustrates the nature of the GP prior for the ReLU nonlinearity by depicting samples of 1D functions z(x) drawn from a ReLU GP, GP(0, K L ), with fixed depth L = 10 and (σ 2 w , σ 2 b ) = (1.8, 0.01). Figure 6: The angular structure of the kernel and its evolution with depth. Also illustrated is the good agreement between the kernel computed using the methods of Section 2.5 (blue, starred) and the analytic form of the kernel (red). The depth l in K l runs from l = 0, ..., 9 (flattened curves for increasing l), and (σ 2 w , σ 2 b ) = (1.6, 0.1).
In the main text, we noted that the recurrence relation Equation 5 can be computed analytically for certain nonlinearities. In particular, this was computed in Cho & Saul (2009) for polynomial rectified nonlinearities. For ReLU, the result including the weight and bias variance is
K l (x, x ) = σ 2 b + σ 2 w 2π K l−1 (x, x)K l−1 (x , x ) sin θ l−1 x,x + (π − θ l−1 x,x ) cos θ l−1 x,x , θ l x,x = cos −1 K l (x, x ) K l (x, x)K l (x , x ) . (11
)
To illustrate the angular form of K l (x, x ) and its evolution with l, in Figure 6 we plot K l (θ) for the ReLU nonlinearity, where θ is the angle between x and x with norms such that ||x|| 2 = ||x || 2 = d in . We observe a flattening of the angular structure with increase in depth l, as predicted from the understanding in Section 3.2. Simultaneously, the figure also illustrates the good agreement between the kernel computed using the numerical implementation of Section 2.5 (blue, starred) and the analytic arccosine kernel, Equation 11 (red), for a particular choice of hyperparameters (σ 2 w , σ 2 b ).

[C BAYESIAN MARGINALIZATION OVER INTERMEDIATE LAYERS]
In this section, we present an alternate derivation of the equivalence between infinitely wide deep neural networks and Gaussian process by marginalization over intermediate layers. For this derivation, we take the weight and bias parameters to be drawn from independent Gaussians, with zero mean and appropriately scaled variance.
We are interested in finding the distribution p(z L |x) over network outputs z L ∈ R dout×B , conditioned on network inputs x ∈ R din×B , for input dimensionality d in , output dimensionality d out , and dataset size B. Intervening layers will have width N l , z l ∈ R N l+1 ×B for L > l > 0. We define the second moment matrix (here post-nonlinearity) for each layer l to be
K l ab = 1 din n x na x nb l = 0 1 N l n φ(z l−1 na )φ(z l−1 nb ) l > 0 . (12
)
Our approach is to think of intermediate random variables corresponding to these second moments defined above. By definition, K l only depends on z l−1 . In turn, the pre-activations z l are described by a Gaussian process conditioned on the second moment matrix K l ,
p(z l |K l ) = N vec z l ; 0, G K l ⊗ I =: GP z l ; 0, G K l ,(13)
where
G K l := σ 2 w K l + σ 2 b 11 T .(14)
This correspondence of each layer to a GP, conditioned on the layer's second moment matrix, is exact even for finite width N l because the parameters are drawn from a Gaussian. Altogether, this justifies the graphical model depicted in Figure 7.
We will write p(z L |x) as an integral over all the intervening second moment matrices K l ,
p(z L |x) = p z L , K 0 , K 1 , • • • , K L |x dK 0•••L . (15
)
This joint distribution can be decomposed as
p(z L |x) = p(z L |K L ) L l=1 p(K l |K l−1 ) p(K 0 |x)dK 0•••L .(16)
The directed decomposition in Equation 16 holds because K L is a function only of z L−1 (see Equation 12), z L−1 depends only on K L−1 (see Equation 13), K L−1 is a function only of z L−2 , etc (Figure 7). The sum in Equation 12for l > 0 is a sum over i.i.d. terms. As N l grows large, the Central Limit Theorem applies, and p K l |K l−1 converges to a Gaussian with variance that shrinks as 1 N l . Further, in the infinite width limit it will go to a delta function,
z L K L ... K 1 K 0 z L−1 K L−1 z L−2 x z 0
lim N l →∞ p K l |K l−1 = δ K l − (F • G) K l−1 ,(17)
with F (•) defined as in Equation 5. Similarly, the dependence of K 0 on x can be expressed as a delta function,
p K 0 |x = δ K 0 − 1 d in x T x .(18)
Substituting p(z L |K L ), p K l |K l−1 and p K 0 |x into Equation 16, we get
lim N L →∞,...,N1→∞ p(z L |x) = GP z L ; 0, G K L L l=1 δ K l − (F • G) K l−1 δ K 0 − 1 d in x T x dK 0•••L = GP z L ; 0, G • (F • G) L 1 d in x T x = GP z L ; 0, G • (F • G) L K 0 . (19
)
So, in the limit of infinite width, z L |x is described by a Gaussian process with kernel G • (F • G)
L K 0 .

[D DETAILS OF THE EXPERIMENTS]
We outline details of the experiments for Section 3. For MNIST we use a 50k/10k/10k split of the training/validation/test dataset. For CIFAR-10, we used a 45k/5k/10k split. The validation set was used for choosing the best hyperparameters and evaluation on the test set is reported.
For training neural networks hyperparameters were optimized via random search on average 250 trials for each choice of (n train , depth, width, nonlinearity).
Random search range: Learning rate was sampled within (10 −4 , 0.2) in log-scale, weight decay constant was sampled from (10 −8 , 1.0) in log-scale, σ w ∈ [0.01, 2.5], σ b ∈ [0, 1.5] was uniformly sampled and mini-batch size was chosen equally among [16,32,64,128,256].
For the GP with given depth and nonlinearity, a grid of 30 points evenly spaced from 0.1 to 5.0 (for σ 2 w ) and 30 points evenly spaced from 0 to 2.0 (for σ 2 b ) was evaluated to generate the heatmap. The best GP run was chosen among the 900 evaluations in the σ 2 w -σ 2 b grid. Computation time: We report computation times for NNGP experiments. The grid generation with took 440-460s with 6 CPUs for n g = 501, n v = 501, n c = 500, which was amortized over all the experiments. For full (50k) MNIST, constructing K DD for each layer took 90-140s (depending on CPU generation) running on 64 CPUs. Solving linear equations via Cholesky decomposition took 180-220s for 1000 test points.

[DETAILS OF NNGP IMPLEMENTAION:]
For all the experiments we used pre-computed lookup tables F with n g = 501, n v = 501, n c = 500, and s max = 100. Default value for the target noise σ 2 was set to 10 −10 and was increased by factor of 10 when Cholesky decomposition failed while solving Equation 8 and 9. We refer to Rasmussen & Williams (2006) for standard numerically stable implementation of GP regression.

[E FURTHER RESULTS]
Here we include more results from experiments described in Section 3.
Uncertainty: Relationship between the target MSE and the GP's uncertainty estimate for smaller training set size is shown in Figure 8.
Performance: Performance of grid points of σ 2 w -σ 2 b for varying depth is shown in Figure 9. The best performing NNGP's hyperparameters are distributed near the critical line (Figure 10) where the phase changes as described in Section 3.2. 

[ACKNOWLEDGMENTS]
We thank Ryan Adams, Samy Bengio, and Matt Hoffman for useful discussions and feedback, and Gamaleldin Elsayed and Daniel Levy for helpful comments on the manuscript.","[TITLE]
DEEP NEURAL NETWORKS AS GAUSSIAN PROCESSES

[ABSTRACT]
It has long been known that a single-layer fully-connected neural network with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP), in the limit of infinite network width. This correspondence enables exact Bayesian inference for infinite width neural networks on regression tasks by means of evaluating the corresponding GP. Recently, kernel functions which mimic multi-layer random neural networks have been developed, but only outside of a Bayesian framework. As such, previous work has not identified that these kernels can be used as covariance functions for GPs and allow fully Bayesian prediction with a deep neural network. In this work, we derive the exact equivalence between infinitely wide deep networks and GPs. We further develop a computationally efficient pipeline to compute the covariance function for these GPs. We then use the resulting GPs to perform Bayesian inference for wide deep neural networks on MNIST and CIFAR-10. We observe that trained neural network accuracy approaches that of the corresponding GP with increasing layer width, and that the GP uncertainty is strongly correlated with trained network prediction error. We further find that test performance increases as finite-width trained networks are made wider and more similar to a GP, and thus that GP predictions typically outperform those of finite-width networks. Finally we connect the performance of these GPs to the recent theory of signal propagation in random neural networks. * Both authors contributed equally to this work. † Work done as a member of the Google AI Residency program (g.co/airesidency)."
PseudoSeg: Designing Pseudo Labels for Semantic Segmentation,-TwO99rbVRu.json,"** Summary
This work addresses the task of semi-supervised learning (SSL) in semantic segmentation. Following recent SOTAs in SSL, this work also advocates for the use of pseudo-labels on unlabeled data and heavy data augmentation. The main novelty of this work is the novel way to construct higher-quality pseudo-labels: besides the pixel-wise classifier's probabilistic outputs, the authors leverage as well CAM-based activation maps, named as SGC, as an additional pseudo-label source.  The final set of pseudo-labels is determined by linear combining the two soft pseudo-label sources with temperature adjustment. The authors conducted extensive experiments with lots of ablation studies to validate the proposed framework.

** Strengths:
- The paper is well-written, easy to follow
- Extensive experiments with adequate discussions
- Improvements over SOTAs on the addressed benchmarks.

** Weakness/concerns:
 - Does ""distribution sharpen operation"" always use temperature < 1? If yes, what is the reason?
 - How is the temperature $T$ is chosen? May the authors produce a performance analysis over T?
 - In Sec 3.4, it's not clear to me the advantage of proposed method on boundaries. CAM-based activations mostly focus on most discriminative areas (usually inner areas). So hardly SGC can find pseudo-labels on boundaries. Why does the proposed method have an advantage there?
 - More and more segmentation works report results in urban datasets like cityscapes or camvid. It would be interesting to see results on those datasets. One interesting aspect in urban datasets is the natural long-tail class distributions, which severely damages performance on minor classes, especially in low-data regime.","[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Overview of unlabeled data training branch. Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss.
Table 2: ing, as suggested by Sohn et al. (2020a); Zoph et al. (2020); Xie et al. (2020); Sohn et al. (2020b).
Table 3: Figure 3 :3Figure 3: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ unlabeled data) on VOC12 val (left) and COCO val (right).
Table 4: Figure 4 :4Figure 4: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ image-level labeled data) on VOC12 val (left) and COCO val (right).
Table 5: Ablation studies on different factors. See Section 4.3 for complete details.
Table 6: +L u +L x ) Decoder (L s +L u ) Classifier (L x ) Self-attention (L sa ) L s : pixel-level labeled data L u : unlabeled/image-level labeled data L x : pixel-level labeled data (converting to image-level) or image-level labeled data L sa : pixel-level labeled data
Table 7: Figure 7 :7Figure 7: Training. For each network component, we show the loss supervision and the corresponding data.
Table 8: Directly applying consistency training approaches validated in image classification renders particular challenges in segmentation. We first demonstrate how well-calibrated soft pseudo labels obtained through wise fusion of predictions from diverse sources can greatly improve consistency training for segmentation. • We conduct extensive experimental studies on the PASCAL VOC 2012 and COCO datasets.
Table 9: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data). We use the official training set (1.4k) as labeled data, and the augmented set (9k) as unlabeled data.
Table 10: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data) using low-data splits. The exact numbers of pixel-labeled images are shown in brackets. All the methods use ResNet-101 as backbone except CCT(Ouali et al.
Table 11: Comparison  
Table 12: Comparison with state of the
Table 13: Comparison to alternative pseudo labeling strategies. We conduct experiments using 1/4, 1/8, 1/16 of the pixel-level labeled data, the exact numbers of images are shown in the brackets.
Table 14: Comparison with self-training. We use our supervised baseline as the teacher to generate one-hot pseudo labels, followingZoph et al. (2020).
Table 15: Improving fully supervised model with extra data. No test-time augmentation is used.
Table 16: Full results of 1/16 split in VOC12.
Table 17: Benchmarking state-of-the-art weakly supervised semantic segmentation methods. All the methods use image-level labels from VOC12 training (1.4k) and augmented (9k) sets.
Table 18: Performance analysis over T.
Table 19: Experiments on Cityscapes (w/ pixel-level labeled data and unlabeled data).
Table 20: Per-class performance analysis on Cityscapes (w/ pixel-level labeled data and unlabeled data).

[INTRODUCTION]
Image semantic segmentation is a core computer vision task that has been studied for decades. Compared with other vision tasks, such as image classification and object detection, human annotation of pixel-accurate segmentation is dramatically more expensive. Given sufficient pixellevel labeled training data (i.e., high-data regime), the current state-of-the-art segmentation models (e.g., DeepLabv3+ (Chen et al., 2018)) produce satisfactory segmentation prediction for common practical usage. Recent exploration demonstrates improvement over high-data regime settings with large-scale data, including self-training (Chen et al., 2020a;Zoph et al., 2020) and backbone pretraining (Zhang et al., 2020a).
In contrast to the high-data regime, the performance of segmentation models drop significantly, given very limited pixel-labeled data (i.e., low-data regime). Such ineffectiveness at the low-data regime hinders the applicability of segmentation models. Therefore, instead of improving high-data regime segmentation, our work focuses on data-efficient segmentation training that only relies on few pixellabeled data and leverages the availability of extra unlabeled or weakly annotated (e.g., image-level) data to improve performance, with the aim of narrowing the gap to the supervised models trained with fully pixel-labeled data.
Our work is inspired by the recent success in semi-supervised learning (SSL) for image classification, demonstrating promising performance given very limited labeled data and a sufficient amount of unlabeled data. Successful examples include MeanTeacher (Tarvainen & Valpola, 2017), UDA (Xie et al., 2019), MixMatch (Berthelot et al., 2019b), FeatMatch (Kuo et al., 2020), and FixMatch (Sohn et al., 2020a). One outstanding idea in this type of SSL is consistency training: making predictions consistent among multiple augmented images. FixMatch (Sohn et al., 2020a) shows that using high-confidence one-hot pseudo labels obtained from weakly-augmented unlabeled data to train strongly-augmented counterpart is the key to the success of SSL in image classification.
However, effective pseudo labels and well-designed data augmentation are non-trivial to satisfy for semantic segmentation. Although we observe that many related works explore the second condition (i.e., augmentation) for image segmentation to enable consistency training framework (French et al., 2020;Ouali et al., 2020), we show that a wise design of pseudo labels for segmentation has great veiled potentials.
In this paper, we propose PseudoSeg, a one-stage training framework to improve image semantic segmentation by leveraging additional data either with image-level labels (weakly-labeled data) or without any labels. PseudoSeg presents a novel design of pseudo-labeling to infer effective structured pseudo labels of additional data. It then optimizes the prediction of strongly-augmented data to match its corresponding pseudo labels. In summary, we make the following contributions:
• We propose a simple one-stage framework to improve semantic segmentation by using a limited amount of pixel-labeled data and sufficient unlabeled data or image-level labeled data. Our framework is simple to apply and therefore network architecture agnostic. 

[RELATED WORK]
Semi-supervised classification. Semi-supervised learning (SSL) aims to improve model performance by incorporating a large amount of unlabeled data during training. Consistency regularization and entropy minimization are two common strategies for SSL. The intuition behind consistencybased approaches (Laine & Aila, 2016;Sajjadi et al., 2016;Miyato et al., 2018;Tarvainen & Valpola, 2017) is that, the model output should remain unchanged when the input is perturbed. On the other hand, the entropy minimization strategy (Grandvalet & Bengio, 2005) argues that the unlabeled data can be used to ensured classes are well-separated, which can be achieved by encouraging the model to output low-entropy predictions. Pseudo-labeling (Lee, 2013) is one of the methods for implicit entropy minimization. Recently, holistic approaches (Berthelot et al., 2019b;a;Sohn et al., 2020a) combining both strategies have been proposed and achieved significant improvement. By redesigning the pseudo label, we propose an efficient one-stage semi-supervised learning framework of semantic segmentation for consistency training. Semi-supervised semantic segmentation. Collecting pixel-level annotations for semantic segmentation is costly and prone to error. Hence, leveraging unlabeled data in semantic segmentation is a natural fit. Early methods utilize a GAN-based model either to generate additional training data (Souly et al., 2017) or to learn a discriminator between the prediction and the ground truth mask (Hung et al., 2018;Mittal et al., 2019). Consistency regularization based approaches have also been proposed recently, by enforcing the predictions to be consistent, either from augmented input images (French et al., 2020;Kim et al., 2020), perturbed feature embeddings (Ouali et al., 2020), or different networks (Ke et al., 2020). Recently, Luo & Yang (2020) proposes a dual-branch training network to jointly learn from pixel-accurate and coarse labeled data, achieving good segmentation performance. To push the performance of state of the arts, iterative self-training approaches (Chen et al., 2020a;Zoph et al., 2020;Zhu et al., 2020) have been proposed. These methods usually assume the available labeled data is enough to train a good teacher model, which will be used to generate pseudo labels for the student model. However, this condition might not satisfy in the low-data regime. Our proposed method, on the other hand, realizing the ideas of both consistency regularization and pseudo-labeling in segmentation, consistently improves the supervised baseline in both low-data and high-data regimes.
Weakly-supervised semantic segmentation. Instead of supervising network training with accurate pixel-level labels, many prior works exploit weaker forms of annotations (e.g., bounding boxes (Dai et al., 2015), scribbles (Lin et al., 2016), image-level labels). Most recent approaches use imagelevel labels as the supervisory signal, which exploits the idea of class activation map (CAM) (Zhou et al., 2016). Since the vanilla CAM only focus on the most discriminative region of objects, dif- Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss. ferent ways to refine CAM have been proposed, including partial image/feature erasing (Hou et al., 2018;Wei et al., 2017;Li et al., 2018), using an additional saliency estimation model (Oh et al., 2017;Wei et al., 2018), utilizing pixel similarity to propagate the initial score map (Ahn & Kwak, 2018;, or mining and co-segment the same category of objects across images (Sun et al., 2020;. While achieving promising results using the approaches mentioned above, most of them require a multi-stage training strategy. The refined score maps are optimized again using a dense-CRF model (Krähenbühl & Koltun, 2011), and then used as the target to train a separate segmentation network. On the other hand, we assume there exists a small number of fully-annotated data, which allows us to learn stronger segmentation models than general methods without needing pixel-labeled data.

[THE PROPOSED METHOD]
In analogous to SSL for classification, our training objective in PseudoSeg consists of a supervised loss L s applied to pixel-level labeled data D l , and a consistency constraint L u applied to unlabeled data D u 1 . Specifically, the supervised loss L s is the standard pixel-wise cross-entropy loss on the weakly augmented pixel-level labeled examples:
L s = 1 N × |D l | x∈D l N −1 i=0 CrossEntropy (y i , f θ (ω(x i ))) ,(1)
where θ represents the learnable parameters of the network function f and N denotes the number of valid labeled pixels in an image x ∈ R H×W ×3 . y i ∈ R C is the ground truth label of a pixel i in H×W dimensions, and f θ (ω(x i )) ∈ R C is the predicted probability of pixel i, where C is the number of classes to predict and ω(•) denotes the weak (common) data augmentation operations used by Chen et al. (2018).
During training, the proposed PseudoSeg estimates a pseudo label y ∈ R H×W ×C for each stronglyaugmented unlabeled data x in D u , which is then used for computing the cross-entropy loss. The unsupervised objective can then be written as:
L u = 1 N × |D u | x∈Du N −1 i=0 CrossEntropy ( y i , f θ (β • ω(x i ))) ,(2)
where β(•) denotes a stronger data augmentation operation, which will be described in Section 3.2. We illustrate the unlabeled data training branch in Figure 1.

[THE DESIGN OF STRUCTURED PSEUDO LABELS]
The next important question is how to generate the desirable pseudo label y. A straightforward solution is directly using the decoder output of a trained segmentation model after confidence threshold- However, as we demonstrate later in the experiments, the generated pseudo hard/soft labels as well as other post-processing of outputs are barely satisfactory in the low-data regime, and thus yield inferior final results. To address this issue, our design of pseudo-labeling has two key insights. First, we seek for a distinct yet efficient decision mechanisms to compensate for the potential errors of decoder outputs. Second, wisely fusing multiple sources of predictions to generate an ensemble and better-calibrated version of pseudo labels.
Starting with localization. Compared with precise segmentation, learning localization is a simpler task as it only needs to provide coarser-grained outputs than pixel level of objects in images. Based on this motivation, we improve decoder predictions from the localization perspective. Class activation map (CAM) (Zhou et al., 2016) is a popular approach to provide localization for class-specific regions. CAM-based methods (Hou et al., 2018;Wei et al., 2017;Ahn & Kwak, 2018) have been successfully adopted to tackle a different weakly supervised semantic segmentation task from us, where they assume only image-level labels are available. In practice, we adopt a variant of class activation map, Grad-CAM (Selvaraju et al., 2017) in PseudoSeg.
From localization to segmentation. CAM estimates the strength of classifier responses on local feature maps. Thus, an inherent limitation of CAM-based approaches is that it is prone to attending only to the most discriminative regions. Although many weakly-supervised segmentation approaches (Ahn & Kwak, 2018;Ahn et al., 2019;Sun et al., 2020) aim at refining CAM localization maps to segmentation masks, most of them have complicated post-processing steps, such as dense CRF (Krähenbühl & Koltun, 2011), which increases the model complexity when used for consistency training. Here we present a computationally efficient yet effective refinement alternative, which is learnable using available pixel-labeled data.
Although CAM only localizes partial regions of interests, if we know the pairwise similarities between regions, we can propagate the CAM scores from the discriminative regions to the rest unattended regions. Actually, it has been shown in many works that the learned high-level deep features are usually good at similarity measurements of visual objects. In this paper, we find hypercolumn (Hariharan et al., 2015) with a learnable similarity measure function works fairly effective.
Given the vanilla Grad-CAM output for all C classes, which can be viewed as a spatially-flatten 2-D vector of weight m ∈ R L×C , where each row m i is the response weight per class for one region i. Using a kernel function K(•, •) : R H ×R H → R that measures element-wise similarity given feature h ∈ R H of two regions, the propagated scorem i ∈ R C can be computed as followŝ
m i =   m i + L−1 j=0 e K(W k hi,Wvhj ) L−1 k=0 e K(W k hi,Wvh k ) m j   • W c .(3)
The goal of this function is to train Θ = {W k , W v ∈ R H×H , W c ∈ R C×C } in order to propagate the high value in m to all adjacent elements in the feature space R H (i.e., hypercolumn features) to region i. Adding m i in equation 3 indicates the skip-connection. To compute propagated score for all regions, the operations in equation 3 can be efficiently implemented with self-attention dotproduct (Vaswani et al., 2017). For brevity, we denote this efficient refinement process output as selfattention Grad-CAM (SGC) maps in R H×H×C . Figure 6 in Appendix A specifies the architecture. Calibrated prediction fusion. SGC maps are obtained from low-resolution feature maps. It is then resized to the desired output resolution, and thus not sufficient at delineating crisp boundaries. However, compared to the segmentation decoder, SGC is capable of generating more locally-consistent masks. Thus, we propose a novel calibrated fusion strategy to take advantage of both decoder and SCG predictions for better pseudo labels.
Specifically, given a batch of decoder outputs (pre-softmax logits)p = f θ (ω(x)) and SGC mapsm computed from weakly-augmented data ω(x), we generate the pseudo labels y by
F(p,m) = Sharpen γ Softmax p Norm(p,m) + (1 − γ) Softmax m Norm(p,m) , T . (4)
Two critical procedures are proposed to use here to make the fusion process successful. First, p andm are from different decision mechanisms and they could have very different degrees of overconfidence. Therefore, we introduce the operation Norm(a, b)
= |a| i (a 2 i + b 2 i ) as a nor- Input Grad-CAM SGC map Decoder Decoder (strong) Pseudo label
Figure 2: Visualization of pseudo labels and other predictions. The generated pseudo label by fusing the predictions from the decoder and SGC map is used to supervise the decoder (strong) predictions of the strongly-augmented counterpart. malization factor. It alleviates the over-confident probability after softmax, which could unfavorably dominate the resulted γ-averaged probability. Second, the distribution sharpening operation Sharpen(a, T
) i = a 1/T i / C j a 1/T j
adjusts the temperature scalar T of categorical distribution (Berthelot et al., 2019b;Chen et al., 2020b). Figure 2 illustrates the predictions from different sources. More importantly, we investigate the pseudo-labeling from a calibration perspective (Section 4.3), demonstrating that the proposed soft pseudo label y leads to a better calibration metric comparing to other possible fusion alternatives, and justifying why it benefits the final segmentation performance. Training. Our final training objective contains two extra losses: a classification loss L x , and a segmentation loss L sa . First, to compute Grad-CAM, we add a one-layer classification head after the segmentation backbone and a multi-label classification loss L x . Second, as specified in Appendix A (Figure 6), SGC maps are scaled as pixel-wise probabilities using one-layer convolution followed by softmax in equation 3. Learning Θ to predict SGC maps needs pixel-labeled data D l . It is achieved by an extra segmentation loss L sa between SGC maps of pixel-labeled data and corresponding ground truth. All the loss terms are jointly optimized (i.e., L u + L s + L x + L sa ), while L sa only optimizes Θ (achieved by stopping gradient). See Figure 7 in the appendix for further details.

[INCORPORATING IMAGE-LEVEL LABELS AND AUGMENTATION]
The proposed PseudoSeg can easily incorporate image-level label information (if available) into our one-stage training framework, which also leads to consistent improvement as we demonstrate in experiments. We utilize the image-level data with two following steps. First, we directly use ground truth image-level labels to generate Grad-CAMs instead of using classifier outputs. Second, they are used to increase classification supervision beyond pixel-level labels for the classifier head.
For strong data augmentation, we simply follow color jittering operations from SimCLR (Chen et al., 2020b) and remove all geometric transformations. The overall strength of augmentation can be controlled by a scalar (studied in experiments). We also apply once random CutOut (DeVries & Taylor, 2017) with a region of 50 × 50 pixels since we find it gives consistent though minor improvement (pixels inside CutOut regions are ignored in computing losses).

[EXPERIMENTAL RESULTS]
We start by specifying the experimental details. Then, we evaluate the method in the settings of using pixel-level labeled data and unlabeled data, as well as using pixel-level labeled data and image-level labeled data, respectively. Next, we conduct various ablation studies to justify our design choices. Lastly, we conduct more comparative experiments in specific settings.
To evaluate the proposed method, we conduct the main experiments and ablation studies on the PAS-CAL VOC 2012 dataset (VOC12) (Everingham et al., 2015), which contains 21 classes including background. The standard VOC12 dataset has 1,449 images as the training set and 1,456 images as the validation set. We randomly subsample 1/2, 1/4, 1/8, and 1/16 of images in the standard training set to construct the pixel-level labeled data. The remaining images in the standard training set, together with the images in the augmented set (Hariharan et al., 2011) (around 9k images), are used as unlabeled or image-level labeled data. To further verify the effectiveness of the proposed method, we also conduct experiments on the COCO dataset (Lin et al., 2014). The COCO dataset has 118,287 images as the training set, and 5,000 images as the validation set. We evaluate on the 80 foreground classes and the background, as in the object detection task. As the COCO dataset is larger than VOC12, we randomly subsample smaller ratios, 1/32, 1/64, 1/128, 1/256, 1/512, of images from the training set to construct the pixel-level labeled data. The remaining images in the training set are used as unlabeled data or image-level labeled data. We evaluate the performance using the standard mean intersection-over-union (mIoU) metric. Implementation details can be found in Appendix B.

[EXPERIMENTS USING PIXEL-LEVEL LABELED DATA AND UNLABELED DATA]
Improvement over a strong baseline. We first demonstrate the effectiveness of the proposed method by comparing it with the DeepLabv3+ model trained with only the pixel-level labeled data. As shown in Figure 3 (a), the proposed method consistently outperforms the supervised training baseline on VOC12, by utilizing the pixel-level labeled data and the unlabeled data. The proposed method not only achieves a large performance boost in the low-data regime (when only 6.25% pixellevel labels available), but also improves the performance when the entire training set (1.4k images) is available. In Figure 3 (b), we again observe consistent improvement on the COCO dataset.
Comparisons with the others. Next, we compare the proposed method with recent state of the arts on both the public 1.4k/9k split (in Table 1) and the created low-data splits (in Table 2), on VOC12.
Our method compares favorably with the others.      Similar to semi-supervised learning using pixel-level labeled data and unlabeled data, we first demonstrate the efficacy of our method by comparing it with a strong supervised baseline. As shown in Figure 4, the proposed method consistently improves the strong baseline on both datasets.
In Table 3, we evaluate on the public 1.4k/9k split. The proposed method compares favorably with the other methods. Moreover, we further compare to best compared CCT on the created low-data splits (in Table 4). Both experiments show that the proposed PseudoSeg is more robust than the compared method given less data. On all splits on both datasets, using pixel-level labeled data and image-labeled data shows higher mIoU than the setting using pixel-level labeled data and unlabeled data.

[ABLATION STUDY]
In this section, we conduct extensive ablation experiments on VOC12 to validate our design choices. How to construct pseudo label? We investigate the effectiveness of the proposed pseudo labeling. Table 5 demonstrates quantitative results, indicating that using either decoder output or SGC alone gives an inferior performance. Naively using decoder output as pseudo labels can hardly work well. The proposed fusion consistently performs better, either with or without additional image-level labels. To further answer why our pseudo labels are effective, we study from the model calibration perspective. We measure the expected calibration error (ECE) (Guo et al., 2017) scores of all the intermediate steps and other fusion variants. As shown in Figure 5 (a), the proposed fusion strategy (denoted as G in the figure) achieves the lowest ECE scores, indicating that the significance of jointly using normalization with sharpening (see equation 4) compared with other fusion alternatives. We hypothesize using well-calibrated soft labels makes model training less affected by label noises. The comprehensive calibration study is left as a future exploration direction. Using hypercolumn feature or not? In Figure 5 (b), we study the effectiveness of using hypercolumn features instead of the last feature maps in equation 3. We conduct the experiments on the 1/16 split of VOC12. As we can see, hypercolumn features substantially improve performance. Soft or hard pseudo label? How to utilize predictions as pseudo labels remains an active question in SSL. Next, we study whether we should use soft or hard one-hot pseudo labels. We conduct  the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (c), using all predictions as soft pseudo label yields better performance than selecting confident predictions. This suggests that well-calibrated soft pseudo labels might be important in segmentation than over-simplified confidence thresholding. Temperature sharpening or not? We study the effect of temperature sharpening in equation 4. We conduct the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (d), temperature sharpening shows consistent and clear improvements. Strong augmentation strength. In Figure 5 (e), we study the effects of color jittering in the strong augmentation. The magnitude of jittering strength is controlled by a scalar (Chen et al., 2020b). We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. If the magnitude is too small, performance drops significantly, suggesting the importance of strong augmentation. Impact of different feature backbones. In Figure 5 (f), we compare the performance of using ResNet-50, ResNet-101, and Xception-65 as backbone architectures, respectively. We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. As we can see, the proposed method consistently improves the baseline by a substantial margin across different backbone architectures.

[COMPARISON WITH SELF-TRAINING]
Several recent approaches (Chen et al., 2020a;Zoph et al., 2020) exploit the Student-Teacher selftraining idea to improve the performance with additional unlabeled data. However, these methods only apply self-training in the high-data regime (i.e., sufficient pixel-labeled data to train teachers). Here we compare these methods in the low-data regimes, where we focus on. To generate offline pseudo labels, we closely follow segmentation experiments in Zoph et al. (2020): pixels with a confidence score higher than 0.5 will be used as one-hot pseudo labels, while the remaining are treated as ignored regions. This step is considered important to suppress noisy labels. A student model is then trained using the combination of unlabeled data in VOC12 train and augmented sets with generated one-hot pseudo labels and all the available pixel-level labeled data. As shown in Table 6, although the self-training pretty well improves over the supervised baseline, it is inferior to the proposed method 2 . We conjecture that the teacher model usually produces low confidence scores to pixels around boundaries, so pseudo labels of these pixels are filtered in student training. However, boundary pixels are important for improving the performance of segmentation (Kirillov et al., 2020). On the other hand, the design of our method (online soft pseudo labeling process) bypass this challenge. We will conduct more verification of this hypothesis in future work.

[IMPROVING THE FULLY-SUPERVISED METHOD WITH ADDITIONAL DATA]
We have validated the effectiveness of the proposed method in the low-data regime. In this section, we want to explore whether the proposed method can further improve supervised training in the full training set using additional data. We use the training set (1.4k) in VOC12 as the pixel-level labeled data. The additional data contains additional VOC 9k (V 9k ), COCO training set (C tr ), and COCO unlabeled data (C u ). More training details can be found in Appendix D. As shown in Table 7, the proposed PseudoSeg is able to improve upon the supervised baseline even in the high-data regime, using additional unlabeled or image-level labeled data. 

[DISCUSSION AND CONCLUSION]
The key to the good performance of our method in the low-data regime is the novel re-design of pseudo-labeling strategy, which pursues a different decision mechanism from weakly-supervised localization to ""remedy"" weak predictions from segmentation head. Then augmentation consistency training progressively improves segmentation head quality. For the first time, we demonstrate that, with well-calibrated soft pseudo labels, utilizing unlabeled or image-labeled data significantly improves segmentation at low-data regimes. Further exploration of fusing stronger and better-calibrated pseudo labels worth more study as future directions (e.g., multi-scaling). Although color jittering works within our method as strong data augmentation, we have extensively explored geometric augmentations (leveraging STN (Jaderberg et al., 2015) to align pixels in pseudo labels and strongly-augmented predictions) for segmentation but find it not helpful. We believe data augmentation needs re-thinking beyond current success in classification for segmentation usage.

[ACKNOWLEDGEMENT]
We thank Liang-Chieh Chen and Barret Zoph for their valuable comments.    5 . We also adopt a slightly different fusion strategy in this setting by using T = 0.7 and γ = 0.3.

[E COMPARISON WITH WEAKLY-SUPERVISED APPROACHES]
In Table 9, we benchmark recent weakly supervised semantic segmentation performance on PAS-CAL VOC 2012 val set. Instead of enforcing the consistency between different augmented images as we do, these approaches tackle the semantic segmentation task from a different perspective, by exploiting the weaker annotations (image-level labels). As we can see, by exploiting the imagelevel labels with careful designs, weakly-supervised semantic segmentation methods could achieve reasonably well performance. We believe that both perspectives are feasible and promising for low-data regime semantic segmentation tasks, and complementary to each other. Therefore, these designs could be potentially integrated into our framework to generate better pseudo labels, which leads to improved performance. We conduct an additional performance analysis for temporal sharpening. We conduct experiments over T on the 1/16 split of VOC using pixel-level labeled data and image-level labeled data. As shown in Table 10, adopting a T < 1 for distribution sharpening generally leads to improved performance.  (Cordts et al., 2016). The Cityscapes dataset contains 50 real-world driving sequences. Among these video sequences, 2,975 frames are selected as the training set, and 500 frames are selected as the validation set. Following previous common practice, we evaluate on 19 semantic classes.
Comparison with state of the art. We compare our method with the current state-of-the-art method (French et al., 2020), in the setting of using pixel-level labeled and unlabeled data. We randomly subsample 1/4, 1/8, and 1/30 of the training set to construct the pixel-level labeled data, using the first random seed provided by French et al. (2020). Both French et al. (2020) and our method use ResNet-101 as the feature backbone and DeepLabv3+ (Chen et al., 2018) as the segmentation model. As shown in Table 11, the proposed method achieves promising results on all the three label ratios. Per-class performance analysis. Next, we provide per-class performance break down analysis. We compare our method with the supervised baseline on the 1/30 split, using pixel-level labeled data and unlabeled data. As shown in Table 12, the distribution of the labeled pixels is severely imbalanced. Although our method does not in particular address the data imbalance issue, our method improves upon the supervised baseline on most of the classes (except for ""Wall"" and ""Pole""). Discussion. Although the scene layouts are quite similar for all the full images, it is still feasible to generate different image-level labels through a more aggressive geometric data augmentation (e.g., scaling, cropping, translation, etc.). In practice, standard segmentation preprocessing steps only crop a sub-region of the whole training images. It only contains partial images with a certain subset of image labels, making the training batches have diverse image-level labels (converted from pixellevel labels, in the fully-labeled+unlabeled setting). Moreover, in the fully-labeled+weakly-labeled setting, in practice, we can collect diverse Internet images and weakly label them, instead of weakly labeling images from Cityscapes.

[H QUALITATIVE RESULTS]
We visualize several model prediction results for PASCAL VOC 2012 (Figure 8) and COCO (Figure 9). As we can see, the supervised baseline struggles to segment some of the categories and small objects, when trained in the low-data regime. On the other hand, PseudoSeg utilizes unlabeled or weakly-labeled data to generate more satisfying predictions.

[APPENDIX A SELF-ATTENTION GRAD-CAM]
We elaborate the detailed pipeline of generating Self-attention Grad-CAM (SGC) maps (equation 3) in Figure 6. To construct the hypercolumn feature, we extract the feature maps from the last two convolutional stages of the backbone network and concatenate them together. We then project the hypercolumn feature to two separate low-dimension embedding spaces to construct ""key"" and ""query"", using two 1 × 1 convolutional layers. An attention matrix can then be computed via matrix multiplication of ""key"" and ""query"". To construct ""value"", we compute Grad-CAM for each foreground class and then concatenate them together. This results in a H × W × (C − 1) score map, where the maximum score of each category is normalized to one separately. We then use image-level labels (either from classifier prediction or ground truth annotation) to set the score maps of non-existing classes to be zero. For each pixel localization, we use one to subtract the maximum score to construct the background score map, which is then concatenated with the foreground score maps to form ""value"" (H × W × C). The attention score matrix can then be used to reweight and propagate the scores in ""value"". The propagated score is added back to the ""value"" score map, and the pass through a 1 × 1 convolution (w/ batch normalization) to output the SGC map. 

[B IMPLEMENTATION DETAILS]
We implement our method on top of the publicly available official DeepLab codebase. 3 Unless specified, we adopt the DeepLabv3+ model with Xception-65 (Chollet, 2017) as the feature backbone, which is pre-trained on the ImageNet dataset (Russakovsky et al., 2015). We train our model following the default hyper-parameters (e.g., an initial learning rate of 0.007 with a polynomial learning rate decay schedule, a crop size of 513 × 513, and an encoder output stride of 16), using 16 GPUs 4 . We use a batch size of 4 for each GPU for pixel-level labeled data, and 4 for unlabeled/image-level labeled data. For VOC12, we train the model for 30,000 iterations. For COCO, we train the model for 200,000 iterations. We set γ = 0.5 and T = 0.5 unless specified. We do not apply any test time augmentations.

[C LOW-DATA SAMPLING IN PASCAL VOC 2012]
Unlike random sampling in image classification, it is difficult to sample uniformly in a low-data case for semantic segmentation due to the imbalance of rare classes. To avoid the missing classes at extremely low data regimes, we repeat the random sampling process for 1/16 three times (while ensuring each class has a certain amount) and report the results. We use Split 1 in the main manuscript. All splits will be released to encourage reproducibility. The results of all the three splits are shown as in Table 8.

[INPUT]
Ground truth Supervised Ours (unlabeled) Ours (img. label) Figure 9: Qualitative results of COCO. Models are trained with 1/512 pixel-level labeled data in the training set. Note that white pixel in the ground truth indicates this pixel is not annotated for evaluation.","[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss."
PseudoSeg: Designing Pseudo Labels for Semantic Segmentation,-TwO99rbVRu.json,"Summary:
This paper focuses on the problem of semi-supervised semantic segmentation, where less pixel-level annotations are used to train the network. A new one-stage training framework is proposed to include the process of localization cue generation, pseudo label refinement and training of semantic segmentation. Inspire by recent success in the semi-supervised learning (SSL), a novel calibrated fusion strategy is proposed to incorporate the concept of consistency training with data augmentation into the framework. Experiments on PASCAL VOC and MSCOCO benchmarks validate the effectiveness of the proposed method.  

Pro:
+ The proposed one-stage training framework is elegant compared with two stage methods in this area which include one step for pseudo-label generation and another step for refinement then semantic segmentation training. 
+ The new designed calibrated fusion strategy well incorporate the concept of consistency training with data augmentation into the same framework.
+ Achieve a new state-of-the-art on both PASCAL VOC and MSCOCO benchmarks compared with recent semi-supervised semantic segmentation methods.

Questions:
- CCT (Ouali et al., 2020) includes the consistency training with perturbations which can be treated as a kind of data augmentation on features. I'm wondering if authors can provide some insights about why the proposed method can achieve better performance than CCT when they both include the consistency training and data augmentation in the designs.  
- In table 3, I suggest to include the segmentation framework used by each method in the table. In early works, old version of deeplab is usually treated as the standard. I understand using deeplab v3 is a fair comparison with CCT. It would be good to make this information clear in the table.
- It is also suggested to report the performance on PASCAL VOC test set as it is a common practice in this area (although CCT does not do so).
- Sine the unlabeled data training branch does not rely on any pixel-level annotations, I'm wondering if the proposed method can also work under weakly-supervised setting, where no pixel-level annotations are available during the training. ","[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Overview of unlabeled data training branch. Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss.
Table 2: ing, as suggested by Sohn et al. (2020a); Zoph et al. (2020); Xie et al. (2020); Sohn et al. (2020b).
Table 3: Figure 3 :3Figure 3: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ unlabeled data) on VOC12 val (left) and COCO val (right).
Table 4: Figure 4 :4Figure 4: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ image-level labeled data) on VOC12 val (left) and COCO val (right).
Table 5: Ablation studies on different factors. See Section 4.3 for complete details.
Table 6: +L u +L x ) Decoder (L s +L u ) Classifier (L x ) Self-attention (L sa ) L s : pixel-level labeled data L u : unlabeled/image-level labeled data L x : pixel-level labeled data (converting to image-level) or image-level labeled data L sa : pixel-level labeled data
Table 7: Figure 7 :7Figure 7: Training. For each network component, we show the loss supervision and the corresponding data.
Table 8: Directly applying consistency training approaches validated in image classification renders particular challenges in segmentation. We first demonstrate how well-calibrated soft pseudo labels obtained through wise fusion of predictions from diverse sources can greatly improve consistency training for segmentation. • We conduct extensive experimental studies on the PASCAL VOC 2012 and COCO datasets.
Table 9: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data). We use the official training set (1.4k) as labeled data, and the augmented set (9k) as unlabeled data.
Table 10: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data) using low-data splits. The exact numbers of pixel-labeled images are shown in brackets. All the methods use ResNet-101 as backbone except CCT(Ouali et al.
Table 11: Comparison  
Table 12: Comparison with state of the
Table 13: Comparison to alternative pseudo labeling strategies. We conduct experiments using 1/4, 1/8, 1/16 of the pixel-level labeled data, the exact numbers of images are shown in the brackets.
Table 14: Comparison with self-training. We use our supervised baseline as the teacher to generate one-hot pseudo labels, followingZoph et al. (2020).
Table 15: Improving fully supervised model with extra data. No test-time augmentation is used.
Table 16: Full results of 1/16 split in VOC12.
Table 17: Benchmarking state-of-the-art weakly supervised semantic segmentation methods. All the methods use image-level labels from VOC12 training (1.4k) and augmented (9k) sets.
Table 18: Performance analysis over T.
Table 19: Experiments on Cityscapes (w/ pixel-level labeled data and unlabeled data).
Table 20: Per-class performance analysis on Cityscapes (w/ pixel-level labeled data and unlabeled data).

[INTRODUCTION]
Image semantic segmentation is a core computer vision task that has been studied for decades. Compared with other vision tasks, such as image classification and object detection, human annotation of pixel-accurate segmentation is dramatically more expensive. Given sufficient pixellevel labeled training data (i.e., high-data regime), the current state-of-the-art segmentation models (e.g., DeepLabv3+ (Chen et al., 2018)) produce satisfactory segmentation prediction for common practical usage. Recent exploration demonstrates improvement over high-data regime settings with large-scale data, including self-training (Chen et al., 2020a;Zoph et al., 2020) and backbone pretraining (Zhang et al., 2020a).
In contrast to the high-data regime, the performance of segmentation models drop significantly, given very limited pixel-labeled data (i.e., low-data regime). Such ineffectiveness at the low-data regime hinders the applicability of segmentation models. Therefore, instead of improving high-data regime segmentation, our work focuses on data-efficient segmentation training that only relies on few pixellabeled data and leverages the availability of extra unlabeled or weakly annotated (e.g., image-level) data to improve performance, with the aim of narrowing the gap to the supervised models trained with fully pixel-labeled data.
Our work is inspired by the recent success in semi-supervised learning (SSL) for image classification, demonstrating promising performance given very limited labeled data and a sufficient amount of unlabeled data. Successful examples include MeanTeacher (Tarvainen & Valpola, 2017), UDA (Xie et al., 2019), MixMatch (Berthelot et al., 2019b), FeatMatch (Kuo et al., 2020), and FixMatch (Sohn et al., 2020a). One outstanding idea in this type of SSL is consistency training: making predictions consistent among multiple augmented images. FixMatch (Sohn et al., 2020a) shows that using high-confidence one-hot pseudo labels obtained from weakly-augmented unlabeled data to train strongly-augmented counterpart is the key to the success of SSL in image classification.
However, effective pseudo labels and well-designed data augmentation are non-trivial to satisfy for semantic segmentation. Although we observe that many related works explore the second condition (i.e., augmentation) for image segmentation to enable consistency training framework (French et al., 2020;Ouali et al., 2020), we show that a wise design of pseudo labels for segmentation has great veiled potentials.
In this paper, we propose PseudoSeg, a one-stage training framework to improve image semantic segmentation by leveraging additional data either with image-level labels (weakly-labeled data) or without any labels. PseudoSeg presents a novel design of pseudo-labeling to infer effective structured pseudo labels of additional data. It then optimizes the prediction of strongly-augmented data to match its corresponding pseudo labels. In summary, we make the following contributions:
• We propose a simple one-stage framework to improve semantic segmentation by using a limited amount of pixel-labeled data and sufficient unlabeled data or image-level labeled data. Our framework is simple to apply and therefore network architecture agnostic. 

[RELATED WORK]
Semi-supervised classification. Semi-supervised learning (SSL) aims to improve model performance by incorporating a large amount of unlabeled data during training. Consistency regularization and entropy minimization are two common strategies for SSL. The intuition behind consistencybased approaches (Laine & Aila, 2016;Sajjadi et al., 2016;Miyato et al., 2018;Tarvainen & Valpola, 2017) is that, the model output should remain unchanged when the input is perturbed. On the other hand, the entropy minimization strategy (Grandvalet & Bengio, 2005) argues that the unlabeled data can be used to ensured classes are well-separated, which can be achieved by encouraging the model to output low-entropy predictions. Pseudo-labeling (Lee, 2013) is one of the methods for implicit entropy minimization. Recently, holistic approaches (Berthelot et al., 2019b;a;Sohn et al., 2020a) combining both strategies have been proposed and achieved significant improvement. By redesigning the pseudo label, we propose an efficient one-stage semi-supervised learning framework of semantic segmentation for consistency training. Semi-supervised semantic segmentation. Collecting pixel-level annotations for semantic segmentation is costly and prone to error. Hence, leveraging unlabeled data in semantic segmentation is a natural fit. Early methods utilize a GAN-based model either to generate additional training data (Souly et al., 2017) or to learn a discriminator between the prediction and the ground truth mask (Hung et al., 2018;Mittal et al., 2019). Consistency regularization based approaches have also been proposed recently, by enforcing the predictions to be consistent, either from augmented input images (French et al., 2020;Kim et al., 2020), perturbed feature embeddings (Ouali et al., 2020), or different networks (Ke et al., 2020). Recently, Luo & Yang (2020) proposes a dual-branch training network to jointly learn from pixel-accurate and coarse labeled data, achieving good segmentation performance. To push the performance of state of the arts, iterative self-training approaches (Chen et al., 2020a;Zoph et al., 2020;Zhu et al., 2020) have been proposed. These methods usually assume the available labeled data is enough to train a good teacher model, which will be used to generate pseudo labels for the student model. However, this condition might not satisfy in the low-data regime. Our proposed method, on the other hand, realizing the ideas of both consistency regularization and pseudo-labeling in segmentation, consistently improves the supervised baseline in both low-data and high-data regimes.
Weakly-supervised semantic segmentation. Instead of supervising network training with accurate pixel-level labels, many prior works exploit weaker forms of annotations (e.g., bounding boxes (Dai et al., 2015), scribbles (Lin et al., 2016), image-level labels). Most recent approaches use imagelevel labels as the supervisory signal, which exploits the idea of class activation map (CAM) (Zhou et al., 2016). Since the vanilla CAM only focus on the most discriminative region of objects, dif- Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss. ferent ways to refine CAM have been proposed, including partial image/feature erasing (Hou et al., 2018;Wei et al., 2017;Li et al., 2018), using an additional saliency estimation model (Oh et al., 2017;Wei et al., 2018), utilizing pixel similarity to propagate the initial score map (Ahn & Kwak, 2018;, or mining and co-segment the same category of objects across images (Sun et al., 2020;. While achieving promising results using the approaches mentioned above, most of them require a multi-stage training strategy. The refined score maps are optimized again using a dense-CRF model (Krähenbühl & Koltun, 2011), and then used as the target to train a separate segmentation network. On the other hand, we assume there exists a small number of fully-annotated data, which allows us to learn stronger segmentation models than general methods without needing pixel-labeled data.

[THE PROPOSED METHOD]
In analogous to SSL for classification, our training objective in PseudoSeg consists of a supervised loss L s applied to pixel-level labeled data D l , and a consistency constraint L u applied to unlabeled data D u 1 . Specifically, the supervised loss L s is the standard pixel-wise cross-entropy loss on the weakly augmented pixel-level labeled examples:
L s = 1 N × |D l | x∈D l N −1 i=0 CrossEntropy (y i , f θ (ω(x i ))) ,(1)
where θ represents the learnable parameters of the network function f and N denotes the number of valid labeled pixels in an image x ∈ R H×W ×3 . y i ∈ R C is the ground truth label of a pixel i in H×W dimensions, and f θ (ω(x i )) ∈ R C is the predicted probability of pixel i, where C is the number of classes to predict and ω(•) denotes the weak (common) data augmentation operations used by Chen et al. (2018).
During training, the proposed PseudoSeg estimates a pseudo label y ∈ R H×W ×C for each stronglyaugmented unlabeled data x in D u , which is then used for computing the cross-entropy loss. The unsupervised objective can then be written as:
L u = 1 N × |D u | x∈Du N −1 i=0 CrossEntropy ( y i , f θ (β • ω(x i ))) ,(2)
where β(•) denotes a stronger data augmentation operation, which will be described in Section 3.2. We illustrate the unlabeled data training branch in Figure 1.

[THE DESIGN OF STRUCTURED PSEUDO LABELS]
The next important question is how to generate the desirable pseudo label y. A straightforward solution is directly using the decoder output of a trained segmentation model after confidence threshold- However, as we demonstrate later in the experiments, the generated pseudo hard/soft labels as well as other post-processing of outputs are barely satisfactory in the low-data regime, and thus yield inferior final results. To address this issue, our design of pseudo-labeling has two key insights. First, we seek for a distinct yet efficient decision mechanisms to compensate for the potential errors of decoder outputs. Second, wisely fusing multiple sources of predictions to generate an ensemble and better-calibrated version of pseudo labels.
Starting with localization. Compared with precise segmentation, learning localization is a simpler task as it only needs to provide coarser-grained outputs than pixel level of objects in images. Based on this motivation, we improve decoder predictions from the localization perspective. Class activation map (CAM) (Zhou et al., 2016) is a popular approach to provide localization for class-specific regions. CAM-based methods (Hou et al., 2018;Wei et al., 2017;Ahn & Kwak, 2018) have been successfully adopted to tackle a different weakly supervised semantic segmentation task from us, where they assume only image-level labels are available. In practice, we adopt a variant of class activation map, Grad-CAM (Selvaraju et al., 2017) in PseudoSeg.
From localization to segmentation. CAM estimates the strength of classifier responses on local feature maps. Thus, an inherent limitation of CAM-based approaches is that it is prone to attending only to the most discriminative regions. Although many weakly-supervised segmentation approaches (Ahn & Kwak, 2018;Ahn et al., 2019;Sun et al., 2020) aim at refining CAM localization maps to segmentation masks, most of them have complicated post-processing steps, such as dense CRF (Krähenbühl & Koltun, 2011), which increases the model complexity when used for consistency training. Here we present a computationally efficient yet effective refinement alternative, which is learnable using available pixel-labeled data.
Although CAM only localizes partial regions of interests, if we know the pairwise similarities between regions, we can propagate the CAM scores from the discriminative regions to the rest unattended regions. Actually, it has been shown in many works that the learned high-level deep features are usually good at similarity measurements of visual objects. In this paper, we find hypercolumn (Hariharan et al., 2015) with a learnable similarity measure function works fairly effective.
Given the vanilla Grad-CAM output for all C classes, which can be viewed as a spatially-flatten 2-D vector of weight m ∈ R L×C , where each row m i is the response weight per class for one region i. Using a kernel function K(•, •) : R H ×R H → R that measures element-wise similarity given feature h ∈ R H of two regions, the propagated scorem i ∈ R C can be computed as followŝ
m i =   m i + L−1 j=0 e K(W k hi,Wvhj ) L−1 k=0 e K(W k hi,Wvh k ) m j   • W c .(3)
The goal of this function is to train Θ = {W k , W v ∈ R H×H , W c ∈ R C×C } in order to propagate the high value in m to all adjacent elements in the feature space R H (i.e., hypercolumn features) to region i. Adding m i in equation 3 indicates the skip-connection. To compute propagated score for all regions, the operations in equation 3 can be efficiently implemented with self-attention dotproduct (Vaswani et al., 2017). For brevity, we denote this efficient refinement process output as selfattention Grad-CAM (SGC) maps in R H×H×C . Figure 6 in Appendix A specifies the architecture. Calibrated prediction fusion. SGC maps are obtained from low-resolution feature maps. It is then resized to the desired output resolution, and thus not sufficient at delineating crisp boundaries. However, compared to the segmentation decoder, SGC is capable of generating more locally-consistent masks. Thus, we propose a novel calibrated fusion strategy to take advantage of both decoder and SCG predictions for better pseudo labels.
Specifically, given a batch of decoder outputs (pre-softmax logits)p = f θ (ω(x)) and SGC mapsm computed from weakly-augmented data ω(x), we generate the pseudo labels y by
F(p,m) = Sharpen γ Softmax p Norm(p,m) + (1 − γ) Softmax m Norm(p,m) , T . (4)
Two critical procedures are proposed to use here to make the fusion process successful. First, p andm are from different decision mechanisms and they could have very different degrees of overconfidence. Therefore, we introduce the operation Norm(a, b)
= |a| i (a 2 i + b 2 i ) as a nor- Input Grad-CAM SGC map Decoder Decoder (strong) Pseudo label
Figure 2: Visualization of pseudo labels and other predictions. The generated pseudo label by fusing the predictions from the decoder and SGC map is used to supervise the decoder (strong) predictions of the strongly-augmented counterpart. malization factor. It alleviates the over-confident probability after softmax, which could unfavorably dominate the resulted γ-averaged probability. Second, the distribution sharpening operation Sharpen(a, T
) i = a 1/T i / C j a 1/T j
adjusts the temperature scalar T of categorical distribution (Berthelot et al., 2019b;Chen et al., 2020b). Figure 2 illustrates the predictions from different sources. More importantly, we investigate the pseudo-labeling from a calibration perspective (Section 4.3), demonstrating that the proposed soft pseudo label y leads to a better calibration metric comparing to other possible fusion alternatives, and justifying why it benefits the final segmentation performance. Training. Our final training objective contains two extra losses: a classification loss L x , and a segmentation loss L sa . First, to compute Grad-CAM, we add a one-layer classification head after the segmentation backbone and a multi-label classification loss L x . Second, as specified in Appendix A (Figure 6), SGC maps are scaled as pixel-wise probabilities using one-layer convolution followed by softmax in equation 3. Learning Θ to predict SGC maps needs pixel-labeled data D l . It is achieved by an extra segmentation loss L sa between SGC maps of pixel-labeled data and corresponding ground truth. All the loss terms are jointly optimized (i.e., L u + L s + L x + L sa ), while L sa only optimizes Θ (achieved by stopping gradient). See Figure 7 in the appendix for further details.

[INCORPORATING IMAGE-LEVEL LABELS AND AUGMENTATION]
The proposed PseudoSeg can easily incorporate image-level label information (if available) into our one-stage training framework, which also leads to consistent improvement as we demonstrate in experiments. We utilize the image-level data with two following steps. First, we directly use ground truth image-level labels to generate Grad-CAMs instead of using classifier outputs. Second, they are used to increase classification supervision beyond pixel-level labels for the classifier head.
For strong data augmentation, we simply follow color jittering operations from SimCLR (Chen et al., 2020b) and remove all geometric transformations. The overall strength of augmentation can be controlled by a scalar (studied in experiments). We also apply once random CutOut (DeVries & Taylor, 2017) with a region of 50 × 50 pixels since we find it gives consistent though minor improvement (pixels inside CutOut regions are ignored in computing losses).

[EXPERIMENTAL RESULTS]
We start by specifying the experimental details. Then, we evaluate the method in the settings of using pixel-level labeled data and unlabeled data, as well as using pixel-level labeled data and image-level labeled data, respectively. Next, we conduct various ablation studies to justify our design choices. Lastly, we conduct more comparative experiments in specific settings.
To evaluate the proposed method, we conduct the main experiments and ablation studies on the PAS-CAL VOC 2012 dataset (VOC12) (Everingham et al., 2015), which contains 21 classes including background. The standard VOC12 dataset has 1,449 images as the training set and 1,456 images as the validation set. We randomly subsample 1/2, 1/4, 1/8, and 1/16 of images in the standard training set to construct the pixel-level labeled data. The remaining images in the standard training set, together with the images in the augmented set (Hariharan et al., 2011) (around 9k images), are used as unlabeled or image-level labeled data. To further verify the effectiveness of the proposed method, we also conduct experiments on the COCO dataset (Lin et al., 2014). The COCO dataset has 118,287 images as the training set, and 5,000 images as the validation set. We evaluate on the 80 foreground classes and the background, as in the object detection task. As the COCO dataset is larger than VOC12, we randomly subsample smaller ratios, 1/32, 1/64, 1/128, 1/256, 1/512, of images from the training set to construct the pixel-level labeled data. The remaining images in the training set are used as unlabeled data or image-level labeled data. We evaluate the performance using the standard mean intersection-over-union (mIoU) metric. Implementation details can be found in Appendix B.

[EXPERIMENTS USING PIXEL-LEVEL LABELED DATA AND UNLABELED DATA]
Improvement over a strong baseline. We first demonstrate the effectiveness of the proposed method by comparing it with the DeepLabv3+ model trained with only the pixel-level labeled data. As shown in Figure 3 (a), the proposed method consistently outperforms the supervised training baseline on VOC12, by utilizing the pixel-level labeled data and the unlabeled data. The proposed method not only achieves a large performance boost in the low-data regime (when only 6.25% pixellevel labels available), but also improves the performance when the entire training set (1.4k images) is available. In Figure 3 (b), we again observe consistent improvement on the COCO dataset.
Comparisons with the others. Next, we compare the proposed method with recent state of the arts on both the public 1.4k/9k split (in Table 1) and the created low-data splits (in Table 2), on VOC12.
Our method compares favorably with the others.      Similar to semi-supervised learning using pixel-level labeled data and unlabeled data, we first demonstrate the efficacy of our method by comparing it with a strong supervised baseline. As shown in Figure 4, the proposed method consistently improves the strong baseline on both datasets.
In Table 3, we evaluate on the public 1.4k/9k split. The proposed method compares favorably with the other methods. Moreover, we further compare to best compared CCT on the created low-data splits (in Table 4). Both experiments show that the proposed PseudoSeg is more robust than the compared method given less data. On all splits on both datasets, using pixel-level labeled data and image-labeled data shows higher mIoU than the setting using pixel-level labeled data and unlabeled data.

[ABLATION STUDY]
In this section, we conduct extensive ablation experiments on VOC12 to validate our design choices. How to construct pseudo label? We investigate the effectiveness of the proposed pseudo labeling. Table 5 demonstrates quantitative results, indicating that using either decoder output or SGC alone gives an inferior performance. Naively using decoder output as pseudo labels can hardly work well. The proposed fusion consistently performs better, either with or without additional image-level labels. To further answer why our pseudo labels are effective, we study from the model calibration perspective. We measure the expected calibration error (ECE) (Guo et al., 2017) scores of all the intermediate steps and other fusion variants. As shown in Figure 5 (a), the proposed fusion strategy (denoted as G in the figure) achieves the lowest ECE scores, indicating that the significance of jointly using normalization with sharpening (see equation 4) compared with other fusion alternatives. We hypothesize using well-calibrated soft labels makes model training less affected by label noises. The comprehensive calibration study is left as a future exploration direction. Using hypercolumn feature or not? In Figure 5 (b), we study the effectiveness of using hypercolumn features instead of the last feature maps in equation 3. We conduct the experiments on the 1/16 split of VOC12. As we can see, hypercolumn features substantially improve performance. Soft or hard pseudo label? How to utilize predictions as pseudo labels remains an active question in SSL. Next, we study whether we should use soft or hard one-hot pseudo labels. We conduct  the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (c), using all predictions as soft pseudo label yields better performance than selecting confident predictions. This suggests that well-calibrated soft pseudo labels might be important in segmentation than over-simplified confidence thresholding. Temperature sharpening or not? We study the effect of temperature sharpening in equation 4. We conduct the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (d), temperature sharpening shows consistent and clear improvements. Strong augmentation strength. In Figure 5 (e), we study the effects of color jittering in the strong augmentation. The magnitude of jittering strength is controlled by a scalar (Chen et al., 2020b). We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. If the magnitude is too small, performance drops significantly, suggesting the importance of strong augmentation. Impact of different feature backbones. In Figure 5 (f), we compare the performance of using ResNet-50, ResNet-101, and Xception-65 as backbone architectures, respectively. We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. As we can see, the proposed method consistently improves the baseline by a substantial margin across different backbone architectures.

[COMPARISON WITH SELF-TRAINING]
Several recent approaches (Chen et al., 2020a;Zoph et al., 2020) exploit the Student-Teacher selftraining idea to improve the performance with additional unlabeled data. However, these methods only apply self-training in the high-data regime (i.e., sufficient pixel-labeled data to train teachers). Here we compare these methods in the low-data regimes, where we focus on. To generate offline pseudo labels, we closely follow segmentation experiments in Zoph et al. (2020): pixels with a confidence score higher than 0.5 will be used as one-hot pseudo labels, while the remaining are treated as ignored regions. This step is considered important to suppress noisy labels. A student model is then trained using the combination of unlabeled data in VOC12 train and augmented sets with generated one-hot pseudo labels and all the available pixel-level labeled data. As shown in Table 6, although the self-training pretty well improves over the supervised baseline, it is inferior to the proposed method 2 . We conjecture that the teacher model usually produces low confidence scores to pixels around boundaries, so pseudo labels of these pixels are filtered in student training. However, boundary pixels are important for improving the performance of segmentation (Kirillov et al., 2020). On the other hand, the design of our method (online soft pseudo labeling process) bypass this challenge. We will conduct more verification of this hypothesis in future work.

[IMPROVING THE FULLY-SUPERVISED METHOD WITH ADDITIONAL DATA]
We have validated the effectiveness of the proposed method in the low-data regime. In this section, we want to explore whether the proposed method can further improve supervised training in the full training set using additional data. We use the training set (1.4k) in VOC12 as the pixel-level labeled data. The additional data contains additional VOC 9k (V 9k ), COCO training set (C tr ), and COCO unlabeled data (C u ). More training details can be found in Appendix D. As shown in Table 7, the proposed PseudoSeg is able to improve upon the supervised baseline even in the high-data regime, using additional unlabeled or image-level labeled data. 

[DISCUSSION AND CONCLUSION]
The key to the good performance of our method in the low-data regime is the novel re-design of pseudo-labeling strategy, which pursues a different decision mechanism from weakly-supervised localization to ""remedy"" weak predictions from segmentation head. Then augmentation consistency training progressively improves segmentation head quality. For the first time, we demonstrate that, with well-calibrated soft pseudo labels, utilizing unlabeled or image-labeled data significantly improves segmentation at low-data regimes. Further exploration of fusing stronger and better-calibrated pseudo labels worth more study as future directions (e.g., multi-scaling). Although color jittering works within our method as strong data augmentation, we have extensively explored geometric augmentations (leveraging STN (Jaderberg et al., 2015) to align pixels in pseudo labels and strongly-augmented predictions) for segmentation but find it not helpful. We believe data augmentation needs re-thinking beyond current success in classification for segmentation usage.

[ACKNOWLEDGEMENT]
We thank Liang-Chieh Chen and Barret Zoph for their valuable comments.    5 . We also adopt a slightly different fusion strategy in this setting by using T = 0.7 and γ = 0.3.

[E COMPARISON WITH WEAKLY-SUPERVISED APPROACHES]
In Table 9, we benchmark recent weakly supervised semantic segmentation performance on PAS-CAL VOC 2012 val set. Instead of enforcing the consistency between different augmented images as we do, these approaches tackle the semantic segmentation task from a different perspective, by exploiting the weaker annotations (image-level labels). As we can see, by exploiting the imagelevel labels with careful designs, weakly-supervised semantic segmentation methods could achieve reasonably well performance. We believe that both perspectives are feasible and promising for low-data regime semantic segmentation tasks, and complementary to each other. Therefore, these designs could be potentially integrated into our framework to generate better pseudo labels, which leads to improved performance. We conduct an additional performance analysis for temporal sharpening. We conduct experiments over T on the 1/16 split of VOC using pixel-level labeled data and image-level labeled data. As shown in Table 10, adopting a T < 1 for distribution sharpening generally leads to improved performance.  (Cordts et al., 2016). The Cityscapes dataset contains 50 real-world driving sequences. Among these video sequences, 2,975 frames are selected as the training set, and 500 frames are selected as the validation set. Following previous common practice, we evaluate on 19 semantic classes.
Comparison with state of the art. We compare our method with the current state-of-the-art method (French et al., 2020), in the setting of using pixel-level labeled and unlabeled data. We randomly subsample 1/4, 1/8, and 1/30 of the training set to construct the pixel-level labeled data, using the first random seed provided by French et al. (2020). Both French et al. (2020) and our method use ResNet-101 as the feature backbone and DeepLabv3+ (Chen et al., 2018) as the segmentation model. As shown in Table 11, the proposed method achieves promising results on all the three label ratios. Per-class performance analysis. Next, we provide per-class performance break down analysis. We compare our method with the supervised baseline on the 1/30 split, using pixel-level labeled data and unlabeled data. As shown in Table 12, the distribution of the labeled pixels is severely imbalanced. Although our method does not in particular address the data imbalance issue, our method improves upon the supervised baseline on most of the classes (except for ""Wall"" and ""Pole""). Discussion. Although the scene layouts are quite similar for all the full images, it is still feasible to generate different image-level labels through a more aggressive geometric data augmentation (e.g., scaling, cropping, translation, etc.). In practice, standard segmentation preprocessing steps only crop a sub-region of the whole training images. It only contains partial images with a certain subset of image labels, making the training batches have diverse image-level labels (converted from pixellevel labels, in the fully-labeled+unlabeled setting). Moreover, in the fully-labeled+weakly-labeled setting, in practice, we can collect diverse Internet images and weakly label them, instead of weakly labeling images from Cityscapes.

[H QUALITATIVE RESULTS]
We visualize several model prediction results for PASCAL VOC 2012 (Figure 8) and COCO (Figure 9). As we can see, the supervised baseline struggles to segment some of the categories and small objects, when trained in the low-data regime. On the other hand, PseudoSeg utilizes unlabeled or weakly-labeled data to generate more satisfying predictions.

[APPENDIX A SELF-ATTENTION GRAD-CAM]
We elaborate the detailed pipeline of generating Self-attention Grad-CAM (SGC) maps (equation 3) in Figure 6. To construct the hypercolumn feature, we extract the feature maps from the last two convolutional stages of the backbone network and concatenate them together. We then project the hypercolumn feature to two separate low-dimension embedding spaces to construct ""key"" and ""query"", using two 1 × 1 convolutional layers. An attention matrix can then be computed via matrix multiplication of ""key"" and ""query"". To construct ""value"", we compute Grad-CAM for each foreground class and then concatenate them together. This results in a H × W × (C − 1) score map, where the maximum score of each category is normalized to one separately. We then use image-level labels (either from classifier prediction or ground truth annotation) to set the score maps of non-existing classes to be zero. For each pixel localization, we use one to subtract the maximum score to construct the background score map, which is then concatenated with the foreground score maps to form ""value"" (H × W × C). The attention score matrix can then be used to reweight and propagate the scores in ""value"". The propagated score is added back to the ""value"" score map, and the pass through a 1 × 1 convolution (w/ batch normalization) to output the SGC map. 

[B IMPLEMENTATION DETAILS]
We implement our method on top of the publicly available official DeepLab codebase. 3 Unless specified, we adopt the DeepLabv3+ model with Xception-65 (Chollet, 2017) as the feature backbone, which is pre-trained on the ImageNet dataset (Russakovsky et al., 2015). We train our model following the default hyper-parameters (e.g., an initial learning rate of 0.007 with a polynomial learning rate decay schedule, a crop size of 513 × 513, and an encoder output stride of 16), using 16 GPUs 4 . We use a batch size of 4 for each GPU for pixel-level labeled data, and 4 for unlabeled/image-level labeled data. For VOC12, we train the model for 30,000 iterations. For COCO, we train the model for 200,000 iterations. We set γ = 0.5 and T = 0.5 unless specified. We do not apply any test time augmentations.

[C LOW-DATA SAMPLING IN PASCAL VOC 2012]
Unlike random sampling in image classification, it is difficult to sample uniformly in a low-data case for semantic segmentation due to the imbalance of rare classes. To avoid the missing classes at extremely low data regimes, we repeat the random sampling process for 1/16 three times (while ensuring each class has a certain amount) and report the results. We use Split 1 in the main manuscript. All splits will be released to encourage reproducibility. The results of all the three splits are shown as in Table 8.

[INPUT]
Ground truth Supervised Ours (unlabeled) Ours (img. label) Figure 9: Qualitative results of COCO. Models are trained with 1/512 pixel-level labeled data in the training set. Note that white pixel in the ground truth indicates this pixel is not annotated for evaluation.","[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss."
PseudoSeg: Designing Pseudo Labels for Semantic Segmentation,-TwO99rbVRu.json,"**Summary:**

This paper introduces a model to improve semantic segmentation by using a limited amount of pixel-labeled data and unlabeled data or image-level labeled data. The authors use a Self-attention Grad-CAM (SGC) and segmenter to generate the pseudo-labels during training. The approach shows good results on Pascal VOC and COCO datasets and is well analyzed. 


**Reasons for score:** 

I do not think the technical contribution is strong enough for ICLR. The paper is incremental and the proposed approach is a combination of a lot of existing approaches. But I also want to highlight that the experimental section is strong and detailed.


**Pros:**

- The idea of using pseudo-labels is interesting because it allows to build larger dataset without increasing the annotation cost.
- The approach is evaluated in the settings of using unlabeled data and using image-level labeled data.
- The ablation study section gives a lot of details about the model. The authors analyzed a lot of things: expected calibration error, hypercolumn feature, soft vs hard label, temperature sharpening, color jittering strength, backbone architecture.
- The approach shows good results on Pascal VOC and COCO datasets.
- The proposed method achieves good performance in the low-data regime


**Cons:**
- The overall approach seems incremental because it is a combination of a lot of existing approaches and there is not a strong technical contribution. For instance, the model uses several loss functions and all the losses are jointly optimized.
- I think the related work section should be in the main paper instead of the supplementary. 
- I feel some parts are a bit difficult to read because of some misleading information. For example, the title of section 3.1 is “Experiments using unlabeled data” but the model still uses some labeled data. 
","[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Overview of unlabeled data training branch. Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss.
Table 2: ing, as suggested by Sohn et al. (2020a); Zoph et al. (2020); Xie et al. (2020); Sohn et al. (2020b).
Table 3: Figure 3 :3Figure 3: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ unlabeled data) on VOC12 val (left) and COCO val (right).
Table 4: Figure 4 :4Figure 4: Improvement over the strong supervised baseline, in a semi-supervised setting (w/ image-level labeled data) on VOC12 val (left) and COCO val (right).
Table 5: Ablation studies on different factors. See Section 4.3 for complete details.
Table 6: +L u +L x ) Decoder (L s +L u ) Classifier (L x ) Self-attention (L sa ) L s : pixel-level labeled data L u : unlabeled/image-level labeled data L x : pixel-level labeled data (converting to image-level) or image-level labeled data L sa : pixel-level labeled data
Table 7: Figure 7 :7Figure 7: Training. For each network component, we show the loss supervision and the corresponding data.
Table 8: Directly applying consistency training approaches validated in image classification renders particular challenges in segmentation. We first demonstrate how well-calibrated soft pseudo labels obtained through wise fusion of predictions from diverse sources can greatly improve consistency training for segmentation. • We conduct extensive experimental studies on the PASCAL VOC 2012 and COCO datasets.
Table 9: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data). We use the official training set (1.4k) as labeled data, and the augmented set (9k) as unlabeled data.
Table 10: Comparison with state of the arts on VOC12 val set (w/ pixel-level labeled data and unlabeled data) using low-data splits. The exact numbers of pixel-labeled images are shown in brackets. All the methods use ResNet-101 as backbone except CCT(Ouali et al.
Table 11: Comparison  
Table 12: Comparison with state of the
Table 13: Comparison to alternative pseudo labeling strategies. We conduct experiments using 1/4, 1/8, 1/16 of the pixel-level labeled data, the exact numbers of images are shown in the brackets.
Table 14: Comparison with self-training. We use our supervised baseline as the teacher to generate one-hot pseudo labels, followingZoph et al. (2020).
Table 15: Improving fully supervised model with extra data. No test-time augmentation is used.
Table 16: Full results of 1/16 split in VOC12.
Table 17: Benchmarking state-of-the-art weakly supervised semantic segmentation methods. All the methods use image-level labels from VOC12 training (1.4k) and augmented (9k) sets.
Table 18: Performance analysis over T.
Table 19: Experiments on Cityscapes (w/ pixel-level labeled data and unlabeled data).
Table 20: Per-class performance analysis on Cityscapes (w/ pixel-level labeled data and unlabeled data).

[INTRODUCTION]
Image semantic segmentation is a core computer vision task that has been studied for decades. Compared with other vision tasks, such as image classification and object detection, human annotation of pixel-accurate segmentation is dramatically more expensive. Given sufficient pixellevel labeled training data (i.e., high-data regime), the current state-of-the-art segmentation models (e.g., DeepLabv3+ (Chen et al., 2018)) produce satisfactory segmentation prediction for common practical usage. Recent exploration demonstrates improvement over high-data regime settings with large-scale data, including self-training (Chen et al., 2020a;Zoph et al., 2020) and backbone pretraining (Zhang et al., 2020a).
In contrast to the high-data regime, the performance of segmentation models drop significantly, given very limited pixel-labeled data (i.e., low-data regime). Such ineffectiveness at the low-data regime hinders the applicability of segmentation models. Therefore, instead of improving high-data regime segmentation, our work focuses on data-efficient segmentation training that only relies on few pixellabeled data and leverages the availability of extra unlabeled or weakly annotated (e.g., image-level) data to improve performance, with the aim of narrowing the gap to the supervised models trained with fully pixel-labeled data.
Our work is inspired by the recent success in semi-supervised learning (SSL) for image classification, demonstrating promising performance given very limited labeled data and a sufficient amount of unlabeled data. Successful examples include MeanTeacher (Tarvainen & Valpola, 2017), UDA (Xie et al., 2019), MixMatch (Berthelot et al., 2019b), FeatMatch (Kuo et al., 2020), and FixMatch (Sohn et al., 2020a). One outstanding idea in this type of SSL is consistency training: making predictions consistent among multiple augmented images. FixMatch (Sohn et al., 2020a) shows that using high-confidence one-hot pseudo labels obtained from weakly-augmented unlabeled data to train strongly-augmented counterpart is the key to the success of SSL in image classification.
However, effective pseudo labels and well-designed data augmentation are non-trivial to satisfy for semantic segmentation. Although we observe that many related works explore the second condition (i.e., augmentation) for image segmentation to enable consistency training framework (French et al., 2020;Ouali et al., 2020), we show that a wise design of pseudo labels for segmentation has great veiled potentials.
In this paper, we propose PseudoSeg, a one-stage training framework to improve image semantic segmentation by leveraging additional data either with image-level labels (weakly-labeled data) or without any labels. PseudoSeg presents a novel design of pseudo-labeling to infer effective structured pseudo labels of additional data. It then optimizes the prediction of strongly-augmented data to match its corresponding pseudo labels. In summary, we make the following contributions:
• We propose a simple one-stage framework to improve semantic segmentation by using a limited amount of pixel-labeled data and sufficient unlabeled data or image-level labeled data. Our framework is simple to apply and therefore network architecture agnostic. 

[RELATED WORK]
Semi-supervised classification. Semi-supervised learning (SSL) aims to improve model performance by incorporating a large amount of unlabeled data during training. Consistency regularization and entropy minimization are two common strategies for SSL. The intuition behind consistencybased approaches (Laine & Aila, 2016;Sajjadi et al., 2016;Miyato et al., 2018;Tarvainen & Valpola, 2017) is that, the model output should remain unchanged when the input is perturbed. On the other hand, the entropy minimization strategy (Grandvalet & Bengio, 2005) argues that the unlabeled data can be used to ensured classes are well-separated, which can be achieved by encouraging the model to output low-entropy predictions. Pseudo-labeling (Lee, 2013) is one of the methods for implicit entropy minimization. Recently, holistic approaches (Berthelot et al., 2019b;a;Sohn et al., 2020a) combining both strategies have been proposed and achieved significant improvement. By redesigning the pseudo label, we propose an efficient one-stage semi-supervised learning framework of semantic segmentation for consistency training. Semi-supervised semantic segmentation. Collecting pixel-level annotations for semantic segmentation is costly and prone to error. Hence, leveraging unlabeled data in semantic segmentation is a natural fit. Early methods utilize a GAN-based model either to generate additional training data (Souly et al., 2017) or to learn a discriminator between the prediction and the ground truth mask (Hung et al., 2018;Mittal et al., 2019). Consistency regularization based approaches have also been proposed recently, by enforcing the predictions to be consistent, either from augmented input images (French et al., 2020;Kim et al., 2020), perturbed feature embeddings (Ouali et al., 2020), or different networks (Ke et al., 2020). Recently, Luo & Yang (2020) proposes a dual-branch training network to jointly learn from pixel-accurate and coarse labeled data, achieving good segmentation performance. To push the performance of state of the arts, iterative self-training approaches (Chen et al., 2020a;Zoph et al., 2020;Zhu et al., 2020) have been proposed. These methods usually assume the available labeled data is enough to train a good teacher model, which will be used to generate pseudo labels for the student model. However, this condition might not satisfy in the low-data regime. Our proposed method, on the other hand, realizing the ideas of both consistency regularization and pseudo-labeling in segmentation, consistently improves the supervised baseline in both low-data and high-data regimes.
Weakly-supervised semantic segmentation. Instead of supervising network training with accurate pixel-level labels, many prior works exploit weaker forms of annotations (e.g., bounding boxes (Dai et al., 2015), scribbles (Lin et al., 2016), image-level labels). Most recent approaches use imagelevel labels as the supervisory signal, which exploits the idea of class activation map (CAM) (Zhou et al., 2016). Since the vanilla CAM only focus on the most discriminative region of objects, dif- Given an image, the weakly augmented version is fed into the network to get the decoder prediction and Self-attention Grad-CAM (SGC). The two sources are then combined via a calibrated fusion strategy to form the pseudo label. The network is trained to make its decoder prediction from strongly augmented image to match the pseudo label by a per-pixel cross-entropy loss. ferent ways to refine CAM have been proposed, including partial image/feature erasing (Hou et al., 2018;Wei et al., 2017;Li et al., 2018), using an additional saliency estimation model (Oh et al., 2017;Wei et al., 2018), utilizing pixel similarity to propagate the initial score map (Ahn & Kwak, 2018;, or mining and co-segment the same category of objects across images (Sun et al., 2020;. While achieving promising results using the approaches mentioned above, most of them require a multi-stage training strategy. The refined score maps are optimized again using a dense-CRF model (Krähenbühl & Koltun, 2011), and then used as the target to train a separate segmentation network. On the other hand, we assume there exists a small number of fully-annotated data, which allows us to learn stronger segmentation models than general methods without needing pixel-labeled data.

[THE PROPOSED METHOD]
In analogous to SSL for classification, our training objective in PseudoSeg consists of a supervised loss L s applied to pixel-level labeled data D l , and a consistency constraint L u applied to unlabeled data D u 1 . Specifically, the supervised loss L s is the standard pixel-wise cross-entropy loss on the weakly augmented pixel-level labeled examples:
L s = 1 N × |D l | x∈D l N −1 i=0 CrossEntropy (y i , f θ (ω(x i ))) ,(1)
where θ represents the learnable parameters of the network function f and N denotes the number of valid labeled pixels in an image x ∈ R H×W ×3 . y i ∈ R C is the ground truth label of a pixel i in H×W dimensions, and f θ (ω(x i )) ∈ R C is the predicted probability of pixel i, where C is the number of classes to predict and ω(•) denotes the weak (common) data augmentation operations used by Chen et al. (2018).
During training, the proposed PseudoSeg estimates a pseudo label y ∈ R H×W ×C for each stronglyaugmented unlabeled data x in D u , which is then used for computing the cross-entropy loss. The unsupervised objective can then be written as:
L u = 1 N × |D u | x∈Du N −1 i=0 CrossEntropy ( y i , f θ (β • ω(x i ))) ,(2)
where β(•) denotes a stronger data augmentation operation, which will be described in Section 3.2. We illustrate the unlabeled data training branch in Figure 1.

[THE DESIGN OF STRUCTURED PSEUDO LABELS]
The next important question is how to generate the desirable pseudo label y. A straightforward solution is directly using the decoder output of a trained segmentation model after confidence threshold- However, as we demonstrate later in the experiments, the generated pseudo hard/soft labels as well as other post-processing of outputs are barely satisfactory in the low-data regime, and thus yield inferior final results. To address this issue, our design of pseudo-labeling has two key insights. First, we seek for a distinct yet efficient decision mechanisms to compensate for the potential errors of decoder outputs. Second, wisely fusing multiple sources of predictions to generate an ensemble and better-calibrated version of pseudo labels.
Starting with localization. Compared with precise segmentation, learning localization is a simpler task as it only needs to provide coarser-grained outputs than pixel level of objects in images. Based on this motivation, we improve decoder predictions from the localization perspective. Class activation map (CAM) (Zhou et al., 2016) is a popular approach to provide localization for class-specific regions. CAM-based methods (Hou et al., 2018;Wei et al., 2017;Ahn & Kwak, 2018) have been successfully adopted to tackle a different weakly supervised semantic segmentation task from us, where they assume only image-level labels are available. In practice, we adopt a variant of class activation map, Grad-CAM (Selvaraju et al., 2017) in PseudoSeg.
From localization to segmentation. CAM estimates the strength of classifier responses on local feature maps. Thus, an inherent limitation of CAM-based approaches is that it is prone to attending only to the most discriminative regions. Although many weakly-supervised segmentation approaches (Ahn & Kwak, 2018;Ahn et al., 2019;Sun et al., 2020) aim at refining CAM localization maps to segmentation masks, most of them have complicated post-processing steps, such as dense CRF (Krähenbühl & Koltun, 2011), which increases the model complexity when used for consistency training. Here we present a computationally efficient yet effective refinement alternative, which is learnable using available pixel-labeled data.
Although CAM only localizes partial regions of interests, if we know the pairwise similarities between regions, we can propagate the CAM scores from the discriminative regions to the rest unattended regions. Actually, it has been shown in many works that the learned high-level deep features are usually good at similarity measurements of visual objects. In this paper, we find hypercolumn (Hariharan et al., 2015) with a learnable similarity measure function works fairly effective.
Given the vanilla Grad-CAM output for all C classes, which can be viewed as a spatially-flatten 2-D vector of weight m ∈ R L×C , where each row m i is the response weight per class for one region i. Using a kernel function K(•, •) : R H ×R H → R that measures element-wise similarity given feature h ∈ R H of two regions, the propagated scorem i ∈ R C can be computed as followŝ
m i =   m i + L−1 j=0 e K(W k hi,Wvhj ) L−1 k=0 e K(W k hi,Wvh k ) m j   • W c .(3)
The goal of this function is to train Θ = {W k , W v ∈ R H×H , W c ∈ R C×C } in order to propagate the high value in m to all adjacent elements in the feature space R H (i.e., hypercolumn features) to region i. Adding m i in equation 3 indicates the skip-connection. To compute propagated score for all regions, the operations in equation 3 can be efficiently implemented with self-attention dotproduct (Vaswani et al., 2017). For brevity, we denote this efficient refinement process output as selfattention Grad-CAM (SGC) maps in R H×H×C . Figure 6 in Appendix A specifies the architecture. Calibrated prediction fusion. SGC maps are obtained from low-resolution feature maps. It is then resized to the desired output resolution, and thus not sufficient at delineating crisp boundaries. However, compared to the segmentation decoder, SGC is capable of generating more locally-consistent masks. Thus, we propose a novel calibrated fusion strategy to take advantage of both decoder and SCG predictions for better pseudo labels.
Specifically, given a batch of decoder outputs (pre-softmax logits)p = f θ (ω(x)) and SGC mapsm computed from weakly-augmented data ω(x), we generate the pseudo labels y by
F(p,m) = Sharpen γ Softmax p Norm(p,m) + (1 − γ) Softmax m Norm(p,m) , T . (4)
Two critical procedures are proposed to use here to make the fusion process successful. First, p andm are from different decision mechanisms and they could have very different degrees of overconfidence. Therefore, we introduce the operation Norm(a, b)
= |a| i (a 2 i + b 2 i ) as a nor- Input Grad-CAM SGC map Decoder Decoder (strong) Pseudo label
Figure 2: Visualization of pseudo labels and other predictions. The generated pseudo label by fusing the predictions from the decoder and SGC map is used to supervise the decoder (strong) predictions of the strongly-augmented counterpart. malization factor. It alleviates the over-confident probability after softmax, which could unfavorably dominate the resulted γ-averaged probability. Second, the distribution sharpening operation Sharpen(a, T
) i = a 1/T i / C j a 1/T j
adjusts the temperature scalar T of categorical distribution (Berthelot et al., 2019b;Chen et al., 2020b). Figure 2 illustrates the predictions from different sources. More importantly, we investigate the pseudo-labeling from a calibration perspective (Section 4.3), demonstrating that the proposed soft pseudo label y leads to a better calibration metric comparing to other possible fusion alternatives, and justifying why it benefits the final segmentation performance. Training. Our final training objective contains two extra losses: a classification loss L x , and a segmentation loss L sa . First, to compute Grad-CAM, we add a one-layer classification head after the segmentation backbone and a multi-label classification loss L x . Second, as specified in Appendix A (Figure 6), SGC maps are scaled as pixel-wise probabilities using one-layer convolution followed by softmax in equation 3. Learning Θ to predict SGC maps needs pixel-labeled data D l . It is achieved by an extra segmentation loss L sa between SGC maps of pixel-labeled data and corresponding ground truth. All the loss terms are jointly optimized (i.e., L u + L s + L x + L sa ), while L sa only optimizes Θ (achieved by stopping gradient). See Figure 7 in the appendix for further details.

[INCORPORATING IMAGE-LEVEL LABELS AND AUGMENTATION]
The proposed PseudoSeg can easily incorporate image-level label information (if available) into our one-stage training framework, which also leads to consistent improvement as we demonstrate in experiments. We utilize the image-level data with two following steps. First, we directly use ground truth image-level labels to generate Grad-CAMs instead of using classifier outputs. Second, they are used to increase classification supervision beyond pixel-level labels for the classifier head.
For strong data augmentation, we simply follow color jittering operations from SimCLR (Chen et al., 2020b) and remove all geometric transformations. The overall strength of augmentation can be controlled by a scalar (studied in experiments). We also apply once random CutOut (DeVries & Taylor, 2017) with a region of 50 × 50 pixels since we find it gives consistent though minor improvement (pixels inside CutOut regions are ignored in computing losses).

[EXPERIMENTAL RESULTS]
We start by specifying the experimental details. Then, we evaluate the method in the settings of using pixel-level labeled data and unlabeled data, as well as using pixel-level labeled data and image-level labeled data, respectively. Next, we conduct various ablation studies to justify our design choices. Lastly, we conduct more comparative experiments in specific settings.
To evaluate the proposed method, we conduct the main experiments and ablation studies on the PAS-CAL VOC 2012 dataset (VOC12) (Everingham et al., 2015), which contains 21 classes including background. The standard VOC12 dataset has 1,449 images as the training set and 1,456 images as the validation set. We randomly subsample 1/2, 1/4, 1/8, and 1/16 of images in the standard training set to construct the pixel-level labeled data. The remaining images in the standard training set, together with the images in the augmented set (Hariharan et al., 2011) (around 9k images), are used as unlabeled or image-level labeled data. To further verify the effectiveness of the proposed method, we also conduct experiments on the COCO dataset (Lin et al., 2014). The COCO dataset has 118,287 images as the training set, and 5,000 images as the validation set. We evaluate on the 80 foreground classes and the background, as in the object detection task. As the COCO dataset is larger than VOC12, we randomly subsample smaller ratios, 1/32, 1/64, 1/128, 1/256, 1/512, of images from the training set to construct the pixel-level labeled data. The remaining images in the training set are used as unlabeled data or image-level labeled data. We evaluate the performance using the standard mean intersection-over-union (mIoU) metric. Implementation details can be found in Appendix B.

[EXPERIMENTS USING PIXEL-LEVEL LABELED DATA AND UNLABELED DATA]
Improvement over a strong baseline. We first demonstrate the effectiveness of the proposed method by comparing it with the DeepLabv3+ model trained with only the pixel-level labeled data. As shown in Figure 3 (a), the proposed method consistently outperforms the supervised training baseline on VOC12, by utilizing the pixel-level labeled data and the unlabeled data. The proposed method not only achieves a large performance boost in the low-data regime (when only 6.25% pixellevel labels available), but also improves the performance when the entire training set (1.4k images) is available. In Figure 3 (b), we again observe consistent improvement on the COCO dataset.
Comparisons with the others. Next, we compare the proposed method with recent state of the arts on both the public 1.4k/9k split (in Table 1) and the created low-data splits (in Table 2), on VOC12.
Our method compares favorably with the others.      Similar to semi-supervised learning using pixel-level labeled data and unlabeled data, we first demonstrate the efficacy of our method by comparing it with a strong supervised baseline. As shown in Figure 4, the proposed method consistently improves the strong baseline on both datasets.
In Table 3, we evaluate on the public 1.4k/9k split. The proposed method compares favorably with the other methods. Moreover, we further compare to best compared CCT on the created low-data splits (in Table 4). Both experiments show that the proposed PseudoSeg is more robust than the compared method given less data. On all splits on both datasets, using pixel-level labeled data and image-labeled data shows higher mIoU than the setting using pixel-level labeled data and unlabeled data.

[ABLATION STUDY]
In this section, we conduct extensive ablation experiments on VOC12 to validate our design choices. How to construct pseudo label? We investigate the effectiveness of the proposed pseudo labeling. Table 5 demonstrates quantitative results, indicating that using either decoder output or SGC alone gives an inferior performance. Naively using decoder output as pseudo labels can hardly work well. The proposed fusion consistently performs better, either with or without additional image-level labels. To further answer why our pseudo labels are effective, we study from the model calibration perspective. We measure the expected calibration error (ECE) (Guo et al., 2017) scores of all the intermediate steps and other fusion variants. As shown in Figure 5 (a), the proposed fusion strategy (denoted as G in the figure) achieves the lowest ECE scores, indicating that the significance of jointly using normalization with sharpening (see equation 4) compared with other fusion alternatives. We hypothesize using well-calibrated soft labels makes model training less affected by label noises. The comprehensive calibration study is left as a future exploration direction. Using hypercolumn feature or not? In Figure 5 (b), we study the effectiveness of using hypercolumn features instead of the last feature maps in equation 3. We conduct the experiments on the 1/16 split of VOC12. As we can see, hypercolumn features substantially improve performance. Soft or hard pseudo label? How to utilize predictions as pseudo labels remains an active question in SSL. Next, we study whether we should use soft or hard one-hot pseudo labels. We conduct  the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (c), using all predictions as soft pseudo label yields better performance than selecting confident predictions. This suggests that well-calibrated soft pseudo labels might be important in segmentation than over-simplified confidence thresholding. Temperature sharpening or not? We study the effect of temperature sharpening in equation 4. We conduct the experiments in the setting where pixel-level labeled data and image-level labeled data are available. As shown in Figure 5 (d), temperature sharpening shows consistent and clear improvements. Strong augmentation strength. In Figure 5 (e), we study the effects of color jittering in the strong augmentation. The magnitude of jittering strength is controlled by a scalar (Chen et al., 2020b). We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. If the magnitude is too small, performance drops significantly, suggesting the importance of strong augmentation. Impact of different feature backbones. In Figure 5 (f), we compare the performance of using ResNet-50, ResNet-101, and Xception-65 as backbone architectures, respectively. We conduct the experiments in the setting where pixel-level labeled data and unlabeled data are available. As we can see, the proposed method consistently improves the baseline by a substantial margin across different backbone architectures.

[COMPARISON WITH SELF-TRAINING]
Several recent approaches (Chen et al., 2020a;Zoph et al., 2020) exploit the Student-Teacher selftraining idea to improve the performance with additional unlabeled data. However, these methods only apply self-training in the high-data regime (i.e., sufficient pixel-labeled data to train teachers). Here we compare these methods in the low-data regimes, where we focus on. To generate offline pseudo labels, we closely follow segmentation experiments in Zoph et al. (2020): pixels with a confidence score higher than 0.5 will be used as one-hot pseudo labels, while the remaining are treated as ignored regions. This step is considered important to suppress noisy labels. A student model is then trained using the combination of unlabeled data in VOC12 train and augmented sets with generated one-hot pseudo labels and all the available pixel-level labeled data. As shown in Table 6, although the self-training pretty well improves over the supervised baseline, it is inferior to the proposed method 2 . We conjecture that the teacher model usually produces low confidence scores to pixels around boundaries, so pseudo labels of these pixels are filtered in student training. However, boundary pixels are important for improving the performance of segmentation (Kirillov et al., 2020). On the other hand, the design of our method (online soft pseudo labeling process) bypass this challenge. We will conduct more verification of this hypothesis in future work.

[IMPROVING THE FULLY-SUPERVISED METHOD WITH ADDITIONAL DATA]
We have validated the effectiveness of the proposed method in the low-data regime. In this section, we want to explore whether the proposed method can further improve supervised training in the full training set using additional data. We use the training set (1.4k) in VOC12 as the pixel-level labeled data. The additional data contains additional VOC 9k (V 9k ), COCO training set (C tr ), and COCO unlabeled data (C u ). More training details can be found in Appendix D. As shown in Table 7, the proposed PseudoSeg is able to improve upon the supervised baseline even in the high-data regime, using additional unlabeled or image-level labeled data. 

[DISCUSSION AND CONCLUSION]
The key to the good performance of our method in the low-data regime is the novel re-design of pseudo-labeling strategy, which pursues a different decision mechanism from weakly-supervised localization to ""remedy"" weak predictions from segmentation head. Then augmentation consistency training progressively improves segmentation head quality. For the first time, we demonstrate that, with well-calibrated soft pseudo labels, utilizing unlabeled or image-labeled data significantly improves segmentation at low-data regimes. Further exploration of fusing stronger and better-calibrated pseudo labels worth more study as future directions (e.g., multi-scaling). Although color jittering works within our method as strong data augmentation, we have extensively explored geometric augmentations (leveraging STN (Jaderberg et al., 2015) to align pixels in pseudo labels and strongly-augmented predictions) for segmentation but find it not helpful. We believe data augmentation needs re-thinking beyond current success in classification for segmentation usage.

[ACKNOWLEDGEMENT]
We thank Liang-Chieh Chen and Barret Zoph for their valuable comments.    5 . We also adopt a slightly different fusion strategy in this setting by using T = 0.7 and γ = 0.3.

[E COMPARISON WITH WEAKLY-SUPERVISED APPROACHES]
In Table 9, we benchmark recent weakly supervised semantic segmentation performance on PAS-CAL VOC 2012 val set. Instead of enforcing the consistency between different augmented images as we do, these approaches tackle the semantic segmentation task from a different perspective, by exploiting the weaker annotations (image-level labels). As we can see, by exploiting the imagelevel labels with careful designs, weakly-supervised semantic segmentation methods could achieve reasonably well performance. We believe that both perspectives are feasible and promising for low-data regime semantic segmentation tasks, and complementary to each other. Therefore, these designs could be potentially integrated into our framework to generate better pseudo labels, which leads to improved performance. We conduct an additional performance analysis for temporal sharpening. We conduct experiments over T on the 1/16 split of VOC using pixel-level labeled data and image-level labeled data. As shown in Table 10, adopting a T < 1 for distribution sharpening generally leads to improved performance.  (Cordts et al., 2016). The Cityscapes dataset contains 50 real-world driving sequences. Among these video sequences, 2,975 frames are selected as the training set, and 500 frames are selected as the validation set. Following previous common practice, we evaluate on 19 semantic classes.
Comparison with state of the art. We compare our method with the current state-of-the-art method (French et al., 2020), in the setting of using pixel-level labeled and unlabeled data. We randomly subsample 1/4, 1/8, and 1/30 of the training set to construct the pixel-level labeled data, using the first random seed provided by French et al. (2020). Both French et al. (2020) and our method use ResNet-101 as the feature backbone and DeepLabv3+ (Chen et al., 2018) as the segmentation model. As shown in Table 11, the proposed method achieves promising results on all the three label ratios. Per-class performance analysis. Next, we provide per-class performance break down analysis. We compare our method with the supervised baseline on the 1/30 split, using pixel-level labeled data and unlabeled data. As shown in Table 12, the distribution of the labeled pixels is severely imbalanced. Although our method does not in particular address the data imbalance issue, our method improves upon the supervised baseline on most of the classes (except for ""Wall"" and ""Pole""). Discussion. Although the scene layouts are quite similar for all the full images, it is still feasible to generate different image-level labels through a more aggressive geometric data augmentation (e.g., scaling, cropping, translation, etc.). In practice, standard segmentation preprocessing steps only crop a sub-region of the whole training images. It only contains partial images with a certain subset of image labels, making the training batches have diverse image-level labels (converted from pixellevel labels, in the fully-labeled+unlabeled setting). Moreover, in the fully-labeled+weakly-labeled setting, in practice, we can collect diverse Internet images and weakly label them, instead of weakly labeling images from Cityscapes.

[H QUALITATIVE RESULTS]
We visualize several model prediction results for PASCAL VOC 2012 (Figure 8) and COCO (Figure 9). As we can see, the supervised baseline struggles to segment some of the categories and small objects, when trained in the low-data regime. On the other hand, PseudoSeg utilizes unlabeled or weakly-labeled data to generate more satisfying predictions.

[APPENDIX A SELF-ATTENTION GRAD-CAM]
We elaborate the detailed pipeline of generating Self-attention Grad-CAM (SGC) maps (equation 3) in Figure 6. To construct the hypercolumn feature, we extract the feature maps from the last two convolutional stages of the backbone network and concatenate them together. We then project the hypercolumn feature to two separate low-dimension embedding spaces to construct ""key"" and ""query"", using two 1 × 1 convolutional layers. An attention matrix can then be computed via matrix multiplication of ""key"" and ""query"". To construct ""value"", we compute Grad-CAM for each foreground class and then concatenate them together. This results in a H × W × (C − 1) score map, where the maximum score of each category is normalized to one separately. We then use image-level labels (either from classifier prediction or ground truth annotation) to set the score maps of non-existing classes to be zero. For each pixel localization, we use one to subtract the maximum score to construct the background score map, which is then concatenated with the foreground score maps to form ""value"" (H × W × C). The attention score matrix can then be used to reweight and propagate the scores in ""value"". The propagated score is added back to the ""value"" score map, and the pass through a 1 × 1 convolution (w/ batch normalization) to output the SGC map. 

[B IMPLEMENTATION DETAILS]
We implement our method on top of the publicly available official DeepLab codebase. 3 Unless specified, we adopt the DeepLabv3+ model with Xception-65 (Chollet, 2017) as the feature backbone, which is pre-trained on the ImageNet dataset (Russakovsky et al., 2015). We train our model following the default hyper-parameters (e.g., an initial learning rate of 0.007 with a polynomial learning rate decay schedule, a crop size of 513 × 513, and an encoder output stride of 16), using 16 GPUs 4 . We use a batch size of 4 for each GPU for pixel-level labeled data, and 4 for unlabeled/image-level labeled data. For VOC12, we train the model for 30,000 iterations. For COCO, we train the model for 200,000 iterations. We set γ = 0.5 and T = 0.5 unless specified. We do not apply any test time augmentations.

[C LOW-DATA SAMPLING IN PASCAL VOC 2012]
Unlike random sampling in image classification, it is difficult to sample uniformly in a low-data case for semantic segmentation due to the imbalance of rare classes. To avoid the missing classes at extremely low data regimes, we repeat the random sampling process for 1/16 three times (while ensuring each class has a certain amount) and report the results. We use Split 1 in the main manuscript. All splits will be released to encourage reproducibility. The results of all the three splits are shown as in Table 8.

[INPUT]
Ground truth Supervised Ours (unlabeled) Ours (img. label) Figure 9: Qualitative results of COCO. Models are trained with 1/512 pixel-level labeled data in the training set. Note that white pixel in the ground truth indicates this pixel is not annotated for evaluation.","[TITLE]
PSEUDOSEG: DESIGNING PSEUDO LABELS FOR SEMANTIC SEGMENTATION

[ABSTRACT]
Recent advances in semi-supervised learning (SSL) demonstrate that a combination of consistency regularization and pseudo-labeling can effectively improve image classification accuracy in the low-data regime. Compared to classification, semantic segmentation tasks require much more intensive labeling costs. Thus, these tasks greatly benefit from data-efficient training methods. However, structured outputs in segmentation render particular difficulties (e.g., designing pseudo-labeling and augmentation) to apply existing SSL strategies. To address this problem, we present a simple and novel re-design of pseudo-labeling to generate well-calibrated structured pseudo labels for training with unlabeled or weaklylabeled data. Our proposed pseudo-labeling strategy is network structure agnostic to apply in a one-stage consistency training framework. We demonstrate the effectiveness of the proposed pseudo-labeling strategy in both low-data and highdata regimes. Extensive experiments have validated that pseudo labels generated from wisely fusing diverse sources and strong data augmentation are crucial to consistency training for semantic segmentation. The source code is available at https://github.com/googleinterns/wss."
Model-Free Model Reconciliation,Byex4T2XcV.json,"This paper extends/generalises an existing explanation approach called 'model reconciliation' in two ways. First, it generalises from classical planning models to MDPs, and second, it generalises by extending to cases in which there are no mental models of the human. The general idea is that offline learning is employed to determine which elements of a set of explanatory messages is the most useful to give to people The idea is evaluated on some synthetic problems, including one in which user studies were performed that evaluated whether there is consistency between what is learnt and what questions users have. No evaluation on the explanations themselves.

Some comments for improvement:

- I think the paper could be improved by defining what question the explanations are answering here. In the context of explainable planning, the explanations are not really answering why particular actions or sequences, but are informing users of the the robot model (modulo the user model). The explanations still require the observer to do quite a bit of work; namely, the judge given the parameters whether this is optimal. So, it would be interesting to define: what question would a user have that this model would be an answer to (if I'm making sense).

- It is not quite clear (to me at least) what the purpose of the theta function/model is at the start of Section 4; that is, the combination of the two models. In the definition of \Epsilon, we have 2^{\Theta} --> \mathcal{M}, but as far as I can tell, \Theta and \mathcal{M} are the same thing and we could just use \mathcal{M}.

- While I understand where the authors are going with the 'model-free' idea, the model-free moniker and the section 5 title could be considered somewhat cheeky considering that the prevailing assumption is that there are a collection of 'training' humans that provide the necessary mental models that are learnt from.

- Section 6 could be improved by clearly outlining the goals of the experiment, as well as giving a slightly better structure to the experiment design: independent variables, measures, etc. 

A question for the authors:

- The explanation process is very much tied to the idea of the explanation being optimal with respect to the human mental model. While the authors point to literature outlining that giving explanations that conform to existing beliefs is often good, is there any evidence to suggest that optimality is a criterion that people typically care about?


Minor things:
- I think both capital theta and mathcal{M} are undefined (first used in Section 4, paragraph 2 to define the epsilon model reconciliation function); but I was able to infer what they meant.

- The title of the paper is accurate, but will probably not be matched by keyword searches, etc. Perhaps adding ""explainable planning"" or something to the title?

- Section 5 title ""without access to human mental models""; perhaps ""without an explicit human mental model"". We cannot access human mental models ever :)

- Lots of non-capitalised terms in the referneces; e.g. ""mdp"" instead of ""MDP""","[TITLE]
Model-Free Model Reconciliation

[ABSTRACT]
Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as modelreconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs.

[CAPTIONS]
Table 1: Figure 3 :3Figure 3: The test accuracy for increasing sizes of training set.

[INTRODUCTION]
A significant barrier to integrating AI systems into our daily lives has been their inability to interact and work with us humans in an intuitive and explicable manner. Orchestrating such interactions would require the agents to have the ability to help users in the loop better understand the rationale behind their various actions. Thankfully there has been a lot of effort within the AI research community to develop systems capable of holding explanatory dialogues with users and thus help them understand the decisions under question [David W. Aha and Magazzeni, 2018;Daniele Magazzeni, 2018]. Such explanatory systems could help users resolve confusions regarding agent decisions that may stem from either a (1) lack of understanding (or even misunderstanding) of the task or (2) from their inferential limitations. While many earlier works Figure 1: A general overview of the explanation as model reconciliation.
in explanation have generally focused on the latter (cf. [Khan et al., 2009;Hayes and Shah, 2017;Seegebarth et al., 2012;Topin and Veloso, 2019]), there is a growing consensus on the importance of explanatory mechanisms that can help bridge the knowledge asymmetry between the system and the user.
In particular, in explanation as model-reconciliation [Chakraborti et al., 2017] we had studied the problem of reconciling knowledge asymmetries between the user and the agent within the context of planning. Works in this direction have generally looked at cases where the user's model of the task (i.e their belief about the initial state, the transition dynamics, and the goal) is known beforehand (in a representation scheme comparable to the one used by the agent) and do not match the agent's model. This mismatch means that the user would not be able to correctly evaluate the validity or the optimality of a given plan. Thus in this paradigm the explanations consist of information about the agent's model that the user could incorporate into his/her own model to correctly evaluate the plan in question.
Unfortunately, it is not always possible to have access to such models. In the most general case, we are dealing with the user's model of the agent and hence the user may not be capable of presenting traces or decisions that could be generated from this model. Even if the system tries to learn such a representation based on interactions with the user, there is no guarantee that the specific representational assumptions of the learned model and the vocabulary used would be satisfied by the user's mental model.
The definition of explanation as model reconciliation may leave one with the idea that there is no way around it. How could one ever truly perform effective reconciliation when there exists no user model guiding us to choose the parts of the model, which when revealed to the user will help them correctly evaluate the current decision? Are we left with revealing the entire agent model to the user as the only option? In this paper, we propose a simple and intuitive way we could still generate minimal explanations in the absence of declarative models. We argue that we could drive such explanations by using learned models that can predict how human expectations could be affected by possible explanations (derived completely from information about the agent model) and in fact show how this method could be viewed as a variation of previous approaches that have been put forth to identify explicable behavior.
We will start by extending model reconciliation to the more general setting of planning with Markov Decision Processes (Section 4). The rest of the paper will investigate how these ideas could be used when the human mental model of the task is unavailable, and will formulate a learning problem that allows us to learn simple models that could be used to identify minimal explanations (Section 5). Finally, we will evaluate our method on a set of standard MDP benchmarks and perform user studies to validate its viability (Section 6).

[BACKGROUND]
Figure 1 presents a general schematic representation for explanation as model reconciliation. The automated agent (henceforth referred to as robot) starts with a model M R that can be used to generate a decision π (where depending on the context, π may be a single action, plan, policy or a program). In this setting, M R h corresponds to the human's preconceived notions about the robot model. The explainer's job then becomes providing information about the model M R , such that the updated human model can correctly evaluate the validity of the robot decisions.
In this case, the robot could have chosen to provide the entire model, but for most realistic tasks, such models could be quite large, so dumping the entire model could be both unnecessary and impractical. It's also well known that people generally prefer explanations that are selective [Miller, 2018;Lombrozo, 2006]. Thus the users would be happier with explanations that asks them to update a subset of beliefs as opposed to a complete update. Note that M R need not be the original agent model, but rather some abstraction/approximation of the underlying robot model (that conserves some desired property of the decision like optimality or validity). In [Chakraborti et al., 2017] where model reconciliation was first introduced, M R was a classical planning model hence inherently interpretable and thus the reconciliation could easily be achieved, but the idea could be applied beyond just planning models. For example, one could understand the explanation methodology used by LIME [Ribeiro et al., 2016] as being a special case of model reconciliation. In their case, they assume the human model is empty and M R is an approximation of the underlying decision model is automatically generated for each decision using a set of prede-fined features.
In this work, we will be looking at the agents that use discounted infinite horizon Markov Decision Processes (or MDPs) [Russell and Norvig, 2003] as the decision making framework. Each MDP M is defined by a tuple S, A, T, R, γ, µ , where the S provides the set of possible atomic states, A defines the set of actions, T is the transition function, R the reward, γ the discounting factor (where 0 ≤ γ < 1) and µ corresponds to the distribution of possible initial states. T : S × A × S → [0, 1] provides the probability that for given state s ∈ S, the execution of an action a would induce a transition to a new state s , and R : S × A × S → R defines the reward corresponding to this transition. The solution concept in MDP takes the form of a policy π that maps each state to a potential action. A policy is said to be optimal for M (denoted as π * M ) if there exists no other policy that dominates the given policy in terms of the expected value of states (where the value of a state s under a policy π for a model M is denoted as V π M (s)). Executing the policy in a state results in a sequence of state action state tuples called execution trajectory or simply a trajectory, denoted as τ = (s 1 , a 1 , s 2 ), ..., (s n−1 , a n−1 , s n ) and we will use P M (τ |π) to denote the probability of sampling the given trajectory τ for a policy π in model M.
In the explanatory setting we are interested in, the robot uses a model M R = S, A, T R , R R , γ R of the task to come up with the policy to act on. For now we will assume this MDP already defines an interpretable model and the human uses a model M R h = S, A, T R h , R R h , γ R h to evaluate it (we will relax this assumption in later sections). Now the task ahead of us will be to formulate how we could still identify minimal information that could resolve user confusion when M R h , but before we can do that we need to reinterpret the ideas of inexplicability and the idea of model reconciliation that was defined in [Chakraborti et al., 2017] in the context of MDPs and we will start by considering a simple scenario.

[ILLUSTRATIVE EXAMPLE]
Consider a warehouse scenario, where a robot is tasked with moving packages from racks and dropping them off at the dispatch chute. The robot is powered by a battery pack that can be recharged by visiting its docking station. The docking station also doubles as a quality assurance station that the robot needs to visit whenever it picks up a box labeled #013 (which means the box is fragile). The robot's operations are mostly deterministic, apart from a small probability of slipping (0.25) in some cells, that could leave the robot in the same position. Now suppose the warehouse has just hired a new part-time employee to oversee the operations. The employee is just getting used to this new setting and is puzzled by the robot's decision to once in a while take a detour from the drop-off activity and visit a specific position of the factory floor (which is, in fact, the docking location). If we wished the robot to be explainable, then it would need to be capable of helping the employee better understand the underlying model used by the robot (i.e achieve some form of model reconciliation). Given the fact that the robot may not have an exact model of the user, one way to achieve this could be by providing robot's entire model to the user. Unfortunately, this could easily overwhelm the user.
Another possibility could be to allow the user to specify which robot actions appear inexplicable, and focus on providing facts relevant to those actions. This explanation may still prove to be quite verbose and may in fact not help resolve their confusion. For example, imagine a case where the robot is visiting the station to recharge its batteries and the human says that the visit action is inexplicable. Now even if the robot mentions that visiting the station recharges it, the employee may still be confused if they are under the incorrect assumption that the robot is operating on full battery. Similarly, if the human had expected the robot to go to the docking station due to some confusion regarding the box codes, the human may mark the robot decision to not go to the dropoff as being inexplicable and the explanations that could resolve the confusion may have little to do with that specific action marked as inexplicable.

[EXPLANATION AS MODEL RECONCILIATION FOR MDPS]
In this setting the human and robot models are captured as MDPs defined over the same set of states and thus we can characterize both models by the tuple θ = θ T , θ R , θ γ , θ µ , where the θ T provides the set of parameters that defines the transition probabilities P (.|s, a), while θ R the parameters corresponding to the reward function, θ γ the parameters corresponding to the discount factor and θ µ the parameters for the initial state distribution. For simple MDP models with atomic states, θ T contains parameters of the categorical distribution for each transition (θ µ will contain similar parameters for the initial state distribution), θ R contains the reward associated with each transition (an s, a, s tuple) and θ γ just contains the value of the discount factor. The specific instantiations of the parameters for each model M is captured as θ(M). For simplicity, we will denote each of the unique parameters in the tuple θ using indexes. For example, θ s,a T (M R ), will correspond to the parameters for the distribution P (.|s, a) for the model M R .
If we use M to capture the set of all possible models and Θ = θ T × θ R × θ γ × θ µ , then model reconciliation operation can be captured as a function E M R h ,M R : 2 Θ → M that takes in a set of model parameters and generates a new version of the model M R h where the set of specified parameters will be set to values from M R . For example,
M = E M R h ,M R (θ s1,a T )
will be a new model such that θ(M) will be identical to θ(M R h ), except that θ s1,a TM , will be equal to θ s1,a T R .
Practically, the model reconciliation operation corresponds to the robot informing the human about some part of its model. This communication could incur cost and we can capture this by using the cost function C : 2 Θ → R that maps a given set of a threshold to a cost. Now the question we need to ask is whether the agent is trying to explain its policy or if it is trying to explain some behavior (i.e an execution trace). Most of the earlier work that looks at model reconciliation explanation (cf. [Chakraborti et al., 2017;Sreedharan et al., 2018a;2018b]) has looked at sequential plans and has generally ignored this differentiation and treated the problem of explaining plans to be same as that of explaining behavior. In general, a given plan or policy compactly represents a set of possible behaviors and the choice of explaining behavior vs explaining the plans/policies could affect the content of the explanation being given. For example, when explaining policies there is the additional challenge of presenting the entire policy to the user and the explainer may need to justify action choices for extremely unlikely states or contingencies. On the other hand, when explaining a given set of behaviors the explainer needs to only justify their action choices for cases they actually witnessed. For example, when explaining traces from the warehouse scenario, given the small probability of slipping, the robot may never have to mention what to do when it slips, but on the other hand if we are dealing with full policies, the agent may need to talk about the states where the robot is in the slipped positions and they need to get up from that position and move on.
Explaining policies or plans becomes more relevant when we consider explanatory dialogues where the agent and the user are trying to jointly come to agreement on what policy/plans to follow (eg: decision support systems), while the latter may be more useful when the user is observing some agent operating in an environment.
With respect to policies, we assume that the user is presented with the entire policy. A given policy is said to be explicable to the human, if the policy is optimal for the human model. Therefore the goal of the explainer becomes that of ensuring the optimality of the given policy Definition 1. A set of parameters θ E corresponds to a complete policy explanation for the given robot policy π * M R , if the policy is also optimal for E M R h ,M R (θ E ) and is said to be the minimally complete policy explanation if there exists no other complete explanation θ E , such that, C(θ E ) < C(θ E ) Finding a complete policy explanation is relatively straightforward (the set of all parameters automatically meets this requirement). The more challenging case becomes that of finding the minimal or the cheapest explanations i.e. the minimally complete explanations. Such minimally complete explanations can be calculated by adopting a search strategy similar to [Chakraborti et al., 2017]. The search can start at the human model and try to find the minimal number of parameters that needs to be updated in the human model for the current policy to become optimal. Similar to generating minimally complete explanations, i.e, we can also generate monotonic explanations (i.e explanations where no further information about parameters in the robot model can affect the optimality of the current plan).
In the case of policies, we can also describe explicable planning and balancing cost of explanations with that of choosing policies that are inherently explicable, where inexplicablity score (I E ) of a policy π is defined as
I E (π, M R h ) = |E[V M R h π * (s)|s ∼ µ R h ] − E[V M R h π (s)|s ∼ µ R h ]
| Where π * is the optimal policy in the human model. Explicable planning thus becomes the problem of choosing Figure 2: Subfigure (a) shows a visualization of a trajectory expected by the user described in the illustrative example, and (b) shows the visualization of a trajectory the user may observe. Subfigure (c), shows the various explanatory messages that could be used in this scenario, note that the messages span information from multiple abstractions of the given task policies that minimize inexplicability score [Kulkarni et al., 2016], while minimizing the potential loss in optimality due to the policy choice (since the most explicable plan may not be an optimal policy). Balanced planning, as studied in [Chakraborti et al., 2019;, proposes going one step further and also takes into account possible savings in inexplicability score that can be achieved by providing explanation (while incurring additional cost of communicating the required explanations).
For explaining behavior, we will look at the simplest case, namely the agent needs to explain a set of behaviors that the user has just observed. We will assume that the observer has full observability of the state and is seeing the robot behavior for the first time. In such a setting, a given trace τ would appear explicable to the user if it could be sampled from their expected MDP policy (i.e a policy optimal in their model) or more generally, i.e P M R h (τ |π) > δ, where δ is some small threshold. 1 Definition 2. A set of parameters θ E corresponds to a complete behavior explanation for a set of traces
T = {τ 1 , ...τ n }, if ∀τ ∈ T, ∃π such that P E M R h ,M R (θ E ) (τ |π) > δ and π is an optimal policy for the model P E M R h ,M R (θ E ) .
The explanation is said to be the minimally complete behavior explanation if there exists no other complete explanation θ E , such that,
C(θ E ) < C(θ E )
Note that given the above definition, if δ is set very high it may not be possible to find a complete explanation, as the trace may genuinely contain low probability transitions. In this work we will assume δ to be zero.
While model reconciliation could be an important component of either policy or behavior explanation, the applicabil-ity of the model reconciliation explanations on their own for policies is limited by the fact that in all but problems with the smallest state spaces, the user would have trouble going over the entire policy. Thus in these settings, explanatory systems would need to also utilize policy approximation or summarization methods, then allow users the ability to drill down on policy details as required. Since our main goals was to focus on developing approaches that allow us to generate model reconciliation explanations without explicitly defined user models, the rest of the paper will mostly focus on behavior explanation. In Section 8, we will have a brief discussion on how these methods could potentially be extended to policy explanation scenarios.

[EXPLAINING WITHOUT EXPLICIT HUMAN MENTAL MODELS]
Now we will look at how we can identify cheap complete behavior explanations when the human model is unknown. We will go one step further from identifying not only the parameters that need to be explained, but also capturing the right modality/abstractions to present the information about the parameters. That is, we will no longer assume that the human is using a full MDP model to come up with their decisions. Instead, the robot starts with a set of explanatory messages Ψ = {m 1 , m 2 , ..., m n } that can be presented to the user. Where the messages correspond to a set of parameter values (the parameters corresponding to a set of messages {m 1 , ..m k } is denoted as E({m 1 , ..m k })) of the model as captured in some abstraction of this model and has a corresponding cost ( C) associated with it. The abstractions to consider may depend on the specific scenario and the previous information about the intended users (laypeople vs. experts). Some simple possibilities may be to consider qualitative models (say non-deterministic ones instead of stochastic)
and considering state abstractions the given task. Note that, technically E(Ψ), need not span the set of all possible model parameters, but could rather be limited to a subset of parameters identified to be relevant to the given problem. One possible way may be to consider variations of explanation techniques like MSE [Khan et al., 2009] to identify set of possible factors that affect the optimality of each action. In Figure 2, the subfigure (c) shows a set of possible explanatory messages for the warehouse domain, that consists of each parameter mapped to some english statement. For models captured using factored representations that use relational or propositional fluents, such statements could be easily generated using templates (cf. [Hayes and Shah, 2017]). Given this setting, we will now make some simplifying assumptions, namely, (1) the order in which the explanatory messages are presented does not matter (2) we have access to a set of observers with similar models and they share this model with the target user (3) the robot is viewing the task at the same level or at a more detailed level of granularity than the user and (4) the user and robot have some shared vocabulary in regards to the task. While assumption ( 1) is easily met since we are mostly dealing with model information and ( 4) is a prerequisite for most explanatory approaches, in section 8 we will discuss how we can possibly relax requirements (2) and (3). Now our goal is to learn a predictive model that is able to predict whether a given user would find a given s, a, s tuple explicable and how the user's perception changes with the given explanatory messages.
For example, at the beginning of an episode the user may be presented with the following explanatory messages,Ψ = {m 1 = ""Robot slips with probability 0.25 at grey cells""}, which corresponds to the fact that P (s i |a, s i ) = 0.25, for all states s i where the feature grey cell is true and for all actions a. Now the user will be presented with a sequence of transitions, say (1, 2), right, (2, 2) and asked whether the transition was explicable or not. Then the tuple (1, 2), right, (2, 2) , {m 1 }, l 1 , where l 1 is the label assigned by the user to the transition, becomes input to our learning method.
The exact function that we would want to learn would be
L( s, a, s , {m 1 , ..., m k }) =      1 if s, a, s ∼ π * E M R h ,M R (θ({m1,...,m k }) (s) 0 otherwise
Note that this is a modified version of the sequential model we introduced in [Zhang et al., 2017] for identifying whether a given plan is explicable or not. Though our methods vary in some significant aspects, namely, (1) we allow for the possibility that the explicability of the actions/traces could be affected by explanations provided by the system; (2) we no longer use labels of high level tasks as a proxy for the explicability of the trace. Instead, we just use a simple binary label on whether the transition is explicable or not; (3) we no longer consider sequence models but rather a much simpler labeling model that maps a single transition to the explicability label. We argue that in cases where the human is markovian on the same set of features as the agent, this rather simpler model suffices.
It is also important that our learning approach is more tractable than the ones studied in [Zhang et al., 2017], since in their case to build a balanced dataset (of explicable and inexplicable plans), they would need to uniformly sample through the entire plan space (an extremely hard endeavour with no obvious known approaches), while we stick to traces generated from the optimal policy and only need to randomly generate possible sets of explanatory messages, which is clearly a smaller set.
Once we have learned an approximation of the above labeling functionL, the problem of explanation generation for a trace τ = s 0 , a 0 , s 1 , ..., s n , a n , s n+1 becomes that of finding the subset of Ψ that balances the cost of communication with the reduction in the inexplicability of the given trace, i.e arg min
Ψ (C M (Ψ) + α * Σ n i=0 (1 −L( s i , a i , s i+1 ,Ψ)))
WhereΨ is a subset of Ψ and α is some scaling factor that balances the cost of explanation with the number of inexplicable transitions for a given trace.

[EVALUATION]
The success of the approach described above would be directly dependent on whether we can learn high accuracy labeling models. Once we have access to such a model, we could be quite confident in our ability to generate useful explanation (provided the user's model is the same as the one the labeler was trained on) and identifying the best explanation becomes a matter of just searching for the required subset of messages that minimizes the objective defined in section 5. So to evaluate the method our focus was on identifying if it was possible to learn high accuracy models. We validated our approach on both simulations and on data collected from users.

[EVALUATION ON SIMULATED DATA]
For simulations, we used a slightly modified versions of the Taxi domain [Dietterich, 1998] (of size 6*6), the Four rooms domain [Sutton et al., 1999] (of size 9*9) and the warehouse scenario (of size 9*9) described before (implemented using the SimpleRL framework [Abel, 2019]). For each domain, we start with an MDP instance (henceforth referred to as the robot model) and then create a space of possible user models by identifying a set of possible values for each MDP parameter. For example, in the taxi domain the parameters include position of the passengers, their destination, the step cost, discounting etc., for the Four rooms this included the goal locations, locations with negative rewards, discounting, step cost, slip probability, etc., and finally for the warehouse, the position of the box, the position of station #1, the step cost, slipping probabilities and the discounting factors were selected as potential parameters that can be updated. In this setting, we assume that there exists a single explanatory message for each possible parameter.
For each individual test, we select a random subset of three parameters and then randomly choose a value for each of these. We then treat this new MDP model as a stand-in for the user model and use it to label traces generated from the original MDP. The traces were generated by choosing a random initial state and then following the optimal policy of the robot until either the terminal state is reached or the trace length reaches a predefined limit. For each trace, a random subset of the explanations was selected and presented to the human. This means updating the MDP parameters to their corresponding values in the robot model only for the parameters specified by the current subset of explanation. Each individual transition was then labeled using this updated MDP. A transition was labeled as inexplicable if the action is not the optimal one in the human model (i.e. Q value is lower) or the next state had a probability of occurring of δ = 0.
We then used this set of labeled transitions to create a training set and test set for a decision tree learner. The input features to the decision tree consist of current state features, (just x and y for Four rooms and the position of the the taxi and passengers for the Taxi domain and for Warehouse it included the position of the robot and the fact whether the agent picked up the box or visited station #1), the index of the action and features capturing the current subset of explanations being considered. In each Warehouse and Four rooms test instance, we collected 900 unique data points as training set and 100 data points as the test set. Due to the complexity of the taxi domain, we generated less data points (since for each different explanation subset we need to solve a new planning problem) and used close to 220 unique points as training data and on average 28 data points as the test set.
We then tested on 20 such instances for each domain. Figure 3 plots the average test accuracy for models trained with training sets of varying sizes. As evident from the graph, a simple decision tree seems to be able to easily model the effect of explanations on labeling for these simulated scenarios. We chose a simple learning model to establish the viability of this method, but one could easily see that the use of more sophisticated learning methods and/or more informed features should lead to better results.

[USER STUDIES]
Next, we wanted to establish if we can still learn such simple models when the labels are collected from naive users. Our goal here is not to consider scenarios with possible differences in the user's knowledge, but rather cases where, even in the presence of a set of users with similar backgrounds, their responses to explanations would be too varied to learn useful models. To test this, we used the Warehouse domain as a test bed and collected feedback on how users would view the explicability of traces generated from this domain when presented with explanatory messages detailed in Figure 2.
For the study, we recruited 45 master turkers from the Amazon Mechanical Turk. Each participant was provided with the URL to a website (https://goo.gl/Hun3ce) where they could view and label various robot behaviors. We considered a setting where the robot had a full battery, but was picking up a fragile box and thus still needs to visit the station #1. The robot could slip on some cells marked in dark grey with probability 0.25 (slipping here meant the robot picture is tilted to give an impression that it slipped on the cell and didn't prevent the robot from moving to the next cell). To make sure that all the users had similar mental models at the start, they were provided with the following facts, (a) that robot couldn't pass through racks, (b) whenever the robot runs low on battery it needs to get to Station 1, (c) whenever the robot has a green battery sign next to the robot, that means their battery is full and (d) the robot needs to take the shortest route to the goal. Also, they were presented with an example trace in this instructions section and were made to take a small pre-test that allowed them to revise the above facts in various scenarios. After the pre-test, they were shown eight traces from the robot policy sampled according to their probabilities. After the first trace, the user was given an explanation message before each trace, where the message was taken from the seven possible messages (the order of the messages was always randomized).
From the data collected from 45 turkers, we removed data from seven users, based on the fact they didn't find any of the transition in the first trace (i.e the case where no explanation was provided) inexplicable. We imagine this number would go down when we move to expert users or users who are invested in the success of the robot. The data generated for the remaining 38 users were then used to train a decision tree. Since the placement of other objects in the environment were fixed, we were able to use rather simple features for the model like the current position of the robot (x and y), previous position (again x and y), the action, whether they have slipped and finally the explanations given. We found the model to have an average 10-fold cross validation score of 0.935. For a randomly generated train and test split (where the test split was 10% and contained around 7% inexpicable labels) the precision score was 0.9637 and the recall score was 0.9568.
Furthermore, we could see that the model was able to correctly predict the usefulness of intuitive minimal explanations for the given scenario. For example, it predicted that while the robots decision to visit station #1 would be considered inexeplicable by the user in the absence of any explanation, the user would mark it as explicable when they are explained about the box being fragile and that fragile boxes need to be inspected at station #1. In fact the model predicted that only the message that ""fragile boxes need to be inspected at station #1"" is enough to convince the user about the need for that action (i.e the user could deduce that the box must have been fragile). This shows that such learned models may help us generate cheaper explanations (the above set of explanations is smaller than the corresponding minimal complete behavior explanation for the domain), by taking into account the users ability to correctly predict missing information in simple cases. Another point of interest was that the model predicted all slipping events as explainable even in the absence of any explanations. The cases where the user saw a slip before being told about the possibility of slipping was rare (since there are two explanatory messages related to slipping and the probability of slipping was 0.25). Furthermore when we went over the data, we found that in most such cases, the users did mark it as explainable. This may be because the effect of slipping may not have been that detrimental to the overall plan (it doesn't take you off the current path). It would be interesting to see if this result would be the same in cases where slipping was a more likely event and if it had a more apparent effect on the robot's plan.

[RELATED WORK]
To the best of our knowledge, this work represents the first attempt at learning proxies for user mental models that allows an agent to predict the potential impact of providing explanations as model reconciliation to observers. With that said, there have been works that have looked at the problem of generating explanations in the presence of model uncertainty for human models. In particular, our previous works like [ Sreedharan et al., 2018a;2018b] have looked at cases where the agent has access to a set of potential human models. One drawback of considering a set of possible models is either they would need to have explicit sensing to identify the user model (which could mean asking a large number of questions to the user) or providing a large amount of information to cover the space of all possible models. In our work, the problem of identifying the specifics of the user model is resolved through an offline training process.
Another work quite related to the discussion covered in this paper is [Reddy et al., 2018], wherein the authors tried to identify cases where they can learn a potential model for the human's expectation of the task transition dynamics when they do not align with the real world dynamics. Unlike their work, we do not assume that the user can provide traces for the given task, rather they may be able to provide some highlevel feedback on the action (i.e. they may not be able to do or even know the right action but may be able to point out actions or transitions that surprise them). Moreover, their work requires that the user and the robot must have the same reward function, which is again an assumption we do not make. Even if we had followed their technique to learn a potential approximation of the human's transition model for the task, there is no guarantee that the learned representation would be one that makes sense to the human.

[DISCUSSION AND CONCLUSION]
This paper proposes a possible way in which model reconciliation explanation could be applied to cases where the user model is unknown. The method described here is a rather simple and general method to identify information that could potentially affect the user's mental model and produce effects that align with the agent's requirements. There is no requirement here that the messages have to align with actual facts about the world. This again points to the rather troubling similarities between the mechanisms needed to generate useful explanations and lies .
Two important assumptions we made throughout the work is that the user only considers the current state (as defined by the robot) to make their decisions and we have access to a model that was learned from interactions to previous users who had similar knowledge level to the current user. Relaxing the first assumption would require us to go beyond learning models that map each transitions to labels. Instead we have to consider sequential labeling models (for example models based on LSTM or CRF) of the type considered in [Zhang et al., 2017] to capture the human's expectations. For example, we considered a simple extension of the warehouse domain where the human believes the robot should visit two locations (i.e the human state contains variables that record whether the user has visited the locations). Even though here the user is considering a more detailed model, we were able to learn labeling models of 80% accuracy by using simple CRFs. As for the second, instead of assuming that all users are of the same type, a more reasonable assumption may be that the users could be clustered into N groups and we could learn a different labeling model for each user type. Now we still have a challenge of identifying the user type of a new user and one way to overcome this would be by adopting a decision-theoretic approach to this problem and modeling it as a POMDP (where user labels become observations and previously learned user models the observation models).
The work discussed in this paper only covers explanations that allow the user and the system to reconcile any model difference. This only covers a part of the entire explanatory dialogue. Even if there is no difference in models, the user may still have questions about parts of the policy or may raise alternative policies they think should be followed. This may arise from a difference in inferential abilities and may require providing information that is already part of their deductive closure eg: help them understand the long term consequences of taking some actions. Once you have access to a set of such messages one could use a method similar to the one described in the paper to find the set of helpful ones. Unlike the model reconciliation setting where the messages stand for information about the model, it is not quite clear how one could automatically generate such messages.

[ACKNOWLEDGMENTS]
This research is supported in part by the ONR grants N00014-16-1-2892, N00014-18-1-2442, N00014-18-1-2840, the AFOSR grant FA9550-18-1-0067, NASA grant NNX17AD06G and a JP Morgan AI Faculty Research grant .","[TITLE]
Model-Free Model Reconciliation

[ABSTRACT]
Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as modelreconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs."
Model-Free Model Reconciliation,Byex4T2XcV.json,"This paper extends previous works from the same authors on model reconciliation.
The novelty here is that they drop the assumption about having the explicit user model. In this work, the model is instead learned.
This is clearly an interesting work, and it's worth presenting and discussing it at the workshop.

Some comments:
1) I noticed that the authors make a number of assumptions, but they are scattered throughout the paper. For example, some of things that the authors assume:
-the order in which the explanations are presented does not matter
-human and robot MDP models only differ on the transition probabilities
-the robot has access to a set of explanatory messages

and others. Perhaps it would be clearer to state from the beginning what the assumptions are, so that the reader do not discover them here and then?

2) I wonder how this approach can actually scale or be generalised to other (more complex) scenarios.
Again, I'm recommending acceptance of this paper, but I have to admit the use case considered is quite basic. 
Can the authors comment on potential/limitation to scale to larger (and more complex/interesting) scenarios? Also, I think that the sentence ""finding a complete policy explanation is relatively straightforward"" should be put in context! As in many cases is not so straightforward!

3) What's the class of questions this approach aims to answer/address?
I didn't manage to understand which questions the authors are actually interested in.

4) User study  
The user study looks nice (I did it myself through the website). But honestly I don't understand how you get from that an assessment of the effectiveness of the proposed explanations.

Minor/typos:
""another point point""
""standins""


In conclusion, I recommend accepting the paper. I have some major concerns, as I wrote, but I believe that XAIP being a new area we should be as inclusive as possible, and we should acknowledge that big contributions are made step by step.
","[TITLE]
Model-Free Model Reconciliation

[ABSTRACT]
Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as modelreconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs.

[CAPTIONS]
Table 1: Figure 3 :3Figure 3: The test accuracy for increasing sizes of training set.

[INTRODUCTION]
A significant barrier to integrating AI systems into our daily lives has been their inability to interact and work with us humans in an intuitive and explicable manner. Orchestrating such interactions would require the agents to have the ability to help users in the loop better understand the rationale behind their various actions. Thankfully there has been a lot of effort within the AI research community to develop systems capable of holding explanatory dialogues with users and thus help them understand the decisions under question [David W. Aha and Magazzeni, 2018;Daniele Magazzeni, 2018]. Such explanatory systems could help users resolve confusions regarding agent decisions that may stem from either a (1) lack of understanding (or even misunderstanding) of the task or (2) from their inferential limitations. While many earlier works Figure 1: A general overview of the explanation as model reconciliation.
in explanation have generally focused on the latter (cf. [Khan et al., 2009;Hayes and Shah, 2017;Seegebarth et al., 2012;Topin and Veloso, 2019]), there is a growing consensus on the importance of explanatory mechanisms that can help bridge the knowledge asymmetry between the system and the user.
In particular, in explanation as model-reconciliation [Chakraborti et al., 2017] we had studied the problem of reconciling knowledge asymmetries between the user and the agent within the context of planning. Works in this direction have generally looked at cases where the user's model of the task (i.e their belief about the initial state, the transition dynamics, and the goal) is known beforehand (in a representation scheme comparable to the one used by the agent) and do not match the agent's model. This mismatch means that the user would not be able to correctly evaluate the validity or the optimality of a given plan. Thus in this paradigm the explanations consist of information about the agent's model that the user could incorporate into his/her own model to correctly evaluate the plan in question.
Unfortunately, it is not always possible to have access to such models. In the most general case, we are dealing with the user's model of the agent and hence the user may not be capable of presenting traces or decisions that could be generated from this model. Even if the system tries to learn such a representation based on interactions with the user, there is no guarantee that the specific representational assumptions of the learned model and the vocabulary used would be satisfied by the user's mental model.
The definition of explanation as model reconciliation may leave one with the idea that there is no way around it. How could one ever truly perform effective reconciliation when there exists no user model guiding us to choose the parts of the model, which when revealed to the user will help them correctly evaluate the current decision? Are we left with revealing the entire agent model to the user as the only option? In this paper, we propose a simple and intuitive way we could still generate minimal explanations in the absence of declarative models. We argue that we could drive such explanations by using learned models that can predict how human expectations could be affected by possible explanations (derived completely from information about the agent model) and in fact show how this method could be viewed as a variation of previous approaches that have been put forth to identify explicable behavior.
We will start by extending model reconciliation to the more general setting of planning with Markov Decision Processes (Section 4). The rest of the paper will investigate how these ideas could be used when the human mental model of the task is unavailable, and will formulate a learning problem that allows us to learn simple models that could be used to identify minimal explanations (Section 5). Finally, we will evaluate our method on a set of standard MDP benchmarks and perform user studies to validate its viability (Section 6).

[BACKGROUND]
Figure 1 presents a general schematic representation for explanation as model reconciliation. The automated agent (henceforth referred to as robot) starts with a model M R that can be used to generate a decision π (where depending on the context, π may be a single action, plan, policy or a program). In this setting, M R h corresponds to the human's preconceived notions about the robot model. The explainer's job then becomes providing information about the model M R , such that the updated human model can correctly evaluate the validity of the robot decisions.
In this case, the robot could have chosen to provide the entire model, but for most realistic tasks, such models could be quite large, so dumping the entire model could be both unnecessary and impractical. It's also well known that people generally prefer explanations that are selective [Miller, 2018;Lombrozo, 2006]. Thus the users would be happier with explanations that asks them to update a subset of beliefs as opposed to a complete update. Note that M R need not be the original agent model, but rather some abstraction/approximation of the underlying robot model (that conserves some desired property of the decision like optimality or validity). In [Chakraborti et al., 2017] where model reconciliation was first introduced, M R was a classical planning model hence inherently interpretable and thus the reconciliation could easily be achieved, but the idea could be applied beyond just planning models. For example, one could understand the explanation methodology used by LIME [Ribeiro et al., 2016] as being a special case of model reconciliation. In their case, they assume the human model is empty and M R is an approximation of the underlying decision model is automatically generated for each decision using a set of prede-fined features.
In this work, we will be looking at the agents that use discounted infinite horizon Markov Decision Processes (or MDPs) [Russell and Norvig, 2003] as the decision making framework. Each MDP M is defined by a tuple S, A, T, R, γ, µ , where the S provides the set of possible atomic states, A defines the set of actions, T is the transition function, R the reward, γ the discounting factor (where 0 ≤ γ < 1) and µ corresponds to the distribution of possible initial states. T : S × A × S → [0, 1] provides the probability that for given state s ∈ S, the execution of an action a would induce a transition to a new state s , and R : S × A × S → R defines the reward corresponding to this transition. The solution concept in MDP takes the form of a policy π that maps each state to a potential action. A policy is said to be optimal for M (denoted as π * M ) if there exists no other policy that dominates the given policy in terms of the expected value of states (where the value of a state s under a policy π for a model M is denoted as V π M (s)). Executing the policy in a state results in a sequence of state action state tuples called execution trajectory or simply a trajectory, denoted as τ = (s 1 , a 1 , s 2 ), ..., (s n−1 , a n−1 , s n ) and we will use P M (τ |π) to denote the probability of sampling the given trajectory τ for a policy π in model M.
In the explanatory setting we are interested in, the robot uses a model M R = S, A, T R , R R , γ R of the task to come up with the policy to act on. For now we will assume this MDP already defines an interpretable model and the human uses a model M R h = S, A, T R h , R R h , γ R h to evaluate it (we will relax this assumption in later sections). Now the task ahead of us will be to formulate how we could still identify minimal information that could resolve user confusion when M R h , but before we can do that we need to reinterpret the ideas of inexplicability and the idea of model reconciliation that was defined in [Chakraborti et al., 2017] in the context of MDPs and we will start by considering a simple scenario.

[ILLUSTRATIVE EXAMPLE]
Consider a warehouse scenario, where a robot is tasked with moving packages from racks and dropping them off at the dispatch chute. The robot is powered by a battery pack that can be recharged by visiting its docking station. The docking station also doubles as a quality assurance station that the robot needs to visit whenever it picks up a box labeled #013 (which means the box is fragile). The robot's operations are mostly deterministic, apart from a small probability of slipping (0.25) in some cells, that could leave the robot in the same position. Now suppose the warehouse has just hired a new part-time employee to oversee the operations. The employee is just getting used to this new setting and is puzzled by the robot's decision to once in a while take a detour from the drop-off activity and visit a specific position of the factory floor (which is, in fact, the docking location). If we wished the robot to be explainable, then it would need to be capable of helping the employee better understand the underlying model used by the robot (i.e achieve some form of model reconciliation). Given the fact that the robot may not have an exact model of the user, one way to achieve this could be by providing robot's entire model to the user. Unfortunately, this could easily overwhelm the user.
Another possibility could be to allow the user to specify which robot actions appear inexplicable, and focus on providing facts relevant to those actions. This explanation may still prove to be quite verbose and may in fact not help resolve their confusion. For example, imagine a case where the robot is visiting the station to recharge its batteries and the human says that the visit action is inexplicable. Now even if the robot mentions that visiting the station recharges it, the employee may still be confused if they are under the incorrect assumption that the robot is operating on full battery. Similarly, if the human had expected the robot to go to the docking station due to some confusion regarding the box codes, the human may mark the robot decision to not go to the dropoff as being inexplicable and the explanations that could resolve the confusion may have little to do with that specific action marked as inexplicable.

[EXPLANATION AS MODEL RECONCILIATION FOR MDPS]
In this setting the human and robot models are captured as MDPs defined over the same set of states and thus we can characterize both models by the tuple θ = θ T , θ R , θ γ , θ µ , where the θ T provides the set of parameters that defines the transition probabilities P (.|s, a), while θ R the parameters corresponding to the reward function, θ γ the parameters corresponding to the discount factor and θ µ the parameters for the initial state distribution. For simple MDP models with atomic states, θ T contains parameters of the categorical distribution for each transition (θ µ will contain similar parameters for the initial state distribution), θ R contains the reward associated with each transition (an s, a, s tuple) and θ γ just contains the value of the discount factor. The specific instantiations of the parameters for each model M is captured as θ(M). For simplicity, we will denote each of the unique parameters in the tuple θ using indexes. For example, θ s,a T (M R ), will correspond to the parameters for the distribution P (.|s, a) for the model M R .
If we use M to capture the set of all possible models and Θ = θ T × θ R × θ γ × θ µ , then model reconciliation operation can be captured as a function E M R h ,M R : 2 Θ → M that takes in a set of model parameters and generates a new version of the model M R h where the set of specified parameters will be set to values from M R . For example,
M = E M R h ,M R (θ s1,a T )
will be a new model such that θ(M) will be identical to θ(M R h ), except that θ s1,a TM , will be equal to θ s1,a T R .
Practically, the model reconciliation operation corresponds to the robot informing the human about some part of its model. This communication could incur cost and we can capture this by using the cost function C : 2 Θ → R that maps a given set of a threshold to a cost. Now the question we need to ask is whether the agent is trying to explain its policy or if it is trying to explain some behavior (i.e an execution trace). Most of the earlier work that looks at model reconciliation explanation (cf. [Chakraborti et al., 2017;Sreedharan et al., 2018a;2018b]) has looked at sequential plans and has generally ignored this differentiation and treated the problem of explaining plans to be same as that of explaining behavior. In general, a given plan or policy compactly represents a set of possible behaviors and the choice of explaining behavior vs explaining the plans/policies could affect the content of the explanation being given. For example, when explaining policies there is the additional challenge of presenting the entire policy to the user and the explainer may need to justify action choices for extremely unlikely states or contingencies. On the other hand, when explaining a given set of behaviors the explainer needs to only justify their action choices for cases they actually witnessed. For example, when explaining traces from the warehouse scenario, given the small probability of slipping, the robot may never have to mention what to do when it slips, but on the other hand if we are dealing with full policies, the agent may need to talk about the states where the robot is in the slipped positions and they need to get up from that position and move on.
Explaining policies or plans becomes more relevant when we consider explanatory dialogues where the agent and the user are trying to jointly come to agreement on what policy/plans to follow (eg: decision support systems), while the latter may be more useful when the user is observing some agent operating in an environment.
With respect to policies, we assume that the user is presented with the entire policy. A given policy is said to be explicable to the human, if the policy is optimal for the human model. Therefore the goal of the explainer becomes that of ensuring the optimality of the given policy Definition 1. A set of parameters θ E corresponds to a complete policy explanation for the given robot policy π * M R , if the policy is also optimal for E M R h ,M R (θ E ) and is said to be the minimally complete policy explanation if there exists no other complete explanation θ E , such that, C(θ E ) < C(θ E ) Finding a complete policy explanation is relatively straightforward (the set of all parameters automatically meets this requirement). The more challenging case becomes that of finding the minimal or the cheapest explanations i.e. the minimally complete explanations. Such minimally complete explanations can be calculated by adopting a search strategy similar to [Chakraborti et al., 2017]. The search can start at the human model and try to find the minimal number of parameters that needs to be updated in the human model for the current policy to become optimal. Similar to generating minimally complete explanations, i.e, we can also generate monotonic explanations (i.e explanations where no further information about parameters in the robot model can affect the optimality of the current plan).
In the case of policies, we can also describe explicable planning and balancing cost of explanations with that of choosing policies that are inherently explicable, where inexplicablity score (I E ) of a policy π is defined as
I E (π, M R h ) = |E[V M R h π * (s)|s ∼ µ R h ] − E[V M R h π (s)|s ∼ µ R h ]
| Where π * is the optimal policy in the human model. Explicable planning thus becomes the problem of choosing Figure 2: Subfigure (a) shows a visualization of a trajectory expected by the user described in the illustrative example, and (b) shows the visualization of a trajectory the user may observe. Subfigure (c), shows the various explanatory messages that could be used in this scenario, note that the messages span information from multiple abstractions of the given task policies that minimize inexplicability score [Kulkarni et al., 2016], while minimizing the potential loss in optimality due to the policy choice (since the most explicable plan may not be an optimal policy). Balanced planning, as studied in [Chakraborti et al., 2019;, proposes going one step further and also takes into account possible savings in inexplicability score that can be achieved by providing explanation (while incurring additional cost of communicating the required explanations).
For explaining behavior, we will look at the simplest case, namely the agent needs to explain a set of behaviors that the user has just observed. We will assume that the observer has full observability of the state and is seeing the robot behavior for the first time. In such a setting, a given trace τ would appear explicable to the user if it could be sampled from their expected MDP policy (i.e a policy optimal in their model) or more generally, i.e P M R h (τ |π) > δ, where δ is some small threshold. 1 Definition 2. A set of parameters θ E corresponds to a complete behavior explanation for a set of traces
T = {τ 1 , ...τ n }, if ∀τ ∈ T, ∃π such that P E M R h ,M R (θ E ) (τ |π) > δ and π is an optimal policy for the model P E M R h ,M R (θ E ) .
The explanation is said to be the minimally complete behavior explanation if there exists no other complete explanation θ E , such that,
C(θ E ) < C(θ E )
Note that given the above definition, if δ is set very high it may not be possible to find a complete explanation, as the trace may genuinely contain low probability transitions. In this work we will assume δ to be zero.
While model reconciliation could be an important component of either policy or behavior explanation, the applicabil-ity of the model reconciliation explanations on their own for policies is limited by the fact that in all but problems with the smallest state spaces, the user would have trouble going over the entire policy. Thus in these settings, explanatory systems would need to also utilize policy approximation or summarization methods, then allow users the ability to drill down on policy details as required. Since our main goals was to focus on developing approaches that allow us to generate model reconciliation explanations without explicitly defined user models, the rest of the paper will mostly focus on behavior explanation. In Section 8, we will have a brief discussion on how these methods could potentially be extended to policy explanation scenarios.

[EXPLAINING WITHOUT EXPLICIT HUMAN MENTAL MODELS]
Now we will look at how we can identify cheap complete behavior explanations when the human model is unknown. We will go one step further from identifying not only the parameters that need to be explained, but also capturing the right modality/abstractions to present the information about the parameters. That is, we will no longer assume that the human is using a full MDP model to come up with their decisions. Instead, the robot starts with a set of explanatory messages Ψ = {m 1 , m 2 , ..., m n } that can be presented to the user. Where the messages correspond to a set of parameter values (the parameters corresponding to a set of messages {m 1 , ..m k } is denoted as E({m 1 , ..m k })) of the model as captured in some abstraction of this model and has a corresponding cost ( C) associated with it. The abstractions to consider may depend on the specific scenario and the previous information about the intended users (laypeople vs. experts). Some simple possibilities may be to consider qualitative models (say non-deterministic ones instead of stochastic)
and considering state abstractions the given task. Note that, technically E(Ψ), need not span the set of all possible model parameters, but could rather be limited to a subset of parameters identified to be relevant to the given problem. One possible way may be to consider variations of explanation techniques like MSE [Khan et al., 2009] to identify set of possible factors that affect the optimality of each action. In Figure 2, the subfigure (c) shows a set of possible explanatory messages for the warehouse domain, that consists of each parameter mapped to some english statement. For models captured using factored representations that use relational or propositional fluents, such statements could be easily generated using templates (cf. [Hayes and Shah, 2017]). Given this setting, we will now make some simplifying assumptions, namely, (1) the order in which the explanatory messages are presented does not matter (2) we have access to a set of observers with similar models and they share this model with the target user (3) the robot is viewing the task at the same level or at a more detailed level of granularity than the user and (4) the user and robot have some shared vocabulary in regards to the task. While assumption ( 1) is easily met since we are mostly dealing with model information and ( 4) is a prerequisite for most explanatory approaches, in section 8 we will discuss how we can possibly relax requirements (2) and (3). Now our goal is to learn a predictive model that is able to predict whether a given user would find a given s, a, s tuple explicable and how the user's perception changes with the given explanatory messages.
For example, at the beginning of an episode the user may be presented with the following explanatory messages,Ψ = {m 1 = ""Robot slips with probability 0.25 at grey cells""}, which corresponds to the fact that P (s i |a, s i ) = 0.25, for all states s i where the feature grey cell is true and for all actions a. Now the user will be presented with a sequence of transitions, say (1, 2), right, (2, 2) and asked whether the transition was explicable or not. Then the tuple (1, 2), right, (2, 2) , {m 1 }, l 1 , where l 1 is the label assigned by the user to the transition, becomes input to our learning method.
The exact function that we would want to learn would be
L( s, a, s , {m 1 , ..., m k }) =      1 if s, a, s ∼ π * E M R h ,M R (θ({m1,...,m k }) (s) 0 otherwise
Note that this is a modified version of the sequential model we introduced in [Zhang et al., 2017] for identifying whether a given plan is explicable or not. Though our methods vary in some significant aspects, namely, (1) we allow for the possibility that the explicability of the actions/traces could be affected by explanations provided by the system; (2) we no longer use labels of high level tasks as a proxy for the explicability of the trace. Instead, we just use a simple binary label on whether the transition is explicable or not; (3) we no longer consider sequence models but rather a much simpler labeling model that maps a single transition to the explicability label. We argue that in cases where the human is markovian on the same set of features as the agent, this rather simpler model suffices.
It is also important that our learning approach is more tractable than the ones studied in [Zhang et al., 2017], since in their case to build a balanced dataset (of explicable and inexplicable plans), they would need to uniformly sample through the entire plan space (an extremely hard endeavour with no obvious known approaches), while we stick to traces generated from the optimal policy and only need to randomly generate possible sets of explanatory messages, which is clearly a smaller set.
Once we have learned an approximation of the above labeling functionL, the problem of explanation generation for a trace τ = s 0 , a 0 , s 1 , ..., s n , a n , s n+1 becomes that of finding the subset of Ψ that balances the cost of communication with the reduction in the inexplicability of the given trace, i.e arg min
Ψ (C M (Ψ) + α * Σ n i=0 (1 −L( s i , a i , s i+1 ,Ψ)))
WhereΨ is a subset of Ψ and α is some scaling factor that balances the cost of explanation with the number of inexplicable transitions for a given trace.

[EVALUATION]
The success of the approach described above would be directly dependent on whether we can learn high accuracy labeling models. Once we have access to such a model, we could be quite confident in our ability to generate useful explanation (provided the user's model is the same as the one the labeler was trained on) and identifying the best explanation becomes a matter of just searching for the required subset of messages that minimizes the objective defined in section 5. So to evaluate the method our focus was on identifying if it was possible to learn high accuracy models. We validated our approach on both simulations and on data collected from users.

[EVALUATION ON SIMULATED DATA]
For simulations, we used a slightly modified versions of the Taxi domain [Dietterich, 1998] (of size 6*6), the Four rooms domain [Sutton et al., 1999] (of size 9*9) and the warehouse scenario (of size 9*9) described before (implemented using the SimpleRL framework [Abel, 2019]). For each domain, we start with an MDP instance (henceforth referred to as the robot model) and then create a space of possible user models by identifying a set of possible values for each MDP parameter. For example, in the taxi domain the parameters include position of the passengers, their destination, the step cost, discounting etc., for the Four rooms this included the goal locations, locations with negative rewards, discounting, step cost, slip probability, etc., and finally for the warehouse, the position of the box, the position of station #1, the step cost, slipping probabilities and the discounting factors were selected as potential parameters that can be updated. In this setting, we assume that there exists a single explanatory message for each possible parameter.
For each individual test, we select a random subset of three parameters and then randomly choose a value for each of these. We then treat this new MDP model as a stand-in for the user model and use it to label traces generated from the original MDP. The traces were generated by choosing a random initial state and then following the optimal policy of the robot until either the terminal state is reached or the trace length reaches a predefined limit. For each trace, a random subset of the explanations was selected and presented to the human. This means updating the MDP parameters to their corresponding values in the robot model only for the parameters specified by the current subset of explanation. Each individual transition was then labeled using this updated MDP. A transition was labeled as inexplicable if the action is not the optimal one in the human model (i.e. Q value is lower) or the next state had a probability of occurring of δ = 0.
We then used this set of labeled transitions to create a training set and test set for a decision tree learner. The input features to the decision tree consist of current state features, (just x and y for Four rooms and the position of the the taxi and passengers for the Taxi domain and for Warehouse it included the position of the robot and the fact whether the agent picked up the box or visited station #1), the index of the action and features capturing the current subset of explanations being considered. In each Warehouse and Four rooms test instance, we collected 900 unique data points as training set and 100 data points as the test set. Due to the complexity of the taxi domain, we generated less data points (since for each different explanation subset we need to solve a new planning problem) and used close to 220 unique points as training data and on average 28 data points as the test set.
We then tested on 20 such instances for each domain. Figure 3 plots the average test accuracy for models trained with training sets of varying sizes. As evident from the graph, a simple decision tree seems to be able to easily model the effect of explanations on labeling for these simulated scenarios. We chose a simple learning model to establish the viability of this method, but one could easily see that the use of more sophisticated learning methods and/or more informed features should lead to better results.

[USER STUDIES]
Next, we wanted to establish if we can still learn such simple models when the labels are collected from naive users. Our goal here is not to consider scenarios with possible differences in the user's knowledge, but rather cases where, even in the presence of a set of users with similar backgrounds, their responses to explanations would be too varied to learn useful models. To test this, we used the Warehouse domain as a test bed and collected feedback on how users would view the explicability of traces generated from this domain when presented with explanatory messages detailed in Figure 2.
For the study, we recruited 45 master turkers from the Amazon Mechanical Turk. Each participant was provided with the URL to a website (https://goo.gl/Hun3ce) where they could view and label various robot behaviors. We considered a setting where the robot had a full battery, but was picking up a fragile box and thus still needs to visit the station #1. The robot could slip on some cells marked in dark grey with probability 0.25 (slipping here meant the robot picture is tilted to give an impression that it slipped on the cell and didn't prevent the robot from moving to the next cell). To make sure that all the users had similar mental models at the start, they were provided with the following facts, (a) that robot couldn't pass through racks, (b) whenever the robot runs low on battery it needs to get to Station 1, (c) whenever the robot has a green battery sign next to the robot, that means their battery is full and (d) the robot needs to take the shortest route to the goal. Also, they were presented with an example trace in this instructions section and were made to take a small pre-test that allowed them to revise the above facts in various scenarios. After the pre-test, they were shown eight traces from the robot policy sampled according to their probabilities. After the first trace, the user was given an explanation message before each trace, where the message was taken from the seven possible messages (the order of the messages was always randomized).
From the data collected from 45 turkers, we removed data from seven users, based on the fact they didn't find any of the transition in the first trace (i.e the case where no explanation was provided) inexplicable. We imagine this number would go down when we move to expert users or users who are invested in the success of the robot. The data generated for the remaining 38 users were then used to train a decision tree. Since the placement of other objects in the environment were fixed, we were able to use rather simple features for the model like the current position of the robot (x and y), previous position (again x and y), the action, whether they have slipped and finally the explanations given. We found the model to have an average 10-fold cross validation score of 0.935. For a randomly generated train and test split (where the test split was 10% and contained around 7% inexpicable labels) the precision score was 0.9637 and the recall score was 0.9568.
Furthermore, we could see that the model was able to correctly predict the usefulness of intuitive minimal explanations for the given scenario. For example, it predicted that while the robots decision to visit station #1 would be considered inexeplicable by the user in the absence of any explanation, the user would mark it as explicable when they are explained about the box being fragile and that fragile boxes need to be inspected at station #1. In fact the model predicted that only the message that ""fragile boxes need to be inspected at station #1"" is enough to convince the user about the need for that action (i.e the user could deduce that the box must have been fragile). This shows that such learned models may help us generate cheaper explanations (the above set of explanations is smaller than the corresponding minimal complete behavior explanation for the domain), by taking into account the users ability to correctly predict missing information in simple cases. Another point of interest was that the model predicted all slipping events as explainable even in the absence of any explanations. The cases where the user saw a slip before being told about the possibility of slipping was rare (since there are two explanatory messages related to slipping and the probability of slipping was 0.25). Furthermore when we went over the data, we found that in most such cases, the users did mark it as explainable. This may be because the effect of slipping may not have been that detrimental to the overall plan (it doesn't take you off the current path). It would be interesting to see if this result would be the same in cases where slipping was a more likely event and if it had a more apparent effect on the robot's plan.

[RELATED WORK]
To the best of our knowledge, this work represents the first attempt at learning proxies for user mental models that allows an agent to predict the potential impact of providing explanations as model reconciliation to observers. With that said, there have been works that have looked at the problem of generating explanations in the presence of model uncertainty for human models. In particular, our previous works like [ Sreedharan et al., 2018a;2018b] have looked at cases where the agent has access to a set of potential human models. One drawback of considering a set of possible models is either they would need to have explicit sensing to identify the user model (which could mean asking a large number of questions to the user) or providing a large amount of information to cover the space of all possible models. In our work, the problem of identifying the specifics of the user model is resolved through an offline training process.
Another work quite related to the discussion covered in this paper is [Reddy et al., 2018], wherein the authors tried to identify cases where they can learn a potential model for the human's expectation of the task transition dynamics when they do not align with the real world dynamics. Unlike their work, we do not assume that the user can provide traces for the given task, rather they may be able to provide some highlevel feedback on the action (i.e. they may not be able to do or even know the right action but may be able to point out actions or transitions that surprise them). Moreover, their work requires that the user and the robot must have the same reward function, which is again an assumption we do not make. Even if we had followed their technique to learn a potential approximation of the human's transition model for the task, there is no guarantee that the learned representation would be one that makes sense to the human.

[DISCUSSION AND CONCLUSION]
This paper proposes a possible way in which model reconciliation explanation could be applied to cases where the user model is unknown. The method described here is a rather simple and general method to identify information that could potentially affect the user's mental model and produce effects that align with the agent's requirements. There is no requirement here that the messages have to align with actual facts about the world. This again points to the rather troubling similarities between the mechanisms needed to generate useful explanations and lies .
Two important assumptions we made throughout the work is that the user only considers the current state (as defined by the robot) to make their decisions and we have access to a model that was learned from interactions to previous users who had similar knowledge level to the current user. Relaxing the first assumption would require us to go beyond learning models that map each transitions to labels. Instead we have to consider sequential labeling models (for example models based on LSTM or CRF) of the type considered in [Zhang et al., 2017] to capture the human's expectations. For example, we considered a simple extension of the warehouse domain where the human believes the robot should visit two locations (i.e the human state contains variables that record whether the user has visited the locations). Even though here the user is considering a more detailed model, we were able to learn labeling models of 80% accuracy by using simple CRFs. As for the second, instead of assuming that all users are of the same type, a more reasonable assumption may be that the users could be clustered into N groups and we could learn a different labeling model for each user type. Now we still have a challenge of identifying the user type of a new user and one way to overcome this would be by adopting a decision-theoretic approach to this problem and modeling it as a POMDP (where user labels become observations and previously learned user models the observation models).
The work discussed in this paper only covers explanations that allow the user and the system to reconcile any model difference. This only covers a part of the entire explanatory dialogue. Even if there is no difference in models, the user may still have questions about parts of the policy or may raise alternative policies they think should be followed. This may arise from a difference in inferential abilities and may require providing information that is already part of their deductive closure eg: help them understand the long term consequences of taking some actions. Once you have access to a set of such messages one could use a method similar to the one described in the paper to find the set of helpful ones. Unlike the model reconciliation setting where the messages stand for information about the model, it is not quite clear how one could automatically generate such messages.

[ACKNOWLEDGMENTS]
This research is supported in part by the ONR grants N00014-16-1-2892, N00014-18-1-2442, N00014-18-1-2840, the AFOSR grant FA9550-18-1-0067, NASA grant NNX17AD06G and a JP Morgan AI Faculty Research grant .","[TITLE]
Model-Free Model Reconciliation

[ABSTRACT]
Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as modelreconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs."
The Early Phase of Neural Network Training,Hkl1iRNFwS.json,"Overview:

This paper is dedicated to examining the changes that networks undergo during the early phase of the network training. The author conducts extensive measurements of the network state and its updates during the early iterations of training. Based on the observations, they find that: i) deep network is not robust to reinitialization with random weights, but maintained signs; ii) the distribution is highly non-i.i.d after the early phase of network training. This is why permuting weight dramatically harms performance. iii) the changes in the supervised networks are label-agnostic. The author claims these results can play an important role in explaining the network changes in the initial critical period.

Strength Bullets:

1. The paper performs exhaustive experiments in the early phase of network training. And it has some interesting implications for lottery tickets. i.e. to some extent, then signs from the rewinding iteration are sufficient to recover the damage caused by permutation.
2. I am very like the analysis of whether weight distributions are i.i.d.. The results in the paper are aligned with my intuition that the weights in the early stage are highly dependent and they share some similarities in the distribution level. And weight in different training stages supposes in a different distribution. Networks aren't robust to these permutations. They also show that the perturbations can be roughly be approximated by adding Gaussian noise to network weights.s
3. The author offers detailed results to analysis the data-dependence of the early phase of network training. The experiment organization is complete and convincing.

Weakness Bullets:

1. Although the paper gives extensive measurements and observations about the early phase about network training, it doesn't provide useful and efficient guidance for the lottery tickets hypothesis. In other words, the observation is interesting but the novelty is arguable. How can we use these ""implications"" to find better tickets? I mean it should be efficient. It is not reasonable cost more the find the best weight magnitude point or sign point for a better initialization.
2. [Minor] The legend in figure 9 is missing. I guess it may be the same as the upper left. But it is still confusing and hard to read.

Recommendation:

I think this paper provides plenty of insightful observations. I still hope the author can solve the above weakness bullets. This is a weak accept.","[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Accuracy of IMP when rewinding to various iterations of the early phase for ResNet-20 sub-networks as a function of sparsity level.
Table 2: Figure 2 :2Figure 2: Rough timeline of the early phase of training for ResNet-20 on CIFAR-10.
Table 3: Figure 3 :Figure 4 :34Figure 3: Basic telemetry about the state of ResNet-20 during the first 4000 iterations (10 epochs). Top row: evaluation accuracy/loss; average weight magnitude; percentage of weights that change sign from initialization; the values of ten randomly-selected weights. Bottom row: gradient magnitude; L2 distance of weights from their initial values and final values at the end of training; cosine similarity of weights from their initial values and final values at the end of training.
Table 4: Figure 5 :5Figure 5: Performance of an IMP-derived ResNet-20 sub-network on CIFAR-10 initialized with the weights at iteration k permuted within various structural elements. Left: k = 500. Right: k = 2000.
Table 5: Figure 6 :6Figure 6: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Left: k = 500. Right: k = 2000.
Table 6: Figure 7 :7Figure 7: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k and Gaussian noise of nσ, where σ is the standard deviation of the initialization distribution for each layer. Left: k = 500. Right: k = 2000.
Table 7: Figure 8 :8Figure 8: The effective standard deviation of various perturbations as a function of mean evaluation accuracy (across 5 seeds) at sparsity 26.2%. The mean of each perturbation was approximately 0. Left: k = 500, r = −0.672, p = 0.008; Right: k = 2000, r = −0.726, p = 0.003.
Table 8: Figure 9 :9Figure 9: The effect of pre-training ResNet-20 on CIFAR-10 with random labels, self-supervised rotation, 4x blurring, and 4x blurring and self-supervised rotation.
Table 9: Figure 10 :10Figure 10: The effect of pretraining sparse sub-networks of Resnet-20 (rewound to iteration 500) with 40 epochs of self-supervised rotation before training on CIFAR-10.
Table 10: Figure A2 :A2Figure A2: The effect of IMP rewinding iteration on the accuracy of sub-networks at various levels of sparsity. Accompanies Figure 1.
Table 11: Figure A3 :Figure A4 :Figure A5 :A3A4A5Figure A3: Basic telemetry about the state of all networks in Table A1 during the first 4000 iterations of training. Accompanies Figure 3.
Table 12: Figure A6 :Figure A7 :A6A7Figure A6: The effect of training an IMP-derived sub-network initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Accompanies Figure 6.
Table 13: Figure A8 :A8Figure A8: The effective standard deviation of each of the perturbations studied in Section 5 as a function of mean evaluation accuracy (across five seeds). Accompanies Figure 8.
Table 14: Figure A9 :Figure A10 :Figure A11 :Figure A12 :A9A10A11A12Figure A9: The effect of pre-training CIFAR-10 with random labels. Accompanies Figure 9.

[INTRODUCTION]
Over the past decade, methods for successfully training big, deep neural networks have revolutionized machine learning. Yet surprisingly, the underlying reasons for the success of these approaches remain poorly understood, despite remarkable empirical performance (Santurkar et al., 2018;Zhang et al., 2017). A large body of work has focused on understanding what happens during the later stages of training (Neyshabur et al., 2019;Yaida, 2019;Chaudhuri & Soatto, 2017;Wei & Schwab, 2019), while the initial phase has been less explored. However, a number of distinct observations indicate that significant and consequential changes are occurring during the most early stage of training. These include the presence of critical periods during training , the dramatic reshaping of the local loss landscape (Sagun et al., 2017;Gur-Ari et al., 2018), and the necessity of rewinding in the context of the lottery ticket hypothesis . Here we perform a thorough investigation of the state of the network in this early stage.
To provide a unified framework for understanding the changes the network undergoes during the early phase, we employ the methodology of iterative magnitude pruning with rewinding (IMP), as detailed below, throughout the bulk of this work . The initial lottery ticket hypothesis, which was validated on comparatively small networks, proposed that small, sparse sub-networks found via pruning of converged larger models could be trained to high performance provided they were initialized with the same values used in the training of the unpruned model . However, follow-up work found that rewinding the weights to their values at some iteration early in the training of the unpruned model, rather than to their initial values, was necessary to achieve good performance on deeper networks such as ResNets . This observation suggests that the changes in the network during this initial phase are vital for the success of the training of small, sparse sub-networks. As a result, this paradigm provides a simple and quantitative scheme for measuring the importance of the weights at various points early in training within an actionable and causal framework.
We make the following contributions, all evaluated across three different network architectures:
1. We provide an in-depth overview of various statistics summarizing learning over the early part of training.
2. We evaluate the impact of perturbing the state of the network in various ways during the early phase of training, finding that:
(i) counter to observations in smaller networks , deeper networks are not robust to reinitializion with random weights, but maintained signs (ii) the distribution of weights after the early phase of training is already highly non-i.i.d., as permuting them dramatically harms performance, even when signs are maintained (iii) both of the above perturbations can roughly be approximated by simply adding noise to the network weights, though this effect is stronger for (ii) than (i)
3. We measure the data-dependence of the early phase of training, finding that pre-training using only p(x) can approximate the changes that occur in the early phase of training, though pre-training must last for far longer (∼32× longer) and not be fed misleading labels.  Lottery ticket rewinding: The original lottery ticket paper  rewound weights to initialization, i.e., k = 0, during IMP. Follow up work on larger models demonstrated that it is necessary to rewind to a later point during training for IMP to succeed, i.e., k << T , where T is total training iterations . Notably, the benefit of rewinding to a later point in training saturates quickly, roughly between 500 and 2000 iterations for ResNet-20 on CIFAR-10 (Figure 1). This timescale is strikingly similar to the changes in the Hessian described below.
Hessian eigenspectrum: The shape of the loss landscape around the network state also appears to change rapidly during the early phase of training (Sagun et al., 2017;Gur-Ari et al., 2018). At initialization, the Hessian of the loss contains a number of large positive and negative eigenvalues. However, very rapidly the curvature is reshaped in a few marked ways: a few large eigenvalues emerge, the bulk eigenvalues are close to zero, and the negative eigenvalues become very small. Moreover, once the Hessian spectrum has reshaped, gradient descent appears to occur largely within the top subspace of the Hessian (Gur-Ari et al., 2018). These results have been largely confirmed in large scale studies (Ghorbani et al., 2019), but note they depend to some extent on architecture and (absence of) batch normalization (Ioffe & Szegedy, 2015). A notable exception to this consistency is the presence of substantial L 1 energy of negative eigenvalues for models trained on ImageNet.
Critical periods in deep learning:  found that perturbing the training process by providing corrupted data early on in training can result in irrevocable damage to the final performance of the network. Note that the timescales over which the authors find a critical period extend well beyond those we study here. However, architecture, learning rate schedule, and regularization all modify the timing of the critical period, and follow-up work found that critical periods were also present for regularization, in particular weight decay and data augmentation (Golatkar et al., 2019).

[PRELIMINARIES AND METHODOLOGY]
Networks: Throughout this paper, we study five standard convolutional neural networks for CIFAR-10. These include the ResNet-20 and ResNet-56 architectures designed for CIFAR-10 (He et al., 2015), the ResNet-18 architecture designed for ImageNet but commonly used on CIFAR-10 ( He et al., 2015), the WRN-16-8 wide residual network (Zagoruyko & Komodakis, 2016)  VGG-13 network (Simonyan & Zisserman (2015) as adapted by ). Throughout the main body of the paper, we show ResNet-20; in Appendix B, we present the same experiments for the other networks. Unless otherwise stated, results were qualitatively similar across all three networks. All experiments in this paper display the mean and standard deviation across five replicates with different random seeds. See Appendix A for further model details.
Iterative magnitude pruning with rewinding: In order to test the effect of various hypotheses about the state of sparse networks early in training, we use the Iterative Magnitude Pruning with rewinding (IMP) procedure of  to extract sub-networks from various points in training that could have learned on their own. The procedure involves training a network to completion, pruning the 20% of weights with the lowest magnitudes globally throughout the network, and rewinding the remaining weights to their values from an earlier iteration k during the initial, pre-pruning training run. This process is iterated to produce networks with high sparsity levels. As demonstrated in , IMP with rewinding leads to sparse sub-networks which can train to high performance even at high sparsity levels > 90%.
Figure 1 shows the results of the IMP with rewinding procedure, showing the accuracy of ResNet-20 at increasing sparsity when performing this procedure for several rewinding values of k. For k ≥ 500, sub-networks can match the performance of the original network with 16.8% of weights remaining. For k > 2000, essentially no further improvement is observed (not shown).

[THE STATE OF THE NETWORK EARLY IN TRAINING]
Many of the aforementioned papers refer to various points in the ""early"" part of training. In this section, we descriptively chart the state of ResNet-20 during the earliest phase of training to provide context for this related work and our subsequent experiments. We specifically focus on the first 4,000 iterations (10 epochs). See Figure A3 for the characterization of additional networks. We include a summary of these results for ResNet-20 as a timeline in Figure 2, and include a broader timeline including results from several previous papers for ResNet-18 in Figure A1.
As shown in Figure 3, during the earliest ten iterations, the network undergoes substantial change. It experiences large gradients that correspond to a rapid increase in distance from the initialization and a large number of sign changes of the weights. After these initial iterations, gradient magnitudes drop and the rate of change in each of the aforementioned quantities gradually slows through the remainder of the period we observe. Interestingly, gradient magnitudes reach a minimum after the first 200 iterations and subsequently increase to a stable level by iteration 500. Evaluation accuracy, improves rapidly, reaching 55% by the end of the first epoch (400 iterations), more than halfway to the final 91.5%. By 2000 iterations, accuracy approaches 80%.
During the first 4000 iterations of training, we observe three sub-phases. In the first phase, lasting only the initial few iterations, gradient magnitudes are very large and, consequently, the network changes rapidly. In the second phase, lasting about 500 iterations, performance quickly improves, weight magnitudes quickly increase, sign differences from initialization quickly increase, and gradient magnitudes reach a minimum before settling at a stable level. Finally, in the third phase, all of these quantities continue to change in the same direction, but begin to decelerate.  

[PERTURBING NEURAL NETWORKS EARLY IN TRAINING]
Figure 1 shows that the changes in the network weights over the first 500 iterations of training are essential to enable high performance at high sparsity levels. What features of this weight transformation are necessary to recover increased performance? Can they be summarized by maintaining the weight signs, but discarding their magnitudes as implied by ? Can they be represented distributionally? In this section, we evaluate these questions by perturbing the early state of the network in various ways. Concretely, we either add noise or shuffle the weights of IMP sub-networks of ResNet-20 across different network sub-compenents and examine the effect on the network's ability to learn thereafter. The sub-networks derived by IMP with rewinding make it possible to understand the causal impact of perturbations on sub-networks that are as capable as the full networks but more visibly decline in performance when improperly configured. To enable comparisons between the experiments in Section 5 and provide a common frame of reference, we measure the effective standard deviation of each perturbation, i.e. stddev(w perturb − w orig ).  show that, for a set of small convolutional networks, signs alone are sufficient to capture the state of lottery ticket sub-networks. However, it is unclear whether signs are still sufficient for larger networks early in training. In Figure 4, we investigate the impact of combining the magnitudes of the weights from one time-point with the signs from another. We found that the signs at iteration 500 paired with the magnitudes from initialization (red line) or from a separate random initialization (green line) were insufficient to maintain the performance reached by using both signs and magnitudes from iteration 500 (orange line), and performance drops to that of using  both magnitudes and signs from initialization (blue line). However, while using the magnitudes from iteration 500 and the signs from initialization, performance is still substantially better than initialization signs and magnitudes. In addition, the overall perturbation to the network by using the magnitudes at iteration 500 and signs from initialization (mean: 0.0, stddev: 0.033) is smaller than by using the signs at iteration 500 and the magnitudes from initialization (0.0 ± 0.042, mean ± std). These results suggest that the change in weight magnitudes over the first 500 iterations of training are substantially more important than the change in the signs for enabling subsequent training.

[ARE SIGNS ALL YOU NEED?]
By iteration 2000, however, pairing the iteration 2000 signs with magnitudes from initialization (red line) reaches similar performance to using the signs from initialization and the magnitudes from iteration 2000 (purple line) though not as high performance as using both from iteration 2000. This result suggests that network signs undergo important changes between iterations 500 and 2000, as only 9% of signs change during this period. Our results also suggest that counter to the observations of  in shallow networks, signs are not sufficient in deeper networks.

[ARE WEIGHT DISTRIBUTIONS I.I.D.?]
Can the changes in weights over the first k iterations be approximated distributionally? To measure this, we permuted the weights at iteration k within various structural sub-components of the network (globally, within layers, and within convolutional filters). If networks are robust to these permutations, it would suggest that the weights in such sub-compenents might be approximated and sampled from. As Figure 5 shows, however, we found that performance was not robust to shuffling weights globally (green line) or within layers (red line), and drops substantially to no better than that of the original initialization (blue line) at both 500 and 2000 iterations. 1 Shuffling within filters (purple line) performs slightly better, but results in a smaller overall perturbation (0.0 ± 0.092 for k = 500) than shuffling layerwise (0.0±0.143) or globally (0.0±0.144), suggesting that this change in perturbation strength may simply account for the difference. Are the signs from the rewinding iteration, k, sufficient to recover the damage caused by permutation? In Figure 6, we also consider shuffling only amongst weights that have the same sign. Doing so substantially improves the performance of the filter-wise shuffle; however, it also reduces the extent of the overall perturbation (0.0 ± 0.049 for k = 500). It also improves the performance of shuffling within layers slightly for k = 500 and substantially for k = 2000. We attribute the behavior for k = 2000 to the signs just as in Figure 4: when the magnitudes are similar in value (Figure 4 red line) or distribution (Figure 6 red and green lines), using the signs improves performance. Reverting back to the initial signs while shuffling magnitudes within layers (brown line), however, damages the network too severely (0.0 ± 0.087 for k = 500) to yield any performance improvement over random noise. These results suggest that, while the signs from initialization are not sufficient for high performance at high sparsity as shown in Section 5.1, the signs from the rewinding iteration are sufficient to recover the damage caused by permutation, at least to some extent.

[IS IT ALL JUST NOISE?]
Some of our previous results suggested that the impact of signs and permutations may simply reduce to adding noise to the weights. To evaluate this hypothesis, we next study the effect of simply adding Gaussian noise to the network weights at iteration k. To add noise appropriately for layers with different scales, the standard deviation of the noise added for each layer was normalized to a multiple of the standard deviation σ of the initialization distribution for that layer. In Figure 7, we see that for iteration k = 500, sub-networks can tolerate 0.5σ to 1σ of noise before performance degrades back to that of the original initialization at higher levels of noise. For iteration k = 2000, networks are surprisingly robust to noise up to 1σ, and even 2σ exhibits nontrivial performance.
In Figure 8, we plot the performance of each network at a fixed sparsity level as a function of the effective standard deviation of the noise imposed by each of the aforementioned perturbations. We find that the standard deviation of the effective noise explained fairly well the resultant performance (k = 500: r = −0.672, p = 0.008; k = 2000: r = −0.726, p = 0.003). As expected, perturbations that preserved the performance of the network generally resulted in smaller changes to the state of the network at iteration k. Interestingly, experiments that mixed signs and magnitudes from different points in training (green points) aligned least well with this pattern: the standard deviation of the perturbation is roughly similar among all of these experiments, but the accuracy of the resulting networks changes substantially. This result suggests that although the standard deviation of the noise is certainly indicative of lower accuracy, there are still specific perturbations that, while small in overall magnitude, can have a large effect on the network's ability to learn, suggesting that the observed perturbation effects are not, in fact, just a consequence of noise.

[THE DATA-DEPENDENCE OF NEURAL NETWORKS EARLY IN TRAINING]
Section 5 suggests that the change in network behavior by iteration k is not due to easilyascertainable, distributional properties of the network weights and signs. Rather, it appears that training is required to reach these network states. It is unclear, however, the extent to which various aspects of the data distribution are necessary. Mainly, is the change in weights during the early phase of training dependent on p(x) or p(y|x)? Here, we attempt to answer this question by mea-  suring the extent to which we can re-create a favorable network state for sub-network training using restricted information from the training data and labels. In particular, we consider pre-training the network with techniques that ignore labels entirely (self-supervised rotation prediction, Section 6.2), provide misleading labels (training with random labels, Section 6.1), or eliminate information from examples (blurring training examples Section 6.3).
We first train a randomly-initialized, unpruned network on CIFAR-10 on the pre-training task for a set number of epochs. After pre-training, we train the network normally as if the pre-trained state were the original initialization. We then use the state of the network at the end of the pre-training phase as the ""initialization"" to find masks for IMP. Finally, we examine the performance of the IMPpruned sub-networks as initialized using the state after pre-training. This experiment determines the extent to which pre-training places the network in a state suitable for sub-network training as compared to using the state of the network at iteration k of training on the original task.

[RANDOM LABELS]
To evaluate whether this phase of training is dependent on underlying structure in the data, we drew inspiration from Zhang et al. (2017) and pre-trained networks on data with randomized labels. This experiment tests whether the input distribution of the training data is sufficient to put the network in a position from which IMP with rewinding can find a sparse, trainable sub-network despite the presence of incorrect (not just missing) labels. Figure 9 (upper left) shows that pre-training on random labels for up to 10 epochs provides no improvement above rewinding to iteration 0 and that pre-training for longer begins to hurt accuracy. This result suggests that, though it is still possible that labels may not be required for learning, the presence incorrect labels is sufficient to prevent learning which approximates the early phase of training.

[SELF-SUPERVISED ROTATION PREDICTION]
What if we remove labels entirely? Is p(x) sufficient to approximate the early phase of training? Historically, neural network training often involved two steps: a self-supervised pre-training phase followed by a supervised phase on the target task (Erhan et al., 2010). Here, we consider one such self-supervised technique: rotation prediction (Gidaris et al., 2018). During the pre-training phase, the network is presented with a training image that has randomly been rotated 90n degrees (where n ∈ {0, 1, 2, 3}). The network must classify examples by the value of n. If self-supervised pretraining can approximate the early phase of training, it would suggest that p(x) is sufficient on its own. Indeed, as shown in Figure 9 (upper right), this pre-training regime leads to well-trainable subnetworks, though networks must be trained for many more epochs compared to supervised training (40 compared to 1.25, or a factor of 32×). This result suggests that the labels for the ultimate task themselves are not necessary to put the network in such a state (although explicitly misleading labels are detrimental). We emphasize that the duration of the pre-training phase required is an order of magnitude larger than the original rewinding iteration, however, suggesting that labels add important information which accelerates the learning process.

[BLURRING TRAINING EXAMPLES]
To probe the importance of p(x) for the early phase of training, we study the extent to which the training input distribution is necessary. Namely, we pretrain using blurred training inputs with the correct labels. Following , we blur training inputs by downsampling by 4x and then upsampling back to the full size. Figure 9 (bottom left) shows that this pre-training method succeeds: after 40 epochs of pre-training, IMP with rewinding can find sub-networks that are similar in performance to those found after training on the original task for 500 iterations (1.25 epochs).
Due to the success of the the rotation and blurring pre-training tasks, we explored the effect of combining these pre-training techniques. Doing so tests the extent to which we can discard both the training labels and some information from the training inputs. Figure 9 (bottom right) shows that doing so provides the network too little information: no amount of pre-training we considered makes it possible for IMP with rewinding to find sub-networks that perform tangibly better than rewinding to iteration 0. Interestingly however, as shown in Appendix B, trainable sub-networks are found for VGG-13 with this pre-training regime, suggesting that different network architectures have different sensitivities to the deprivation of labels and input content.

[SPARSE PRETRAINING]
Since sparse sub-networks are often challenging to train from scratch without the proper initialization (Han et al., 2015;, does pre-training make it easier for sparse neural networks to learn? Doing so would serve as a rough form of curriculum learning (Bengio et al., 2009) for sparse neural networks. We experimented with training sparse sub-networks of ResNet-20 (IMP sub-networks, randomly reinitialized sub-networks, and randomly pruned subnetworks) first on self-supervised rotation and then on the main task, but found no benefit beyond rewinding to iteration 0 (Figure 10). Moreover, doing so when starting from a sub-network rewound to iteration 500 actually hurts final accuracy. This result suggests that while pre-training is sufficient to approximate the early phase of supervised training with an appropriately structured mask, it is not sufficient to do so with an inappropriate mask. 

[DISCUSSION]
In this paper, we first performed extensive measurements of various statistics summarizing learning over the early part of training. Notably, we uncovered 3 sub-phases: in the very first iterations, gradient magnitudes are anomalously large and motion is rapid. Subsequently, gradients overshoot to smaller magnitudes before leveling off while performance increases rapidly. Then, learning slowly begins to decelerate. We then studied a suite of perturbations to the network state in the early phase finding that, counter to observations in smaller networks , deeper networks are not robust to reinitializing with random weights with maintained signs. We also found that the weight distribution after the early phase of training is highly non-independent. Finally, we measured the data-dependence of the early phase with the surprising result that pre-training on a self-supervised task yields equivalent performance to late rewinding with IMP.
These results have significant implications for the lottery ticket hypothesis. The seeming necessity of late rewinding calls into question certain interpretations of lottery tickets as well as the ability to identify sub-networks at initialization. Our observation that weights are highly non-independent at the rewinding point suggests that the weights at this point cannot be easily approximated, making approaches which attempt to ""jump"" directly to the rewinding point unlikely to succeed. However, our result that labels are not necessary to approximate the rewinding point suggests that the learning during this phase does not require task-specific information, suggesting that rewinding may not be necessary if networks are pre-trained appropriately.","[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning."
The Early Phase of Neural Network Training,Hkl1iRNFwS.json,"This paper aims at exploring the properties of neural network training during the early phase. By some studies on the lottery ticket hypothesis, something important happens during the early phase of training so rewinding the network should go to these early phases instead of the initial phase. So, what is important during training? The paper explores this problem from four aspects through empirical studies: 
1. By showing the various statistics through different training iterations, especially the gradient magnitude, the training phase is divided into three parts, and each part has different behaviors.
2. The paper explores what is more important for the early phase of training: signs of the weights or magnitude of the weights.
3. The paper explores what is more important for the early phase of training if we redistribute the weights, signs or magnitudes.
4. The paper explores how training depends on data. Giving weak information such as self-supervised information may work but giving wrong information such as random labels will hurt the performance.

This paper studies the properties of deep neural networks. Through a series of carefully designed experiments, the paper shows what is important for the weights (magnitude or signs), and what is important for the data (weak information or random information). It enables a deep understanding of neural networks and may motivate new neural network compression methods to be proposed. Generally, the paper is well-written, although some parts can be improved. I would vote for acceptance of the paper.

Some questions/suggestions to make the paper clearer:
1. It is better to have a table summarizing various results of the paper, to give readers an overall impression after going through so many detailed experimental results.
2. The results of Fig. 4 and Fig.6 can be inconsistent. In Figure 4, it says that signs are less important than magnitudes. In Figure 6, it says that signs are more important than magnitudes if shuffling filters and keep signs. Any explanation on the inconsistency?
3. There is no explanation of the “weight trace” in Figure 3.
4. It is not clear what is the difference between “Init” and “Final” in “L2 Dist” and “Cosine Similarity” in Figure 3. 

Finally,  it is said in the “call for papers”, “Reviewers will be instructed to apply a higher standard to papers in excess of 8 pages”. This paper is nearly nine pages, and higher standards should be applied. 

--------------------------------------
I am satisfied with the rebuttal. Since the paper is now within the 8 pages limit, I would not apply a high standard and increase my score. 
","[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Accuracy of IMP when rewinding to various iterations of the early phase for ResNet-20 sub-networks as a function of sparsity level.
Table 2: Figure 2 :2Figure 2: Rough timeline of the early phase of training for ResNet-20 on CIFAR-10.
Table 3: Figure 3 :Figure 4 :34Figure 3: Basic telemetry about the state of ResNet-20 during the first 4000 iterations (10 epochs). Top row: evaluation accuracy/loss; average weight magnitude; percentage of weights that change sign from initialization; the values of ten randomly-selected weights. Bottom row: gradient magnitude; L2 distance of weights from their initial values and final values at the end of training; cosine similarity of weights from their initial values and final values at the end of training.
Table 4: Figure 5 :5Figure 5: Performance of an IMP-derived ResNet-20 sub-network on CIFAR-10 initialized with the weights at iteration k permuted within various structural elements. Left: k = 500. Right: k = 2000.
Table 5: Figure 6 :6Figure 6: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Left: k = 500. Right: k = 2000.
Table 6: Figure 7 :7Figure 7: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k and Gaussian noise of nσ, where σ is the standard deviation of the initialization distribution for each layer. Left: k = 500. Right: k = 2000.
Table 7: Figure 8 :8Figure 8: The effective standard deviation of various perturbations as a function of mean evaluation accuracy (across 5 seeds) at sparsity 26.2%. The mean of each perturbation was approximately 0. Left: k = 500, r = −0.672, p = 0.008; Right: k = 2000, r = −0.726, p = 0.003.
Table 8: Figure 9 :9Figure 9: The effect of pre-training ResNet-20 on CIFAR-10 with random labels, self-supervised rotation, 4x blurring, and 4x blurring and self-supervised rotation.
Table 9: Figure 10 :10Figure 10: The effect of pretraining sparse sub-networks of Resnet-20 (rewound to iteration 500) with 40 epochs of self-supervised rotation before training on CIFAR-10.
Table 10: Figure A2 :A2Figure A2: The effect of IMP rewinding iteration on the accuracy of sub-networks at various levels of sparsity. Accompanies Figure 1.
Table 11: Figure A3 :Figure A4 :Figure A5 :A3A4A5Figure A3: Basic telemetry about the state of all networks in Table A1 during the first 4000 iterations of training. Accompanies Figure 3.
Table 12: Figure A6 :Figure A7 :A6A7Figure A6: The effect of training an IMP-derived sub-network initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Accompanies Figure 6.
Table 13: Figure A8 :A8Figure A8: The effective standard deviation of each of the perturbations studied in Section 5 as a function of mean evaluation accuracy (across five seeds). Accompanies Figure 8.
Table 14: Figure A9 :Figure A10 :Figure A11 :Figure A12 :A9A10A11A12Figure A9: The effect of pre-training CIFAR-10 with random labels. Accompanies Figure 9.

[INTRODUCTION]
Over the past decade, methods for successfully training big, deep neural networks have revolutionized machine learning. Yet surprisingly, the underlying reasons for the success of these approaches remain poorly understood, despite remarkable empirical performance (Santurkar et al., 2018;Zhang et al., 2017). A large body of work has focused on understanding what happens during the later stages of training (Neyshabur et al., 2019;Yaida, 2019;Chaudhuri & Soatto, 2017;Wei & Schwab, 2019), while the initial phase has been less explored. However, a number of distinct observations indicate that significant and consequential changes are occurring during the most early stage of training. These include the presence of critical periods during training , the dramatic reshaping of the local loss landscape (Sagun et al., 2017;Gur-Ari et al., 2018), and the necessity of rewinding in the context of the lottery ticket hypothesis . Here we perform a thorough investigation of the state of the network in this early stage.
To provide a unified framework for understanding the changes the network undergoes during the early phase, we employ the methodology of iterative magnitude pruning with rewinding (IMP), as detailed below, throughout the bulk of this work . The initial lottery ticket hypothesis, which was validated on comparatively small networks, proposed that small, sparse sub-networks found via pruning of converged larger models could be trained to high performance provided they were initialized with the same values used in the training of the unpruned model . However, follow-up work found that rewinding the weights to their values at some iteration early in the training of the unpruned model, rather than to their initial values, was necessary to achieve good performance on deeper networks such as ResNets . This observation suggests that the changes in the network during this initial phase are vital for the success of the training of small, sparse sub-networks. As a result, this paradigm provides a simple and quantitative scheme for measuring the importance of the weights at various points early in training within an actionable and causal framework.
We make the following contributions, all evaluated across three different network architectures:
1. We provide an in-depth overview of various statistics summarizing learning over the early part of training.
2. We evaluate the impact of perturbing the state of the network in various ways during the early phase of training, finding that:
(i) counter to observations in smaller networks , deeper networks are not robust to reinitializion with random weights, but maintained signs (ii) the distribution of weights after the early phase of training is already highly non-i.i.d., as permuting them dramatically harms performance, even when signs are maintained (iii) both of the above perturbations can roughly be approximated by simply adding noise to the network weights, though this effect is stronger for (ii) than (i)
3. We measure the data-dependence of the early phase of training, finding that pre-training using only p(x) can approximate the changes that occur in the early phase of training, though pre-training must last for far longer (∼32× longer) and not be fed misleading labels.  Lottery ticket rewinding: The original lottery ticket paper  rewound weights to initialization, i.e., k = 0, during IMP. Follow up work on larger models demonstrated that it is necessary to rewind to a later point during training for IMP to succeed, i.e., k << T , where T is total training iterations . Notably, the benefit of rewinding to a later point in training saturates quickly, roughly between 500 and 2000 iterations for ResNet-20 on CIFAR-10 (Figure 1). This timescale is strikingly similar to the changes in the Hessian described below.
Hessian eigenspectrum: The shape of the loss landscape around the network state also appears to change rapidly during the early phase of training (Sagun et al., 2017;Gur-Ari et al., 2018). At initialization, the Hessian of the loss contains a number of large positive and negative eigenvalues. However, very rapidly the curvature is reshaped in a few marked ways: a few large eigenvalues emerge, the bulk eigenvalues are close to zero, and the negative eigenvalues become very small. Moreover, once the Hessian spectrum has reshaped, gradient descent appears to occur largely within the top subspace of the Hessian (Gur-Ari et al., 2018). These results have been largely confirmed in large scale studies (Ghorbani et al., 2019), but note they depend to some extent on architecture and (absence of) batch normalization (Ioffe & Szegedy, 2015). A notable exception to this consistency is the presence of substantial L 1 energy of negative eigenvalues for models trained on ImageNet.
Critical periods in deep learning:  found that perturbing the training process by providing corrupted data early on in training can result in irrevocable damage to the final performance of the network. Note that the timescales over which the authors find a critical period extend well beyond those we study here. However, architecture, learning rate schedule, and regularization all modify the timing of the critical period, and follow-up work found that critical periods were also present for regularization, in particular weight decay and data augmentation (Golatkar et al., 2019).

[PRELIMINARIES AND METHODOLOGY]
Networks: Throughout this paper, we study five standard convolutional neural networks for CIFAR-10. These include the ResNet-20 and ResNet-56 architectures designed for CIFAR-10 (He et al., 2015), the ResNet-18 architecture designed for ImageNet but commonly used on CIFAR-10 ( He et al., 2015), the WRN-16-8 wide residual network (Zagoruyko & Komodakis, 2016)  VGG-13 network (Simonyan & Zisserman (2015) as adapted by ). Throughout the main body of the paper, we show ResNet-20; in Appendix B, we present the same experiments for the other networks. Unless otherwise stated, results were qualitatively similar across all three networks. All experiments in this paper display the mean and standard deviation across five replicates with different random seeds. See Appendix A for further model details.
Iterative magnitude pruning with rewinding: In order to test the effect of various hypotheses about the state of sparse networks early in training, we use the Iterative Magnitude Pruning with rewinding (IMP) procedure of  to extract sub-networks from various points in training that could have learned on their own. The procedure involves training a network to completion, pruning the 20% of weights with the lowest magnitudes globally throughout the network, and rewinding the remaining weights to their values from an earlier iteration k during the initial, pre-pruning training run. This process is iterated to produce networks with high sparsity levels. As demonstrated in , IMP with rewinding leads to sparse sub-networks which can train to high performance even at high sparsity levels > 90%.
Figure 1 shows the results of the IMP with rewinding procedure, showing the accuracy of ResNet-20 at increasing sparsity when performing this procedure for several rewinding values of k. For k ≥ 500, sub-networks can match the performance of the original network with 16.8% of weights remaining. For k > 2000, essentially no further improvement is observed (not shown).

[THE STATE OF THE NETWORK EARLY IN TRAINING]
Many of the aforementioned papers refer to various points in the ""early"" part of training. In this section, we descriptively chart the state of ResNet-20 during the earliest phase of training to provide context for this related work and our subsequent experiments. We specifically focus on the first 4,000 iterations (10 epochs). See Figure A3 for the characterization of additional networks. We include a summary of these results for ResNet-20 as a timeline in Figure 2, and include a broader timeline including results from several previous papers for ResNet-18 in Figure A1.
As shown in Figure 3, during the earliest ten iterations, the network undergoes substantial change. It experiences large gradients that correspond to a rapid increase in distance from the initialization and a large number of sign changes of the weights. After these initial iterations, gradient magnitudes drop and the rate of change in each of the aforementioned quantities gradually slows through the remainder of the period we observe. Interestingly, gradient magnitudes reach a minimum after the first 200 iterations and subsequently increase to a stable level by iteration 500. Evaluation accuracy, improves rapidly, reaching 55% by the end of the first epoch (400 iterations), more than halfway to the final 91.5%. By 2000 iterations, accuracy approaches 80%.
During the first 4000 iterations of training, we observe three sub-phases. In the first phase, lasting only the initial few iterations, gradient magnitudes are very large and, consequently, the network changes rapidly. In the second phase, lasting about 500 iterations, performance quickly improves, weight magnitudes quickly increase, sign differences from initialization quickly increase, and gradient magnitudes reach a minimum before settling at a stable level. Finally, in the third phase, all of these quantities continue to change in the same direction, but begin to decelerate.  

[PERTURBING NEURAL NETWORKS EARLY IN TRAINING]
Figure 1 shows that the changes in the network weights over the first 500 iterations of training are essential to enable high performance at high sparsity levels. What features of this weight transformation are necessary to recover increased performance? Can they be summarized by maintaining the weight signs, but discarding their magnitudes as implied by ? Can they be represented distributionally? In this section, we evaluate these questions by perturbing the early state of the network in various ways. Concretely, we either add noise or shuffle the weights of IMP sub-networks of ResNet-20 across different network sub-compenents and examine the effect on the network's ability to learn thereafter. The sub-networks derived by IMP with rewinding make it possible to understand the causal impact of perturbations on sub-networks that are as capable as the full networks but more visibly decline in performance when improperly configured. To enable comparisons between the experiments in Section 5 and provide a common frame of reference, we measure the effective standard deviation of each perturbation, i.e. stddev(w perturb − w orig ).  show that, for a set of small convolutional networks, signs alone are sufficient to capture the state of lottery ticket sub-networks. However, it is unclear whether signs are still sufficient for larger networks early in training. In Figure 4, we investigate the impact of combining the magnitudes of the weights from one time-point with the signs from another. We found that the signs at iteration 500 paired with the magnitudes from initialization (red line) or from a separate random initialization (green line) were insufficient to maintain the performance reached by using both signs and magnitudes from iteration 500 (orange line), and performance drops to that of using  both magnitudes and signs from initialization (blue line). However, while using the magnitudes from iteration 500 and the signs from initialization, performance is still substantially better than initialization signs and magnitudes. In addition, the overall perturbation to the network by using the magnitudes at iteration 500 and signs from initialization (mean: 0.0, stddev: 0.033) is smaller than by using the signs at iteration 500 and the magnitudes from initialization (0.0 ± 0.042, mean ± std). These results suggest that the change in weight magnitudes over the first 500 iterations of training are substantially more important than the change in the signs for enabling subsequent training.

[ARE SIGNS ALL YOU NEED?]
By iteration 2000, however, pairing the iteration 2000 signs with magnitudes from initialization (red line) reaches similar performance to using the signs from initialization and the magnitudes from iteration 2000 (purple line) though not as high performance as using both from iteration 2000. This result suggests that network signs undergo important changes between iterations 500 and 2000, as only 9% of signs change during this period. Our results also suggest that counter to the observations of  in shallow networks, signs are not sufficient in deeper networks.

[ARE WEIGHT DISTRIBUTIONS I.I.D.?]
Can the changes in weights over the first k iterations be approximated distributionally? To measure this, we permuted the weights at iteration k within various structural sub-components of the network (globally, within layers, and within convolutional filters). If networks are robust to these permutations, it would suggest that the weights in such sub-compenents might be approximated and sampled from. As Figure 5 shows, however, we found that performance was not robust to shuffling weights globally (green line) or within layers (red line), and drops substantially to no better than that of the original initialization (blue line) at both 500 and 2000 iterations. 1 Shuffling within filters (purple line) performs slightly better, but results in a smaller overall perturbation (0.0 ± 0.092 for k = 500) than shuffling layerwise (0.0±0.143) or globally (0.0±0.144), suggesting that this change in perturbation strength may simply account for the difference. Are the signs from the rewinding iteration, k, sufficient to recover the damage caused by permutation? In Figure 6, we also consider shuffling only amongst weights that have the same sign. Doing so substantially improves the performance of the filter-wise shuffle; however, it also reduces the extent of the overall perturbation (0.0 ± 0.049 for k = 500). It also improves the performance of shuffling within layers slightly for k = 500 and substantially for k = 2000. We attribute the behavior for k = 2000 to the signs just as in Figure 4: when the magnitudes are similar in value (Figure 4 red line) or distribution (Figure 6 red and green lines), using the signs improves performance. Reverting back to the initial signs while shuffling magnitudes within layers (brown line), however, damages the network too severely (0.0 ± 0.087 for k = 500) to yield any performance improvement over random noise. These results suggest that, while the signs from initialization are not sufficient for high performance at high sparsity as shown in Section 5.1, the signs from the rewinding iteration are sufficient to recover the damage caused by permutation, at least to some extent.

[IS IT ALL JUST NOISE?]
Some of our previous results suggested that the impact of signs and permutations may simply reduce to adding noise to the weights. To evaluate this hypothesis, we next study the effect of simply adding Gaussian noise to the network weights at iteration k. To add noise appropriately for layers with different scales, the standard deviation of the noise added for each layer was normalized to a multiple of the standard deviation σ of the initialization distribution for that layer. In Figure 7, we see that for iteration k = 500, sub-networks can tolerate 0.5σ to 1σ of noise before performance degrades back to that of the original initialization at higher levels of noise. For iteration k = 2000, networks are surprisingly robust to noise up to 1σ, and even 2σ exhibits nontrivial performance.
In Figure 8, we plot the performance of each network at a fixed sparsity level as a function of the effective standard deviation of the noise imposed by each of the aforementioned perturbations. We find that the standard deviation of the effective noise explained fairly well the resultant performance (k = 500: r = −0.672, p = 0.008; k = 2000: r = −0.726, p = 0.003). As expected, perturbations that preserved the performance of the network generally resulted in smaller changes to the state of the network at iteration k. Interestingly, experiments that mixed signs and magnitudes from different points in training (green points) aligned least well with this pattern: the standard deviation of the perturbation is roughly similar among all of these experiments, but the accuracy of the resulting networks changes substantially. This result suggests that although the standard deviation of the noise is certainly indicative of lower accuracy, there are still specific perturbations that, while small in overall magnitude, can have a large effect on the network's ability to learn, suggesting that the observed perturbation effects are not, in fact, just a consequence of noise.

[THE DATA-DEPENDENCE OF NEURAL NETWORKS EARLY IN TRAINING]
Section 5 suggests that the change in network behavior by iteration k is not due to easilyascertainable, distributional properties of the network weights and signs. Rather, it appears that training is required to reach these network states. It is unclear, however, the extent to which various aspects of the data distribution are necessary. Mainly, is the change in weights during the early phase of training dependent on p(x) or p(y|x)? Here, we attempt to answer this question by mea-  suring the extent to which we can re-create a favorable network state for sub-network training using restricted information from the training data and labels. In particular, we consider pre-training the network with techniques that ignore labels entirely (self-supervised rotation prediction, Section 6.2), provide misleading labels (training with random labels, Section 6.1), or eliminate information from examples (blurring training examples Section 6.3).
We first train a randomly-initialized, unpruned network on CIFAR-10 on the pre-training task for a set number of epochs. After pre-training, we train the network normally as if the pre-trained state were the original initialization. We then use the state of the network at the end of the pre-training phase as the ""initialization"" to find masks for IMP. Finally, we examine the performance of the IMPpruned sub-networks as initialized using the state after pre-training. This experiment determines the extent to which pre-training places the network in a state suitable for sub-network training as compared to using the state of the network at iteration k of training on the original task.

[RANDOM LABELS]
To evaluate whether this phase of training is dependent on underlying structure in the data, we drew inspiration from Zhang et al. (2017) and pre-trained networks on data with randomized labels. This experiment tests whether the input distribution of the training data is sufficient to put the network in a position from which IMP with rewinding can find a sparse, trainable sub-network despite the presence of incorrect (not just missing) labels. Figure 9 (upper left) shows that pre-training on random labels for up to 10 epochs provides no improvement above rewinding to iteration 0 and that pre-training for longer begins to hurt accuracy. This result suggests that, though it is still possible that labels may not be required for learning, the presence incorrect labels is sufficient to prevent learning which approximates the early phase of training.

[SELF-SUPERVISED ROTATION PREDICTION]
What if we remove labels entirely? Is p(x) sufficient to approximate the early phase of training? Historically, neural network training often involved two steps: a self-supervised pre-training phase followed by a supervised phase on the target task (Erhan et al., 2010). Here, we consider one such self-supervised technique: rotation prediction (Gidaris et al., 2018). During the pre-training phase, the network is presented with a training image that has randomly been rotated 90n degrees (where n ∈ {0, 1, 2, 3}). The network must classify examples by the value of n. If self-supervised pretraining can approximate the early phase of training, it would suggest that p(x) is sufficient on its own. Indeed, as shown in Figure 9 (upper right), this pre-training regime leads to well-trainable subnetworks, though networks must be trained for many more epochs compared to supervised training (40 compared to 1.25, or a factor of 32×). This result suggests that the labels for the ultimate task themselves are not necessary to put the network in such a state (although explicitly misleading labels are detrimental). We emphasize that the duration of the pre-training phase required is an order of magnitude larger than the original rewinding iteration, however, suggesting that labels add important information which accelerates the learning process.

[BLURRING TRAINING EXAMPLES]
To probe the importance of p(x) for the early phase of training, we study the extent to which the training input distribution is necessary. Namely, we pretrain using blurred training inputs with the correct labels. Following , we blur training inputs by downsampling by 4x and then upsampling back to the full size. Figure 9 (bottom left) shows that this pre-training method succeeds: after 40 epochs of pre-training, IMP with rewinding can find sub-networks that are similar in performance to those found after training on the original task for 500 iterations (1.25 epochs).
Due to the success of the the rotation and blurring pre-training tasks, we explored the effect of combining these pre-training techniques. Doing so tests the extent to which we can discard both the training labels and some information from the training inputs. Figure 9 (bottom right) shows that doing so provides the network too little information: no amount of pre-training we considered makes it possible for IMP with rewinding to find sub-networks that perform tangibly better than rewinding to iteration 0. Interestingly however, as shown in Appendix B, trainable sub-networks are found for VGG-13 with this pre-training regime, suggesting that different network architectures have different sensitivities to the deprivation of labels and input content.

[SPARSE PRETRAINING]
Since sparse sub-networks are often challenging to train from scratch without the proper initialization (Han et al., 2015;, does pre-training make it easier for sparse neural networks to learn? Doing so would serve as a rough form of curriculum learning (Bengio et al., 2009) for sparse neural networks. We experimented with training sparse sub-networks of ResNet-20 (IMP sub-networks, randomly reinitialized sub-networks, and randomly pruned subnetworks) first on self-supervised rotation and then on the main task, but found no benefit beyond rewinding to iteration 0 (Figure 10). Moreover, doing so when starting from a sub-network rewound to iteration 500 actually hurts final accuracy. This result suggests that while pre-training is sufficient to approximate the early phase of supervised training with an appropriately structured mask, it is not sufficient to do so with an inappropriate mask. 

[DISCUSSION]
In this paper, we first performed extensive measurements of various statistics summarizing learning over the early part of training. Notably, we uncovered 3 sub-phases: in the very first iterations, gradient magnitudes are anomalously large and motion is rapid. Subsequently, gradients overshoot to smaller magnitudes before leveling off while performance increases rapidly. Then, learning slowly begins to decelerate. We then studied a suite of perturbations to the network state in the early phase finding that, counter to observations in smaller networks , deeper networks are not robust to reinitializing with random weights with maintained signs. We also found that the weight distribution after the early phase of training is highly non-independent. Finally, we measured the data-dependence of the early phase with the surprising result that pre-training on a self-supervised task yields equivalent performance to late rewinding with IMP.
These results have significant implications for the lottery ticket hypothesis. The seeming necessity of late rewinding calls into question certain interpretations of lottery tickets as well as the ability to identify sub-networks at initialization. Our observation that weights are highly non-independent at the rewinding point suggests that the weights at this point cannot be easily approximated, making approaches which attempt to ""jump"" directly to the rewinding point unlikely to succeed. However, our result that labels are not necessary to approximate the rewinding point suggests that the learning during this phase does not require task-specific information, suggesting that rewinding may not be necessary if networks are pre-trained appropriately.","[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning."
The Early Phase of Neural Network Training,Hkl1iRNFwS.json,"Pros
+ This work provided a good summary of observations and network properties that worth studying during the early stage of network’s training.
+ The authors conducted extensive and detailed experiments to study the statistics of weights and their gradients. Ablation studies also considered network’s accuracy under perturbation of weight signs, weight shuffling, and different standard deviations.
+ The authors also verified the effectiveness of weak labels used in self-supervised learning.

Cons
- The work itself did not propose any new network properties or any new metric to measure. Most experiments are designed for previous observations and mostly for verification purpose. I am concern about the core motivation of this work, like to identify or solve any new problems, in addition to experimentally verify the observations during network’s training.
- The conclusion in this work is still very empirical: it remains uncertain whether in other vision tasks and with more complex networks (e.g. multi-branch network) these conclusions would hold.","[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Accuracy of IMP when rewinding to various iterations of the early phase for ResNet-20 sub-networks as a function of sparsity level.
Table 2: Figure 2 :2Figure 2: Rough timeline of the early phase of training for ResNet-20 on CIFAR-10.
Table 3: Figure 3 :Figure 4 :34Figure 3: Basic telemetry about the state of ResNet-20 during the first 4000 iterations (10 epochs). Top row: evaluation accuracy/loss; average weight magnitude; percentage of weights that change sign from initialization; the values of ten randomly-selected weights. Bottom row: gradient magnitude; L2 distance of weights from their initial values and final values at the end of training; cosine similarity of weights from their initial values and final values at the end of training.
Table 4: Figure 5 :5Figure 5: Performance of an IMP-derived ResNet-20 sub-network on CIFAR-10 initialized with the weights at iteration k permuted within various structural elements. Left: k = 500. Right: k = 2000.
Table 5: Figure 6 :6Figure 6: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Left: k = 500. Right: k = 2000.
Table 6: Figure 7 :7Figure 7: The effect of training an IMP-derived sub-network of ResNet-20 on CIFAR-10 initialized with the weights at iteration k and Gaussian noise of nσ, where σ is the standard deviation of the initialization distribution for each layer. Left: k = 500. Right: k = 2000.
Table 7: Figure 8 :8Figure 8: The effective standard deviation of various perturbations as a function of mean evaluation accuracy (across 5 seeds) at sparsity 26.2%. The mean of each perturbation was approximately 0. Left: k = 500, r = −0.672, p = 0.008; Right: k = 2000, r = −0.726, p = 0.003.
Table 8: Figure 9 :9Figure 9: The effect of pre-training ResNet-20 on CIFAR-10 with random labels, self-supervised rotation, 4x blurring, and 4x blurring and self-supervised rotation.
Table 9: Figure 10 :10Figure 10: The effect of pretraining sparse sub-networks of Resnet-20 (rewound to iteration 500) with 40 epochs of self-supervised rotation before training on CIFAR-10.
Table 10: Figure A2 :A2Figure A2: The effect of IMP rewinding iteration on the accuracy of sub-networks at various levels of sparsity. Accompanies Figure 1.
Table 11: Figure A3 :Figure A4 :Figure A5 :A3A4A5Figure A3: Basic telemetry about the state of all networks in Table A1 during the first 4000 iterations of training. Accompanies Figure 3.
Table 12: Figure A6 :Figure A7 :A6A7Figure A6: The effect of training an IMP-derived sub-network initialized with the weights at iteration k as shuffled within various structural elements where shuffling only occurs between weights with the same sign. Accompanies Figure 6.
Table 13: Figure A8 :A8Figure A8: The effective standard deviation of each of the perturbations studied in Section 5 as a function of mean evaluation accuracy (across five seeds). Accompanies Figure 8.
Table 14: Figure A9 :Figure A10 :Figure A11 :Figure A12 :A9A10A11A12Figure A9: The effect of pre-training CIFAR-10 with random labels. Accompanies Figure 9.

[INTRODUCTION]
Over the past decade, methods for successfully training big, deep neural networks have revolutionized machine learning. Yet surprisingly, the underlying reasons for the success of these approaches remain poorly understood, despite remarkable empirical performance (Santurkar et al., 2018;Zhang et al., 2017). A large body of work has focused on understanding what happens during the later stages of training (Neyshabur et al., 2019;Yaida, 2019;Chaudhuri & Soatto, 2017;Wei & Schwab, 2019), while the initial phase has been less explored. However, a number of distinct observations indicate that significant and consequential changes are occurring during the most early stage of training. These include the presence of critical periods during training , the dramatic reshaping of the local loss landscape (Sagun et al., 2017;Gur-Ari et al., 2018), and the necessity of rewinding in the context of the lottery ticket hypothesis . Here we perform a thorough investigation of the state of the network in this early stage.
To provide a unified framework for understanding the changes the network undergoes during the early phase, we employ the methodology of iterative magnitude pruning with rewinding (IMP), as detailed below, throughout the bulk of this work . The initial lottery ticket hypothesis, which was validated on comparatively small networks, proposed that small, sparse sub-networks found via pruning of converged larger models could be trained to high performance provided they were initialized with the same values used in the training of the unpruned model . However, follow-up work found that rewinding the weights to their values at some iteration early in the training of the unpruned model, rather than to their initial values, was necessary to achieve good performance on deeper networks such as ResNets . This observation suggests that the changes in the network during this initial phase are vital for the success of the training of small, sparse sub-networks. As a result, this paradigm provides a simple and quantitative scheme for measuring the importance of the weights at various points early in training within an actionable and causal framework.
We make the following contributions, all evaluated across three different network architectures:
1. We provide an in-depth overview of various statistics summarizing learning over the early part of training.
2. We evaluate the impact of perturbing the state of the network in various ways during the early phase of training, finding that:
(i) counter to observations in smaller networks , deeper networks are not robust to reinitializion with random weights, but maintained signs (ii) the distribution of weights after the early phase of training is already highly non-i.i.d., as permuting them dramatically harms performance, even when signs are maintained (iii) both of the above perturbations can roughly be approximated by simply adding noise to the network weights, though this effect is stronger for (ii) than (i)
3. We measure the data-dependence of the early phase of training, finding that pre-training using only p(x) can approximate the changes that occur in the early phase of training, though pre-training must last for far longer (∼32× longer) and not be fed misleading labels.  Lottery ticket rewinding: The original lottery ticket paper  rewound weights to initialization, i.e., k = 0, during IMP. Follow up work on larger models demonstrated that it is necessary to rewind to a later point during training for IMP to succeed, i.e., k << T , where T is total training iterations . Notably, the benefit of rewinding to a later point in training saturates quickly, roughly between 500 and 2000 iterations for ResNet-20 on CIFAR-10 (Figure 1). This timescale is strikingly similar to the changes in the Hessian described below.
Hessian eigenspectrum: The shape of the loss landscape around the network state also appears to change rapidly during the early phase of training (Sagun et al., 2017;Gur-Ari et al., 2018). At initialization, the Hessian of the loss contains a number of large positive and negative eigenvalues. However, very rapidly the curvature is reshaped in a few marked ways: a few large eigenvalues emerge, the bulk eigenvalues are close to zero, and the negative eigenvalues become very small. Moreover, once the Hessian spectrum has reshaped, gradient descent appears to occur largely within the top subspace of the Hessian (Gur-Ari et al., 2018). These results have been largely confirmed in large scale studies (Ghorbani et al., 2019), but note they depend to some extent on architecture and (absence of) batch normalization (Ioffe & Szegedy, 2015). A notable exception to this consistency is the presence of substantial L 1 energy of negative eigenvalues for models trained on ImageNet.
Critical periods in deep learning:  found that perturbing the training process by providing corrupted data early on in training can result in irrevocable damage to the final performance of the network. Note that the timescales over which the authors find a critical period extend well beyond those we study here. However, architecture, learning rate schedule, and regularization all modify the timing of the critical period, and follow-up work found that critical periods were also present for regularization, in particular weight decay and data augmentation (Golatkar et al., 2019).

[PRELIMINARIES AND METHODOLOGY]
Networks: Throughout this paper, we study five standard convolutional neural networks for CIFAR-10. These include the ResNet-20 and ResNet-56 architectures designed for CIFAR-10 (He et al., 2015), the ResNet-18 architecture designed for ImageNet but commonly used on CIFAR-10 ( He et al., 2015), the WRN-16-8 wide residual network (Zagoruyko & Komodakis, 2016)  VGG-13 network (Simonyan & Zisserman (2015) as adapted by ). Throughout the main body of the paper, we show ResNet-20; in Appendix B, we present the same experiments for the other networks. Unless otherwise stated, results were qualitatively similar across all three networks. All experiments in this paper display the mean and standard deviation across five replicates with different random seeds. See Appendix A for further model details.
Iterative magnitude pruning with rewinding: In order to test the effect of various hypotheses about the state of sparse networks early in training, we use the Iterative Magnitude Pruning with rewinding (IMP) procedure of  to extract sub-networks from various points in training that could have learned on their own. The procedure involves training a network to completion, pruning the 20% of weights with the lowest magnitudes globally throughout the network, and rewinding the remaining weights to their values from an earlier iteration k during the initial, pre-pruning training run. This process is iterated to produce networks with high sparsity levels. As demonstrated in , IMP with rewinding leads to sparse sub-networks which can train to high performance even at high sparsity levels > 90%.
Figure 1 shows the results of the IMP with rewinding procedure, showing the accuracy of ResNet-20 at increasing sparsity when performing this procedure for several rewinding values of k. For k ≥ 500, sub-networks can match the performance of the original network with 16.8% of weights remaining. For k > 2000, essentially no further improvement is observed (not shown).

[THE STATE OF THE NETWORK EARLY IN TRAINING]
Many of the aforementioned papers refer to various points in the ""early"" part of training. In this section, we descriptively chart the state of ResNet-20 during the earliest phase of training to provide context for this related work and our subsequent experiments. We specifically focus on the first 4,000 iterations (10 epochs). See Figure A3 for the characterization of additional networks. We include a summary of these results for ResNet-20 as a timeline in Figure 2, and include a broader timeline including results from several previous papers for ResNet-18 in Figure A1.
As shown in Figure 3, during the earliest ten iterations, the network undergoes substantial change. It experiences large gradients that correspond to a rapid increase in distance from the initialization and a large number of sign changes of the weights. After these initial iterations, gradient magnitudes drop and the rate of change in each of the aforementioned quantities gradually slows through the remainder of the period we observe. Interestingly, gradient magnitudes reach a minimum after the first 200 iterations and subsequently increase to a stable level by iteration 500. Evaluation accuracy, improves rapidly, reaching 55% by the end of the first epoch (400 iterations), more than halfway to the final 91.5%. By 2000 iterations, accuracy approaches 80%.
During the first 4000 iterations of training, we observe three sub-phases. In the first phase, lasting only the initial few iterations, gradient magnitudes are very large and, consequently, the network changes rapidly. In the second phase, lasting about 500 iterations, performance quickly improves, weight magnitudes quickly increase, sign differences from initialization quickly increase, and gradient magnitudes reach a minimum before settling at a stable level. Finally, in the third phase, all of these quantities continue to change in the same direction, but begin to decelerate.  

[PERTURBING NEURAL NETWORKS EARLY IN TRAINING]
Figure 1 shows that the changes in the network weights over the first 500 iterations of training are essential to enable high performance at high sparsity levels. What features of this weight transformation are necessary to recover increased performance? Can they be summarized by maintaining the weight signs, but discarding their magnitudes as implied by ? Can they be represented distributionally? In this section, we evaluate these questions by perturbing the early state of the network in various ways. Concretely, we either add noise or shuffle the weights of IMP sub-networks of ResNet-20 across different network sub-compenents and examine the effect on the network's ability to learn thereafter. The sub-networks derived by IMP with rewinding make it possible to understand the causal impact of perturbations on sub-networks that are as capable as the full networks but more visibly decline in performance when improperly configured. To enable comparisons between the experiments in Section 5 and provide a common frame of reference, we measure the effective standard deviation of each perturbation, i.e. stddev(w perturb − w orig ).  show that, for a set of small convolutional networks, signs alone are sufficient to capture the state of lottery ticket sub-networks. However, it is unclear whether signs are still sufficient for larger networks early in training. In Figure 4, we investigate the impact of combining the magnitudes of the weights from one time-point with the signs from another. We found that the signs at iteration 500 paired with the magnitudes from initialization (red line) or from a separate random initialization (green line) were insufficient to maintain the performance reached by using both signs and magnitudes from iteration 500 (orange line), and performance drops to that of using  both magnitudes and signs from initialization (blue line). However, while using the magnitudes from iteration 500 and the signs from initialization, performance is still substantially better than initialization signs and magnitudes. In addition, the overall perturbation to the network by using the magnitudes at iteration 500 and signs from initialization (mean: 0.0, stddev: 0.033) is smaller than by using the signs at iteration 500 and the magnitudes from initialization (0.0 ± 0.042, mean ± std). These results suggest that the change in weight magnitudes over the first 500 iterations of training are substantially more important than the change in the signs for enabling subsequent training.

[ARE SIGNS ALL YOU NEED?]
By iteration 2000, however, pairing the iteration 2000 signs with magnitudes from initialization (red line) reaches similar performance to using the signs from initialization and the magnitudes from iteration 2000 (purple line) though not as high performance as using both from iteration 2000. This result suggests that network signs undergo important changes between iterations 500 and 2000, as only 9% of signs change during this period. Our results also suggest that counter to the observations of  in shallow networks, signs are not sufficient in deeper networks.

[ARE WEIGHT DISTRIBUTIONS I.I.D.?]
Can the changes in weights over the first k iterations be approximated distributionally? To measure this, we permuted the weights at iteration k within various structural sub-components of the network (globally, within layers, and within convolutional filters). If networks are robust to these permutations, it would suggest that the weights in such sub-compenents might be approximated and sampled from. As Figure 5 shows, however, we found that performance was not robust to shuffling weights globally (green line) or within layers (red line), and drops substantially to no better than that of the original initialization (blue line) at both 500 and 2000 iterations. 1 Shuffling within filters (purple line) performs slightly better, but results in a smaller overall perturbation (0.0 ± 0.092 for k = 500) than shuffling layerwise (0.0±0.143) or globally (0.0±0.144), suggesting that this change in perturbation strength may simply account for the difference. Are the signs from the rewinding iteration, k, sufficient to recover the damage caused by permutation? In Figure 6, we also consider shuffling only amongst weights that have the same sign. Doing so substantially improves the performance of the filter-wise shuffle; however, it also reduces the extent of the overall perturbation (0.0 ± 0.049 for k = 500). It also improves the performance of shuffling within layers slightly for k = 500 and substantially for k = 2000. We attribute the behavior for k = 2000 to the signs just as in Figure 4: when the magnitudes are similar in value (Figure 4 red line) or distribution (Figure 6 red and green lines), using the signs improves performance. Reverting back to the initial signs while shuffling magnitudes within layers (brown line), however, damages the network too severely (0.0 ± 0.087 for k = 500) to yield any performance improvement over random noise. These results suggest that, while the signs from initialization are not sufficient for high performance at high sparsity as shown in Section 5.1, the signs from the rewinding iteration are sufficient to recover the damage caused by permutation, at least to some extent.

[IS IT ALL JUST NOISE?]
Some of our previous results suggested that the impact of signs and permutations may simply reduce to adding noise to the weights. To evaluate this hypothesis, we next study the effect of simply adding Gaussian noise to the network weights at iteration k. To add noise appropriately for layers with different scales, the standard deviation of the noise added for each layer was normalized to a multiple of the standard deviation σ of the initialization distribution for that layer. In Figure 7, we see that for iteration k = 500, sub-networks can tolerate 0.5σ to 1σ of noise before performance degrades back to that of the original initialization at higher levels of noise. For iteration k = 2000, networks are surprisingly robust to noise up to 1σ, and even 2σ exhibits nontrivial performance.
In Figure 8, we plot the performance of each network at a fixed sparsity level as a function of the effective standard deviation of the noise imposed by each of the aforementioned perturbations. We find that the standard deviation of the effective noise explained fairly well the resultant performance (k = 500: r = −0.672, p = 0.008; k = 2000: r = −0.726, p = 0.003). As expected, perturbations that preserved the performance of the network generally resulted in smaller changes to the state of the network at iteration k. Interestingly, experiments that mixed signs and magnitudes from different points in training (green points) aligned least well with this pattern: the standard deviation of the perturbation is roughly similar among all of these experiments, but the accuracy of the resulting networks changes substantially. This result suggests that although the standard deviation of the noise is certainly indicative of lower accuracy, there are still specific perturbations that, while small in overall magnitude, can have a large effect on the network's ability to learn, suggesting that the observed perturbation effects are not, in fact, just a consequence of noise.

[THE DATA-DEPENDENCE OF NEURAL NETWORKS EARLY IN TRAINING]
Section 5 suggests that the change in network behavior by iteration k is not due to easilyascertainable, distributional properties of the network weights and signs. Rather, it appears that training is required to reach these network states. It is unclear, however, the extent to which various aspects of the data distribution are necessary. Mainly, is the change in weights during the early phase of training dependent on p(x) or p(y|x)? Here, we attempt to answer this question by mea-  suring the extent to which we can re-create a favorable network state for sub-network training using restricted information from the training data and labels. In particular, we consider pre-training the network with techniques that ignore labels entirely (self-supervised rotation prediction, Section 6.2), provide misleading labels (training with random labels, Section 6.1), or eliminate information from examples (blurring training examples Section 6.3).
We first train a randomly-initialized, unpruned network on CIFAR-10 on the pre-training task for a set number of epochs. After pre-training, we train the network normally as if the pre-trained state were the original initialization. We then use the state of the network at the end of the pre-training phase as the ""initialization"" to find masks for IMP. Finally, we examine the performance of the IMPpruned sub-networks as initialized using the state after pre-training. This experiment determines the extent to which pre-training places the network in a state suitable for sub-network training as compared to using the state of the network at iteration k of training on the original task.

[RANDOM LABELS]
To evaluate whether this phase of training is dependent on underlying structure in the data, we drew inspiration from Zhang et al. (2017) and pre-trained networks on data with randomized labels. This experiment tests whether the input distribution of the training data is sufficient to put the network in a position from which IMP with rewinding can find a sparse, trainable sub-network despite the presence of incorrect (not just missing) labels. Figure 9 (upper left) shows that pre-training on random labels for up to 10 epochs provides no improvement above rewinding to iteration 0 and that pre-training for longer begins to hurt accuracy. This result suggests that, though it is still possible that labels may not be required for learning, the presence incorrect labels is sufficient to prevent learning which approximates the early phase of training.

[SELF-SUPERVISED ROTATION PREDICTION]
What if we remove labels entirely? Is p(x) sufficient to approximate the early phase of training? Historically, neural network training often involved two steps: a self-supervised pre-training phase followed by a supervised phase on the target task (Erhan et al., 2010). Here, we consider one such self-supervised technique: rotation prediction (Gidaris et al., 2018). During the pre-training phase, the network is presented with a training image that has randomly been rotated 90n degrees (where n ∈ {0, 1, 2, 3}). The network must classify examples by the value of n. If self-supervised pretraining can approximate the early phase of training, it would suggest that p(x) is sufficient on its own. Indeed, as shown in Figure 9 (upper right), this pre-training regime leads to well-trainable subnetworks, though networks must be trained for many more epochs compared to supervised training (40 compared to 1.25, or a factor of 32×). This result suggests that the labels for the ultimate task themselves are not necessary to put the network in such a state (although explicitly misleading labels are detrimental). We emphasize that the duration of the pre-training phase required is an order of magnitude larger than the original rewinding iteration, however, suggesting that labels add important information which accelerates the learning process.

[BLURRING TRAINING EXAMPLES]
To probe the importance of p(x) for the early phase of training, we study the extent to which the training input distribution is necessary. Namely, we pretrain using blurred training inputs with the correct labels. Following , we blur training inputs by downsampling by 4x and then upsampling back to the full size. Figure 9 (bottom left) shows that this pre-training method succeeds: after 40 epochs of pre-training, IMP with rewinding can find sub-networks that are similar in performance to those found after training on the original task for 500 iterations (1.25 epochs).
Due to the success of the the rotation and blurring pre-training tasks, we explored the effect of combining these pre-training techniques. Doing so tests the extent to which we can discard both the training labels and some information from the training inputs. Figure 9 (bottom right) shows that doing so provides the network too little information: no amount of pre-training we considered makes it possible for IMP with rewinding to find sub-networks that perform tangibly better than rewinding to iteration 0. Interestingly however, as shown in Appendix B, trainable sub-networks are found for VGG-13 with this pre-training regime, suggesting that different network architectures have different sensitivities to the deprivation of labels and input content.

[SPARSE PRETRAINING]
Since sparse sub-networks are often challenging to train from scratch without the proper initialization (Han et al., 2015;, does pre-training make it easier for sparse neural networks to learn? Doing so would serve as a rough form of curriculum learning (Bengio et al., 2009) for sparse neural networks. We experimented with training sparse sub-networks of ResNet-20 (IMP sub-networks, randomly reinitialized sub-networks, and randomly pruned subnetworks) first on self-supervised rotation and then on the main task, but found no benefit beyond rewinding to iteration 0 (Figure 10). Moreover, doing so when starting from a sub-network rewound to iteration 500 actually hurts final accuracy. This result suggests that while pre-training is sufficient to approximate the early phase of supervised training with an appropriately structured mask, it is not sufficient to do so with an inappropriate mask. 

[DISCUSSION]
In this paper, we first performed extensive measurements of various statistics summarizing learning over the early part of training. Notably, we uncovered 3 sub-phases: in the very first iterations, gradient magnitudes are anomalously large and motion is rapid. Subsequently, gradients overshoot to smaller magnitudes before leveling off while performance increases rapidly. Then, learning slowly begins to decelerate. We then studied a suite of perturbations to the network state in the early phase finding that, counter to observations in smaller networks , deeper networks are not robust to reinitializing with random weights with maintained signs. We also found that the weight distribution after the early phase of training is highly non-independent. Finally, we measured the data-dependence of the early phase with the surprising result that pre-training on a self-supervised task yields equivalent performance to late rewinding with IMP.
These results have significant implications for the lottery ticket hypothesis. The seeming necessity of late rewinding calls into question certain interpretations of lottery tickets as well as the ability to identify sub-networks at initialization. Our observation that weights are highly non-independent at the rewinding point suggests that the weights at this point cannot be easily approximated, making approaches which attempt to ""jump"" directly to the rewinding point unlikely to succeed. However, our result that labels are not necessary to approximate the rewinding point suggests that the learning during this phase does not require task-specific information, suggesting that rewinding may not be necessary if networks are pre-trained appropriately.","[TITLE]
THE EARLY PHASE OF NEURAL NETWORK TRAINING

[ABSTRACT]
Recent studies have shown that many important aspects of neural network learning take place within the very earliest iterations or epochs of training. For example, sparse, trainable sub-networks emerge , gradient descent moves into a small subspace (Gur-Ari et al., 2018), and the network undergoes a critical period . Here we examine the changes that deep neural networks undergo during this early phase of training. We perform extensive measurements of the network state during these early iterations of training and leverage the framework of  to quantitatively probe the weight distribution and its reliance on various aspects of the dataset. We find that, within this framework, deep networks are not robust to reinitializing with random weights while maintaining signs, and that weight distributions are highly non-independent even after only a few hundred iterations. Despite this behavior, pre-training with blurred inputs or an auxiliary self-supervised task can approximate the changes in supervised networks, suggesting that these changes are not inherently label-dependent, though labels significantly accelerate this process. Together, these results help to elucidate the network changes occurring during this pivotal initial period of learning."
Cyclic orthogonal convolutions for long-range integration of features,868DWd46dv2.json,"
The paper proposes cyclic orthogonal convolutions as a means to grow receptive fields fast in CNNs. The authors show a small improvement of their cyclic convolution model over a simple CNN baseline on CIFAR-10, ImageNet and Stylized ImageNet. Overall it's an interesting idea, but not executed very convincingly.

The biological motivation is weak at best. Long-range horizontal connections in cortex, which the authors use as motivation, are feature specific, i.e. between corresponding orientation domains. In contrast, in the authors' setup, they interact across all features. Moreover, the long-range connections are only along the x and y direction, but not in oblique directions. In my opinion, the author's proposal is not closer to biology than vision transformers, which also provide an all-to-all spatial interaction, albeit with a different mechanism and arguably stronger performance on large-scale datasets.

The experiments with simple CNNs are nice and show a trend in the right direction, but in order to show that the cyclic convolutions are also of practical use, more extensive and competitive results would be necessary. The authors argue that also in ResNets receptive fields grow sublinearly with depth. If that's the case, why don't they show that incorporating cyclic orthogonal convolutions improves a standard ResNet-50 model on ImageNet?

I don't find the pathfinder results very convincing. It has been shown before (this workshop, last year) that CNNs can also learn Pathfinder once the training setup is slightly adjusted [1]


[1] https://openreview.net/forum?id=dPwyQnHUVvw","[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual.

[CAPTIONS]
Table 1: Figure 2 :2Figure2: Illustration of CycleNet. (a) One cycle is defined as three convolutions performed in sequence: first on (x, y) (green), second on (x, z) (red), and third on (y, z) (blue), each convolution is followed by BatchNorm, ReLU and Dropout (not shown). (b) The full model consists of a stack of cycles, plus a first convolutional layer and a final dense layer.
Table 2: Figure 3 :3Figure 3: Test accuracy on image classification. (a) Performance of CycleNet and the CNN baseline on CIFAR-10, as a function of the number of parameters. Each point corresponds to a different network depth, from one to six cycles, and is an average of five experiments with identical optimized hyperparameters. ResNet-18 is taken from (He et al. (2016)). (b). Left: performance of CycleNet, the CNN baseline and 2:3 hybrid model on ImageNet (128×128) as a function of the number of parameters. MobileNet is taken from (Howard et al. (2017)). Right: we progressively substitute CycleNet with standard convolutional cycles. Non-cubic: experiments with non-cubic tensor shape. Details about different CycleNet architectures an be found in appendix B.1.
Table 3: Figure 4 :4Figure 4: Receptive field sizes and shapes. (a) Normalized receptive field size as a function of activation depth (number of layers) of CycleNet and the CNN baseline, for ImageNet and CIFAR-10. Each point is the mean and standard deviation computed on 100 receptive fields chosen at random. CycleNet achieves large size after one cycle (3 layers), while the size increases with depth in the baseline, and achieves similar size after 18 layers. (b) Sample saliency maps in CycleNet (top) and the baseline (bottom) on ImageNet, after 3 (left) and 15 layers (right). Yellow ellipses show the receptive field shape, covering 3 standard deviations.
Table 4: Figure 5 :5Figure 5: Pathfinder challenge. Each panel shows the accuracy of CycleNet and the CNN baseline as a function of kernel size. Task difficulty increases from left to right (path length n =6, 9, 14). Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[INTRODUCTION]
Several computer vision tasks require capturing long-range dependencies between features. For example, in order to recognize a teapot, it is necessary to identify and integrate several of its parts in their correct spatial relationship, e.g. a spout, a handle, a lid, while each part by itself is usually not enough to determine the object class. Convolutional layers have proven very useful at several computer vision tasks such as classification, and they are at the core of most state-of-the-art neural networks (LeCun et al. (2015)). A convolution transforms each pixel depending on a few neighboring pixels, and the transformation is shared across all pixels; however, the small size of the neighbourhood implies that in a single layer features at distant locations cannot be integrated. In deep convolutional networks, the receptive field size increases sub-linearly with depth (Luo et al. (2017)), therefore a large number of layers is necessary for integrating features across long distances. We propose a novel neural network architecture that retains the efficient parameterization of convolutions, while promoting long-range interactions of distant features.
Similar to the human visual system, individual 'neurons' within each CNN have receptive fields with size increasing with the number of layers. However, while the human visual system achieves large receptive fields within a handful of layers, CNNs need many more (Luo et al. (2017)). In fact, the number of layers keeps expanding to 100s or 1000s of layers for state of the art CNN models (Tan & Le (2019); He et al. (2016)). A neuron of the visual system connects not only to other neurons responding to similar spatial locations, but also to neurons at distant locations, provided that they share similar visual features, such as edge orientation (see Fig. 1a). These connections not only enlarge the receptive fields, but they also facilitate solving tasks that require long-range integration of features. For example, they allow tracking curved contours (Fig. 1b). The neurons responsible for the long-range connections are excitatory pyramidal cells, which are known to have a long range connections in the cortex. Instead, inhibitory GABA-ergic cells do not have this property.
Our model architecture agrees with these biological principles of information processing taking place in the brain. Given a tensor of horizontal (x) and vertical (y) coordinates and features (z), we propose to apply convolutions not only on (x, y) coordinates, but also on (x, z) and (y, z) in sequence. These orthogonal convolutions allow the interaction of 'neurons' at distant locations, provided that they share similar visual features, similarly to the human visual system. We show that, after a cycle of three convolutions over the three axes, each element of the tensor depends on all elements of the input, and features at all locations have a chance to interact. Our model, named CycleNet, is obtained by adding a permutation of the axes to a convolution, and therefore is easy to compare with a standard CNN. In order to evaluate our model, we compare it with a CNN baseline network with identical architecture, i.e. same number of parameters and tensor shape in all layers.
In this paper we show that Cyclenet: outperforms the CNN baseline at image classification on CIFAR-10 and ImageNet (Krizhevsky (2009), Deng et al. (2009)), and approaches the performance of ResNet and MobileNet (He et al. (2016), Howard et al. (2017)) with a similar number of parameters; achieves the maximum receptive field size after a single cycle and outperforms the baseline CNN in transfer learning to stylised data, suggesting the use of more features dependent on shape rather than texture; outperforms the baseline by a large margin at the Pathfinder challenge, a task inspired by the study of human visual perception where CNNs are known to fail dramatically (Linsley et al. (2019)).   (Linsley et al. (2019)). Each image has two circles attached to a path. The goal of this task is to classify images into connected and disconnected, two examples shown for each class.

[RELATED WORK]
A few recent papers showed that deep convolutional networks struggle to learn features integrating large spatial domains. For instance Luo et al. (2017) showed that the receptive field size of a deep CNN is smaller than the sum of its kernel sizes. Geirhos et al. (2019) showed that ImageNet-trained ResNet relies on small image patches and textures, rather than object shapes, and does not transfer well to stylised images. Linsley et al. (2019) showed that ResNet struggles on the Pathfinder challenge, a simple task that requires integrating features over long distances.
Other methods have been proposed to learn large scale features. Pooling layers increase the receptive field size in CNNs, but they lose a significant amount of information, including the spatial relationship between features (Boureau et al. (2010)). Multiscale pyramids use kernels of different sizes arranged in parallel, but larger kernels have lower resolution in order to keep a reasonable number of parameters (Farabet et al. (2013)). Deformable convolutions adaptively learn the shape of the kernel, at the cost of an increased complexity of the model (Dai et al. (2017)). Dilated convolutions increase the kernel size along network depth (Yu & Koltun (2015)) and they are equivalent to a tensor decomposition (Huszar (2016)). Tensor decompositions have been widely used to reduce the complexity of convolutional and dense layers (Kuzmin et al. (2019); Novikov et al. (2015)). We show below that one cycle of CycleNet is equivalent to a decomposition of a dense layer, in the simple case of 1 × 1 convolutions and linear activations, but not in the general case. CycleNet is also different from a 3D convolution (Ji et al. (2013)), since spatial coordinates are fully connected in the second and third layers of a cycle.
Among non-convolutional architectures, self-attention networks naturally capture long-range dependencies. Initially designed for natural language processing (Vaswani et al. (2017)), transformers were recently shown to exhibit good performance on vision tasks ). Similar to self-attention networks, CycleNet breaks translational symmetry and has the best performance on ImageNet when standard convolutions are included in the first layers (Ramachandran et al. (2019)).

[MODEL: A CYCLE OF THREE ORTHOGONAL CONVOLUTIONS]
In this section we describe a single cycle, the basic building block of CycleNet, which is composed of three layers arranged in sequence (Fig. 2a). We start by describing a single convolutional layer. We denote the input tensor as I(x, y, z), at coordinates x (horizontal), y (vertical) and feature z. The output tensor S is equal to:
S(x, y, z) = dx,dy,z K(dx, dy, z, z )I(x + dx, y + dy, z ).(1)
where K denotes the convolutional kernel. We assume zero padding and kernel stride equal to one, while other parameters vary in different experiments (kernel size, input and output tensor shapes). The convolution operation is local: the output at a given location (x, y) depends only on the input displaced by dx, dy, spanning neighboring pixels up to the kernel size, which is typically much smaller than the size of the input. On the other hand, features z are all-to-all fully connected. This is a standard convolutional layer and is depicted as the first layer of Fig. 2a (green). Convolutional layers are usually stacked and interleaved with other types of layers, such as nonlinearities, down(up)sampling, skip connections, normalization, regularization, etc. In this study, we use BatchNorm (Ioffe & Szegedy (2015)), ReLU (Glorot et al. (2011)), Dropout (Srivastava et al. (2014)), and no skip connections. In order to promote long-distance integration, we propose to apply convolutions not only in the (x, y) plane, but also in the (x, z) and (y, z) planes. With some abuse of notation, we denote I as the output of the previous layer, and S as the output of the current layer. The second layer in a cycle is given by the following convolution,
S(x, y, z) = dx,y ,dz K(dx, dz, y, y )I(x + dx, y , z + dz),(2)
also illustrated in Fig. 2a (red). Here, vertical coordinates y are fully connected. Subsequently, after another BatchNorm, ReLU and Dropout, we apply a convolution in the (y, z) plane:
S(x, y, z) =
x ,dy,dz K(dy, dz, x, x )I(x , y + dy, z + dz).
(3) This is the third and final layer in a cycle, illustrated in Fig. 2a (blue), and is also followed by BatchNorm, ReLU and Dropout. Here, horizontal coordinates x are fully connected. The sequence of three convolutions along the three different axes constitutes a cycle. Note that a cycle could be defined with a different ordering of the three convolutions, but we did not explore other configurations.
Further architecture details about the long range integration, the number of parameters, the comparison with a convolutional baseline and the scaling to deeper networks can be found in Appendix A.
3 Experiments

[RECEPTIVE FIELDS]
We train CycleNet and the baseline CNN on CIFAR-10 and ImageNet, benchmarks for image classification over which CNNs have been extensively optimized. Experimental results can be seen in Figure 3 and detailed description and discussion of these experiments can be found in appendix B.1. Figure 3 shows that CycleNet has a higher accuracy than the baseline and is comparable to state of the art CNN models of similar size. Is this because CycleNet learns substantially different representations of the images compared to standard CNN? We investigate this using receptive fields. At the end of a cycle, each element of the output tensor may depend on all elements of the input tensor (see appendix A.1), thus CycleNet is expected to have large receptive fields. In contrast, the receptive fields of CNNs are usually small (Luo et al. (2017)). Here, we compute the receptive fields at the output of each cycle of our models with 18 layers for CIFAR and 15 layers for ImageNet: first, we compute saliency maps following Simonyan et al. (2014), then we use those maps to compute receptive fields following Luo et al. (2017). Details can be found in section 3.3.
Figure 4a shows receptive field size as a function of depth, where the receptive field is normalized by the resolution (32 for CIFAR, 128 for ImageNet). Each bar is computed across 100 receptive fields sampled at random, 10 activations times 10 images. As expected, CycleNet achieves a large receptive field size after one cycle (3 layers), while the CNN baseline increases the receptive field size with depth, and approaches the size of CycleNet after 18 layers. Fig. 4b shows a few visual examples of receptive field shapes for both CycleNet and the CNN baseline at different layer positions.
Larger receptive fields are fundamental in tasks requiring long-range integration of features and the features developed implicitly rely on more global patters, favouring classification of images by shape rather than texture. These are investigated in Section 3.2 and section 3.3. 

[PATHFINDER CHALLENGE]
In order to test whether large receptive fields help integrating long-range features, we evaluated CycleNet on the Pathfinder challenge (Fig. 1d). This task is inspired by the study of human visual perception, and requires tracking paths over long distances to distinguish whether two white circles are connected. Deep convolutional networks struggle on this task (e.g. ResNet50), because of their limited ability of integrating features over long distances (Linsley et al. (2019)). Since CycleNet integrates all pixels after one cycle, we predict that it should perform well on this task. Linsley et al. (2019) proposes a biologically-inspired horizontal gated recurrent neural network that performs 100% on the task, but their model is hard to compare with standard CNNs. Details about the Pathfinder dataset as well as the architectures used can be found in appendix B.3.
Figure 5 shows the performance of CycleNet and the CNN baseline in 3 challenges of increasing difficulty, i.e. increasing path lengths: n = 6, 9, and 14. For kernel sizes 4, 8, and 12, the CNN baseline does not learn the task. It performs at chance level (50%), except in one experiment of path length 6. On the other hand, CycleNet shows good accuracy, up to nearly 100%, suggesting that it is able to take advantage of its larger receptive fields. Furthermore, its performance decreases with path length and increases with kernel size. Note that large receptive fields may not be enough to solve this task, it also requires an increasing level of expressivity at larger path lengths. This is provided by a larger kernel size in CycleNet. For kernel size 20 the CNN baseline is able to perform the task, sometimes better than CycleNet; note that however for this kernel size the training has not converged as we were limited by computational resources and the models were very large due to the size of the kernel. We can only observe that for this case the CNN converged faster. At this size, a single kernel covers a substantial part of the image and is effectively long-range. However, the number of parameters is large in this case and the efficiency of the parameterization is lost. . Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[STYLISED IMAGES]
Given that CycleNet has large receptive fields, we hypothesise that it should classify images by the shape rather than texture of objects. Geirhos et al. (2019) designed new benchmarks to explore the shape vs texture bias of deep neural networks trained on ImageNet (IN). In the Stylised ImageNet dataset (SIN), the texture of each image is replaced with a randomly selected painting style (Fig. 6, top). In the Cue-Conflict dataset (CC), a few images are generated by iterative style transfer from selected textures consisting of patches of other classes (Fig. 6, bottom). We predict that our IN pre-trained CycleNet should transfer better than the baseline to both SIN and CC. We generate SIN following the scripts at https://github.com/rgeirhos/Stylized-ImageNet, and data for CC is sourced from https://github.com/rgeirhos/texture-vs-shape. Both experiments are evaluated at resolution 128x128.
Table 1 shows evaluation top-5 accuracy on Stylised ImageNet and top-1 accuracy on the Cue-Conflict dataset. The results confirm that CycleNet transfers better to both datasets. Although the overall accuracy on ImageNet is higher for a hybrid network where the first two cycles have standard convolutions (2:3), the best transfer to stylised images is obtained by CycleNet, where all cycles use orthogonal convolutions (0:5). These results confirm that large receptive fields are instrumental for biasing a neural network towards shape, and that the base performance on ImageNet by itself is not a good predictor of such bias. 

[DISCUSSION]
We proposed a biologically inspired neural network architecture, CycleNet, that achieves better performance than a CNN baseline in classification tasks, and it develops a significantly different representation of the input image, where node activations have large receptive fields and are thus able to represent large portions of an image. This is in agreement with the human visual system that achieves large receptive fields within few layers and in sharp contrast with CNNs, which have smaller receptive fields. CycleNet transfers better than the CNN baseline to stylised images, suggesting that the large receptive fields bias the model towards shape rather than texture of objects, and performs much better at the Pathfinder task, which requires long-range integration of features.
We emphasize that CycleNet loses translational symmetry, which is considered a strong feature of CNNs. However, recent evidence suggests that this property may not be crucial. Symmetries do not have to be hard coded in the architecture: they can be learned by stochastic gradient descent (Achille & Soatto (2018)) and data augmentation (Taylor & Nitschke (2017)). Furthermore, several nontranslational symmetric architectures recently achieved near state-of-the-art performance in image classification (Ramachandran et al. (2019)). The good performance of CycleNet on classification adds to this line of research, suggesting that built-in translational symmetry may not be necessary.
Performance in other tasks is likely to benefit from long-range integration, such as image segmentation, generation, reconstruction, etc. Future work may focus on testing CycleNet on those tasks.
A Appendix: Architecture details

[A.1 LONG-RANGE INTEGRATION]
We combine Eqs.1, 2 and 3 to express the effect of one cycle. We consider the simple case of 1 × 1 convolutions (thus dx = dy = dz = 0), and we ignore all nonlinearities in between layers, obtaining S(x, y, z) =
x ,y ,z
K 3 (x, x )K 2 (y, y )K 1 (z, z )I(x , y , z ).(4)
where K 1 , K 2 and K 3 are the three successive kernels. Therefore, after a cycle of three convolutions, each element of the output tensor depends on all elements of the input tensor. It is straightforward to check that, even for k × k convolutions (k > 1) and nonlinearities, each element of the output tensor may still depend on all elements of the input tensor. However, it is not guaranteed that CycleNet makes use of the entire receptive range after training on a given task, and therefore we test this hypothesis in Section 3.1.
Note that if we substitute K(x, y, z, x , y , z
) = K 3 (x, x )K 2 (y, y )K 1 (z, z )(5)
into Eq.4, then the expression becomes equivalent to a dense layer, where the kernel is decomposed into three factors. Thus, Eq.4 is equivalent to a tensor decomposition. However, CycleNet is different from a tensor decomposition when nonlinearities are included between layers, and in the more general case for convolutions of kernel size k > 1.

[A.2 NUMBER OF PARAMETERS AND OPERATIONS]
The expression to calculate the number of parameters in CycleNet is the same as in a standard convolutional layer, simply the convolutions are performed along a different axis. We denote by k the size of the kernel, by X in , Y in , Z in the shape of the input tensor and by X out , Y out , Z out the shape of the output tensor. The number of parameters of the first layer is k 2 Z in Z out , each one operated on X out Y out times. Similarly, the number of parameters of the second layer is k 2 Y in Y out , times X out Z out operations, and for the third k 2 X in X out parameters, times Y out Z out operations. Note that the number of parameters of CycleNet may differ from a standard convolution if the number of horizontal pixels X or vertical pixels Y is different from the number of features Z.

[A.3 CUBIC BASELINE]
In order to compare the performance of CycleNet with standard CNNs, we define a baseline to study the relative improvement introduced by orthogonal convolutions. The baseline is exactly identical to CycleNet, except that we do not permute the axes, instead we always perform convolutions along the same axis. This corresponds to a standard stack of convolutional layers, interleaved with BatchNorm, ReLU and Dropout. However, in order to keep the number of parameters equal between CycleNet and the CNN baseline, we consider cubic tensors in most of our experiments, i.e. in which the number of horizontal pixels X, vertical pixels Y and the number of features Z are equal within each cycle. This is a strong constraint that makes such baseline quite specific, but we still refer to it simply as ""baseline"". It is likely that a better performance could be obtained, both by CycleNet and the CNN, without this cubic constraint. Therefore, we also perform experiments on CycleNet with non-cubic tensors, and compare its performance with other convolutional architectures, e.g. ResNet and MobileNet, which are also non-cubic.

[A.4 DEEP NETWORK]
As illustrated in Fig. 2b, we stack cycles in sequence to construct a deep network model. We add a standard convolution as a first layer of the network, to obtain the appropriate number of features to be used in the first cycle, and a dense layer as the last layer, thus reducing the (flattened) tensor to the size of the final output. In cubic models, tensor width is changed from one cycle to the next using tri-linear interpolation. In non-cubic models, tensor shape is changed by controlling the output size of the fully connected coordinate at each layer. In the case of the Pathfinder challenge, we use global pooling before the dense layer, in order to match the baseline architectures of Linsley et al. (2019). Code will be made available to the reviewers in the supplementary material, and to the public upon acceptance of the paper.

[B APPENDIX: EXPERIMENTAL DETAILS B.1 IMAGE CLASSIFICATION]
We start by training CycleNet on CIFAR-10 and ImageNet (at 128 × 128 resolution), standard benchmarks for image classification on which CNNs have been extensively optimized (Fig. 1a,b). We do not aim at beating the current state-of-the-art. Our goal is to compare CycleNet with standard CNNs of equal or similar size, and use them to test our working hypotheses in the next sections. We use relatively small models for ease of implementation, in order to explore a variety of hyperparameters. The CIFAR dataset consists of 60,000 32x32 colour images in 10 classes, 50,000 training images and 10,000 test images. We use the standard cross-entropy loss with L2 regularization. We train on a single GPU (GeForce RTX 2080 Ti) using the Keras API in TensorFlow. The training cycle is 300 epochs and the network hyperparameters are optimized by running twelve experiments varying dropout rates and L2 lambda. RMSprop was used with batch size 64 and initial learning rate 0.001, which is divided by 10 when the error plateaus. Data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. The first convolution outputs 80 features, and the deepest architecture has the following tensor widths in successive cycles: 75, 60, 45, 30, 15, 5. For the shallower networks, we use the smaller end of the sequence, e.g. width 5 for one cycle, widths 15, 5 for two cycles, widths 30, 15, 5 for three cycles and so on and so forth. The non-cubic CycleNet starts with a convolution with 100 features (32 × 32 × 100) followed by four cycles (45 × 45 × 100, 30 × 30 × 66, 15 × 15 × 33, 5 × 5 × 8). Fig. 3a shows the performance of models of up to six cycles (18 layers, k = 3) on CIFAR-10. CycleNet performs better than the CNN baseline, and the non-cubic CycleNet approaches the performance of a ResNet-18 (He et al. (2016)) with a similar number of parameters. These results suggest that long-range integration may be instrumental for image classification, but their significance is limited by the small resolution of CIFAR-10 (32 × 32).
To confirm our results on higher resolution, we next tested CycleNet on ImageNet (128 × 128). Fig. 3b (left) shows the performance of five-cycle models (15 layers, k = 3) on ImageNet. The ImageNet dataset consists of 1.28 million training and 50,000 test images in 1000 classes. We use 128 × 128 resultion, and the standard cross-entropy loss. We train on a 24 GPUs (GeForce RTX 2080 Ti) distributed training system (Sergeev & Del Balso (2018)) for 120 epochs. Adam optimizer was used with effective batch size 384 and learning rate 0.024 (using warm-up as in (Goyal et al. (2017)), which is divided by 10 when the error plateaus. Similar to CIFAR, data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. In the cubic models, the first convolution outputs 128 features, followed by cycles of the following tensor widths: 106, 106, 106, 106, 12. The non-cubic CycleNet starts with a convolution with 260 features (128 × 128 × 260) followed by five cycles (106 × 106 × 260, 106 × 106 × 190, 106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11). The non-cubic 2:3 CNN:CycleNet model has a convolution of 200 filters, followed by two CNN cycles (equivalent to 6 convolutions of output features 200,200,200,190,170,159) and three CycleNet cycles (106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11).
CycleNet performs better than the baseline, but its accuracy is significantly lower when compared to MobileNet (Howard et al. (2017)), which has a slightly larger size. We experiment with larger models, and we followed two approaches to increase performance up to a comparable level. First, similar to our CIFAR-10 experiment, we try a non-cubic architecture and find a 10% gain in relative performance. Second, following previous work (Ramachandran et al. (2019)), we hypothesise that image classification benefits from standard convolutions in the first layers, therefore we vary the number of initial cycles with standard convolutions. Fig.3b (right) shows that a model with two cycles of standard convolutions (2:3) has the best performance, and combining this with a noncubic architecture, the model approaches the performance of MobileNet with a similar number of parameters. This motivates using CycleNet and the CNN baseline for testing our working hypotheses in the next sections.
We used relatively small CycleNet models, in order to have a baseline that can be easily compared with, and to be able to explore CycleNet with a variety of choices of hyperparameters. It would be interesting to look at whether state of the art performance can be achieved on datasets of higher resolution and larger models, for example by introducing residual connections. This would however require significantly more computational resources.

[B.2 CALCULATION OF THE RECEPTIVE FIELD]
To calculate the receptive field, for a given node in the network and a given input image, we compute the gradient of the activation of that node with respect to the image. The saliency map is a grayscale image, obtained by taking the absolute value of the gradient for each pixel and summing across the three color channels (Simonyan et al. (2014)). The saliency map is normalized, such that the sum across pixels is one, and is interpreted as a 2-dimensional probability density. The receptive field size is computed by the square root of the total variance (the trace of the covariance matrix) of that density. We use the total variance since it quantifies the scatter of the saliency map along both axes independently. The receptive field shape is defined as the ellipsoid containing 3 standard deviations on both axes.

[B.3 THE PATHFINDER CHALLENGE]
The Pathfinder dataset is composed of 900, 000 training and 100, 000 test images, and has two classes. We use the standard cross-entropy loss with L2 regularization. In our experiment, in addition to the first convolution, CycleNet has a single cycle at 128 × 128 × 128 features, global pooling (as in Linsley et al. (2019)) and a dense layer. The experiments run on a single GPU (GeForce RTX 2080 Ti), Adam optimizer was used with learning rate 0.001. We use 5 times more distractors than the base dataset.
We generate three datasets of increasing difficulty, for different path lengths: n = 6, 9, and 14, at 128 × 128 resolution, using the code available at https://github.com/drewlinsley/pathfinder. We study performance of one-cycle models as a function of kernel size, k = 4, 8, 12 and 20.
For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease (see Figure 5). For kernel size 20, which allows convolutions to integrate long-range, the CNN is able to learn the task. However, for this kernel size, the accuracy values reported here are not final because the models had not converged.
The training stopped before convergence due to limitations in computational resources.
Note that CNNs typically have kernel size smaller than 10, since the number of parameters becomes substantial for larger kernels.","[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual."
Cyclic orthogonal convolutions for long-range integration of features,868DWd46dv2.json,"This paper attempts to enable CNNs to learn long range spatial dependencies, typically only possible at great depth, in the early layers. To acheive this the authors propose CycleNet, a network of 'cycles' of orthogonal convolutions. These convolutions are performed across the three coordinate planes and have a subtaintially larger receptive field than a typical convolution without a dramatic increase in the number of parameters. The motivation for this work is comprehensive and the architecture is well described and intuitive. Experimental results show that CycleNet significantly improves performance over a baseline on the pathfinder challenge and also provides a modest improvement / increase in parameter efficiency on CIFAR-10.

The authors touch on a biological basis for the ideas explored here but I feel that the full potential of this line of reasoning is not realised. For example, the authors show improved generalisation to stylised ImageNet only in an Appendix when it is arguably among the most exciting results of the paper. These could be further augmented with the addition of other biological similarity measures such as the brain-score (https://www.brain-score.org/). Finally, it would be valuable for the authors to delve deeper into the related biology, perhaps identifying specific cell types or psychophysical results that they feel are better represented by the CycleNet model.

Overall, this is a well presented and clearly motivated work with promising results, a strong accept.","[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual.

[CAPTIONS]
Table 1: Figure 2 :2Figure2: Illustration of CycleNet. (a) One cycle is defined as three convolutions performed in sequence: first on (x, y) (green), second on (x, z) (red), and third on (y, z) (blue), each convolution is followed by BatchNorm, ReLU and Dropout (not shown). (b) The full model consists of a stack of cycles, plus a first convolutional layer and a final dense layer.
Table 2: Figure 3 :3Figure 3: Test accuracy on image classification. (a) Performance of CycleNet and the CNN baseline on CIFAR-10, as a function of the number of parameters. Each point corresponds to a different network depth, from one to six cycles, and is an average of five experiments with identical optimized hyperparameters. ResNet-18 is taken from (He et al. (2016)). (b). Left: performance of CycleNet, the CNN baseline and 2:3 hybrid model on ImageNet (128×128) as a function of the number of parameters. MobileNet is taken from (Howard et al. (2017)). Right: we progressively substitute CycleNet with standard convolutional cycles. Non-cubic: experiments with non-cubic tensor shape. Details about different CycleNet architectures an be found in appendix B.1.
Table 3: Figure 4 :4Figure 4: Receptive field sizes and shapes. (a) Normalized receptive field size as a function of activation depth (number of layers) of CycleNet and the CNN baseline, for ImageNet and CIFAR-10. Each point is the mean and standard deviation computed on 100 receptive fields chosen at random. CycleNet achieves large size after one cycle (3 layers), while the size increases with depth in the baseline, and achieves similar size after 18 layers. (b) Sample saliency maps in CycleNet (top) and the baseline (bottom) on ImageNet, after 3 (left) and 15 layers (right). Yellow ellipses show the receptive field shape, covering 3 standard deviations.
Table 4: Figure 5 :5Figure 5: Pathfinder challenge. Each panel shows the accuracy of CycleNet and the CNN baseline as a function of kernel size. Task difficulty increases from left to right (path length n =6, 9, 14). Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[INTRODUCTION]
Several computer vision tasks require capturing long-range dependencies between features. For example, in order to recognize a teapot, it is necessary to identify and integrate several of its parts in their correct spatial relationship, e.g. a spout, a handle, a lid, while each part by itself is usually not enough to determine the object class. Convolutional layers have proven very useful at several computer vision tasks such as classification, and they are at the core of most state-of-the-art neural networks (LeCun et al. (2015)). A convolution transforms each pixel depending on a few neighboring pixels, and the transformation is shared across all pixels; however, the small size of the neighbourhood implies that in a single layer features at distant locations cannot be integrated. In deep convolutional networks, the receptive field size increases sub-linearly with depth (Luo et al. (2017)), therefore a large number of layers is necessary for integrating features across long distances. We propose a novel neural network architecture that retains the efficient parameterization of convolutions, while promoting long-range interactions of distant features.
Similar to the human visual system, individual 'neurons' within each CNN have receptive fields with size increasing with the number of layers. However, while the human visual system achieves large receptive fields within a handful of layers, CNNs need many more (Luo et al. (2017)). In fact, the number of layers keeps expanding to 100s or 1000s of layers for state of the art CNN models (Tan & Le (2019); He et al. (2016)). A neuron of the visual system connects not only to other neurons responding to similar spatial locations, but also to neurons at distant locations, provided that they share similar visual features, such as edge orientation (see Fig. 1a). These connections not only enlarge the receptive fields, but they also facilitate solving tasks that require long-range integration of features. For example, they allow tracking curved contours (Fig. 1b). The neurons responsible for the long-range connections are excitatory pyramidal cells, which are known to have a long range connections in the cortex. Instead, inhibitory GABA-ergic cells do not have this property.
Our model architecture agrees with these biological principles of information processing taking place in the brain. Given a tensor of horizontal (x) and vertical (y) coordinates and features (z), we propose to apply convolutions not only on (x, y) coordinates, but also on (x, z) and (y, z) in sequence. These orthogonal convolutions allow the interaction of 'neurons' at distant locations, provided that they share similar visual features, similarly to the human visual system. We show that, after a cycle of three convolutions over the three axes, each element of the tensor depends on all elements of the input, and features at all locations have a chance to interact. Our model, named CycleNet, is obtained by adding a permutation of the axes to a convolution, and therefore is easy to compare with a standard CNN. In order to evaluate our model, we compare it with a CNN baseline network with identical architecture, i.e. same number of parameters and tensor shape in all layers.
In this paper we show that Cyclenet: outperforms the CNN baseline at image classification on CIFAR-10 and ImageNet (Krizhevsky (2009), Deng et al. (2009)), and approaches the performance of ResNet and MobileNet (He et al. (2016), Howard et al. (2017)) with a similar number of parameters; achieves the maximum receptive field size after a single cycle and outperforms the baseline CNN in transfer learning to stylised data, suggesting the use of more features dependent on shape rather than texture; outperforms the baseline by a large margin at the Pathfinder challenge, a task inspired by the study of human visual perception where CNNs are known to fail dramatically (Linsley et al. (2019)).   (Linsley et al. (2019)). Each image has two circles attached to a path. The goal of this task is to classify images into connected and disconnected, two examples shown for each class.

[RELATED WORK]
A few recent papers showed that deep convolutional networks struggle to learn features integrating large spatial domains. For instance Luo et al. (2017) showed that the receptive field size of a deep CNN is smaller than the sum of its kernel sizes. Geirhos et al. (2019) showed that ImageNet-trained ResNet relies on small image patches and textures, rather than object shapes, and does not transfer well to stylised images. Linsley et al. (2019) showed that ResNet struggles on the Pathfinder challenge, a simple task that requires integrating features over long distances.
Other methods have been proposed to learn large scale features. Pooling layers increase the receptive field size in CNNs, but they lose a significant amount of information, including the spatial relationship between features (Boureau et al. (2010)). Multiscale pyramids use kernels of different sizes arranged in parallel, but larger kernels have lower resolution in order to keep a reasonable number of parameters (Farabet et al. (2013)). Deformable convolutions adaptively learn the shape of the kernel, at the cost of an increased complexity of the model (Dai et al. (2017)). Dilated convolutions increase the kernel size along network depth (Yu & Koltun (2015)) and they are equivalent to a tensor decomposition (Huszar (2016)). Tensor decompositions have been widely used to reduce the complexity of convolutional and dense layers (Kuzmin et al. (2019); Novikov et al. (2015)). We show below that one cycle of CycleNet is equivalent to a decomposition of a dense layer, in the simple case of 1 × 1 convolutions and linear activations, but not in the general case. CycleNet is also different from a 3D convolution (Ji et al. (2013)), since spatial coordinates are fully connected in the second and third layers of a cycle.
Among non-convolutional architectures, self-attention networks naturally capture long-range dependencies. Initially designed for natural language processing (Vaswani et al. (2017)), transformers were recently shown to exhibit good performance on vision tasks ). Similar to self-attention networks, CycleNet breaks translational symmetry and has the best performance on ImageNet when standard convolutions are included in the first layers (Ramachandran et al. (2019)).

[MODEL: A CYCLE OF THREE ORTHOGONAL CONVOLUTIONS]
In this section we describe a single cycle, the basic building block of CycleNet, which is composed of three layers arranged in sequence (Fig. 2a). We start by describing a single convolutional layer. We denote the input tensor as I(x, y, z), at coordinates x (horizontal), y (vertical) and feature z. The output tensor S is equal to:
S(x, y, z) = dx,dy,z K(dx, dy, z, z )I(x + dx, y + dy, z ).(1)
where K denotes the convolutional kernel. We assume zero padding and kernel stride equal to one, while other parameters vary in different experiments (kernel size, input and output tensor shapes). The convolution operation is local: the output at a given location (x, y) depends only on the input displaced by dx, dy, spanning neighboring pixels up to the kernel size, which is typically much smaller than the size of the input. On the other hand, features z are all-to-all fully connected. This is a standard convolutional layer and is depicted as the first layer of Fig. 2a (green). Convolutional layers are usually stacked and interleaved with other types of layers, such as nonlinearities, down(up)sampling, skip connections, normalization, regularization, etc. In this study, we use BatchNorm (Ioffe & Szegedy (2015)), ReLU (Glorot et al. (2011)), Dropout (Srivastava et al. (2014)), and no skip connections. In order to promote long-distance integration, we propose to apply convolutions not only in the (x, y) plane, but also in the (x, z) and (y, z) planes. With some abuse of notation, we denote I as the output of the previous layer, and S as the output of the current layer. The second layer in a cycle is given by the following convolution,
S(x, y, z) = dx,y ,dz K(dx, dz, y, y )I(x + dx, y , z + dz),(2)
also illustrated in Fig. 2a (red). Here, vertical coordinates y are fully connected. Subsequently, after another BatchNorm, ReLU and Dropout, we apply a convolution in the (y, z) plane:
S(x, y, z) =
x ,dy,dz K(dy, dz, x, x )I(x , y + dy, z + dz).
(3) This is the third and final layer in a cycle, illustrated in Fig. 2a (blue), and is also followed by BatchNorm, ReLU and Dropout. Here, horizontal coordinates x are fully connected. The sequence of three convolutions along the three different axes constitutes a cycle. Note that a cycle could be defined with a different ordering of the three convolutions, but we did not explore other configurations.
Further architecture details about the long range integration, the number of parameters, the comparison with a convolutional baseline and the scaling to deeper networks can be found in Appendix A.
3 Experiments

[RECEPTIVE FIELDS]
We train CycleNet and the baseline CNN on CIFAR-10 and ImageNet, benchmarks for image classification over which CNNs have been extensively optimized. Experimental results can be seen in Figure 3 and detailed description and discussion of these experiments can be found in appendix B.1. Figure 3 shows that CycleNet has a higher accuracy than the baseline and is comparable to state of the art CNN models of similar size. Is this because CycleNet learns substantially different representations of the images compared to standard CNN? We investigate this using receptive fields. At the end of a cycle, each element of the output tensor may depend on all elements of the input tensor (see appendix A.1), thus CycleNet is expected to have large receptive fields. In contrast, the receptive fields of CNNs are usually small (Luo et al. (2017)). Here, we compute the receptive fields at the output of each cycle of our models with 18 layers for CIFAR and 15 layers for ImageNet: first, we compute saliency maps following Simonyan et al. (2014), then we use those maps to compute receptive fields following Luo et al. (2017). Details can be found in section 3.3.
Figure 4a shows receptive field size as a function of depth, where the receptive field is normalized by the resolution (32 for CIFAR, 128 for ImageNet). Each bar is computed across 100 receptive fields sampled at random, 10 activations times 10 images. As expected, CycleNet achieves a large receptive field size after one cycle (3 layers), while the CNN baseline increases the receptive field size with depth, and approaches the size of CycleNet after 18 layers. Fig. 4b shows a few visual examples of receptive field shapes for both CycleNet and the CNN baseline at different layer positions.
Larger receptive fields are fundamental in tasks requiring long-range integration of features and the features developed implicitly rely on more global patters, favouring classification of images by shape rather than texture. These are investigated in Section 3.2 and section 3.3. 

[PATHFINDER CHALLENGE]
In order to test whether large receptive fields help integrating long-range features, we evaluated CycleNet on the Pathfinder challenge (Fig. 1d). This task is inspired by the study of human visual perception, and requires tracking paths over long distances to distinguish whether two white circles are connected. Deep convolutional networks struggle on this task (e.g. ResNet50), because of their limited ability of integrating features over long distances (Linsley et al. (2019)). Since CycleNet integrates all pixels after one cycle, we predict that it should perform well on this task. Linsley et al. (2019) proposes a biologically-inspired horizontal gated recurrent neural network that performs 100% on the task, but their model is hard to compare with standard CNNs. Details about the Pathfinder dataset as well as the architectures used can be found in appendix B.3.
Figure 5 shows the performance of CycleNet and the CNN baseline in 3 challenges of increasing difficulty, i.e. increasing path lengths: n = 6, 9, and 14. For kernel sizes 4, 8, and 12, the CNN baseline does not learn the task. It performs at chance level (50%), except in one experiment of path length 6. On the other hand, CycleNet shows good accuracy, up to nearly 100%, suggesting that it is able to take advantage of its larger receptive fields. Furthermore, its performance decreases with path length and increases with kernel size. Note that large receptive fields may not be enough to solve this task, it also requires an increasing level of expressivity at larger path lengths. This is provided by a larger kernel size in CycleNet. For kernel size 20 the CNN baseline is able to perform the task, sometimes better than CycleNet; note that however for this kernel size the training has not converged as we were limited by computational resources and the models were very large due to the size of the kernel. We can only observe that for this case the CNN converged faster. At this size, a single kernel covers a substantial part of the image and is effectively long-range. However, the number of parameters is large in this case and the efficiency of the parameterization is lost. . Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[STYLISED IMAGES]
Given that CycleNet has large receptive fields, we hypothesise that it should classify images by the shape rather than texture of objects. Geirhos et al. (2019) designed new benchmarks to explore the shape vs texture bias of deep neural networks trained on ImageNet (IN). In the Stylised ImageNet dataset (SIN), the texture of each image is replaced with a randomly selected painting style (Fig. 6, top). In the Cue-Conflict dataset (CC), a few images are generated by iterative style transfer from selected textures consisting of patches of other classes (Fig. 6, bottom). We predict that our IN pre-trained CycleNet should transfer better than the baseline to both SIN and CC. We generate SIN following the scripts at https://github.com/rgeirhos/Stylized-ImageNet, and data for CC is sourced from https://github.com/rgeirhos/texture-vs-shape. Both experiments are evaluated at resolution 128x128.
Table 1 shows evaluation top-5 accuracy on Stylised ImageNet and top-1 accuracy on the Cue-Conflict dataset. The results confirm that CycleNet transfers better to both datasets. Although the overall accuracy on ImageNet is higher for a hybrid network where the first two cycles have standard convolutions (2:3), the best transfer to stylised images is obtained by CycleNet, where all cycles use orthogonal convolutions (0:5). These results confirm that large receptive fields are instrumental for biasing a neural network towards shape, and that the base performance on ImageNet by itself is not a good predictor of such bias. 

[DISCUSSION]
We proposed a biologically inspired neural network architecture, CycleNet, that achieves better performance than a CNN baseline in classification tasks, and it develops a significantly different representation of the input image, where node activations have large receptive fields and are thus able to represent large portions of an image. This is in agreement with the human visual system that achieves large receptive fields within few layers and in sharp contrast with CNNs, which have smaller receptive fields. CycleNet transfers better than the CNN baseline to stylised images, suggesting that the large receptive fields bias the model towards shape rather than texture of objects, and performs much better at the Pathfinder task, which requires long-range integration of features.
We emphasize that CycleNet loses translational symmetry, which is considered a strong feature of CNNs. However, recent evidence suggests that this property may not be crucial. Symmetries do not have to be hard coded in the architecture: they can be learned by stochastic gradient descent (Achille & Soatto (2018)) and data augmentation (Taylor & Nitschke (2017)). Furthermore, several nontranslational symmetric architectures recently achieved near state-of-the-art performance in image classification (Ramachandran et al. (2019)). The good performance of CycleNet on classification adds to this line of research, suggesting that built-in translational symmetry may not be necessary.
Performance in other tasks is likely to benefit from long-range integration, such as image segmentation, generation, reconstruction, etc. Future work may focus on testing CycleNet on those tasks.
A Appendix: Architecture details

[A.1 LONG-RANGE INTEGRATION]
We combine Eqs.1, 2 and 3 to express the effect of one cycle. We consider the simple case of 1 × 1 convolutions (thus dx = dy = dz = 0), and we ignore all nonlinearities in between layers, obtaining S(x, y, z) =
x ,y ,z
K 3 (x, x )K 2 (y, y )K 1 (z, z )I(x , y , z ).(4)
where K 1 , K 2 and K 3 are the three successive kernels. Therefore, after a cycle of three convolutions, each element of the output tensor depends on all elements of the input tensor. It is straightforward to check that, even for k × k convolutions (k > 1) and nonlinearities, each element of the output tensor may still depend on all elements of the input tensor. However, it is not guaranteed that CycleNet makes use of the entire receptive range after training on a given task, and therefore we test this hypothesis in Section 3.1.
Note that if we substitute K(x, y, z, x , y , z
) = K 3 (x, x )K 2 (y, y )K 1 (z, z )(5)
into Eq.4, then the expression becomes equivalent to a dense layer, where the kernel is decomposed into three factors. Thus, Eq.4 is equivalent to a tensor decomposition. However, CycleNet is different from a tensor decomposition when nonlinearities are included between layers, and in the more general case for convolutions of kernel size k > 1.

[A.2 NUMBER OF PARAMETERS AND OPERATIONS]
The expression to calculate the number of parameters in CycleNet is the same as in a standard convolutional layer, simply the convolutions are performed along a different axis. We denote by k the size of the kernel, by X in , Y in , Z in the shape of the input tensor and by X out , Y out , Z out the shape of the output tensor. The number of parameters of the first layer is k 2 Z in Z out , each one operated on X out Y out times. Similarly, the number of parameters of the second layer is k 2 Y in Y out , times X out Z out operations, and for the third k 2 X in X out parameters, times Y out Z out operations. Note that the number of parameters of CycleNet may differ from a standard convolution if the number of horizontal pixels X or vertical pixels Y is different from the number of features Z.

[A.3 CUBIC BASELINE]
In order to compare the performance of CycleNet with standard CNNs, we define a baseline to study the relative improvement introduced by orthogonal convolutions. The baseline is exactly identical to CycleNet, except that we do not permute the axes, instead we always perform convolutions along the same axis. This corresponds to a standard stack of convolutional layers, interleaved with BatchNorm, ReLU and Dropout. However, in order to keep the number of parameters equal between CycleNet and the CNN baseline, we consider cubic tensors in most of our experiments, i.e. in which the number of horizontal pixels X, vertical pixels Y and the number of features Z are equal within each cycle. This is a strong constraint that makes such baseline quite specific, but we still refer to it simply as ""baseline"". It is likely that a better performance could be obtained, both by CycleNet and the CNN, without this cubic constraint. Therefore, we also perform experiments on CycleNet with non-cubic tensors, and compare its performance with other convolutional architectures, e.g. ResNet and MobileNet, which are also non-cubic.

[A.4 DEEP NETWORK]
As illustrated in Fig. 2b, we stack cycles in sequence to construct a deep network model. We add a standard convolution as a first layer of the network, to obtain the appropriate number of features to be used in the first cycle, and a dense layer as the last layer, thus reducing the (flattened) tensor to the size of the final output. In cubic models, tensor width is changed from one cycle to the next using tri-linear interpolation. In non-cubic models, tensor shape is changed by controlling the output size of the fully connected coordinate at each layer. In the case of the Pathfinder challenge, we use global pooling before the dense layer, in order to match the baseline architectures of Linsley et al. (2019). Code will be made available to the reviewers in the supplementary material, and to the public upon acceptance of the paper.

[B APPENDIX: EXPERIMENTAL DETAILS B.1 IMAGE CLASSIFICATION]
We start by training CycleNet on CIFAR-10 and ImageNet (at 128 × 128 resolution), standard benchmarks for image classification on which CNNs have been extensively optimized (Fig. 1a,b). We do not aim at beating the current state-of-the-art. Our goal is to compare CycleNet with standard CNNs of equal or similar size, and use them to test our working hypotheses in the next sections. We use relatively small models for ease of implementation, in order to explore a variety of hyperparameters. The CIFAR dataset consists of 60,000 32x32 colour images in 10 classes, 50,000 training images and 10,000 test images. We use the standard cross-entropy loss with L2 regularization. We train on a single GPU (GeForce RTX 2080 Ti) using the Keras API in TensorFlow. The training cycle is 300 epochs and the network hyperparameters are optimized by running twelve experiments varying dropout rates and L2 lambda. RMSprop was used with batch size 64 and initial learning rate 0.001, which is divided by 10 when the error plateaus. Data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. The first convolution outputs 80 features, and the deepest architecture has the following tensor widths in successive cycles: 75, 60, 45, 30, 15, 5. For the shallower networks, we use the smaller end of the sequence, e.g. width 5 for one cycle, widths 15, 5 for two cycles, widths 30, 15, 5 for three cycles and so on and so forth. The non-cubic CycleNet starts with a convolution with 100 features (32 × 32 × 100) followed by four cycles (45 × 45 × 100, 30 × 30 × 66, 15 × 15 × 33, 5 × 5 × 8). Fig. 3a shows the performance of models of up to six cycles (18 layers, k = 3) on CIFAR-10. CycleNet performs better than the CNN baseline, and the non-cubic CycleNet approaches the performance of a ResNet-18 (He et al. (2016)) with a similar number of parameters. These results suggest that long-range integration may be instrumental for image classification, but their significance is limited by the small resolution of CIFAR-10 (32 × 32).
To confirm our results on higher resolution, we next tested CycleNet on ImageNet (128 × 128). Fig. 3b (left) shows the performance of five-cycle models (15 layers, k = 3) on ImageNet. The ImageNet dataset consists of 1.28 million training and 50,000 test images in 1000 classes. We use 128 × 128 resultion, and the standard cross-entropy loss. We train on a 24 GPUs (GeForce RTX 2080 Ti) distributed training system (Sergeev & Del Balso (2018)) for 120 epochs. Adam optimizer was used with effective batch size 384 and learning rate 0.024 (using warm-up as in (Goyal et al. (2017)), which is divided by 10 when the error plateaus. Similar to CIFAR, data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. In the cubic models, the first convolution outputs 128 features, followed by cycles of the following tensor widths: 106, 106, 106, 106, 12. The non-cubic CycleNet starts with a convolution with 260 features (128 × 128 × 260) followed by five cycles (106 × 106 × 260, 106 × 106 × 190, 106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11). The non-cubic 2:3 CNN:CycleNet model has a convolution of 200 filters, followed by two CNN cycles (equivalent to 6 convolutions of output features 200,200,200,190,170,159) and three CycleNet cycles (106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11).
CycleNet performs better than the baseline, but its accuracy is significantly lower when compared to MobileNet (Howard et al. (2017)), which has a slightly larger size. We experiment with larger models, and we followed two approaches to increase performance up to a comparable level. First, similar to our CIFAR-10 experiment, we try a non-cubic architecture and find a 10% gain in relative performance. Second, following previous work (Ramachandran et al. (2019)), we hypothesise that image classification benefits from standard convolutions in the first layers, therefore we vary the number of initial cycles with standard convolutions. Fig.3b (right) shows that a model with two cycles of standard convolutions (2:3) has the best performance, and combining this with a noncubic architecture, the model approaches the performance of MobileNet with a similar number of parameters. This motivates using CycleNet and the CNN baseline for testing our working hypotheses in the next sections.
We used relatively small CycleNet models, in order to have a baseline that can be easily compared with, and to be able to explore CycleNet with a variety of choices of hyperparameters. It would be interesting to look at whether state of the art performance can be achieved on datasets of higher resolution and larger models, for example by introducing residual connections. This would however require significantly more computational resources.

[B.2 CALCULATION OF THE RECEPTIVE FIELD]
To calculate the receptive field, for a given node in the network and a given input image, we compute the gradient of the activation of that node with respect to the image. The saliency map is a grayscale image, obtained by taking the absolute value of the gradient for each pixel and summing across the three color channels (Simonyan et al. (2014)). The saliency map is normalized, such that the sum across pixels is one, and is interpreted as a 2-dimensional probability density. The receptive field size is computed by the square root of the total variance (the trace of the covariance matrix) of that density. We use the total variance since it quantifies the scatter of the saliency map along both axes independently. The receptive field shape is defined as the ellipsoid containing 3 standard deviations on both axes.

[B.3 THE PATHFINDER CHALLENGE]
The Pathfinder dataset is composed of 900, 000 training and 100, 000 test images, and has two classes. We use the standard cross-entropy loss with L2 regularization. In our experiment, in addition to the first convolution, CycleNet has a single cycle at 128 × 128 × 128 features, global pooling (as in Linsley et al. (2019)) and a dense layer. The experiments run on a single GPU (GeForce RTX 2080 Ti), Adam optimizer was used with learning rate 0.001. We use 5 times more distractors than the base dataset.
We generate three datasets of increasing difficulty, for different path lengths: n = 6, 9, and 14, at 128 × 128 resolution, using the code available at https://github.com/drewlinsley/pathfinder. We study performance of one-cycle models as a function of kernel size, k = 4, 8, 12 and 20.
For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease (see Figure 5). For kernel size 20, which allows convolutions to integrate long-range, the CNN is able to learn the task. However, for this kernel size, the accuracy values reported here are not final because the models had not converged.
The training stopped before convergence due to limitations in computational resources.
Note that CNNs typically have kernel size smaller than 10, since the number of parameters becomes substantial for larger kernels.","[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual."
Cyclic orthogonal convolutions for long-range integration of features,868DWd46dv2.json,"This article proposes a convolutional network architecture to address the lack of connectivity between features of spatially distant locations within a layer. The authors propose CycleNet, which consists of the concatenation of convolutional operations on the three pairs dimensions - (x, y), (x, z) and (y, z) - instead of only on (x, y). The paper studies several properties of CycleNet compared to some baselines models: the performance on CIFAR-10, the receptive field size of the learnt features and the performance on the Pathfinder challenge.

This is a well written paper, which presents a simple and reasonable idea to address a weakness of standard convolutional models - the lack of connectivity between distant pixels or features. While the analysis of the proposed architecture does not outperform standard models on image classification tasks, the performance is close enough and, importantly, the experiments show the advantageous properties of CycleNet on other dimensions beyond classification accuracy, such as the receptive field size of the features and the performance on other tasks such as Pathfinder. I think the choice of experiments is sound and extensive enough for a workshop submission. Therefore, I have a generally positive impression of this paper and I recommend its acceptance to the SVRHM 2021.

Nonetheless, I have a few comments about potential weakness or aspects that could be improved, as well as some questions. First, I believe that the paper should more transparently present the less positive results of CycleNet from the experimental setup. For example, the authors show the performance on CIFAR-10 compared to a basic CNN baseline in Figure 3 of the main body of the paper, but leave for the supplementary material the results on ImageNet, where CycleNet achieve comparably worse classification accuracy. I argue that this introduces an analytical bias that can be misleading. Second, I think the paper could be improved by more in-depth discussion of the limitations of the proposal and directions for future work. Finally, I would also have appreciated a longer discussion on what the gap is that this new architecture aims to fill if the issue it addresses can be mitigated or solved by architectures such as transformers. I encourage the authors to consider these changes for their camera-ready version, if the paper is accepted.","[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual.

[CAPTIONS]
Table 1: Figure 2 :2Figure2: Illustration of CycleNet. (a) One cycle is defined as three convolutions performed in sequence: first on (x, y) (green), second on (x, z) (red), and third on (y, z) (blue), each convolution is followed by BatchNorm, ReLU and Dropout (not shown). (b) The full model consists of a stack of cycles, plus a first convolutional layer and a final dense layer.
Table 2: Figure 3 :3Figure 3: Test accuracy on image classification. (a) Performance of CycleNet and the CNN baseline on CIFAR-10, as a function of the number of parameters. Each point corresponds to a different network depth, from one to six cycles, and is an average of five experiments with identical optimized hyperparameters. ResNet-18 is taken from (He et al. (2016)). (b). Left: performance of CycleNet, the CNN baseline and 2:3 hybrid model on ImageNet (128×128) as a function of the number of parameters. MobileNet is taken from (Howard et al. (2017)). Right: we progressively substitute CycleNet with standard convolutional cycles. Non-cubic: experiments with non-cubic tensor shape. Details about different CycleNet architectures an be found in appendix B.1.
Table 3: Figure 4 :4Figure 4: Receptive field sizes and shapes. (a) Normalized receptive field size as a function of activation depth (number of layers) of CycleNet and the CNN baseline, for ImageNet and CIFAR-10. Each point is the mean and standard deviation computed on 100 receptive fields chosen at random. CycleNet achieves large size after one cycle (3 layers), while the size increases with depth in the baseline, and achieves similar size after 18 layers. (b) Sample saliency maps in CycleNet (top) and the baseline (bottom) on ImageNet, after 3 (left) and 15 layers (right). Yellow ellipses show the receptive field shape, covering 3 standard deviations.
Table 4: Figure 5 :5Figure 5: Pathfinder challenge. Each panel shows the accuracy of CycleNet and the CNN baseline as a function of kernel size. Task difficulty increases from left to right (path length n =6, 9, 14). Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[INTRODUCTION]
Several computer vision tasks require capturing long-range dependencies between features. For example, in order to recognize a teapot, it is necessary to identify and integrate several of its parts in their correct spatial relationship, e.g. a spout, a handle, a lid, while each part by itself is usually not enough to determine the object class. Convolutional layers have proven very useful at several computer vision tasks such as classification, and they are at the core of most state-of-the-art neural networks (LeCun et al. (2015)). A convolution transforms each pixel depending on a few neighboring pixels, and the transformation is shared across all pixels; however, the small size of the neighbourhood implies that in a single layer features at distant locations cannot be integrated. In deep convolutional networks, the receptive field size increases sub-linearly with depth (Luo et al. (2017)), therefore a large number of layers is necessary for integrating features across long distances. We propose a novel neural network architecture that retains the efficient parameterization of convolutions, while promoting long-range interactions of distant features.
Similar to the human visual system, individual 'neurons' within each CNN have receptive fields with size increasing with the number of layers. However, while the human visual system achieves large receptive fields within a handful of layers, CNNs need many more (Luo et al. (2017)). In fact, the number of layers keeps expanding to 100s or 1000s of layers for state of the art CNN models (Tan & Le (2019); He et al. (2016)). A neuron of the visual system connects not only to other neurons responding to similar spatial locations, but also to neurons at distant locations, provided that they share similar visual features, such as edge orientation (see Fig. 1a). These connections not only enlarge the receptive fields, but they also facilitate solving tasks that require long-range integration of features. For example, they allow tracking curved contours (Fig. 1b). The neurons responsible for the long-range connections are excitatory pyramidal cells, which are known to have a long range connections in the cortex. Instead, inhibitory GABA-ergic cells do not have this property.
Our model architecture agrees with these biological principles of information processing taking place in the brain. Given a tensor of horizontal (x) and vertical (y) coordinates and features (z), we propose to apply convolutions not only on (x, y) coordinates, but also on (x, z) and (y, z) in sequence. These orthogonal convolutions allow the interaction of 'neurons' at distant locations, provided that they share similar visual features, similarly to the human visual system. We show that, after a cycle of three convolutions over the three axes, each element of the tensor depends on all elements of the input, and features at all locations have a chance to interact. Our model, named CycleNet, is obtained by adding a permutation of the axes to a convolution, and therefore is easy to compare with a standard CNN. In order to evaluate our model, we compare it with a CNN baseline network with identical architecture, i.e. same number of parameters and tensor shape in all layers.
In this paper we show that Cyclenet: outperforms the CNN baseline at image classification on CIFAR-10 and ImageNet (Krizhevsky (2009), Deng et al. (2009)), and approaches the performance of ResNet and MobileNet (He et al. (2016), Howard et al. (2017)) with a similar number of parameters; achieves the maximum receptive field size after a single cycle and outperforms the baseline CNN in transfer learning to stylised data, suggesting the use of more features dependent on shape rather than texture; outperforms the baseline by a large margin at the Pathfinder challenge, a task inspired by the study of human visual perception where CNNs are known to fail dramatically (Linsley et al. (2019)).   (Linsley et al. (2019)). Each image has two circles attached to a path. The goal of this task is to classify images into connected and disconnected, two examples shown for each class.

[RELATED WORK]
A few recent papers showed that deep convolutional networks struggle to learn features integrating large spatial domains. For instance Luo et al. (2017) showed that the receptive field size of a deep CNN is smaller than the sum of its kernel sizes. Geirhos et al. (2019) showed that ImageNet-trained ResNet relies on small image patches and textures, rather than object shapes, and does not transfer well to stylised images. Linsley et al. (2019) showed that ResNet struggles on the Pathfinder challenge, a simple task that requires integrating features over long distances.
Other methods have been proposed to learn large scale features. Pooling layers increase the receptive field size in CNNs, but they lose a significant amount of information, including the spatial relationship between features (Boureau et al. (2010)). Multiscale pyramids use kernels of different sizes arranged in parallel, but larger kernels have lower resolution in order to keep a reasonable number of parameters (Farabet et al. (2013)). Deformable convolutions adaptively learn the shape of the kernel, at the cost of an increased complexity of the model (Dai et al. (2017)). Dilated convolutions increase the kernel size along network depth (Yu & Koltun (2015)) and they are equivalent to a tensor decomposition (Huszar (2016)). Tensor decompositions have been widely used to reduce the complexity of convolutional and dense layers (Kuzmin et al. (2019); Novikov et al. (2015)). We show below that one cycle of CycleNet is equivalent to a decomposition of a dense layer, in the simple case of 1 × 1 convolutions and linear activations, but not in the general case. CycleNet is also different from a 3D convolution (Ji et al. (2013)), since spatial coordinates are fully connected in the second and third layers of a cycle.
Among non-convolutional architectures, self-attention networks naturally capture long-range dependencies. Initially designed for natural language processing (Vaswani et al. (2017)), transformers were recently shown to exhibit good performance on vision tasks ). Similar to self-attention networks, CycleNet breaks translational symmetry and has the best performance on ImageNet when standard convolutions are included in the first layers (Ramachandran et al. (2019)).

[MODEL: A CYCLE OF THREE ORTHOGONAL CONVOLUTIONS]
In this section we describe a single cycle, the basic building block of CycleNet, which is composed of three layers arranged in sequence (Fig. 2a). We start by describing a single convolutional layer. We denote the input tensor as I(x, y, z), at coordinates x (horizontal), y (vertical) and feature z. The output tensor S is equal to:
S(x, y, z) = dx,dy,z K(dx, dy, z, z )I(x + dx, y + dy, z ).(1)
where K denotes the convolutional kernel. We assume zero padding and kernel stride equal to one, while other parameters vary in different experiments (kernel size, input and output tensor shapes). The convolution operation is local: the output at a given location (x, y) depends only on the input displaced by dx, dy, spanning neighboring pixels up to the kernel size, which is typically much smaller than the size of the input. On the other hand, features z are all-to-all fully connected. This is a standard convolutional layer and is depicted as the first layer of Fig. 2a (green). Convolutional layers are usually stacked and interleaved with other types of layers, such as nonlinearities, down(up)sampling, skip connections, normalization, regularization, etc. In this study, we use BatchNorm (Ioffe & Szegedy (2015)), ReLU (Glorot et al. (2011)), Dropout (Srivastava et al. (2014)), and no skip connections. In order to promote long-distance integration, we propose to apply convolutions not only in the (x, y) plane, but also in the (x, z) and (y, z) planes. With some abuse of notation, we denote I as the output of the previous layer, and S as the output of the current layer. The second layer in a cycle is given by the following convolution,
S(x, y, z) = dx,y ,dz K(dx, dz, y, y )I(x + dx, y , z + dz),(2)
also illustrated in Fig. 2a (red). Here, vertical coordinates y are fully connected. Subsequently, after another BatchNorm, ReLU and Dropout, we apply a convolution in the (y, z) plane:
S(x, y, z) =
x ,dy,dz K(dy, dz, x, x )I(x , y + dy, z + dz).
(3) This is the third and final layer in a cycle, illustrated in Fig. 2a (blue), and is also followed by BatchNorm, ReLU and Dropout. Here, horizontal coordinates x are fully connected. The sequence of three convolutions along the three different axes constitutes a cycle. Note that a cycle could be defined with a different ordering of the three convolutions, but we did not explore other configurations.
Further architecture details about the long range integration, the number of parameters, the comparison with a convolutional baseline and the scaling to deeper networks can be found in Appendix A.
3 Experiments

[RECEPTIVE FIELDS]
We train CycleNet and the baseline CNN on CIFAR-10 and ImageNet, benchmarks for image classification over which CNNs have been extensively optimized. Experimental results can be seen in Figure 3 and detailed description and discussion of these experiments can be found in appendix B.1. Figure 3 shows that CycleNet has a higher accuracy than the baseline and is comparable to state of the art CNN models of similar size. Is this because CycleNet learns substantially different representations of the images compared to standard CNN? We investigate this using receptive fields. At the end of a cycle, each element of the output tensor may depend on all elements of the input tensor (see appendix A.1), thus CycleNet is expected to have large receptive fields. In contrast, the receptive fields of CNNs are usually small (Luo et al. (2017)). Here, we compute the receptive fields at the output of each cycle of our models with 18 layers for CIFAR and 15 layers for ImageNet: first, we compute saliency maps following Simonyan et al. (2014), then we use those maps to compute receptive fields following Luo et al. (2017). Details can be found in section 3.3.
Figure 4a shows receptive field size as a function of depth, where the receptive field is normalized by the resolution (32 for CIFAR, 128 for ImageNet). Each bar is computed across 100 receptive fields sampled at random, 10 activations times 10 images. As expected, CycleNet achieves a large receptive field size after one cycle (3 layers), while the CNN baseline increases the receptive field size with depth, and approaches the size of CycleNet after 18 layers. Fig. 4b shows a few visual examples of receptive field shapes for both CycleNet and the CNN baseline at different layer positions.
Larger receptive fields are fundamental in tasks requiring long-range integration of features and the features developed implicitly rely on more global patters, favouring classification of images by shape rather than texture. These are investigated in Section 3.2 and section 3.3. 

[PATHFINDER CHALLENGE]
In order to test whether large receptive fields help integrating long-range features, we evaluated CycleNet on the Pathfinder challenge (Fig. 1d). This task is inspired by the study of human visual perception, and requires tracking paths over long distances to distinguish whether two white circles are connected. Deep convolutional networks struggle on this task (e.g. ResNet50), because of their limited ability of integrating features over long distances (Linsley et al. (2019)). Since CycleNet integrates all pixels after one cycle, we predict that it should perform well on this task. Linsley et al. (2019) proposes a biologically-inspired horizontal gated recurrent neural network that performs 100% on the task, but their model is hard to compare with standard CNNs. Details about the Pathfinder dataset as well as the architectures used can be found in appendix B.3.
Figure 5 shows the performance of CycleNet and the CNN baseline in 3 challenges of increasing difficulty, i.e. increasing path lengths: n = 6, 9, and 14. For kernel sizes 4, 8, and 12, the CNN baseline does not learn the task. It performs at chance level (50%), except in one experiment of path length 6. On the other hand, CycleNet shows good accuracy, up to nearly 100%, suggesting that it is able to take advantage of its larger receptive fields. Furthermore, its performance decreases with path length and increases with kernel size. Note that large receptive fields may not be enough to solve this task, it also requires an increasing level of expressivity at larger path lengths. This is provided by a larger kernel size in CycleNet. For kernel size 20 the CNN baseline is able to perform the task, sometimes better than CycleNet; note that however for this kernel size the training has not converged as we were limited by computational resources and the models were very large due to the size of the kernel. We can only observe that for this case the CNN converged faster. At this size, a single kernel covers a substantial part of the image and is effectively long-range. However, the number of parameters is large in this case and the efficiency of the parameterization is lost. . Each point is one experiment, the mean and standard deviation across multiple experiments is shown by error bars. For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease. For learning the task the CNN needs to use a kernel size of 20, much larger than commonly used CNN's kernels.

[STYLISED IMAGES]
Given that CycleNet has large receptive fields, we hypothesise that it should classify images by the shape rather than texture of objects. Geirhos et al. (2019) designed new benchmarks to explore the shape vs texture bias of deep neural networks trained on ImageNet (IN). In the Stylised ImageNet dataset (SIN), the texture of each image is replaced with a randomly selected painting style (Fig. 6, top). In the Cue-Conflict dataset (CC), a few images are generated by iterative style transfer from selected textures consisting of patches of other classes (Fig. 6, bottom). We predict that our IN pre-trained CycleNet should transfer better than the baseline to both SIN and CC. We generate SIN following the scripts at https://github.com/rgeirhos/Stylized-ImageNet, and data for CC is sourced from https://github.com/rgeirhos/texture-vs-shape. Both experiments are evaluated at resolution 128x128.
Table 1 shows evaluation top-5 accuracy on Stylised ImageNet and top-1 accuracy on the Cue-Conflict dataset. The results confirm that CycleNet transfers better to both datasets. Although the overall accuracy on ImageNet is higher for a hybrid network where the first two cycles have standard convolutions (2:3), the best transfer to stylised images is obtained by CycleNet, where all cycles use orthogonal convolutions (0:5). These results confirm that large receptive fields are instrumental for biasing a neural network towards shape, and that the base performance on ImageNet by itself is not a good predictor of such bias. 

[DISCUSSION]
We proposed a biologically inspired neural network architecture, CycleNet, that achieves better performance than a CNN baseline in classification tasks, and it develops a significantly different representation of the input image, where node activations have large receptive fields and are thus able to represent large portions of an image. This is in agreement with the human visual system that achieves large receptive fields within few layers and in sharp contrast with CNNs, which have smaller receptive fields. CycleNet transfers better than the CNN baseline to stylised images, suggesting that the large receptive fields bias the model towards shape rather than texture of objects, and performs much better at the Pathfinder task, which requires long-range integration of features.
We emphasize that CycleNet loses translational symmetry, which is considered a strong feature of CNNs. However, recent evidence suggests that this property may not be crucial. Symmetries do not have to be hard coded in the architecture: they can be learned by stochastic gradient descent (Achille & Soatto (2018)) and data augmentation (Taylor & Nitschke (2017)). Furthermore, several nontranslational symmetric architectures recently achieved near state-of-the-art performance in image classification (Ramachandran et al. (2019)). The good performance of CycleNet on classification adds to this line of research, suggesting that built-in translational symmetry may not be necessary.
Performance in other tasks is likely to benefit from long-range integration, such as image segmentation, generation, reconstruction, etc. Future work may focus on testing CycleNet on those tasks.
A Appendix: Architecture details

[A.1 LONG-RANGE INTEGRATION]
We combine Eqs.1, 2 and 3 to express the effect of one cycle. We consider the simple case of 1 × 1 convolutions (thus dx = dy = dz = 0), and we ignore all nonlinearities in between layers, obtaining S(x, y, z) =
x ,y ,z
K 3 (x, x )K 2 (y, y )K 1 (z, z )I(x , y , z ).(4)
where K 1 , K 2 and K 3 are the three successive kernels. Therefore, after a cycle of three convolutions, each element of the output tensor depends on all elements of the input tensor. It is straightforward to check that, even for k × k convolutions (k > 1) and nonlinearities, each element of the output tensor may still depend on all elements of the input tensor. However, it is not guaranteed that CycleNet makes use of the entire receptive range after training on a given task, and therefore we test this hypothesis in Section 3.1.
Note that if we substitute K(x, y, z, x , y , z
) = K 3 (x, x )K 2 (y, y )K 1 (z, z )(5)
into Eq.4, then the expression becomes equivalent to a dense layer, where the kernel is decomposed into three factors. Thus, Eq.4 is equivalent to a tensor decomposition. However, CycleNet is different from a tensor decomposition when nonlinearities are included between layers, and in the more general case for convolutions of kernel size k > 1.

[A.2 NUMBER OF PARAMETERS AND OPERATIONS]
The expression to calculate the number of parameters in CycleNet is the same as in a standard convolutional layer, simply the convolutions are performed along a different axis. We denote by k the size of the kernel, by X in , Y in , Z in the shape of the input tensor and by X out , Y out , Z out the shape of the output tensor. The number of parameters of the first layer is k 2 Z in Z out , each one operated on X out Y out times. Similarly, the number of parameters of the second layer is k 2 Y in Y out , times X out Z out operations, and for the third k 2 X in X out parameters, times Y out Z out operations. Note that the number of parameters of CycleNet may differ from a standard convolution if the number of horizontal pixels X or vertical pixels Y is different from the number of features Z.

[A.3 CUBIC BASELINE]
In order to compare the performance of CycleNet with standard CNNs, we define a baseline to study the relative improvement introduced by orthogonal convolutions. The baseline is exactly identical to CycleNet, except that we do not permute the axes, instead we always perform convolutions along the same axis. This corresponds to a standard stack of convolutional layers, interleaved with BatchNorm, ReLU and Dropout. However, in order to keep the number of parameters equal between CycleNet and the CNN baseline, we consider cubic tensors in most of our experiments, i.e. in which the number of horizontal pixels X, vertical pixels Y and the number of features Z are equal within each cycle. This is a strong constraint that makes such baseline quite specific, but we still refer to it simply as ""baseline"". It is likely that a better performance could be obtained, both by CycleNet and the CNN, without this cubic constraint. Therefore, we also perform experiments on CycleNet with non-cubic tensors, and compare its performance with other convolutional architectures, e.g. ResNet and MobileNet, which are also non-cubic.

[A.4 DEEP NETWORK]
As illustrated in Fig. 2b, we stack cycles in sequence to construct a deep network model. We add a standard convolution as a first layer of the network, to obtain the appropriate number of features to be used in the first cycle, and a dense layer as the last layer, thus reducing the (flattened) tensor to the size of the final output. In cubic models, tensor width is changed from one cycle to the next using tri-linear interpolation. In non-cubic models, tensor shape is changed by controlling the output size of the fully connected coordinate at each layer. In the case of the Pathfinder challenge, we use global pooling before the dense layer, in order to match the baseline architectures of Linsley et al. (2019). Code will be made available to the reviewers in the supplementary material, and to the public upon acceptance of the paper.

[B APPENDIX: EXPERIMENTAL DETAILS B.1 IMAGE CLASSIFICATION]
We start by training CycleNet on CIFAR-10 and ImageNet (at 128 × 128 resolution), standard benchmarks for image classification on which CNNs have been extensively optimized (Fig. 1a,b). We do not aim at beating the current state-of-the-art. Our goal is to compare CycleNet with standard CNNs of equal or similar size, and use them to test our working hypotheses in the next sections. We use relatively small models for ease of implementation, in order to explore a variety of hyperparameters. The CIFAR dataset consists of 60,000 32x32 colour images in 10 classes, 50,000 training images and 10,000 test images. We use the standard cross-entropy loss with L2 regularization. We train on a single GPU (GeForce RTX 2080 Ti) using the Keras API in TensorFlow. The training cycle is 300 epochs and the network hyperparameters are optimized by running twelve experiments varying dropout rates and L2 lambda. RMSprop was used with batch size 64 and initial learning rate 0.001, which is divided by 10 when the error plateaus. Data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. The first convolution outputs 80 features, and the deepest architecture has the following tensor widths in successive cycles: 75, 60, 45, 30, 15, 5. For the shallower networks, we use the smaller end of the sequence, e.g. width 5 for one cycle, widths 15, 5 for two cycles, widths 30, 15, 5 for three cycles and so on and so forth. The non-cubic CycleNet starts with a convolution with 100 features (32 × 32 × 100) followed by four cycles (45 × 45 × 100, 30 × 30 × 66, 15 × 15 × 33, 5 × 5 × 8). Fig. 3a shows the performance of models of up to six cycles (18 layers, k = 3) on CIFAR-10. CycleNet performs better than the CNN baseline, and the non-cubic CycleNet approaches the performance of a ResNet-18 (He et al. (2016)) with a similar number of parameters. These results suggest that long-range integration may be instrumental for image classification, but their significance is limited by the small resolution of CIFAR-10 (32 × 32).
To confirm our results on higher resolution, we next tested CycleNet on ImageNet (128 × 128). Fig. 3b (left) shows the performance of five-cycle models (15 layers, k = 3) on ImageNet. The ImageNet dataset consists of 1.28 million training and 50,000 test images in 1000 classes. We use 128 × 128 resultion, and the standard cross-entropy loss. We train on a 24 GPUs (GeForce RTX 2080 Ti) distributed training system (Sergeev & Del Balso (2018)) for 120 epochs. Adam optimizer was used with effective batch size 384 and learning rate 0.024 (using warm-up as in (Goyal et al. (2017)), which is divided by 10 when the error plateaus. Similar to CIFAR, data augmentation is implemented with 20 • rotation range, up to 20% shift of height and width, and horizontal flip on 50% of the data. All convolutions have kernel size k = 3. In the cubic models, the first convolution outputs 128 features, followed by cycles of the following tensor widths: 106, 106, 106, 106, 12. The non-cubic CycleNet starts with a convolution with 260 features (128 × 128 × 260) followed by five cycles (106 × 106 × 260, 106 × 106 × 190, 106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11). The non-cubic 2:3 CNN:CycleNet model has a convolution of 200 filters, followed by two CNN cycles (equivalent to 6 convolutions of output features 200,200,200,190,170,159) and three CycleNet cycles (106 × 106 × 159, 106 × 106 × 159, 10 × 10 × 11).
CycleNet performs better than the baseline, but its accuracy is significantly lower when compared to MobileNet (Howard et al. (2017)), which has a slightly larger size. We experiment with larger models, and we followed two approaches to increase performance up to a comparable level. First, similar to our CIFAR-10 experiment, we try a non-cubic architecture and find a 10% gain in relative performance. Second, following previous work (Ramachandran et al. (2019)), we hypothesise that image classification benefits from standard convolutions in the first layers, therefore we vary the number of initial cycles with standard convolutions. Fig.3b (right) shows that a model with two cycles of standard convolutions (2:3) has the best performance, and combining this with a noncubic architecture, the model approaches the performance of MobileNet with a similar number of parameters. This motivates using CycleNet and the CNN baseline for testing our working hypotheses in the next sections.
We used relatively small CycleNet models, in order to have a baseline that can be easily compared with, and to be able to explore CycleNet with a variety of choices of hyperparameters. It would be interesting to look at whether state of the art performance can be achieved on datasets of higher resolution and larger models, for example by introducing residual connections. This would however require significantly more computational resources.

[B.2 CALCULATION OF THE RECEPTIVE FIELD]
To calculate the receptive field, for a given node in the network and a given input image, we compute the gradient of the activation of that node with respect to the image. The saliency map is a grayscale image, obtained by taking the absolute value of the gradient for each pixel and summing across the three color channels (Simonyan et al. (2014)). The saliency map is normalized, such that the sum across pixels is one, and is interpreted as a 2-dimensional probability density. The receptive field size is computed by the square root of the total variance (the trace of the covariance matrix) of that density. We use the total variance since it quantifies the scatter of the saliency map along both axes independently. The receptive field shape is defined as the ellipsoid containing 3 standard deviations on both axes.

[B.3 THE PATHFINDER CHALLENGE]
The Pathfinder dataset is composed of 900, 000 training and 100, 000 test images, and has two classes. We use the standard cross-entropy loss with L2 regularization. In our experiment, in addition to the first convolution, CycleNet has a single cycle at 128 × 128 × 128 features, global pooling (as in Linsley et al. (2019)) and a dense layer. The experiments run on a single GPU (GeForce RTX 2080 Ti), Adam optimizer was used with learning rate 0.001. We use 5 times more distractors than the base dataset.
We generate three datasets of increasing difficulty, for different path lengths: n = 6, 9, and 14, at 128 × 128 resolution, using the code available at https://github.com/drewlinsley/pathfinder. We study performance of one-cycle models as a function of kernel size, k = 4, 8, 12 and 20.
For any kernel sizes 4, 8, 12, the baseline does not learn the task (chance = 0.5), while CycleNet shows a good accuracy that increases with kernel size and task ease (see Figure 5). For kernel size 20, which allows convolutions to integrate long-range, the CNN is able to learn the task. However, for this kernel size, the accuracy values reported here are not final because the models had not converged.
The training stopped before convergence due to limitations in computational resources.
Note that CNNs typically have kernel size smaller than 10, since the number of parameters becomes substantial for larger kernels.","[TITLE]
Cyclic orthogonal convolutions for long-range integration of features

[ABSTRACT]
In Convolutional Neural Networks (CNNs) information flows across a small neighbourhood of each pixel of an image, preventing long-range integration of features before reaching deep layers in the network. Inspired by the neurons of the human visual cortex responding to similar but distant visual features, we propose a novel architecture that allows efficient information flow between features z and locations (x, y) across the entire image with a small number of layers. This architecture uses a cycle of three orthogonal convolutions, not only in (x, y) coordinates, but also in (x, z) and (y, z) coordinates. We stack a sequence of such cycles to obtain our deep network, named CycleNet. When compared to CNNs of similar size, our model obtains competitive results at image classification on CIFAR-10 and ImageNet datasets. We hypothesise that long-range integration favours recognition of objects by shape rather than texture, and we show that CycleNet transfers better than CNNs to stylised images. On the Pathfinder challenge, where integration of distant features is crucial, CycleNet outperforms CNNs by a large margin. Code has been made available at: https://github.com/netX21/Submission * equal contribution 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual."
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR,boIlH5V81C8.json,"Please explain your method with an example / or a flow chart
""However, in our solution instead of a fixed cost for substitutions, we allow it to be dynamically computed as
the character error rate (CER)""

Can you compare your approach to other approaches, is it better? Is there any take home message?

Fix reference capitalisation

Per Erik Solberg, Pablo Ortiz, Phoebe Parsons, 942 Torbjørn Svendsen, and Giampiero Salvi. Improving generalization of norwegian asr with limited linguistic resources. Submitted for publication.
=>
Per Erik Solberg, Pablo Ortiz, Phoebe Parsons, 942 Torbjørn Svendsen, and Giampiero Salvi. Improving generalization of Norwegian asr with limited linguistic resources. Submitted for publication.
","[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Instances of ""r"" becoming ""l"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 2: Figure 2 :2Figure 2: Instances of ""l"" becoming ""r"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 3: for the NB Tale data and Figure 4 for Rundkast. The first column (a) shows results from the 300m parameter model, second column (b) shows results from 1b parameter model. Darker colors represent higher errors.For both the NB Tale and Rundkast corpora we,
Table 4: Example of the vectors for ""k"", ""g"", and ""n"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 5: Example of the vectors for ""a"", and ""e"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 6: 

[INTRODUCTION]
Automatic Speech Recognition (ASR) has, like all machine learning tasks, struggled with generalization. That is, a model will perform well on the task and data it was trained on but when presented with new examples, especially examples that differ in some dimension from the training data, the model will perform markedly less well. In the task of ASR, this means that models often struggle with generating correct transcriptions for speakers whose age, gender, or dialect differs from that of the speakers on which the model was originally trained. Of specific focus in this paper is the impact of dialect on a modern ASR system.
Dialect information has been used in different ways in ASR. In some applications, such as Dialect Identification (DID), the goal is to correctly identify the dialect for a given sample of speech. Hämäläinen et al. (2021), for example, used a combination of speech and text features to perform DID. In other cases, DID is combined with ASR systems to improve transcription accuracy. For example, Zhang and Hansen (2018) used bottleneck features extracted via unsupervised deep learning to perform DID for both Chinese and Arabic. Similarly, Imaizumi et al. (2022) used a multitask model for both DID and ASR. This multitask approach outperformed the single task systems on both DID and ASR.
Beyond DID, the behavior of ASR systems has been analyzed with respect to dialectal speech (as we do in this paper). This in order to explore phonetic phenomena, as well as to gain insights into the way those complex systems work. In these studies, even when dialectal information is not an explicit target, there is still an interest to understand what phonetic and dialectal information has been captured in ASR models. With traditional ASR models, this investigation has been fairly straightforward as these models have consisted of three semi-independent components: the acoustic model, the language model, and the lexicon. Because of the separate acoustic models within these multi-component models, one could, for example, perform clustering on the model parameters themselves such as (Salvi, 2003a(Salvi, ,b, 2005. In this work, Salvi performed clustering on the acoustic model features and correlated the resulting clusters with known dialectally realized phonemes. Instead of directly using an acoustic model from an ASR system, Chen et al. (2014) adapted the concept of an HMM acoustic model to automatically discover dialect-specific phonetic rules.
Unlike multi-component ASR systems, investigating modern, end-to-end models for phonetic and dialectal information is quite different.
Whereas parameters from an acoustic model may be extracted and used independently, the acoustic information in an end-to-end model cannot be so easily excised. This design makes it more challenging, but not impossible, to investigate what acoustic information is captured where in the network. Belinkov and collaborators used the output from each layer of an end-to-end system to train phonetic, grapheme, and articulatory classifiers (Belinkov and Glass, 2017;Belinkov et al., 2019). Prasad and Jyothi (2020) investigated dialectal information captured by an end-to-end system using not only layer-wise classification but also gradient and information-theoric analysis. All of these works are focused on analyzing the networkinternal representations detached from actual network output.
The output from ASR models is constrained by the model architecture. Traditional ASR models with lexicons are bound to output only words contained within that lexicon. This means that all transcripts generated by these models contained only real, known words even if the transcribed output did not necessarily match the word that was spoken. Additionally, these models do not allow for acceptable variation in spelling. For example, the word, ""favorite,"" would always be spelled ""favorite"" never ""favourite,"" even if the latter might better reflect the preference of a British English speaker. Conversely, these newer end-to-end architectures, trained using connectionist temporal classification (CTC) loss, produce output at the character instead of word level. This permits the model to create novel words and spellings, potentially better reflecting the phonetic realization of the spoken word. Given that CTC models are allowed to generate novel spellings, there exists the potential that dialectal information will be captured by the model output itself via non-standard spellings.
The goal of this paper is to investigate whether dialectal acoustic information can impact spellings with an end-to-end model. In order to test this, we used wav2vec 2.0 (Baevski et al., 2020) to generate transcriptions of Norwegian speech. We then performed an analysis of the resulting transcripts for captured dialectal knowledge via a dialectalregion based evaluation of character error patterns. From this analysis we are able to see known Norwegian dialectally-based phonological patterns, specifically around ""r"" and ""l"" confus-ability and stop consonant voicing. Thus we illustrate that strong enough acoustic dialectal cues can effect the character output of an end-to-end ASR system.

[NORWEGIAN LANGUAGE AND DIALECTS]
In this paper, we focus our analysis on the Norwegian language. Though spoken by a relatively small population of a little over 5 million speakers, Norwegian contains many dialects differentiated in phonology, syntax, and lexicon. In addition to dialectal variation, Norwegian also maintains two official written standards: Bokmål and Nynorsk; though neither written standard directly corresponds with a spoken variant. Furthermore, Norway does not recognize any official language standard. Indeed, people are encouraged to use their preferred written standard and native dialect in all aspects of work and life.
The variety in dialects stems from Norway's challenging and rugged topography that has historically forced the populace to organize into many, smaller communities. Over time, the diversity we see in Norwegian dialects developed in these small, isolated communities. As described by phoneticians, there now exist large dialectal phonetic variations ranging from infinitive verb endings to palatalization of consonants, to /r/ and /l/ realizations, to the various pronunciations for the personal pronoun for ""I"", jeg -ranging from [jaei] to [eg] to [i] and more (Skjekkeland, 1997).
While the number of specific Norwegian dialects is quite large, we can group these dialects into larger dialect groups for the purpose of this investigation. These grouping could be either into the regional names used by Skjekkeland or into the even larger, cardinal regions of ""East,"" ""West,"" ""North,"" ""South,"" and ""Mid."" The analysis outlined in this paper relies on these cardinal regions.

[EXPERIMENTAL SETUP AND DATA]
In order to investigate the impact of dialect on an end-to-end ASR system, a well-performing baseline model was required. Therefore, we used three models trained by the Norwegian National Library AI Lab and released publicly on the Hugging Face repository for our analysis 123 . The first model contained one billion parameters and was originally trained on the XLS-R (Babu et al., 2021). It was then fine tuned using the Norwegian Parliamentary Speech Corpus (NPSC) to transcribe Norwegian Bokmål text. The other two models were fine tuned from the 300 million parameter VoxRex model (Malmsten et al., 2022). One of these 300 million parameter models was fine-tuned to transcribe Bokmål, the other Nynorsk. All models use a 5-gram word-based language model. In all cases, the NPSC corpus was used to fine-tune the models (Solberg and Ortiz, 2022). When evaluated against the NPSC corpus, the Norwegian AI lab reports a word error rate (WER) of 6.33% for the 1 billion parameter model, 7.03% for the 300 million parameter Bokmål model, and 12.22% for the Nynorsk model. These results indicate that these models will make excellent candidates for our analysis.
As stated earlier, the models to be used were trained on the NPSC. This consists of recordings from the Norwegian Parliament and thus the speech style can be considered mostly spontaneous, with perhaps slightly more planning than everyday speech. For analysis purposes, the NPSC was excluded. This is due to data sparsity in the NPSC test set. While the whole test set is acceptable for model evaluation, data becomes untenably sparse when considered dialect-by-dialect. Thus our analysis focuses on results from two unrelated and more dialectally robust corpora: Rundkast and NB Tale.
The Rundkast corpus consists of radio broadcasts from the Norwegian Broadcasting Corporation (NRK) (Amdal et al., 2008). These transcripts are in both Bokmål and Nynorsk which are treated separately for analysis in this paper. Dialectal annotations were added by the transcribers during corpus creation and are provided directly in the speaker metadata.
NB Tale is publicly available from the National Library of Norway's Language Bank and consist of recordings and transcripts of native and non-native speakers of Norwegian. All speech was transcribed using the Bokmål standard. Read

[WORD AND CHARACTER ALIGNMENT]
As our investigation into dialectal impact revolves around analyzing trends in character errors, we require an alignment between reference text and model-generated hypothesis text where words that only differ by a few characters are prioritized for alignment. While character error rate (CER) computed across a whole utterance is useful in understanding an aggregate of character errors, this method loses awareness of word boundaries. For example, ""også kalt"" and ""og såkalt"" would be aligned in whole-utterance CER with an insertion and a deletion of a space (resulting in ""og så kalt""). However, we prefer an alignment where we recognize that ""så"" was removed from the first word and ""så"" as added to the second word. Thus CER, as it is generally used across entire utterances, does not answer for our analysis purposes.
With traditional, word-level Levenshtein-based alignments, word similarity is not considered. Any pair of words that do not exactly match are treated as completely different. However, by considering word similarity, the resulting alignments can be used for analysis of broad trends of spellings (e.g., a word ending in ""a"" instead of ""e"") that can indicate dialectal impact.
To accomplish such an alignment, an extension to the traditional Levenshtein alignment was developed (Levenshtein, 1965). Typically edit costs are fixed at a value before alignment is computed. However, in our solution instead of a fixed cost for substitutions, we allow it to be dynamically computed as the CER between the two candidate words. This still ensures that there is no cost for aligning words that are the same while also preferring substitutions of similarly spelled words. voiced class nasal place rounding  Once word-level alignment is computed using the dynamic substitution cost, we can investigate spelling errors. To ensure characters within a word are aligned optimally, we continue to use the dynamic substitution cost idea and compute the substitution cost between characters as the Euclidean distance between two feature vectors. To support this, articulatory feature vectors were created for each letter in the Norwegian alphabet using the International Phonetic Alphabet (IPA) charts as a guide. Articulatory features were considered as indexes in the vector and the values correspond to the realization. For our work, consonants (see examples in Table 1) were defined and treated separately from vowels (see examples in Table 2). As the goal with these vectors is not to create an accurate grapheme-to-phoneme mapping, nor to perfectly illustrate all possible IPA nuance, but instead to align letters in a more logical way, these vectors were sufficient.
""k"" 0 0 0 5 0 ""g"" 1 0 0 5 0 ""n"" 1 0 1 2 0
To illustrate the necessity of these vectors, consider the word pair of inngang (meaning ""entrance"") and enkel (""easy""). Using a traditional alignment method 4 , where all characters substitutions have the same cost, an alignment like in 4 Alignment generated using the Python Levenshtein package:
https://github.com/maxbachmann/ python-Levenshtein reference i n n g a n g hypothesis e n k e l Table 3: A possible alignment between inngang and enkel, generated without accounting for character similarity. reference i n n g a n g hypothesis e n k e l Table 4: A possible alignment between inngang and enkel, generated by accounting for character similarity.
Table 3 is generated. However, using articulatory features as a distance, we are able to generate the alignment in Table 4 where ""g"" and ""k"" (only differing by voicing), ""a"" and ""e"" (both being front vowels), and ""n"" and ""l"" (both being sonorants) are aligned. While this solution is slightly phonologically flawed -wholly ignoring the di-and trigraphs that exist in Norwegian and instead treating the component letters individually, for example -these feature vectors do accomplish the goal of creating a logical character-level alignment. With confidence in our word and character alignment we can perform the investigation into character substitution trends that constitutes our results.

[WER BY DIALECT]
To first understand the general trend in recognition across dialects, the WER was calculated for each dialect across the whole of the Rundkast and NB Tale corpora. Transcriptions were generated using both the 300 million and 1 billion parameter Bokmål models for both corpora. Rundkast was further transcribed with the 300 million parameter Nynorsk model (since Rundkast actually contains Nynorsk utterances, unlike NB Tale).
As displayed in Table 5 that shows WER across both corpora and dialects, we can see WER values ranging from the low teens to nearly 40%. These values are markedly higher than the 6.33% WER that was reported on the NPSC which highlights the impact of domain mismatch on ASR; models trained on one domain (the Norwegian Parliament) do not generalize well to new domains (radio and studio recordings).  For the Bokmål text in both corpora, we can see that models perform best on the ""East"" dialect region whereas the ""West"" region has the worst performance. It is unclear which model is generally the best. The 1 billion parameter model performs better than the 300 million parameter model on the NB Tale text, but the 300 million parameter model outperforms the 1 billion on the Rundkast text.
With the Rundkast corpus, we can see that the Bokmål models perform, as expected, poorly on the Nynorsk text with the converse (Nynorsk model evaluated against Bokmål text) being true as well. However, even when the Nynorsk model is evaluated against Nynorsk text, the results are still worse than the Bokmål model of the same size evaluated against Bokmål text.
Of more concern than model accuracy, however, is data scarcity for Nynorsk text. Given that Nynorsk is primarily used in the western part of Norway, the nearly equal split of speakers between Bokmål and Nynorsk for the ""West"" region is understandable. Moreover, for the other regions (""North"" and ""Mid"" in particular) there are too few speakers to draw conclusions from. Therefore, as we move forward with the character-based analy-sis, we will be focusing on the Bokmål models and their performance on the Bokmål text.

[/R/ AND /L/ CONFUSIBLITY]
In Norwegian, /r/ is generally realized as either a voiced apical tap or a voiced velar approximant (Kvale and Foldvik, 1992). These two different pronunciations are considered dialect features, with the approximant version predominating in the ""South"" and ""West"" of the country and the tap being the norm in the rest of country. The maps in (Kvale and Foldvik, 1999) and (Skjekkeland, 1997) nicely illustrate this distribution.
Similar to the Norwegian /r/, which can be realized in several variants, the Norwegian /l/ also has dialectally motivated realizations. Many speakers in the ""East"", ""Mid"", and southern part of the ""North"" region of the country produce a voiced retroflex flap. The norm for speakers in the rest of the country (""West"", ""South"", and the remaining part of the ""North"") is a voiced dental/alveolar lateral (Kvale and Foldvik, 1995).
Understanding these phonetic realizations, we can anticipate that the tapped [R] and the lateral approximant [l] should be minimally confusing for NB Tale Rundkast (a) the model. The former being a brief interruption in the airflow and the latter being a continuous, smooth approximant. However, for speakers in the ""East"" and ""Mid"" parts of the country, where both the tapped [R] and flapped [ó] dialect features are present, we would anticipate a greater degree of confusion. Both tapped [R] and flapped [ó] are seen as brief closures with acoustic differentiation relegated to the F3 and F4 trajectories (Kvale and Foldvik, 1995).
N/A 2.79% 1.34% 1.81% 2.16% (c) 0.67% 4.24% 2.24% 1.56% 2.95% (b) 0.42% 3.44% 2.79% 1.61% 1.95% (d) 0.78% 3.87% 3.69% 1.67% 2.85%
Therefore to evaluate how much of an impact these potentially similar realizations have on the model, we used the aligned Bokmål texts (as described in Section 3.2) and calculated how frequently ""r"" was transcribed instead of ""l"" and vice versa. When analyzing instances of ""r"" transforming into ""l"", we only considered instances where the ""r"" did not precede another alveolar consonant (""t"", ""d"", ""n"", ""l"", ""s""). This is due to the fact that ""r"", when followed by an alveolar consonant, can be interpreted as a digraph. In dialect regions with the alveolar [R], speakers will realize the second alveolar consonant as a retroflex instead of pronouncing two distinct sounds. That is, ""rt"" would be realized as [ú]). To ensure these realizations did not cloud our analysis, we excluded all ""r""s followed by an alveolar consonant.
The maps in Figures 1 and 2 show the percentage of error. That is, for those instances where an ""r"" was not transcribed correctly, the maps show what percentage of those errors were because an ""l"" was transcribed instead (Figure 1). And vice versa for the ""l"" to ""r"" transformation (Figure 2). This error calculation and plotting was done for each of the cardinal dialect region. Darker colors represent higher errors. In both figures the first column (a, c) show results on the NB Tale utterances; second column (b, d)   For all Figures, except 2(b) and 2(d), the regions with the most confusability between ""r"" and ""l"" are the ""East"", ""Mid"", and ""North"". Indeed, for all Figures except 2(d) the ""South"" has the lowest incidences of ""r"" and ""l"" confusion. By and large we also see much clearer, more consistent trends with the NB Tale data. This could be because the utterances in the NB Tale corpora were selected for phonological coverage and thus there were more environments for ""r"" and ""l"" confusion.

[VOICELESS STOP LENITION]
In addition to /r/ and /l/ confusability, we also investigated the distribution of voiceless stop consonants. In the ""South"" region, voiceless stops tend to lenite to their voiced counterparts in post-vocalic environments (Skjekkeland, 1997). Thus, we would expect [p], [t], and [k] to lenite to [b], [d], and [g] when preceded by a vowel. To understand if this change is captured by the wav2vec model, we found instances where a voiceless stop was changed and then ensured that the change was to its voiced counterpart. If a voiceless to voice change occurred, we then ensured that both the voice and voiceless stops were preceded by a vowel. We counted occurrences of this postvocalic voicing change across all three stops of interest. Results can be see in Figure 3  Figure 3: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the NB Tale dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model 300m 1b
(a)
4.94%% 

[2.15%]
Figure 4: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the Rundkast dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model can see that the ""South"" region has the highest instances of voicing. Though once again, we see stronger trends in the NB Tale data then in Rundkast.

[PERSONAL PRONOUN JEG]
As mentioned when discussing the Norwegian language in Section 2, there are many ways for Norwegain speakers to say the first person pronoun jeg. This was briefly investigated as well.
Confusion pairs for jeg were aggregated and trends sought. Regardless, no trends in the words substituted for jeg in the transcripts could be found. This lack of results could indicate that a word like jeg occurs so frequently in all dialects that there is an abundance of training examples for the model to generalize from. Or, perhaps, the 5-gram language model used, in addition to the wav2vec component, had enough influence to ensure that only jeg was produced.

[DISCUSSION]
Due to the fact that we have been able to largely see acoustic dialectal features surfacing through our analysis, we find that this method of carefully aligning text and aggregating results has promise. Furthermore, we infer that the models have learned enough about Norwegian to understand standard spellings and apply these generalizations to broader contexts. Additionally, the phonetic information in the dialects is strong enough to cause the models to utilize this general spelling knowledge and create more acoustically aligned outputs. However, going so far as to say that the models have internalized some knowledge about the dialects themselves (e.g., phonetic features) is perhaps more than can be reasonably asserted from this analysis.
Through this paper we have explored a couple of known dialectally-motivated phonological realizations. There still, however, exist more that could be explored. As mentioned in Section 4.2, there exists a pattern of retroflexting of alveolar consonants for certain Norwegian dialects. This analysis could certainly be extended to those environments. However, there are also phonological changes that are hard, or potentially impossible to see in spelling changes. For example, alveolars are palatalized (most strongly) in the ""Mid"" region as well as in certain phonological environments in the ""North"" and the northern parts of the ""West"" and ""East"" regions. This palatalization would be hard to see in spellings since there is no standard way in Norwegian orthography of representing a palatalized sound. Additional Norwegian phonological features that have no written representation (such as toneme) would also be invisible to the analysis performed in this paper.
As the NPSC is derived from parliamentary speeches, the distribution of parliament speakers emulates the population distribution of the country. Thus our models, all of which were trained on NPSC, have the same speaker representation. That is, the ""East"" region would be the most represented in the training data. Given this, and the results in Table 5, it would seem that the models have best learnt the features which they saw the most, as machine learning models are wont to do. Therefore, if models are to be robust against dialects, it seems necessary to increase the training data for the other regions. Additionally, it might be possible to assign greater weight to these dialectal character changes during training to encourage the models to learn a better representation.

[CONCLUSION]
Through this paper, we demonstrate how an analysis of character errors in transcriptions generated by an end-to-end ASR system can contain dialectal trends mirroring those known through linguistic descriptions. We showed increased confusability between ""r"" and ""l"" in regions where those phonemes are realized similarly. We also showed increased incidences of voiceless stop lenition in a region known for that phenomena. These errors indicate that the end-to-end system has successfully learnt to spell in Norwegian, going so far so as to slightly spell in dialect.

[ACKNOWLEDGEMENTS]
This work has been done as part of the SCRIBE project as funded by the Norwegian Research Council, project number: 322964.","[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems."
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR,boIlH5V81C8.json,"This paper aims to investigate how certain dialectal features impact the performance of ASR systems. The authors compare three pre-trained Norwegian wav2vec2 models on three datasets, five dialect areas and three dialectal features. This rather large set of experimental variables makes the paper a bit difficult to follow, but overall, the goals, methods and results are well explained. While the character alignment method is not particularly original, it is well suited for the task. The results are limited to two variables that can be expressed by single graphemes, r/l confusion and voiceless stop lenition, but they aptly show the potential of the approach.

Questions:
- L226: does the ""5-gram language model"" refer to characters or words?
- L270: do you also use the non-native speakers for your experiments? Is the childhood municipality available for all of them?
- §3.2: If I understand correctly, your approach consists of two steps, word alignment (with Levenshtein distance) and character alignment within aligned words (CER). Wouldn't it work equally well to compute CER directly on the entire utterance/sentence?
- §3.2: Can you share some more details of your phonetic feature vector design? For example, the `epitran.vector` library provides conversion routines for a wide range of languages (although not for Norwegian). Also, the general idea of comparing characters based on their pronunciation has been explored thoroughly by the Groningen dialectometry team in the early 2000s (Nerbonne, Heeringa, Wieling).

Typos and minor remarks:
- L036: system*s* + add full stop
- L092: *the* language model
- L096: such as Salvi did in (Salvi...) > (Salvi...)
- L121: capture*d*
- L154: model.In (add space)
- L163: Thus illustrating > Thus we illustrate
- L175: both > all of
- L198: ""quite large"" - it is probably hard to determine a precise number anyway
- §3.1: Experimental setup *and data*
- L219: Some words missing in the sentence
- L235: an excellent candidate > excellent candidates
- L255: were created ... during corpus creating > were added ... during corpus creation
- L282: capital*i*zations
- L379: add full stop
- Figure 1: Please use the same scale for all graphs - currently the 4.24% of (b) are darker than the 8.04% of (c) and the 10.00% of (g). It might also help to add the dialect region names to one of the maps (although the cardinal directions should be self explanatory).
- Table 3: What happened to the Nynorsk Unknown utterances? WER of 0 would mean everything correct?
- L565: Nynorks > Nynorsk
- L624: proceed > precede
- L841: postvocallic > postvocalic
- Regarding the presentation of the results in general, I wonder if tables would not be easier to grasp than the maps. I find it a bit hard to get the gist of your experiments with so many maps at the same time. You could still show one or two maps as examples.
","[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Instances of ""r"" becoming ""l"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 2: Figure 2 :2Figure 2: Instances of ""l"" becoming ""r"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 3: for the NB Tale data and Figure 4 for Rundkast. The first column (a) shows results from the 300m parameter model, second column (b) shows results from 1b parameter model. Darker colors represent higher errors.For both the NB Tale and Rundkast corpora we,
Table 4: Example of the vectors for ""k"", ""g"", and ""n"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 5: Example of the vectors for ""a"", and ""e"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 6: 

[INTRODUCTION]
Automatic Speech Recognition (ASR) has, like all machine learning tasks, struggled with generalization. That is, a model will perform well on the task and data it was trained on but when presented with new examples, especially examples that differ in some dimension from the training data, the model will perform markedly less well. In the task of ASR, this means that models often struggle with generating correct transcriptions for speakers whose age, gender, or dialect differs from that of the speakers on which the model was originally trained. Of specific focus in this paper is the impact of dialect on a modern ASR system.
Dialect information has been used in different ways in ASR. In some applications, such as Dialect Identification (DID), the goal is to correctly identify the dialect for a given sample of speech. Hämäläinen et al. (2021), for example, used a combination of speech and text features to perform DID. In other cases, DID is combined with ASR systems to improve transcription accuracy. For example, Zhang and Hansen (2018) used bottleneck features extracted via unsupervised deep learning to perform DID for both Chinese and Arabic. Similarly, Imaizumi et al. (2022) used a multitask model for both DID and ASR. This multitask approach outperformed the single task systems on both DID and ASR.
Beyond DID, the behavior of ASR systems has been analyzed with respect to dialectal speech (as we do in this paper). This in order to explore phonetic phenomena, as well as to gain insights into the way those complex systems work. In these studies, even when dialectal information is not an explicit target, there is still an interest to understand what phonetic and dialectal information has been captured in ASR models. With traditional ASR models, this investigation has been fairly straightforward as these models have consisted of three semi-independent components: the acoustic model, the language model, and the lexicon. Because of the separate acoustic models within these multi-component models, one could, for example, perform clustering on the model parameters themselves such as (Salvi, 2003a(Salvi, ,b, 2005. In this work, Salvi performed clustering on the acoustic model features and correlated the resulting clusters with known dialectally realized phonemes. Instead of directly using an acoustic model from an ASR system, Chen et al. (2014) adapted the concept of an HMM acoustic model to automatically discover dialect-specific phonetic rules.
Unlike multi-component ASR systems, investigating modern, end-to-end models for phonetic and dialectal information is quite different.
Whereas parameters from an acoustic model may be extracted and used independently, the acoustic information in an end-to-end model cannot be so easily excised. This design makes it more challenging, but not impossible, to investigate what acoustic information is captured where in the network. Belinkov and collaborators used the output from each layer of an end-to-end system to train phonetic, grapheme, and articulatory classifiers (Belinkov and Glass, 2017;Belinkov et al., 2019). Prasad and Jyothi (2020) investigated dialectal information captured by an end-to-end system using not only layer-wise classification but also gradient and information-theoric analysis. All of these works are focused on analyzing the networkinternal representations detached from actual network output.
The output from ASR models is constrained by the model architecture. Traditional ASR models with lexicons are bound to output only words contained within that lexicon. This means that all transcripts generated by these models contained only real, known words even if the transcribed output did not necessarily match the word that was spoken. Additionally, these models do not allow for acceptable variation in spelling. For example, the word, ""favorite,"" would always be spelled ""favorite"" never ""favourite,"" even if the latter might better reflect the preference of a British English speaker. Conversely, these newer end-to-end architectures, trained using connectionist temporal classification (CTC) loss, produce output at the character instead of word level. This permits the model to create novel words and spellings, potentially better reflecting the phonetic realization of the spoken word. Given that CTC models are allowed to generate novel spellings, there exists the potential that dialectal information will be captured by the model output itself via non-standard spellings.
The goal of this paper is to investigate whether dialectal acoustic information can impact spellings with an end-to-end model. In order to test this, we used wav2vec 2.0 (Baevski et al., 2020) to generate transcriptions of Norwegian speech. We then performed an analysis of the resulting transcripts for captured dialectal knowledge via a dialectalregion based evaluation of character error patterns. From this analysis we are able to see known Norwegian dialectally-based phonological patterns, specifically around ""r"" and ""l"" confus-ability and stop consonant voicing. Thus we illustrate that strong enough acoustic dialectal cues can effect the character output of an end-to-end ASR system.

[NORWEGIAN LANGUAGE AND DIALECTS]
In this paper, we focus our analysis on the Norwegian language. Though spoken by a relatively small population of a little over 5 million speakers, Norwegian contains many dialects differentiated in phonology, syntax, and lexicon. In addition to dialectal variation, Norwegian also maintains two official written standards: Bokmål and Nynorsk; though neither written standard directly corresponds with a spoken variant. Furthermore, Norway does not recognize any official language standard. Indeed, people are encouraged to use their preferred written standard and native dialect in all aspects of work and life.
The variety in dialects stems from Norway's challenging and rugged topography that has historically forced the populace to organize into many, smaller communities. Over time, the diversity we see in Norwegian dialects developed in these small, isolated communities. As described by phoneticians, there now exist large dialectal phonetic variations ranging from infinitive verb endings to palatalization of consonants, to /r/ and /l/ realizations, to the various pronunciations for the personal pronoun for ""I"", jeg -ranging from [jaei] to [eg] to [i] and more (Skjekkeland, 1997).
While the number of specific Norwegian dialects is quite large, we can group these dialects into larger dialect groups for the purpose of this investigation. These grouping could be either into the regional names used by Skjekkeland or into the even larger, cardinal regions of ""East,"" ""West,"" ""North,"" ""South,"" and ""Mid."" The analysis outlined in this paper relies on these cardinal regions.

[EXPERIMENTAL SETUP AND DATA]
In order to investigate the impact of dialect on an end-to-end ASR system, a well-performing baseline model was required. Therefore, we used three models trained by the Norwegian National Library AI Lab and released publicly on the Hugging Face repository for our analysis 123 . The first model contained one billion parameters and was originally trained on the XLS-R (Babu et al., 2021). It was then fine tuned using the Norwegian Parliamentary Speech Corpus (NPSC) to transcribe Norwegian Bokmål text. The other two models were fine tuned from the 300 million parameter VoxRex model (Malmsten et al., 2022). One of these 300 million parameter models was fine-tuned to transcribe Bokmål, the other Nynorsk. All models use a 5-gram word-based language model. In all cases, the NPSC corpus was used to fine-tune the models (Solberg and Ortiz, 2022). When evaluated against the NPSC corpus, the Norwegian AI lab reports a word error rate (WER) of 6.33% for the 1 billion parameter model, 7.03% for the 300 million parameter Bokmål model, and 12.22% for the Nynorsk model. These results indicate that these models will make excellent candidates for our analysis.
As stated earlier, the models to be used were trained on the NPSC. This consists of recordings from the Norwegian Parliament and thus the speech style can be considered mostly spontaneous, with perhaps slightly more planning than everyday speech. For analysis purposes, the NPSC was excluded. This is due to data sparsity in the NPSC test set. While the whole test set is acceptable for model evaluation, data becomes untenably sparse when considered dialect-by-dialect. Thus our analysis focuses on results from two unrelated and more dialectally robust corpora: Rundkast and NB Tale.
The Rundkast corpus consists of radio broadcasts from the Norwegian Broadcasting Corporation (NRK) (Amdal et al., 2008). These transcripts are in both Bokmål and Nynorsk which are treated separately for analysis in this paper. Dialectal annotations were added by the transcribers during corpus creation and are provided directly in the speaker metadata.
NB Tale is publicly available from the National Library of Norway's Language Bank and consist of recordings and transcripts of native and non-native speakers of Norwegian. All speech was transcribed using the Bokmål standard. Read

[WORD AND CHARACTER ALIGNMENT]
As our investigation into dialectal impact revolves around analyzing trends in character errors, we require an alignment between reference text and model-generated hypothesis text where words that only differ by a few characters are prioritized for alignment. While character error rate (CER) computed across a whole utterance is useful in understanding an aggregate of character errors, this method loses awareness of word boundaries. For example, ""også kalt"" and ""og såkalt"" would be aligned in whole-utterance CER with an insertion and a deletion of a space (resulting in ""og så kalt""). However, we prefer an alignment where we recognize that ""så"" was removed from the first word and ""så"" as added to the second word. Thus CER, as it is generally used across entire utterances, does not answer for our analysis purposes.
With traditional, word-level Levenshtein-based alignments, word similarity is not considered. Any pair of words that do not exactly match are treated as completely different. However, by considering word similarity, the resulting alignments can be used for analysis of broad trends of spellings (e.g., a word ending in ""a"" instead of ""e"") that can indicate dialectal impact.
To accomplish such an alignment, an extension to the traditional Levenshtein alignment was developed (Levenshtein, 1965). Typically edit costs are fixed at a value before alignment is computed. However, in our solution instead of a fixed cost for substitutions, we allow it to be dynamically computed as the CER between the two candidate words. This still ensures that there is no cost for aligning words that are the same while also preferring substitutions of similarly spelled words. voiced class nasal place rounding  Once word-level alignment is computed using the dynamic substitution cost, we can investigate spelling errors. To ensure characters within a word are aligned optimally, we continue to use the dynamic substitution cost idea and compute the substitution cost between characters as the Euclidean distance between two feature vectors. To support this, articulatory feature vectors were created for each letter in the Norwegian alphabet using the International Phonetic Alphabet (IPA) charts as a guide. Articulatory features were considered as indexes in the vector and the values correspond to the realization. For our work, consonants (see examples in Table 1) were defined and treated separately from vowels (see examples in Table 2). As the goal with these vectors is not to create an accurate grapheme-to-phoneme mapping, nor to perfectly illustrate all possible IPA nuance, but instead to align letters in a more logical way, these vectors were sufficient.
""k"" 0 0 0 5 0 ""g"" 1 0 0 5 0 ""n"" 1 0 1 2 0
To illustrate the necessity of these vectors, consider the word pair of inngang (meaning ""entrance"") and enkel (""easy""). Using a traditional alignment method 4 , where all characters substitutions have the same cost, an alignment like in 4 Alignment generated using the Python Levenshtein package:
https://github.com/maxbachmann/ python-Levenshtein reference i n n g a n g hypothesis e n k e l Table 3: A possible alignment between inngang and enkel, generated without accounting for character similarity. reference i n n g a n g hypothesis e n k e l Table 4: A possible alignment between inngang and enkel, generated by accounting for character similarity.
Table 3 is generated. However, using articulatory features as a distance, we are able to generate the alignment in Table 4 where ""g"" and ""k"" (only differing by voicing), ""a"" and ""e"" (both being front vowels), and ""n"" and ""l"" (both being sonorants) are aligned. While this solution is slightly phonologically flawed -wholly ignoring the di-and trigraphs that exist in Norwegian and instead treating the component letters individually, for example -these feature vectors do accomplish the goal of creating a logical character-level alignment. With confidence in our word and character alignment we can perform the investigation into character substitution trends that constitutes our results.

[WER BY DIALECT]
To first understand the general trend in recognition across dialects, the WER was calculated for each dialect across the whole of the Rundkast and NB Tale corpora. Transcriptions were generated using both the 300 million and 1 billion parameter Bokmål models for both corpora. Rundkast was further transcribed with the 300 million parameter Nynorsk model (since Rundkast actually contains Nynorsk utterances, unlike NB Tale).
As displayed in Table 5 that shows WER across both corpora and dialects, we can see WER values ranging from the low teens to nearly 40%. These values are markedly higher than the 6.33% WER that was reported on the NPSC which highlights the impact of domain mismatch on ASR; models trained on one domain (the Norwegian Parliament) do not generalize well to new domains (radio and studio recordings).  For the Bokmål text in both corpora, we can see that models perform best on the ""East"" dialect region whereas the ""West"" region has the worst performance. It is unclear which model is generally the best. The 1 billion parameter model performs better than the 300 million parameter model on the NB Tale text, but the 300 million parameter model outperforms the 1 billion on the Rundkast text.
With the Rundkast corpus, we can see that the Bokmål models perform, as expected, poorly on the Nynorsk text with the converse (Nynorsk model evaluated against Bokmål text) being true as well. However, even when the Nynorsk model is evaluated against Nynorsk text, the results are still worse than the Bokmål model of the same size evaluated against Bokmål text.
Of more concern than model accuracy, however, is data scarcity for Nynorsk text. Given that Nynorsk is primarily used in the western part of Norway, the nearly equal split of speakers between Bokmål and Nynorsk for the ""West"" region is understandable. Moreover, for the other regions (""North"" and ""Mid"" in particular) there are too few speakers to draw conclusions from. Therefore, as we move forward with the character-based analy-sis, we will be focusing on the Bokmål models and their performance on the Bokmål text.

[/R/ AND /L/ CONFUSIBLITY]
In Norwegian, /r/ is generally realized as either a voiced apical tap or a voiced velar approximant (Kvale and Foldvik, 1992). These two different pronunciations are considered dialect features, with the approximant version predominating in the ""South"" and ""West"" of the country and the tap being the norm in the rest of country. The maps in (Kvale and Foldvik, 1999) and (Skjekkeland, 1997) nicely illustrate this distribution.
Similar to the Norwegian /r/, which can be realized in several variants, the Norwegian /l/ also has dialectally motivated realizations. Many speakers in the ""East"", ""Mid"", and southern part of the ""North"" region of the country produce a voiced retroflex flap. The norm for speakers in the rest of the country (""West"", ""South"", and the remaining part of the ""North"") is a voiced dental/alveolar lateral (Kvale and Foldvik, 1995).
Understanding these phonetic realizations, we can anticipate that the tapped [R] and the lateral approximant [l] should be minimally confusing for NB Tale Rundkast (a) the model. The former being a brief interruption in the airflow and the latter being a continuous, smooth approximant. However, for speakers in the ""East"" and ""Mid"" parts of the country, where both the tapped [R] and flapped [ó] dialect features are present, we would anticipate a greater degree of confusion. Both tapped [R] and flapped [ó] are seen as brief closures with acoustic differentiation relegated to the F3 and F4 trajectories (Kvale and Foldvik, 1995).
N/A 2.79% 1.34% 1.81% 2.16% (c) 0.67% 4.24% 2.24% 1.56% 2.95% (b) 0.42% 3.44% 2.79% 1.61% 1.95% (d) 0.78% 3.87% 3.69% 1.67% 2.85%
Therefore to evaluate how much of an impact these potentially similar realizations have on the model, we used the aligned Bokmål texts (as described in Section 3.2) and calculated how frequently ""r"" was transcribed instead of ""l"" and vice versa. When analyzing instances of ""r"" transforming into ""l"", we only considered instances where the ""r"" did not precede another alveolar consonant (""t"", ""d"", ""n"", ""l"", ""s""). This is due to the fact that ""r"", when followed by an alveolar consonant, can be interpreted as a digraph. In dialect regions with the alveolar [R], speakers will realize the second alveolar consonant as a retroflex instead of pronouncing two distinct sounds. That is, ""rt"" would be realized as [ú]). To ensure these realizations did not cloud our analysis, we excluded all ""r""s followed by an alveolar consonant.
The maps in Figures 1 and 2 show the percentage of error. That is, for those instances where an ""r"" was not transcribed correctly, the maps show what percentage of those errors were because an ""l"" was transcribed instead (Figure 1). And vice versa for the ""l"" to ""r"" transformation (Figure 2). This error calculation and plotting was done for each of the cardinal dialect region. Darker colors represent higher errors. In both figures the first column (a, c) show results on the NB Tale utterances; second column (b, d)   For all Figures, except 2(b) and 2(d), the regions with the most confusability between ""r"" and ""l"" are the ""East"", ""Mid"", and ""North"". Indeed, for all Figures except 2(d) the ""South"" has the lowest incidences of ""r"" and ""l"" confusion. By and large we also see much clearer, more consistent trends with the NB Tale data. This could be because the utterances in the NB Tale corpora were selected for phonological coverage and thus there were more environments for ""r"" and ""l"" confusion.

[VOICELESS STOP LENITION]
In addition to /r/ and /l/ confusability, we also investigated the distribution of voiceless stop consonants. In the ""South"" region, voiceless stops tend to lenite to their voiced counterparts in post-vocalic environments (Skjekkeland, 1997). Thus, we would expect [p], [t], and [k] to lenite to [b], [d], and [g] when preceded by a vowel. To understand if this change is captured by the wav2vec model, we found instances where a voiceless stop was changed and then ensured that the change was to its voiced counterpart. If a voiceless to voice change occurred, we then ensured that both the voice and voiceless stops were preceded by a vowel. We counted occurrences of this postvocalic voicing change across all three stops of interest. Results can be see in Figure 3  Figure 3: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the NB Tale dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model 300m 1b
(a)
4.94%% 

[2.15%]
Figure 4: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the Rundkast dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model can see that the ""South"" region has the highest instances of voicing. Though once again, we see stronger trends in the NB Tale data then in Rundkast.

[PERSONAL PRONOUN JEG]
As mentioned when discussing the Norwegian language in Section 2, there are many ways for Norwegain speakers to say the first person pronoun jeg. This was briefly investigated as well.
Confusion pairs for jeg were aggregated and trends sought. Regardless, no trends in the words substituted for jeg in the transcripts could be found. This lack of results could indicate that a word like jeg occurs so frequently in all dialects that there is an abundance of training examples for the model to generalize from. Or, perhaps, the 5-gram language model used, in addition to the wav2vec component, had enough influence to ensure that only jeg was produced.

[DISCUSSION]
Due to the fact that we have been able to largely see acoustic dialectal features surfacing through our analysis, we find that this method of carefully aligning text and aggregating results has promise. Furthermore, we infer that the models have learned enough about Norwegian to understand standard spellings and apply these generalizations to broader contexts. Additionally, the phonetic information in the dialects is strong enough to cause the models to utilize this general spelling knowledge and create more acoustically aligned outputs. However, going so far as to say that the models have internalized some knowledge about the dialects themselves (e.g., phonetic features) is perhaps more than can be reasonably asserted from this analysis.
Through this paper we have explored a couple of known dialectally-motivated phonological realizations. There still, however, exist more that could be explored. As mentioned in Section 4.2, there exists a pattern of retroflexting of alveolar consonants for certain Norwegian dialects. This analysis could certainly be extended to those environments. However, there are also phonological changes that are hard, or potentially impossible to see in spelling changes. For example, alveolars are palatalized (most strongly) in the ""Mid"" region as well as in certain phonological environments in the ""North"" and the northern parts of the ""West"" and ""East"" regions. This palatalization would be hard to see in spellings since there is no standard way in Norwegian orthography of representing a palatalized sound. Additional Norwegian phonological features that have no written representation (such as toneme) would also be invisible to the analysis performed in this paper.
As the NPSC is derived from parliamentary speeches, the distribution of parliament speakers emulates the population distribution of the country. Thus our models, all of which were trained on NPSC, have the same speaker representation. That is, the ""East"" region would be the most represented in the training data. Given this, and the results in Table 5, it would seem that the models have best learnt the features which they saw the most, as machine learning models are wont to do. Therefore, if models are to be robust against dialects, it seems necessary to increase the training data for the other regions. Additionally, it might be possible to assign greater weight to these dialectal character changes during training to encourage the models to learn a better representation.

[CONCLUSION]
Through this paper, we demonstrate how an analysis of character errors in transcriptions generated by an end-to-end ASR system can contain dialectal trends mirroring those known through linguistic descriptions. We showed increased confusability between ""r"" and ""l"" in regions where those phonemes are realized similarly. We also showed increased incidences of voiceless stop lenition in a region known for that phenomena. These errors indicate that the end-to-end system has successfully learnt to spell in Norwegian, going so far so as to slightly spell in dialect.

[ACKNOWLEDGEMENTS]
This work has been done as part of the SCRIBE project as funded by the Norwegian Research Council, project number: 322964.","[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems."
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR,boIlH5V81C8.json,"
This paper investigates the following question: can one identify
Norwegian dialects from the output of a standard state of the art ASR
system?

Dialectal variation and identification is an interesting topic, and
leveraging state of the art models should always be a goal. This paper
is therefore quite relevant for the venue and original enough.
Unfortunately I see three serious flaws with the present paper.

1. To actually answer the scientific question that you pose, you
   should establish a 'ground truth' (say, what is the linguistically
   established rate of Voiceless stop lenition per region) and compute
   a correlation coefficient (or some other metric) with what the
   model computes. In the present state of the paper, the reader must
   already be aware of the regional Norwegian dialects and perform a
   correlation by themselves to answer the question.

   
2. The methodology is somewhat roundabout. Why not use a speech to
   phoneme model instead of speech to text? Current methodology seem
   weirdly dependent on the idiosyncrasies of the writing systems and
   the exact peculiarities of the models employed (which does suppress
   dialectic differences--- exactly what you observe with the 1st
   person pronoun experiment.)

   The natural reaction of many readers will be to ask if this method
   is promising or not. You should strive to answer this question by
   comparing your method to prior art (some of which you cite) and
   models which output phonemes.

3. The authors do not convey thorough understanding of some of the
   methods that they employ (alignment, see below).


Reading notes:


111: ""has been obscured by the network itself."" -> ""is not transparent, by design.""

154: missing space after dot.

195,202,301,351,etc. Non standard opening double quotes (for English “ is the preferred opening double quotes)

216-220: this sentence appears to be malformed

295: ""With traditional, Levenshtein-based alignments, this sort of
word similarity is not considered. Any pair of words that do not
exactly match are treated as completely different."" There is such a
thing as character-based Levenshtein alignment. The paper should state
why is it not considered and authors prefer to go a complicated route.

308: why is it dynamic? Dynamic usually means dependent on context,
but surely this measure is only a function of the words encountered?

320: ""compute the substitution cost between characters as the
Euclidean distance between two feature vectors."" What are those
feature vectors? Typically one would use a cosine
similarity.

Table 1: Speaks about non-character aware, but is also done at a
character level. This is self-contradictory. You mean
not aware of phonetics.

347: ""treated equally"": you mean that all character differences have
the same cost.

632: ""we excluded all possibilities."" What does ""all"" mean here?

797: ""machine learning models are wont to do."" (?)
","[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Instances of ""r"" becoming ""l"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 2: Figure 2 :2Figure 2: Instances of ""l"" becoming ""r"". The first column (a, c) show results on the NB Tale utterances; second column (b, d) shows results on the Rundkast utterances.The first row (a, b) being results from the 300m model and the second row (c, d) being results from the 1b parameter model.
Table 3: for the NB Tale data and Figure 4 for Rundkast. The first column (a) shows results from the 300m parameter model, second column (b) shows results from 1b parameter model. Darker colors represent higher errors.For both the NB Tale and Rundkast corpora we,
Table 4: Example of the vectors for ""k"", ""g"", and ""n"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 5: Example of the vectors for ""a"", and ""e"" for Norwegian. Indexes of the vector represent features and values represent their realization.
Table 6: 

[INTRODUCTION]
Automatic Speech Recognition (ASR) has, like all machine learning tasks, struggled with generalization. That is, a model will perform well on the task and data it was trained on but when presented with new examples, especially examples that differ in some dimension from the training data, the model will perform markedly less well. In the task of ASR, this means that models often struggle with generating correct transcriptions for speakers whose age, gender, or dialect differs from that of the speakers on which the model was originally trained. Of specific focus in this paper is the impact of dialect on a modern ASR system.
Dialect information has been used in different ways in ASR. In some applications, such as Dialect Identification (DID), the goal is to correctly identify the dialect for a given sample of speech. Hämäläinen et al. (2021), for example, used a combination of speech and text features to perform DID. In other cases, DID is combined with ASR systems to improve transcription accuracy. For example, Zhang and Hansen (2018) used bottleneck features extracted via unsupervised deep learning to perform DID for both Chinese and Arabic. Similarly, Imaizumi et al. (2022) used a multitask model for both DID and ASR. This multitask approach outperformed the single task systems on both DID and ASR.
Beyond DID, the behavior of ASR systems has been analyzed with respect to dialectal speech (as we do in this paper). This in order to explore phonetic phenomena, as well as to gain insights into the way those complex systems work. In these studies, even when dialectal information is not an explicit target, there is still an interest to understand what phonetic and dialectal information has been captured in ASR models. With traditional ASR models, this investigation has been fairly straightforward as these models have consisted of three semi-independent components: the acoustic model, the language model, and the lexicon. Because of the separate acoustic models within these multi-component models, one could, for example, perform clustering on the model parameters themselves such as (Salvi, 2003a(Salvi, ,b, 2005. In this work, Salvi performed clustering on the acoustic model features and correlated the resulting clusters with known dialectally realized phonemes. Instead of directly using an acoustic model from an ASR system, Chen et al. (2014) adapted the concept of an HMM acoustic model to automatically discover dialect-specific phonetic rules.
Unlike multi-component ASR systems, investigating modern, end-to-end models for phonetic and dialectal information is quite different.
Whereas parameters from an acoustic model may be extracted and used independently, the acoustic information in an end-to-end model cannot be so easily excised. This design makes it more challenging, but not impossible, to investigate what acoustic information is captured where in the network. Belinkov and collaborators used the output from each layer of an end-to-end system to train phonetic, grapheme, and articulatory classifiers (Belinkov and Glass, 2017;Belinkov et al., 2019). Prasad and Jyothi (2020) investigated dialectal information captured by an end-to-end system using not only layer-wise classification but also gradient and information-theoric analysis. All of these works are focused on analyzing the networkinternal representations detached from actual network output.
The output from ASR models is constrained by the model architecture. Traditional ASR models with lexicons are bound to output only words contained within that lexicon. This means that all transcripts generated by these models contained only real, known words even if the transcribed output did not necessarily match the word that was spoken. Additionally, these models do not allow for acceptable variation in spelling. For example, the word, ""favorite,"" would always be spelled ""favorite"" never ""favourite,"" even if the latter might better reflect the preference of a British English speaker. Conversely, these newer end-to-end architectures, trained using connectionist temporal classification (CTC) loss, produce output at the character instead of word level. This permits the model to create novel words and spellings, potentially better reflecting the phonetic realization of the spoken word. Given that CTC models are allowed to generate novel spellings, there exists the potential that dialectal information will be captured by the model output itself via non-standard spellings.
The goal of this paper is to investigate whether dialectal acoustic information can impact spellings with an end-to-end model. In order to test this, we used wav2vec 2.0 (Baevski et al., 2020) to generate transcriptions of Norwegian speech. We then performed an analysis of the resulting transcripts for captured dialectal knowledge via a dialectalregion based evaluation of character error patterns. From this analysis we are able to see known Norwegian dialectally-based phonological patterns, specifically around ""r"" and ""l"" confus-ability and stop consonant voicing. Thus we illustrate that strong enough acoustic dialectal cues can effect the character output of an end-to-end ASR system.

[NORWEGIAN LANGUAGE AND DIALECTS]
In this paper, we focus our analysis on the Norwegian language. Though spoken by a relatively small population of a little over 5 million speakers, Norwegian contains many dialects differentiated in phonology, syntax, and lexicon. In addition to dialectal variation, Norwegian also maintains two official written standards: Bokmål and Nynorsk; though neither written standard directly corresponds with a spoken variant. Furthermore, Norway does not recognize any official language standard. Indeed, people are encouraged to use their preferred written standard and native dialect in all aspects of work and life.
The variety in dialects stems from Norway's challenging and rugged topography that has historically forced the populace to organize into many, smaller communities. Over time, the diversity we see in Norwegian dialects developed in these small, isolated communities. As described by phoneticians, there now exist large dialectal phonetic variations ranging from infinitive verb endings to palatalization of consonants, to /r/ and /l/ realizations, to the various pronunciations for the personal pronoun for ""I"", jeg -ranging from [jaei] to [eg] to [i] and more (Skjekkeland, 1997).
While the number of specific Norwegian dialects is quite large, we can group these dialects into larger dialect groups for the purpose of this investigation. These grouping could be either into the regional names used by Skjekkeland or into the even larger, cardinal regions of ""East,"" ""West,"" ""North,"" ""South,"" and ""Mid."" The analysis outlined in this paper relies on these cardinal regions.

[EXPERIMENTAL SETUP AND DATA]
In order to investigate the impact of dialect on an end-to-end ASR system, a well-performing baseline model was required. Therefore, we used three models trained by the Norwegian National Library AI Lab and released publicly on the Hugging Face repository for our analysis 123 . The first model contained one billion parameters and was originally trained on the XLS-R (Babu et al., 2021). It was then fine tuned using the Norwegian Parliamentary Speech Corpus (NPSC) to transcribe Norwegian Bokmål text. The other two models were fine tuned from the 300 million parameter VoxRex model (Malmsten et al., 2022). One of these 300 million parameter models was fine-tuned to transcribe Bokmål, the other Nynorsk. All models use a 5-gram word-based language model. In all cases, the NPSC corpus was used to fine-tune the models (Solberg and Ortiz, 2022). When evaluated against the NPSC corpus, the Norwegian AI lab reports a word error rate (WER) of 6.33% for the 1 billion parameter model, 7.03% for the 300 million parameter Bokmål model, and 12.22% for the Nynorsk model. These results indicate that these models will make excellent candidates for our analysis.
As stated earlier, the models to be used were trained on the NPSC. This consists of recordings from the Norwegian Parliament and thus the speech style can be considered mostly spontaneous, with perhaps slightly more planning than everyday speech. For analysis purposes, the NPSC was excluded. This is due to data sparsity in the NPSC test set. While the whole test set is acceptable for model evaluation, data becomes untenably sparse when considered dialect-by-dialect. Thus our analysis focuses on results from two unrelated and more dialectally robust corpora: Rundkast and NB Tale.
The Rundkast corpus consists of radio broadcasts from the Norwegian Broadcasting Corporation (NRK) (Amdal et al., 2008). These transcripts are in both Bokmål and Nynorsk which are treated separately for analysis in this paper. Dialectal annotations were added by the transcribers during corpus creation and are provided directly in the speaker metadata.
NB Tale is publicly available from the National Library of Norway's Language Bank and consist of recordings and transcripts of native and non-native speakers of Norwegian. All speech was transcribed using the Bokmål standard. Read

[WORD AND CHARACTER ALIGNMENT]
As our investigation into dialectal impact revolves around analyzing trends in character errors, we require an alignment between reference text and model-generated hypothesis text where words that only differ by a few characters are prioritized for alignment. While character error rate (CER) computed across a whole utterance is useful in understanding an aggregate of character errors, this method loses awareness of word boundaries. For example, ""også kalt"" and ""og såkalt"" would be aligned in whole-utterance CER with an insertion and a deletion of a space (resulting in ""og så kalt""). However, we prefer an alignment where we recognize that ""så"" was removed from the first word and ""så"" as added to the second word. Thus CER, as it is generally used across entire utterances, does not answer for our analysis purposes.
With traditional, word-level Levenshtein-based alignments, word similarity is not considered. Any pair of words that do not exactly match are treated as completely different. However, by considering word similarity, the resulting alignments can be used for analysis of broad trends of spellings (e.g., a word ending in ""a"" instead of ""e"") that can indicate dialectal impact.
To accomplish such an alignment, an extension to the traditional Levenshtein alignment was developed (Levenshtein, 1965). Typically edit costs are fixed at a value before alignment is computed. However, in our solution instead of a fixed cost for substitutions, we allow it to be dynamically computed as the CER between the two candidate words. This still ensures that there is no cost for aligning words that are the same while also preferring substitutions of similarly spelled words. voiced class nasal place rounding  Once word-level alignment is computed using the dynamic substitution cost, we can investigate spelling errors. To ensure characters within a word are aligned optimally, we continue to use the dynamic substitution cost idea and compute the substitution cost between characters as the Euclidean distance between two feature vectors. To support this, articulatory feature vectors were created for each letter in the Norwegian alphabet using the International Phonetic Alphabet (IPA) charts as a guide. Articulatory features were considered as indexes in the vector and the values correspond to the realization. For our work, consonants (see examples in Table 1) were defined and treated separately from vowels (see examples in Table 2). As the goal with these vectors is not to create an accurate grapheme-to-phoneme mapping, nor to perfectly illustrate all possible IPA nuance, but instead to align letters in a more logical way, these vectors were sufficient.
""k"" 0 0 0 5 0 ""g"" 1 0 0 5 0 ""n"" 1 0 1 2 0
To illustrate the necessity of these vectors, consider the word pair of inngang (meaning ""entrance"") and enkel (""easy""). Using a traditional alignment method 4 , where all characters substitutions have the same cost, an alignment like in 4 Alignment generated using the Python Levenshtein package:
https://github.com/maxbachmann/ python-Levenshtein reference i n n g a n g hypothesis e n k e l Table 3: A possible alignment between inngang and enkel, generated without accounting for character similarity. reference i n n g a n g hypothesis e n k e l Table 4: A possible alignment between inngang and enkel, generated by accounting for character similarity.
Table 3 is generated. However, using articulatory features as a distance, we are able to generate the alignment in Table 4 where ""g"" and ""k"" (only differing by voicing), ""a"" and ""e"" (both being front vowels), and ""n"" and ""l"" (both being sonorants) are aligned. While this solution is slightly phonologically flawed -wholly ignoring the di-and trigraphs that exist in Norwegian and instead treating the component letters individually, for example -these feature vectors do accomplish the goal of creating a logical character-level alignment. With confidence in our word and character alignment we can perform the investigation into character substitution trends that constitutes our results.

[WER BY DIALECT]
To first understand the general trend in recognition across dialects, the WER was calculated for each dialect across the whole of the Rundkast and NB Tale corpora. Transcriptions were generated using both the 300 million and 1 billion parameter Bokmål models for both corpora. Rundkast was further transcribed with the 300 million parameter Nynorsk model (since Rundkast actually contains Nynorsk utterances, unlike NB Tale).
As displayed in Table 5 that shows WER across both corpora and dialects, we can see WER values ranging from the low teens to nearly 40%. These values are markedly higher than the 6.33% WER that was reported on the NPSC which highlights the impact of domain mismatch on ASR; models trained on one domain (the Norwegian Parliament) do not generalize well to new domains (radio and studio recordings).  For the Bokmål text in both corpora, we can see that models perform best on the ""East"" dialect region whereas the ""West"" region has the worst performance. It is unclear which model is generally the best. The 1 billion parameter model performs better than the 300 million parameter model on the NB Tale text, but the 300 million parameter model outperforms the 1 billion on the Rundkast text.
With the Rundkast corpus, we can see that the Bokmål models perform, as expected, poorly on the Nynorsk text with the converse (Nynorsk model evaluated against Bokmål text) being true as well. However, even when the Nynorsk model is evaluated against Nynorsk text, the results are still worse than the Bokmål model of the same size evaluated against Bokmål text.
Of more concern than model accuracy, however, is data scarcity for Nynorsk text. Given that Nynorsk is primarily used in the western part of Norway, the nearly equal split of speakers between Bokmål and Nynorsk for the ""West"" region is understandable. Moreover, for the other regions (""North"" and ""Mid"" in particular) there are too few speakers to draw conclusions from. Therefore, as we move forward with the character-based analy-sis, we will be focusing on the Bokmål models and their performance on the Bokmål text.

[/R/ AND /L/ CONFUSIBLITY]
In Norwegian, /r/ is generally realized as either a voiced apical tap or a voiced velar approximant (Kvale and Foldvik, 1992). These two different pronunciations are considered dialect features, with the approximant version predominating in the ""South"" and ""West"" of the country and the tap being the norm in the rest of country. The maps in (Kvale and Foldvik, 1999) and (Skjekkeland, 1997) nicely illustrate this distribution.
Similar to the Norwegian /r/, which can be realized in several variants, the Norwegian /l/ also has dialectally motivated realizations. Many speakers in the ""East"", ""Mid"", and southern part of the ""North"" region of the country produce a voiced retroflex flap. The norm for speakers in the rest of the country (""West"", ""South"", and the remaining part of the ""North"") is a voiced dental/alveolar lateral (Kvale and Foldvik, 1995).
Understanding these phonetic realizations, we can anticipate that the tapped [R] and the lateral approximant [l] should be minimally confusing for NB Tale Rundkast (a) the model. The former being a brief interruption in the airflow and the latter being a continuous, smooth approximant. However, for speakers in the ""East"" and ""Mid"" parts of the country, where both the tapped [R] and flapped [ó] dialect features are present, we would anticipate a greater degree of confusion. Both tapped [R] and flapped [ó] are seen as brief closures with acoustic differentiation relegated to the F3 and F4 trajectories (Kvale and Foldvik, 1995).
N/A 2.79% 1.34% 1.81% 2.16% (c) 0.67% 4.24% 2.24% 1.56% 2.95% (b) 0.42% 3.44% 2.79% 1.61% 1.95% (d) 0.78% 3.87% 3.69% 1.67% 2.85%
Therefore to evaluate how much of an impact these potentially similar realizations have on the model, we used the aligned Bokmål texts (as described in Section 3.2) and calculated how frequently ""r"" was transcribed instead of ""l"" and vice versa. When analyzing instances of ""r"" transforming into ""l"", we only considered instances where the ""r"" did not precede another alveolar consonant (""t"", ""d"", ""n"", ""l"", ""s""). This is due to the fact that ""r"", when followed by an alveolar consonant, can be interpreted as a digraph. In dialect regions with the alveolar [R], speakers will realize the second alveolar consonant as a retroflex instead of pronouncing two distinct sounds. That is, ""rt"" would be realized as [ú]). To ensure these realizations did not cloud our analysis, we excluded all ""r""s followed by an alveolar consonant.
The maps in Figures 1 and 2 show the percentage of error. That is, for those instances where an ""r"" was not transcribed correctly, the maps show what percentage of those errors were because an ""l"" was transcribed instead (Figure 1). And vice versa for the ""l"" to ""r"" transformation (Figure 2). This error calculation and plotting was done for each of the cardinal dialect region. Darker colors represent higher errors. In both figures the first column (a, c) show results on the NB Tale utterances; second column (b, d)   For all Figures, except 2(b) and 2(d), the regions with the most confusability between ""r"" and ""l"" are the ""East"", ""Mid"", and ""North"". Indeed, for all Figures except 2(d) the ""South"" has the lowest incidences of ""r"" and ""l"" confusion. By and large we also see much clearer, more consistent trends with the NB Tale data. This could be because the utterances in the NB Tale corpora were selected for phonological coverage and thus there were more environments for ""r"" and ""l"" confusion.

[VOICELESS STOP LENITION]
In addition to /r/ and /l/ confusability, we also investigated the distribution of voiceless stop consonants. In the ""South"" region, voiceless stops tend to lenite to their voiced counterparts in post-vocalic environments (Skjekkeland, 1997). Thus, we would expect [p], [t], and [k] to lenite to [b], [d], and [g] when preceded by a vowel. To understand if this change is captured by the wav2vec model, we found instances where a voiceless stop was changed and then ensured that the change was to its voiced counterpart. If a voiceless to voice change occurred, we then ensured that both the voice and voiceless stops were preceded by a vowel. We counted occurrences of this postvocalic voicing change across all three stops of interest. Results can be see in Figure 3  Figure 3: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the NB Tale dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model 300m 1b
(a)
4.94%% 

[2.15%]
Figure 4: Percentage of postvocalic voicing error; that is, instances of (""p"", ""t"", ""k"") realized as (""b"", ""d"", ""g"") as a percentage of total (""p"", ""t"", ""k"") errors on the Rundkast dataset. First column (a) shows results from the 300m parameter model, second column (b) from the 1b parameter model can see that the ""South"" region has the highest instances of voicing. Though once again, we see stronger trends in the NB Tale data then in Rundkast.

[PERSONAL PRONOUN JEG]
As mentioned when discussing the Norwegian language in Section 2, there are many ways for Norwegain speakers to say the first person pronoun jeg. This was briefly investigated as well.
Confusion pairs for jeg were aggregated and trends sought. Regardless, no trends in the words substituted for jeg in the transcripts could be found. This lack of results could indicate that a word like jeg occurs so frequently in all dialects that there is an abundance of training examples for the model to generalize from. Or, perhaps, the 5-gram language model used, in addition to the wav2vec component, had enough influence to ensure that only jeg was produced.

[DISCUSSION]
Due to the fact that we have been able to largely see acoustic dialectal features surfacing through our analysis, we find that this method of carefully aligning text and aggregating results has promise. Furthermore, we infer that the models have learned enough about Norwegian to understand standard spellings and apply these generalizations to broader contexts. Additionally, the phonetic information in the dialects is strong enough to cause the models to utilize this general spelling knowledge and create more acoustically aligned outputs. However, going so far as to say that the models have internalized some knowledge about the dialects themselves (e.g., phonetic features) is perhaps more than can be reasonably asserted from this analysis.
Through this paper we have explored a couple of known dialectally-motivated phonological realizations. There still, however, exist more that could be explored. As mentioned in Section 4.2, there exists a pattern of retroflexting of alveolar consonants for certain Norwegian dialects. This analysis could certainly be extended to those environments. However, there are also phonological changes that are hard, or potentially impossible to see in spelling changes. For example, alveolars are palatalized (most strongly) in the ""Mid"" region as well as in certain phonological environments in the ""North"" and the northern parts of the ""West"" and ""East"" regions. This palatalization would be hard to see in spellings since there is no standard way in Norwegian orthography of representing a palatalized sound. Additional Norwegian phonological features that have no written representation (such as toneme) would also be invisible to the analysis performed in this paper.
As the NPSC is derived from parliamentary speeches, the distribution of parliament speakers emulates the population distribution of the country. Thus our models, all of which were trained on NPSC, have the same speaker representation. That is, the ""East"" region would be the most represented in the training data. Given this, and the results in Table 5, it would seem that the models have best learnt the features which they saw the most, as machine learning models are wont to do. Therefore, if models are to be robust against dialects, it seems necessary to increase the training data for the other regions. Additionally, it might be possible to assign greater weight to these dialectal character changes during training to encourage the models to learn a better representation.

[CONCLUSION]
Through this paper, we demonstrate how an analysis of character errors in transcriptions generated by an end-to-end ASR system can contain dialectal trends mirroring those known through linguistic descriptions. We showed increased confusability between ""r"" and ""l"" in regions where those phonemes are realized similarly. We also showed increased incidences of voiceless stop lenition in a region known for that phenomena. These errors indicate that the end-to-end system has successfully learnt to spell in Norwegian, going so far so as to slightly spell in dialect.

[ACKNOWLEDGEMENTS]
This work has been done as part of the SCRIBE project as funded by the Norwegian Research Council, project number: 322964.","[TITLE]
A character-based analysis of impacts of dialects on end-to-end Norwegian ASR

[ABSTRACT]
We present a method for analyzing character errors for use with character-based, end-to-end ASR systems, as used herein for investigating dialectal speech. As endto-end systems are able to produce novel spellings, there exists a possibility that the spelling variants produced by these systems can capture phonological information beyond the intended target word. We therefore first introduce a way of guaranteeing that similar words and characters are paired during alignment, thus ensuring that any resulting analysis of character errors is founded on sound substitutions. Then, from such a careful character alignment, we find trends in system-generated spellings that align with known phonological features of Norwegian dialects, in particular, ""r"" and ""l"" confusability and voiceless stop lenition. Through this analysis, we demonstrate that cues from acoustic dialectal features can influence the output of an end-to-end ASR systems."
Multi-Time Attention Networks for Irregularly Sampled Time Series,4c0J6lwQ4_.json,"This paper proposed a new model (mTANs) for sparse and irregularly sampled multivariate time series. It incorporates the time attention mechanism to learn embedding for continuous time-series based on a kernel smoothing method. Results on real-world dataset such as EHR data has outperformed other baselines.

I have the following comments that I think is worthwhile to consider to improve the paper:

1. I didn't see much on the main method sections that specifically address/discuss sparsity and irregular sample problems, it seems like the proposed method also works for general time-series data.

2. In the discretization part, it would be better to make it clear how to find the set of reference time points r in practice. It may not be efficient enough if one needs to track each time stamp for each variable to obtain this set $r$.

3. How does an extra module of mTAND in encoding and decoding procedure help intuitively? Given that RNN has already captured temporal information.

4. The noise distribution for sparse / irregularly sampled data can often be heavily skewed, so the Gaussian noise assumption in this paper may not hold.

Additional questions/suggestions:
1. It is better to specify how to compute the gradient for ELBO, e.g., are there any approximations used, are methods like REINFORCE or Gumbel softmax implemented for non-continuous cases, as well as how does the parameter initialize for unsupervised learning problems.

2. Does the unified supervised and unsupervised objective in Eq (15) mean the proposed method combines imputation/interpolation with learning/inference for time series data?

3. From the time series perspective, the positional encoding in Eq (1) is more like a season and trend decomposition, not sure if this plays a similar role here.

4. It would be very helpful for me to understand the paper if the authors could add a schematic figure to demonstrate the complete structure of the model.","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1   

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Architecture of the mTAND module. It takes irregularly sampled time points and corresponding values as keys and values and produces a fixed dimensional representation at the query time points. The attention blocks (ATT) perform a scaled dot product attention over the observed values using the time embedding of the query and key time points. Equation 3 and 4 defines this operation.Note that the output at all query points can be computed in parallel.
Table 2: Figure 2 :2Figure 2: Architecture of the proposed encoder-decoder framework mTAND-Full. The classifier is required only for performing classification tasks. The mTAND module is shown in Figure 1.
Table 3: Figure 3 :3Figure 3: Interpolations on the synthetic interpolation dataset. The columns represent 3 different examples. First row: Ground truth trajectories with observed points, second row: reconstructions on the complete range t ∈ [0, 1] using the proposed model mTAN, third row: reconstructions on the complete range t ∈ [0, 1] using the Latent ODE model with ODE encoder.
Table 4: As shown in Equation 2, dimension j of the mTAN embedding mTAN(t, s)[j] is given by a linear combination of intermediate univariate continuous-time functionsx hd (t, s). There is one such function defined for each input data dimension d and each time embedding h. The parameters U hdj are learnable linear combination weights.As shown in Equation3, the structure of the intermediate continuous-time functionx hd (t, s) is essentially a kernel smoother applied to the d th dimension of the time series. However, the interpolation weights κ h (t, t id ) are defined based on a time attention mechanism that leverages time embeddings, as shown in Equation4. As we can see, the same time embedding function φ h (t) is applied for all data dimensions. The form of the attention mechanism is a softmax function over the observed time points t id for dimension d. The activation within the softmax is a scaled inner product between the time embedding φ h (t) of the query time point t and the time embedding φ h (t id ) of the observed time point, the key. The parameters w and v are each d r × d k matrices where d k ≤ d r . We use a scaling factor 1
Table 5: 13.8% of examples are in the positive class.
Table 6: Interpolation performance versus percent observed time points on PhysioNet
Table 7: compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods.
Table 8: Table 2 compares the predictive performance of the models on the mortality prediction task on MIMIC-III. The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While ODE-RNN and L-ODE-ODE both have slightly better A APPENDIX A.1 ABLATION STUDY In this section, we perform ablation experiments to show the performance gain achieved by learning similarity kernel and time embedding. Table
Table 9: Ablation with time embeddingSince mTANs are fundamentally continuous-time interpolation-based models, we perform an ablation study by comparing mTANs with the IP-nets
Table 10: Comparing interpolation kernels
Table 11: Synthetic Data: Mean Squared Error

[INTRODUCTION]
Irregularly sampled time series occur in application domains including healthcare, climate science, ecology, astronomy, biology and others. It is well understood that irregular sampling poses a significant challenge to machine learning models, which typically assume fully-observed, fixed-size feature representations (Marlin et al., 2012;Yadav et al., 2018). While recurrent neural networks (RNNs) have been widely used to model such data because of their ability to handle variable length sequences, basic RNNs assume regular spacing between observation times as well as alignment of the time points where observations occur for different variables (i.e., fully-observed vectors). In practice, both of these assumptions can fail to hold for real-world sparse and irregularly observed time series. To respond to these challenges, there has been significant progress over the last decade on building and adapting machine learning models that can better capture the structure of irregularly sampled multivariate time series (Li & Marlin, 2015;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018;Shukla & Marlin, 2019;Rubanova et al., 2019).
In this work, we introduce a new model for multivariate, sparse and irregularly sampled time series that we refer to as Multi-Time Attention networks or mTANs. mTANs are fundamentally continuous-time, interpolation-based models. Their primary innovations are the inclusion of a learned continuous-time embedding mechanism coupled with a time attention mechanism that replaces the use of a fixed similarity kernel when forming representation from continuous time inputs. This gives mTANs more representational flexibility than previous interpolation-based models (Shukla & Marlin, 2019).
Our approach re-represents an irregularly sampled time series at a fixed set of reference points. The proposed time attention mechanism uses reference time points as queries and the observed time points as keys. We propose an encoder-decoder framework for end-to-end learning using an mTAN module to interface with given multivariate, sparse and irregularly sampled time series inputs. The encoder takes the irregularly sampled time series as input and produces a fixed-length latent representation over a set of reference points, while the decoder uses the latent representations to produce reconstructions conditioned on the set of observed time points. Learning uses established methods for variational autoencoders (Rezende et al., 2014;Kingma & Welling, 2014).
The main contributions of the mTAN model framework are: (1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. (2) It uses a temporally distributed latent representation to better capture local structure in time series data. (3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.

[RELATED WORK]
An irregularly sampled time series is a time series with irregular time intervals between observations. In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records (Marlin et al., 2012;Yadav et al., 2018), climate science (Schulz & Stattegger, 1997), ecology (Clark & Bjørnstad, 2004), biology (Ruf, 1999), and astronomy (Scargle, 1982). It is well understood that such data cause significant issues for standard supervised machine learning models that typically assume fully observed, fixed-size feature representations (Marlin et al., 2012).
A basic approach to dealing with irregular sampling is fixed temporal discretization. For example, Marlin et al. (2012) and Lipton et al. (2016) discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.
The alternative to temporal discretization is to construct models with the ability to directly use an irregularly sampled time series as input. Che et al. (2018) present several methods based on gated recurrent unit networks (GRUs, Chung et al. (2014)), including an approach that takes as input a sequence consisting of observed values, missing data indicators, and time intervals since the last observation. Pham et al. (2017) proposed to capture time irregularity by modifying the forget gate of an LSTM (Hochreiter & Schmidhuber, 1997), while Neil et al. (2016) introduced a new time gate that regulates access to the hidden and cell state of the LSTM. While these approaches allow the network to handle event-based sequences with irregularly spaced vector-valued observations, they do not support learning directly from vectors that are partially observed, which commonly occurs in the multivariate setting because of lack of alignment of observation times across different variables.
Another line of work has looked at using observations from the future as well as from the past for interpolation. Yoon et al. (2019) and Yoon et al. (2018) presented an approach based on the multi-directional RNN (M-RNN) that can leverage observations from the relative past and future of a given time point. Shukla & Marlin (2019) proposed the interpolation-prediction network framework, consisting of several semi-parametric RBF interpolation layers that interpolate multivariate, sparse, and irregularly sampled input time series against a set of reference time points while taking into account all observed data in a time series. Horn et al. (2020) proposed a set function-based approach for classifying time-series with irregularly sampled and unaligned observation.
Chen et al. (2018) proposed a variational auto-encoder model (Kingma & Welling, 2014;Rezende et al., 2014) for continuous time data based on the use of a neural network decoder combined with a latent ordinary differential equation (ODE) model. They model time series data via a latent continuous-time function that is defined via a neural network representation of its gradient field. Building on this, Rubanova et al. (2019) proposed a latent ODE model that uses an ODE-RNN model as the encoder. ODE-RNNs use neural ODEs to model the hidden state dynamics and an RNN to update the hidden state in the presence of a new observation. De Brouwer et al. (2019) proposed GRU-ODE-Bayes, a continuous-time version of the Gated Recurrent Unit (Chung et al., 2014). Instead of the encoder-decoder architecture where the ODE is decoupled from the input processing, GRU-ODE-Bayes provides a tighter integration by interleaving the ODE and the input processing steps.
Several recent approaches have also used attention mechanisms to model irregularly sampled time series (Song et al., 2018;Tan et al., 2020;Zhang et al., 2019) as well as medical concepts (Peng et al., 2019;Cai et al., 2018). Most of these approaches are similar to Vaswani et al. (2017) where they replace the positional encoding with an encoding of time and model sequences using self-attention.
However, instead of adding the time encoding to the input representation as in Vaswani et al. (2017), they concatenate it with the input representation. These methods use a fixed time encoding similar to the positional encoding of Vaswani et al. (2017). Xu et al. (2019) learn a functional time representation and concatenate it with the input event embedding to model time-event interactions.
Like Xu et al. (2019) and Kazemi et al. (2019), our proposed method learns a time representation. However, instead of concatenating it with the input embedding, our model learns to attend to observations at different time points by computing a similarity weighting using only the time embedding. Our proposed model uses the time embedding as both the queries and keys in the attention formulation. It learns an interpolation over the query time points by attending to the observed values at key time points. Our proposed method is thus similar to kernel-based interpolation, but learning the time attention based similarity kernel gives our model more flexibility compared to methods like that of Shukla & Marlin (2019) that use similarity kernels with fixed functional forms. Another important difference relative to many of these previous methods is that our proposed approach attends only to the observed data dimensions at each time point and hence does not require a separate imputation step to handle vector valued observations with an arbitrary collection of dimensions missing at any given time point.

[THE MULTI-TIME ATTENTION MODULE]
In this section, we present the proposed Multi-Time Attention Module (mTAN). The role of this module is to re-represent a sparse and irregularly sampled time series in a fixed-dimensional space. This module uses multiple continuous-time embeddings and attention-based interpolation. We begin by presenting notation followed by the time embedding and attention components.
Notation: In the case of a supervised learning task, we let D = {(s n , y n )|n = 1, ..., N } represent a data set containing N data cases. An individual data case consists of a single target value y n (discrete for classification), as well as a D-dimensional, sparse and irregularly sampled multivariate time series s n . Different dimensions d of the multivariate time series can have observations at different times, as well as different total numbers of observations L dn . Thus, we represent time series d for data case n as a tuple s dn = (t dn , x dn ) where t dn = [t 1dn , ..., t L dn dn ] is the list of time points at which observations are defined and x dn = [x 1dn , ..., x L dn dn ] is the corresponding list of observed values. In the case of an unsupervised task such as interpolation, each data case consists of a multivariate time series s n only. We drop the data case index n for brevity when the context is clear. Time Embedding: Time attention module is based on embedding continuous time points into a vector space. We generalize the notion of a positional encoding used in transformer-based models to continuous time. Time attention networks simultaneously leverage H embedding functions φ h (t), each outputting a representation of size d r . Dimension i of embedding h is defined as follows:
φ h (t)[i] = ω 0h • t + α 0h , if i = 0 sin(ω ih • t + α ih ), if 0 < i < d r (1)
where the ω ih 's and α ih 's are learnable parameters. The periodic terms can capture periodicity in time series data. In this case, ω ih and α ih represent the frequency and phase of the sine function. The linear term, on the other hand, can capture non-periodic patterns dependent on the progression of time. For a given difference ∆, φ h (t + ∆) can be represented as a linear function of φ h (t).
Learning the periodic time embedding functions is equivalent to using a one-layer fully connected network with a sine function non-linearity to map the time values into a higher dimensional space. By contrast, the positional encoding used in transformer models is defined only for discrete positions. We note that our time embedding functions subsume positional encodings when evaluated at discrete positions.
Multi-Time Attention: The time embedding component described above takes a continuous time point and embeds it into H different d r -dimensional spaces. In this section, we describe how we leverage time embeddings to produce a continuous-time embedding module for sparse and irregularly sampled time series. This multi-time attention embedding module mTAN(t, s) takes as input a query time point t and a set of keys and values in the form of a D-dimensional multivariate sparse and irregularly sampled time series s (as defined in the notation section above), and returns a J- Note that the output at all query points can be computed in parallel. dimensional embedding at time t. This process leverages a continuous-time attention mechanism applied to the H time embeddings. The complete computation is described below. to normalize the dot product to counteract the growth in the dot product magnitude with increase in the dimension d k .
mTAN(t, s)[j] = H h=1 D d=1x hd (t, s) • U hdj (2) x hd (t, s) = L d i=1 κ h (t, t id ) x id (3) κ h (t, t id ) = exp φ h (t)wv T φ h (t id ) T / √ d k L d i =1 exp φ h (t)wv T φ h (t i d ) T / √ d k (4
Learning the time embeddings provides our model with flexibility to learn complex temporal kernel functions κ h (t, t ). The use of multiple simultaneous time embeddings φ h (t) and a final linear combination across time embedding dimensions and data dimensions means that the final output representation function mTAN(t, s) is extremely flexible. Different input dimensions can leverage different time embeddings via learned sparsity patterns in the parameter tensor U . Information from different data dimensions can also be mixed together to create compact reduced dimensional representations. We note that all of the required computations can be parallelized using masking variables to deal with unobserved dimensions, allowing for efficient implementation on a GPU. Discretization: Since the mTAN module defines a continuous function of t given s, it can not be directly incorporated into neural network architectures that expect inputs in the form of fixeddimensional vectors or discrete sequences. However, the mTAN module can easily be adapted to produce such an output representation by materializing its output at a set of reference time points
r = [r 1 , ..., r K ].
In some cases, we may have a fixed set of such points. In other cases, the set of reference time points may need to depend on s itself. In particular, we define the auxiliary function ρ(s) to return the set of time points at which there is an observation on any dimension of s.
Given a collection of reference time points r, we define the discretized mTAN module mTAND(r, s) as mTAND(r, s)[i] = mTAN(r i , s). This module takes as input the set of reference time points r and the time series s and outputs a sequence of mTAN embeddings of length |r|, each of dimension J. The architecture of the mTAND module is shown in Figure 1. The mTAND module can be used to interface sparse and irregularly sampled multivariate time series data with any deep neural network layer type including fully-connected, recurrent, and convolutional layers. In the next section, we describe the construction of a temporal encoder-decoder architecture leveraging the mTAND module, which can be applied to both classification and interpolation tasks.

[ENCODER-DECODER FRAMEWORK]
As described in the last section, we leverage the discretized mTAN module in an encoder-decoder framework as the primary model in this paper, which we refer to as an mTAN network. We develop the encoder-decoder framework within the variational autoencoder (VAE) framework in this section.
The architecture for the model framework is shown in Figure 2.
Model Architecture: As we are modeling time series data, we begin by defining a sequence of latent states z i . Each of these latent states are IID-distributed according to a standard multivariate normal distribution p(z i ). We define the set of latent states z = [z 1 , ..., z K ] at K reference time points.
We define a three-stage decoder. First, the latent states are processed through an RNN decoder module to induce temporal dependencies resulting in a first set of deterministic latent variables h
dec RN N = [h dec 1,RN N , ..., h dec K,RN N ].
Second, the output of the RNN decoder stage and the K time points h dec RN N are provided to the mTAND module along with a set of T query time points t. The mTAND module outputs a sequence of embeddings h dec T AN = [h dec 1,T AN , ..., h dec T,T AN ] of length |t|. Third, the mTAN embeddings are independently decoded using a fully connected decoder f dec () and the result is used to parameterize an output distribution. In this work, we use a diagonal covariance Gaussian distribution with mean given by the final decoded representation and a fixed variance σ 2 . The final generated time series is given byŝ = (t, x) with all data dimensions observed. The full generative process is shown below. We let p θ (x|z, t) define the probability distribution over the values of the time series x given the time points t and the latent variables z. θ represents the parameters of all components of the decoder.
z k ∼ p(z k ) (5) h dec RN N = RNN dec (z)(6)
h dec T AN = mTAND dec (t, h dec RN N )(7)
x id ∼ N (x id ; f dec (h dec i,T AN )[d], σ 2 I)(8)
For an encoder, we simply invert the structure of the generative process. We begin by mapping the input time series s through the mTAND module along with a collection of K reference time points r.
We apply an RNN encoder to the mTAND model that outputs h enc T AN to encode longer-range temporal structure. Finally, we construct a distribution over latent variables at each reference time point using a diagonal Gaussian distribution with mean and variance output by fully connected layers applied to the RNN outputs h enc RN N . The complete encoder architecture is described below. We define q γ (z|r, s) to be the distribution over the latent variables induced by the input time series s and the reference time points r. γ represents all of the parameters in all of the encoder components.
h enc T AN = mTAND enc (r, s) (9) h enc RN N = RNN enc (h enc T AN ) (10) z k ∼ q γ (z k |µ k , σ 2 k ), µ k = f enc µ (h enc k,RN N ), σ 2 k = exp(f enc σ (h enc k,RN N ))(11)
Unsupervised Learning: To learn the parameters of our encoder-decoder model given a data set of sparse and irregularly sampled time series, we follow a slightly modified VAE training approach and maximize a normalized variational lower bound on the log marginal likelihood based on the evidence lower bound or ELBO. The learning objective is defined below where p θ (x jdn |z, t n ) and q γ (z|r, s n ) are defined in the previous section.
L NVAE (θ, γ) = N n=1 1 d L dn E qγ (z|r,sn) [log p θ (x n |z, t n )] − D KL (q γ (z|r, s n )||p(z)) (12
)
D KL (q γ (z|r, s n )||p(z)) = K i=1 D KL (q γ (z i |r, s n )||p(z i )) (13
)
log p θ (x n |z, t n ) = D d=1 L dn j=1 log p θ (x jdn |z, t jdn )(14)
Since irregularly sampled time series can have different numbers of observations across different dimensions as well as across different data cases, it can be helpful to normalize the terms in the standard ELBO objective to avoid the model focusing more on sequences that are longer at the expense of sequences that are shorter. The objective above normalizes the contribution of each data case by the total number of observations it contains. The fact that all data dimensions are not observed at all time points is accounted for in Equation 14. In practice, we use k samples from the variational distribution q γ (z|r, s n ) to compute the learning objective.
Supervised Learning: We can also augment the encoder-decoder model with a supervised learning component that leverages the latent states as a feature extractor. We define this component to be of the form p δ (y n |z) where δ are the model parameters. This leads to an augmented learning objective as shown in Equation 15where the λ term trades off the supervised and unsupervised terms.
L supervised (θ, γ, δ) = L NVAE (θ, γ) + λE qγ (z|r,sn) log p δ (y n |z)(15)
In this work, we focus on classification as an illustrative supervised learning problem. For the classification model p δ (y n |z), we use a GRU followed by a 2-layer fully connected network. We use a small number of samples to approximate the required intractable expectations during both learning and prediction. Predictions are computed by marginalizing over the latent variable as shown below.
y * = arg max y∈Y E qγ (z|r,s) [log p δ (y|z)](16)

[EXPERIMENTS]
In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset).
Additional illustrative results on synthetic data can be found in Appendix A.2.

[DATASETS:]
The PhysioNet Challenge 2012 dataset (Silva et al., 2012) consists of multivariate time series data with 37 variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first 48 hours after admission to ICU. We follow the procedures of Rubanova et al. (2019) and round the observation times to the nearest minute. This leads to 2880 possible measurement times per time series. The data set includes 4000 labeled instances and 4000 unlabeled instances. We use all 8000 instances for interpolation experiments and the 4000 labeled instances for classification experiments. We focus on predicting in-hospital mortality.  2019), we extract 53, 211 records each containing 12 physiological variables. We focus on predicting in-hospital mortality using the first 48 hours of data. 8.1% of the instances have positive labels.
The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of Rubanova et al. (2019) and construct a dataset of 6, 554 sequences with 12 channels and 50 time points. We focus on classifying each time point in the sequence into one of eleven types of activities.
Experimental Protocols: We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80% of the instances, and a test set containing the remaining 20% of instances. We use 20% of the training data for validation. In the interpolation task, we condition on a subset of available points and predict values for rest of the time points. We perform interpolation experiments with a varying percentage of observed points ranging from 50% to 90% of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at rest of the available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).
We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification problems. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for the human activity dataset. We randomly divide each data set into a training set containing 80% of the time series, and a test set containing the remaining 20% of instances. We use 20% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy.
For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix A.4.

[MODELS:]
The model we focus on is the encoder-decoder architecture based on the discretized multitime attention module (mTAND-Full). In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (mTAND-Enc). We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) (Chung et al., 2014) module as the recurrent network throughout. Architecture details can be found in Appendix A.3.
• RNN-Impute: Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples (Che et al., 2018).
• RNN-∆ t : Input is concatenated with masking variable and time interval ∆ t indicating how long the particular variable is missing.
• RNN-Decay: RNN with exponential decay on hidden states (Mozer et al., 2017;Che et al., 2018).
• GRU-D: combining hidden state decay with input decay (Che et al., 2018).
• Phased-LSTM: Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM (Neil et al., 2016) with forward filling to handle partially observed vectors.
• IP-Nets: Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU (Shukla & Marlin, 2019).
• SeFT: Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach (Horn et al., 2020).
• RNN-VAE: A VAE-based model where the encoder and decoder are standard RNN models.
• ODE-RNN: Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation (Rubanova et al., 2019).
• L-ODE-RNN: Latent ODE where the encoder is an RNN and decoder is a neural ODE (Chen et al., 2018).
• L-ODE-ODE: Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE (Rubanova et al., 2019).
Physionet Experiments: Table 1 compares the performance of all methods on the interpolation task where we observe 50% − 90% of the values in the test instances. As we can see, the proposed method (mTAND-Full) consistently and substantially outperforms all of the previous approaches across all of the settings of observed time points. We note that in this experiment, different columns correspond to different setting (for example, in the case of 70%, we condition on 70% of data and predict the rest of the data; i.e., 30%) and, hence the results across columns are not comparable. We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver (Chen et al., 2018;Rubanova et al., 2019). These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.  3 shows the ablation results by substituting fixed positional encoding (Vaswani et al., 2017) in place of learnable time embedding defined in Equation 1in mTAND-Full model on PhysioNet and MIMIC-III dataset for classification task. We report the average AUC score over 5 runs. As we can see from Table 3, learning the time embedding improves AUC score by 1% as compared to using fixed positional encodings.  (Shukla & Marlin, 2019). IP-Nets use several semiparametric RBF interpolation layers, followed by a GRU to model irregularly sampled time series. In this framework, we replace the RBK kernel with a learnable similarity kernel using mTAND module, the corresponding model is mTAND-Enc. Table 4 compares the performance of the two methods on classification task on PhysioNet, MIMIC-III and Human Activity dataset. We report the average AUC score over 5 runs. Table 4 shows that learning the similarity kernel using mTAND module performs as well or better than using a fixed RBF kernel. 

[A.2 SYNTHETIC INTERPOLATION EXPERIMENTS]
To demonstrate the capabilities of our model on the interpolation task, we generate a synthetic dataset consisting of 1000 trajectories each of 100 time points sampled over t ∈ [0, 1]. We fix 10 reference points and use RBF kernel with a fixed bandwidth of 100 for constructing local interpolations at 100 time points over [0,1]. The values at the reference points are drawn from a standard normal distribution.
We randomly sample 20 observations from each trajectory to simulate a sparse and irregularly sampled multivariate time series. We use 80% of the data for training and 20% for testing. At test time, encoder conditions on 20 irregularly sampled time points and the decoder generates interpolations on all 100 time points. Figure 3 illustrates the interpolation results on the test set for the Multi-Time Attention Network and Latent ODE model with ODE encoder (Rubanova et al., 2019). For both the models, we draw 100 samples from the approximate posterior distribution. As we can see from Figure 3, the ODE interpolations are much smoother and haven't been able to capture the local structure as well as mTANS.

[OBSERVED DATA]
Ground truth Reconstructions  Table 5 compares the proposed model with best performing baseline Latent-ODE with ODE encoder (L-ODE-ODE) on reconstruction and interpolation task. For both the tasks, we condition on the 20 irregularly sampled time points and reconstruct the input points (reconstruction) and the whole set of 100 time points (interpolation). We report the mean squared error on test set.

[A.3 ARCHITECTURE DETAILS]
Multi-Time Attention Network (mTAND-Full): In our proposed encoder-decoder framework (Figure 2), we use bi-directional GRU as the recurrent model in both encoder and decoder. In encoder, we use a 2 layer fully connected network with 50 hidden units and ReLU activations to map the RNN hidden state at each reference point to mean and variance. Similarly in decoder, mTAN embeddings are independently decoded using a 2 layer fully connected network with 50 hidden units and ReLU activations, and the result is used to parameterize the output distribution. For classification tasks, we use a separate GRU layer on top of the latent states followed by a 2-layer fully connected layer with 300 units and ReLU activations to output the class probabilities.
Multi-Time Attention Encoder (mTAND-Enc): As we show in the experiments, the proposed mTAN module can standalone be used for classification tasks. The mTAND-Enc consists of Multi-Time attention module followed by GRU to extract the final hidden state which is then passed to a 2-layer fully connected layer to output the class probabilities.

[ACKNOWLEDGEMENTS]
Research reported in this paper was partially supported by the National Institutes of Health under award numbers 5U01CA229445 and 1P41EB028242.

[]
mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.
Human Activity Experiments: Table 2 shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets.
Additional Experiments: In Appendix A.2, we demonstrate the effectiveness of learning temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.
We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix A.1. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix A.1 shows that learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.

[DISCUSSION AND CONCLUSIONS]
In this paper, we have presented the Multi-Time Attention (mTAN) module for learning from sparse and irregularly sampled data along with a VAE-based encoder-decoder model leveraging this module.
Our results show that the resulting model performs as well or better than a range of baseline and state-of-the-art models on both the interpolation and classification tasks, while offering training times that are one to two orders of magnitude faster than previous state of the art methods. While in this work we have focused on a VAE-based encoder-decoder architecture, the proposed mTAN module can be used to provide an interface between sparse and irregularly sampled time series and many different types of deep neural network architectures including GAN-based models. Composing the mTAN module with convolutional networks instead of recurrent architectures may also provide further computational enhancements due to improved parallelism.
Loss Function: For computing the evidence lower bound (ELBO) during training, we use negative log-likelihood with fixed variance as the reconstruction loss. For all the datasets, we use a fixed variance of 0.01. For computing ELBO, we use 5 samples for interpolation task and 1 sample for classification tasks. We use cross entropy loss for classification. For the classification tasks, we tune the λ parameter in the supervised learning loss function (Equation 15). We achieved best performance using λ as 100 and 5 for Physionet, MIMIC-III respectively. For human activity dataset, we achieved best results without using the regulaizer or ELBO component. We found that KL annealing with coeff 0.99 improved the performance of interpolation and classification tasks on Physionet.

[A.4 HYPERPARAMETERS]
Baselines: For Physionet and Human Activity dataset, we use the reported hyperparameters for RNN baselines as well as ODE models from Rubanova et al. (2019). For MIMIC-III dataset, we independently tune the hyperparameters of the baseline models on the validation set. We search for GRU hidden units, latent dimension, number of hidden units in fully connected network for ODE function in recognition and generative model over the range {20, 32, 64, 128, 256}. For ODEs, we also searched the number of layers in fully connected network in the range {1, 2, 3}.  In this section, we visualize the attention weights learned by our proposed model. We experiment using synthetic dataset (described in A.2) which consists of univariate time series. Figure 4 shows the attention weights learned by the encoder mTAND module. The input shown in the figure is the irregularly sampled time points and the edges show how the output at reference points attends to the values on the input time points. The final output can be computed by substituting the attention weights in Equation 3. We rescale each feature to be between 0 and 1 for Physionet and MIMIC-III dataset. We also rescale the time to be in [0, 1] for all datasets. In case of MIMIC-III dataset, for the time series missing entirely, we follow the preprocessing steps of Shukla & Marlin (2019) and assign the starting point (time t=0) value of the time series to the global mean for that variable.

[A.6.2 SOURCE CODE]
The code for reproducing the results in this paper is available at https://github.com/ reml-lab/mTAN.

[A.6.3 COMPUTING INFRASTRUCTURE]
All experiments were run on a Nvidia Titan X GPU.","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1"
Multi-Time Attention Networks for Irregularly Sampled Time Series,4c0J6lwQ4_.json,"The paper proposes a novel deep learning framework for handling irregularly sampled time series. 
The paper is well-written and easy to follow. 
The key idea of the paper is to learn embeddings for continuous time values and leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. 
The time embedding component takes a continuous time point and embeds it into multiple fixed-dimensional spaces. The multi-time attention mechanism takes a query time t and a multivariate sparse and irregularly sampled time series, and returns a fixed dimensional embedding for the query time t. This mechanism is used twice in an encoder-decoder VAE framework with varying reference (input) and query (output) time points.

The idea of the paper is novel, impactful, and well-explained. 
The evaluation and benchmarking is proper with significant improvements over the baselines for classification and interpolation tasks. The approach has significant computational gains over the best performing baseline making it more useful in practice while achieving better or similar classification/interpolation performance.

One query I had is regarding the application of the proposed framework for extrapolation or forecasting tasks. Is the framework directly applicable to such tasks given the way time is handled to get the embeddings as the 0th dimension would keep growing with time (Eqn. 1)? This can have practical implications or challenges even in variable length classification tasks where longer duration time series can be present at test time.

Typos:
time series data data
with mroe than one
In sec 4, input and the generated time series are both denoted by vector s.
","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1   

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Architecture of the mTAND module. It takes irregularly sampled time points and corresponding values as keys and values and produces a fixed dimensional representation at the query time points. The attention blocks (ATT) perform a scaled dot product attention over the observed values using the time embedding of the query and key time points. Equation 3 and 4 defines this operation.Note that the output at all query points can be computed in parallel.
Table 2: Figure 2 :2Figure 2: Architecture of the proposed encoder-decoder framework mTAND-Full. The classifier is required only for performing classification tasks. The mTAND module is shown in Figure 1.
Table 3: Figure 3 :3Figure 3: Interpolations on the synthetic interpolation dataset. The columns represent 3 different examples. First row: Ground truth trajectories with observed points, second row: reconstructions on the complete range t ∈ [0, 1] using the proposed model mTAN, third row: reconstructions on the complete range t ∈ [0, 1] using the Latent ODE model with ODE encoder.
Table 4: As shown in Equation 2, dimension j of the mTAN embedding mTAN(t, s)[j] is given by a linear combination of intermediate univariate continuous-time functionsx hd (t, s). There is one such function defined for each input data dimension d and each time embedding h. The parameters U hdj are learnable linear combination weights.As shown in Equation3, the structure of the intermediate continuous-time functionx hd (t, s) is essentially a kernel smoother applied to the d th dimension of the time series. However, the interpolation weights κ h (t, t id ) are defined based on a time attention mechanism that leverages time embeddings, as shown in Equation4. As we can see, the same time embedding function φ h (t) is applied for all data dimensions. The form of the attention mechanism is a softmax function over the observed time points t id for dimension d. The activation within the softmax is a scaled inner product between the time embedding φ h (t) of the query time point t and the time embedding φ h (t id ) of the observed time point, the key. The parameters w and v are each d r × d k matrices where d k ≤ d r . We use a scaling factor 1
Table 5: 13.8% of examples are in the positive class.
Table 6: Interpolation performance versus percent observed time points on PhysioNet
Table 7: compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods.
Table 8: Table 2 compares the predictive performance of the models on the mortality prediction task on MIMIC-III. The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While ODE-RNN and L-ODE-ODE both have slightly better A APPENDIX A.1 ABLATION STUDY In this section, we perform ablation experiments to show the performance gain achieved by learning similarity kernel and time embedding. Table
Table 9: Ablation with time embeddingSince mTANs are fundamentally continuous-time interpolation-based models, we perform an ablation study by comparing mTANs with the IP-nets
Table 10: Comparing interpolation kernels
Table 11: Synthetic Data: Mean Squared Error

[INTRODUCTION]
Irregularly sampled time series occur in application domains including healthcare, climate science, ecology, astronomy, biology and others. It is well understood that irregular sampling poses a significant challenge to machine learning models, which typically assume fully-observed, fixed-size feature representations (Marlin et al., 2012;Yadav et al., 2018). While recurrent neural networks (RNNs) have been widely used to model such data because of their ability to handle variable length sequences, basic RNNs assume regular spacing between observation times as well as alignment of the time points where observations occur for different variables (i.e., fully-observed vectors). In practice, both of these assumptions can fail to hold for real-world sparse and irregularly observed time series. To respond to these challenges, there has been significant progress over the last decade on building and adapting machine learning models that can better capture the structure of irregularly sampled multivariate time series (Li & Marlin, 2015;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018;Shukla & Marlin, 2019;Rubanova et al., 2019).
In this work, we introduce a new model for multivariate, sparse and irregularly sampled time series that we refer to as Multi-Time Attention networks or mTANs. mTANs are fundamentally continuous-time, interpolation-based models. Their primary innovations are the inclusion of a learned continuous-time embedding mechanism coupled with a time attention mechanism that replaces the use of a fixed similarity kernel when forming representation from continuous time inputs. This gives mTANs more representational flexibility than previous interpolation-based models (Shukla & Marlin, 2019).
Our approach re-represents an irregularly sampled time series at a fixed set of reference points. The proposed time attention mechanism uses reference time points as queries and the observed time points as keys. We propose an encoder-decoder framework for end-to-end learning using an mTAN module to interface with given multivariate, sparse and irregularly sampled time series inputs. The encoder takes the irregularly sampled time series as input and produces a fixed-length latent representation over a set of reference points, while the decoder uses the latent representations to produce reconstructions conditioned on the set of observed time points. Learning uses established methods for variational autoencoders (Rezende et al., 2014;Kingma & Welling, 2014).
The main contributions of the mTAN model framework are: (1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. (2) It uses a temporally distributed latent representation to better capture local structure in time series data. (3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.

[RELATED WORK]
An irregularly sampled time series is a time series with irregular time intervals between observations. In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records (Marlin et al., 2012;Yadav et al., 2018), climate science (Schulz & Stattegger, 1997), ecology (Clark & Bjørnstad, 2004), biology (Ruf, 1999), and astronomy (Scargle, 1982). It is well understood that such data cause significant issues for standard supervised machine learning models that typically assume fully observed, fixed-size feature representations (Marlin et al., 2012).
A basic approach to dealing with irregular sampling is fixed temporal discretization. For example, Marlin et al. (2012) and Lipton et al. (2016) discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.
The alternative to temporal discretization is to construct models with the ability to directly use an irregularly sampled time series as input. Che et al. (2018) present several methods based on gated recurrent unit networks (GRUs, Chung et al. (2014)), including an approach that takes as input a sequence consisting of observed values, missing data indicators, and time intervals since the last observation. Pham et al. (2017) proposed to capture time irregularity by modifying the forget gate of an LSTM (Hochreiter & Schmidhuber, 1997), while Neil et al. (2016) introduced a new time gate that regulates access to the hidden and cell state of the LSTM. While these approaches allow the network to handle event-based sequences with irregularly spaced vector-valued observations, they do not support learning directly from vectors that are partially observed, which commonly occurs in the multivariate setting because of lack of alignment of observation times across different variables.
Another line of work has looked at using observations from the future as well as from the past for interpolation. Yoon et al. (2019) and Yoon et al. (2018) presented an approach based on the multi-directional RNN (M-RNN) that can leverage observations from the relative past and future of a given time point. Shukla & Marlin (2019) proposed the interpolation-prediction network framework, consisting of several semi-parametric RBF interpolation layers that interpolate multivariate, sparse, and irregularly sampled input time series against a set of reference time points while taking into account all observed data in a time series. Horn et al. (2020) proposed a set function-based approach for classifying time-series with irregularly sampled and unaligned observation.
Chen et al. (2018) proposed a variational auto-encoder model (Kingma & Welling, 2014;Rezende et al., 2014) for continuous time data based on the use of a neural network decoder combined with a latent ordinary differential equation (ODE) model. They model time series data via a latent continuous-time function that is defined via a neural network representation of its gradient field. Building on this, Rubanova et al. (2019) proposed a latent ODE model that uses an ODE-RNN model as the encoder. ODE-RNNs use neural ODEs to model the hidden state dynamics and an RNN to update the hidden state in the presence of a new observation. De Brouwer et al. (2019) proposed GRU-ODE-Bayes, a continuous-time version of the Gated Recurrent Unit (Chung et al., 2014). Instead of the encoder-decoder architecture where the ODE is decoupled from the input processing, GRU-ODE-Bayes provides a tighter integration by interleaving the ODE and the input processing steps.
Several recent approaches have also used attention mechanisms to model irregularly sampled time series (Song et al., 2018;Tan et al., 2020;Zhang et al., 2019) as well as medical concepts (Peng et al., 2019;Cai et al., 2018). Most of these approaches are similar to Vaswani et al. (2017) where they replace the positional encoding with an encoding of time and model sequences using self-attention.
However, instead of adding the time encoding to the input representation as in Vaswani et al. (2017), they concatenate it with the input representation. These methods use a fixed time encoding similar to the positional encoding of Vaswani et al. (2017). Xu et al. (2019) learn a functional time representation and concatenate it with the input event embedding to model time-event interactions.
Like Xu et al. (2019) and Kazemi et al. (2019), our proposed method learns a time representation. However, instead of concatenating it with the input embedding, our model learns to attend to observations at different time points by computing a similarity weighting using only the time embedding. Our proposed model uses the time embedding as both the queries and keys in the attention formulation. It learns an interpolation over the query time points by attending to the observed values at key time points. Our proposed method is thus similar to kernel-based interpolation, but learning the time attention based similarity kernel gives our model more flexibility compared to methods like that of Shukla & Marlin (2019) that use similarity kernels with fixed functional forms. Another important difference relative to many of these previous methods is that our proposed approach attends only to the observed data dimensions at each time point and hence does not require a separate imputation step to handle vector valued observations with an arbitrary collection of dimensions missing at any given time point.

[THE MULTI-TIME ATTENTION MODULE]
In this section, we present the proposed Multi-Time Attention Module (mTAN). The role of this module is to re-represent a sparse and irregularly sampled time series in a fixed-dimensional space. This module uses multiple continuous-time embeddings and attention-based interpolation. We begin by presenting notation followed by the time embedding and attention components.
Notation: In the case of a supervised learning task, we let D = {(s n , y n )|n = 1, ..., N } represent a data set containing N data cases. An individual data case consists of a single target value y n (discrete for classification), as well as a D-dimensional, sparse and irregularly sampled multivariate time series s n . Different dimensions d of the multivariate time series can have observations at different times, as well as different total numbers of observations L dn . Thus, we represent time series d for data case n as a tuple s dn = (t dn , x dn ) where t dn = [t 1dn , ..., t L dn dn ] is the list of time points at which observations are defined and x dn = [x 1dn , ..., x L dn dn ] is the corresponding list of observed values. In the case of an unsupervised task such as interpolation, each data case consists of a multivariate time series s n only. We drop the data case index n for brevity when the context is clear. Time Embedding: Time attention module is based on embedding continuous time points into a vector space. We generalize the notion of a positional encoding used in transformer-based models to continuous time. Time attention networks simultaneously leverage H embedding functions φ h (t), each outputting a representation of size d r . Dimension i of embedding h is defined as follows:
φ h (t)[i] = ω 0h • t + α 0h , if i = 0 sin(ω ih • t + α ih ), if 0 < i < d r (1)
where the ω ih 's and α ih 's are learnable parameters. The periodic terms can capture periodicity in time series data. In this case, ω ih and α ih represent the frequency and phase of the sine function. The linear term, on the other hand, can capture non-periodic patterns dependent on the progression of time. For a given difference ∆, φ h (t + ∆) can be represented as a linear function of φ h (t).
Learning the periodic time embedding functions is equivalent to using a one-layer fully connected network with a sine function non-linearity to map the time values into a higher dimensional space. By contrast, the positional encoding used in transformer models is defined only for discrete positions. We note that our time embedding functions subsume positional encodings when evaluated at discrete positions.
Multi-Time Attention: The time embedding component described above takes a continuous time point and embeds it into H different d r -dimensional spaces. In this section, we describe how we leverage time embeddings to produce a continuous-time embedding module for sparse and irregularly sampled time series. This multi-time attention embedding module mTAN(t, s) takes as input a query time point t and a set of keys and values in the form of a D-dimensional multivariate sparse and irregularly sampled time series s (as defined in the notation section above), and returns a J- Note that the output at all query points can be computed in parallel. dimensional embedding at time t. This process leverages a continuous-time attention mechanism applied to the H time embeddings. The complete computation is described below. to normalize the dot product to counteract the growth in the dot product magnitude with increase in the dimension d k .
mTAN(t, s)[j] = H h=1 D d=1x hd (t, s) • U hdj (2) x hd (t, s) = L d i=1 κ h (t, t id ) x id (3) κ h (t, t id ) = exp φ h (t)wv T φ h (t id ) T / √ d k L d i =1 exp φ h (t)wv T φ h (t i d ) T / √ d k (4
Learning the time embeddings provides our model with flexibility to learn complex temporal kernel functions κ h (t, t ). The use of multiple simultaneous time embeddings φ h (t) and a final linear combination across time embedding dimensions and data dimensions means that the final output representation function mTAN(t, s) is extremely flexible. Different input dimensions can leverage different time embeddings via learned sparsity patterns in the parameter tensor U . Information from different data dimensions can also be mixed together to create compact reduced dimensional representations. We note that all of the required computations can be parallelized using masking variables to deal with unobserved dimensions, allowing for efficient implementation on a GPU. Discretization: Since the mTAN module defines a continuous function of t given s, it can not be directly incorporated into neural network architectures that expect inputs in the form of fixeddimensional vectors or discrete sequences. However, the mTAN module can easily be adapted to produce such an output representation by materializing its output at a set of reference time points
r = [r 1 , ..., r K ].
In some cases, we may have a fixed set of such points. In other cases, the set of reference time points may need to depend on s itself. In particular, we define the auxiliary function ρ(s) to return the set of time points at which there is an observation on any dimension of s.
Given a collection of reference time points r, we define the discretized mTAN module mTAND(r, s) as mTAND(r, s)[i] = mTAN(r i , s). This module takes as input the set of reference time points r and the time series s and outputs a sequence of mTAN embeddings of length |r|, each of dimension J. The architecture of the mTAND module is shown in Figure 1. The mTAND module can be used to interface sparse and irregularly sampled multivariate time series data with any deep neural network layer type including fully-connected, recurrent, and convolutional layers. In the next section, we describe the construction of a temporal encoder-decoder architecture leveraging the mTAND module, which can be applied to both classification and interpolation tasks.

[ENCODER-DECODER FRAMEWORK]
As described in the last section, we leverage the discretized mTAN module in an encoder-decoder framework as the primary model in this paper, which we refer to as an mTAN network. We develop the encoder-decoder framework within the variational autoencoder (VAE) framework in this section.
The architecture for the model framework is shown in Figure 2.
Model Architecture: As we are modeling time series data, we begin by defining a sequence of latent states z i . Each of these latent states are IID-distributed according to a standard multivariate normal distribution p(z i ). We define the set of latent states z = [z 1 , ..., z K ] at K reference time points.
We define a three-stage decoder. First, the latent states are processed through an RNN decoder module to induce temporal dependencies resulting in a first set of deterministic latent variables h
dec RN N = [h dec 1,RN N , ..., h dec K,RN N ].
Second, the output of the RNN decoder stage and the K time points h dec RN N are provided to the mTAND module along with a set of T query time points t. The mTAND module outputs a sequence of embeddings h dec T AN = [h dec 1,T AN , ..., h dec T,T AN ] of length |t|. Third, the mTAN embeddings are independently decoded using a fully connected decoder f dec () and the result is used to parameterize an output distribution. In this work, we use a diagonal covariance Gaussian distribution with mean given by the final decoded representation and a fixed variance σ 2 . The final generated time series is given byŝ = (t, x) with all data dimensions observed. The full generative process is shown below. We let p θ (x|z, t) define the probability distribution over the values of the time series x given the time points t and the latent variables z. θ represents the parameters of all components of the decoder.
z k ∼ p(z k ) (5) h dec RN N = RNN dec (z)(6)
h dec T AN = mTAND dec (t, h dec RN N )(7)
x id ∼ N (x id ; f dec (h dec i,T AN )[d], σ 2 I)(8)
For an encoder, we simply invert the structure of the generative process. We begin by mapping the input time series s through the mTAND module along with a collection of K reference time points r.
We apply an RNN encoder to the mTAND model that outputs h enc T AN to encode longer-range temporal structure. Finally, we construct a distribution over latent variables at each reference time point using a diagonal Gaussian distribution with mean and variance output by fully connected layers applied to the RNN outputs h enc RN N . The complete encoder architecture is described below. We define q γ (z|r, s) to be the distribution over the latent variables induced by the input time series s and the reference time points r. γ represents all of the parameters in all of the encoder components.
h enc T AN = mTAND enc (r, s) (9) h enc RN N = RNN enc (h enc T AN ) (10) z k ∼ q γ (z k |µ k , σ 2 k ), µ k = f enc µ (h enc k,RN N ), σ 2 k = exp(f enc σ (h enc k,RN N ))(11)
Unsupervised Learning: To learn the parameters of our encoder-decoder model given a data set of sparse and irregularly sampled time series, we follow a slightly modified VAE training approach and maximize a normalized variational lower bound on the log marginal likelihood based on the evidence lower bound or ELBO. The learning objective is defined below where p θ (x jdn |z, t n ) and q γ (z|r, s n ) are defined in the previous section.
L NVAE (θ, γ) = N n=1 1 d L dn E qγ (z|r,sn) [log p θ (x n |z, t n )] − D KL (q γ (z|r, s n )||p(z)) (12
)
D KL (q γ (z|r, s n )||p(z)) = K i=1 D KL (q γ (z i |r, s n )||p(z i )) (13
)
log p θ (x n |z, t n ) = D d=1 L dn j=1 log p θ (x jdn |z, t jdn )(14)
Since irregularly sampled time series can have different numbers of observations across different dimensions as well as across different data cases, it can be helpful to normalize the terms in the standard ELBO objective to avoid the model focusing more on sequences that are longer at the expense of sequences that are shorter. The objective above normalizes the contribution of each data case by the total number of observations it contains. The fact that all data dimensions are not observed at all time points is accounted for in Equation 14. In practice, we use k samples from the variational distribution q γ (z|r, s n ) to compute the learning objective.
Supervised Learning: We can also augment the encoder-decoder model with a supervised learning component that leverages the latent states as a feature extractor. We define this component to be of the form p δ (y n |z) where δ are the model parameters. This leads to an augmented learning objective as shown in Equation 15where the λ term trades off the supervised and unsupervised terms.
L supervised (θ, γ, δ) = L NVAE (θ, γ) + λE qγ (z|r,sn) log p δ (y n |z)(15)
In this work, we focus on classification as an illustrative supervised learning problem. For the classification model p δ (y n |z), we use a GRU followed by a 2-layer fully connected network. We use a small number of samples to approximate the required intractable expectations during both learning and prediction. Predictions are computed by marginalizing over the latent variable as shown below.
y * = arg max y∈Y E qγ (z|r,s) [log p δ (y|z)](16)

[EXPERIMENTS]
In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset).
Additional illustrative results on synthetic data can be found in Appendix A.2.

[DATASETS:]
The PhysioNet Challenge 2012 dataset (Silva et al., 2012) consists of multivariate time series data with 37 variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first 48 hours after admission to ICU. We follow the procedures of Rubanova et al. (2019) and round the observation times to the nearest minute. This leads to 2880 possible measurement times per time series. The data set includes 4000 labeled instances and 4000 unlabeled instances. We use all 8000 instances for interpolation experiments and the 4000 labeled instances for classification experiments. We focus on predicting in-hospital mortality.  2019), we extract 53, 211 records each containing 12 physiological variables. We focus on predicting in-hospital mortality using the first 48 hours of data. 8.1% of the instances have positive labels.
The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of Rubanova et al. (2019) and construct a dataset of 6, 554 sequences with 12 channels and 50 time points. We focus on classifying each time point in the sequence into one of eleven types of activities.
Experimental Protocols: We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80% of the instances, and a test set containing the remaining 20% of instances. We use 20% of the training data for validation. In the interpolation task, we condition on a subset of available points and predict values for rest of the time points. We perform interpolation experiments with a varying percentage of observed points ranging from 50% to 90% of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at rest of the available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).
We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification problems. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for the human activity dataset. We randomly divide each data set into a training set containing 80% of the time series, and a test set containing the remaining 20% of instances. We use 20% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy.
For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix A.4.

[MODELS:]
The model we focus on is the encoder-decoder architecture based on the discretized multitime attention module (mTAND-Full). In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (mTAND-Enc). We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) (Chung et al., 2014) module as the recurrent network throughout. Architecture details can be found in Appendix A.3.
• RNN-Impute: Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples (Che et al., 2018).
• RNN-∆ t : Input is concatenated with masking variable and time interval ∆ t indicating how long the particular variable is missing.
• RNN-Decay: RNN with exponential decay on hidden states (Mozer et al., 2017;Che et al., 2018).
• GRU-D: combining hidden state decay with input decay (Che et al., 2018).
• Phased-LSTM: Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM (Neil et al., 2016) with forward filling to handle partially observed vectors.
• IP-Nets: Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU (Shukla & Marlin, 2019).
• SeFT: Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach (Horn et al., 2020).
• RNN-VAE: A VAE-based model where the encoder and decoder are standard RNN models.
• ODE-RNN: Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation (Rubanova et al., 2019).
• L-ODE-RNN: Latent ODE where the encoder is an RNN and decoder is a neural ODE (Chen et al., 2018).
• L-ODE-ODE: Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE (Rubanova et al., 2019).
Physionet Experiments: Table 1 compares the performance of all methods on the interpolation task where we observe 50% − 90% of the values in the test instances. As we can see, the proposed method (mTAND-Full) consistently and substantially outperforms all of the previous approaches across all of the settings of observed time points. We note that in this experiment, different columns correspond to different setting (for example, in the case of 70%, we condition on 70% of data and predict the rest of the data; i.e., 30%) and, hence the results across columns are not comparable. We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver (Chen et al., 2018;Rubanova et al., 2019). These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.  3 shows the ablation results by substituting fixed positional encoding (Vaswani et al., 2017) in place of learnable time embedding defined in Equation 1in mTAND-Full model on PhysioNet and MIMIC-III dataset for classification task. We report the average AUC score over 5 runs. As we can see from Table 3, learning the time embedding improves AUC score by 1% as compared to using fixed positional encodings.  (Shukla & Marlin, 2019). IP-Nets use several semiparametric RBF interpolation layers, followed by a GRU to model irregularly sampled time series. In this framework, we replace the RBK kernel with a learnable similarity kernel using mTAND module, the corresponding model is mTAND-Enc. Table 4 compares the performance of the two methods on classification task on PhysioNet, MIMIC-III and Human Activity dataset. We report the average AUC score over 5 runs. Table 4 shows that learning the similarity kernel using mTAND module performs as well or better than using a fixed RBF kernel. 

[A.2 SYNTHETIC INTERPOLATION EXPERIMENTS]
To demonstrate the capabilities of our model on the interpolation task, we generate a synthetic dataset consisting of 1000 trajectories each of 100 time points sampled over t ∈ [0, 1]. We fix 10 reference points and use RBF kernel with a fixed bandwidth of 100 for constructing local interpolations at 100 time points over [0,1]. The values at the reference points are drawn from a standard normal distribution.
We randomly sample 20 observations from each trajectory to simulate a sparse and irregularly sampled multivariate time series. We use 80% of the data for training and 20% for testing. At test time, encoder conditions on 20 irregularly sampled time points and the decoder generates interpolations on all 100 time points. Figure 3 illustrates the interpolation results on the test set for the Multi-Time Attention Network and Latent ODE model with ODE encoder (Rubanova et al., 2019). For both the models, we draw 100 samples from the approximate posterior distribution. As we can see from Figure 3, the ODE interpolations are much smoother and haven't been able to capture the local structure as well as mTANS.

[OBSERVED DATA]
Ground truth Reconstructions  Table 5 compares the proposed model with best performing baseline Latent-ODE with ODE encoder (L-ODE-ODE) on reconstruction and interpolation task. For both the tasks, we condition on the 20 irregularly sampled time points and reconstruct the input points (reconstruction) and the whole set of 100 time points (interpolation). We report the mean squared error on test set.

[A.3 ARCHITECTURE DETAILS]
Multi-Time Attention Network (mTAND-Full): In our proposed encoder-decoder framework (Figure 2), we use bi-directional GRU as the recurrent model in both encoder and decoder. In encoder, we use a 2 layer fully connected network with 50 hidden units and ReLU activations to map the RNN hidden state at each reference point to mean and variance. Similarly in decoder, mTAN embeddings are independently decoded using a 2 layer fully connected network with 50 hidden units and ReLU activations, and the result is used to parameterize the output distribution. For classification tasks, we use a separate GRU layer on top of the latent states followed by a 2-layer fully connected layer with 300 units and ReLU activations to output the class probabilities.
Multi-Time Attention Encoder (mTAND-Enc): As we show in the experiments, the proposed mTAN module can standalone be used for classification tasks. The mTAND-Enc consists of Multi-Time attention module followed by GRU to extract the final hidden state which is then passed to a 2-layer fully connected layer to output the class probabilities.

[ACKNOWLEDGEMENTS]
Research reported in this paper was partially supported by the National Institutes of Health under award numbers 5U01CA229445 and 1P41EB028242.

[]
mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.
Human Activity Experiments: Table 2 shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets.
Additional Experiments: In Appendix A.2, we demonstrate the effectiveness of learning temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.
We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix A.1. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix A.1 shows that learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.

[DISCUSSION AND CONCLUSIONS]
In this paper, we have presented the Multi-Time Attention (mTAN) module for learning from sparse and irregularly sampled data along with a VAE-based encoder-decoder model leveraging this module.
Our results show that the resulting model performs as well or better than a range of baseline and state-of-the-art models on both the interpolation and classification tasks, while offering training times that are one to two orders of magnitude faster than previous state of the art methods. While in this work we have focused on a VAE-based encoder-decoder architecture, the proposed mTAN module can be used to provide an interface between sparse and irregularly sampled time series and many different types of deep neural network architectures including GAN-based models. Composing the mTAN module with convolutional networks instead of recurrent architectures may also provide further computational enhancements due to improved parallelism.
Loss Function: For computing the evidence lower bound (ELBO) during training, we use negative log-likelihood with fixed variance as the reconstruction loss. For all the datasets, we use a fixed variance of 0.01. For computing ELBO, we use 5 samples for interpolation task and 1 sample for classification tasks. We use cross entropy loss for classification. For the classification tasks, we tune the λ parameter in the supervised learning loss function (Equation 15). We achieved best performance using λ as 100 and 5 for Physionet, MIMIC-III respectively. For human activity dataset, we achieved best results without using the regulaizer or ELBO component. We found that KL annealing with coeff 0.99 improved the performance of interpolation and classification tasks on Physionet.

[A.4 HYPERPARAMETERS]
Baselines: For Physionet and Human Activity dataset, we use the reported hyperparameters for RNN baselines as well as ODE models from Rubanova et al. (2019). For MIMIC-III dataset, we independently tune the hyperparameters of the baseline models on the validation set. We search for GRU hidden units, latent dimension, number of hidden units in fully connected network for ODE function in recognition and generative model over the range {20, 32, 64, 128, 256}. For ODEs, we also searched the number of layers in fully connected network in the range {1, 2, 3}.  In this section, we visualize the attention weights learned by our proposed model. We experiment using synthetic dataset (described in A.2) which consists of univariate time series. Figure 4 shows the attention weights learned by the encoder mTAND module. The input shown in the figure is the irregularly sampled time points and the edges show how the output at reference points attends to the values on the input time points. The final output can be computed by substituting the attention weights in Equation 3. We rescale each feature to be between 0 and 1 for Physionet and MIMIC-III dataset. We also rescale the time to be in [0, 1] for all datasets. In case of MIMIC-III dataset, for the time series missing entirely, we follow the preprocessing steps of Shukla & Marlin (2019) and assign the starting point (time t=0) value of the time series to the global mean for that variable.

[A.6.2 SOURCE CODE]
The code for reproducing the results in this paper is available at https://github.com/ reml-lab/mTAN.

[A.6.3 COMPUTING INFRASTRUCTURE]
All experiments were run on a Nvidia Titan X GPU.","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1"
Multi-Time Attention Networks for Irregularly Sampled Time Series,4c0J6lwQ4_.json,"This paper proposes a novel approach to learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. In particular, it proposes an mTAN network to leverage the mTAN module in an encoder-decoder framework for both unsupervised and supervised Learning.  The main contribution of this paper is the introduction of Multi-Time Attention Networks to learns a time representation and learns to attend to observations at different time points by computing a similarity weighting by the learning time embedding.  Empirical studies are performed to show the superiority of the proposed model mTANs over several baseline approaches on the tasks unsupervised and supervised learning. 

Pros: 
 
1. The paper proposes a novel model to learns a continuous time representation and adapt to fixed dimensional vectors or discrete sequences. For me, the problem itself is real and practical. 
2. The proposed mTAN is novel for capturing the time dependencies time-series, sparse, irregularly sampled, and multivariate data. 
3. This paper provides comprehensive experiments, including both unsupervised and supervised learning results, to show the effectiveness of the proposed framework.  

Cons: 
 
1.	The paper uses a lot of notations in equations and descriptions, which cause a little bit confusion to follow the authors’ idea. Please consider providing a table to list all the import symbols.
2.	It is better to depict this proposed model structure give the audience a main picture to model.
3.	As the model is attention-based, it has the ability to find the relationships among sequential events. Is it possible to provide one or two case studies to demonstrate the dependencies between time points?
 

Questions during rebuttal period: 
 
Please address and clarify the cons above 

Minor comments: 
1.A few references only list authors, title, and year, but miss publisher or conference, such as “ Michael Mozer, Denis Kazakov, and Robert Lindsey. Discrete event, continuous time rnns. 2017“
","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1   

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Architecture of the mTAND module. It takes irregularly sampled time points and corresponding values as keys and values and produces a fixed dimensional representation at the query time points. The attention blocks (ATT) perform a scaled dot product attention over the observed values using the time embedding of the query and key time points. Equation 3 and 4 defines this operation.Note that the output at all query points can be computed in parallel.
Table 2: Figure 2 :2Figure 2: Architecture of the proposed encoder-decoder framework mTAND-Full. The classifier is required only for performing classification tasks. The mTAND module is shown in Figure 1.
Table 3: Figure 3 :3Figure 3: Interpolations on the synthetic interpolation dataset. The columns represent 3 different examples. First row: Ground truth trajectories with observed points, second row: reconstructions on the complete range t ∈ [0, 1] using the proposed model mTAN, third row: reconstructions on the complete range t ∈ [0, 1] using the Latent ODE model with ODE encoder.
Table 4: As shown in Equation 2, dimension j of the mTAN embedding mTAN(t, s)[j] is given by a linear combination of intermediate univariate continuous-time functionsx hd (t, s). There is one such function defined for each input data dimension d and each time embedding h. The parameters U hdj are learnable linear combination weights.As shown in Equation3, the structure of the intermediate continuous-time functionx hd (t, s) is essentially a kernel smoother applied to the d th dimension of the time series. However, the interpolation weights κ h (t, t id ) are defined based on a time attention mechanism that leverages time embeddings, as shown in Equation4. As we can see, the same time embedding function φ h (t) is applied for all data dimensions. The form of the attention mechanism is a softmax function over the observed time points t id for dimension d. The activation within the softmax is a scaled inner product between the time embedding φ h (t) of the query time point t and the time embedding φ h (t id ) of the observed time point, the key. The parameters w and v are each d r × d k matrices where d k ≤ d r . We use a scaling factor 1
Table 5: 13.8% of examples are in the positive class.
Table 6: Interpolation performance versus percent observed time points on PhysioNet
Table 7: compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods.
Table 8: Table 2 compares the predictive performance of the models on the mortality prediction task on MIMIC-III. The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While ODE-RNN and L-ODE-ODE both have slightly better A APPENDIX A.1 ABLATION STUDY In this section, we perform ablation experiments to show the performance gain achieved by learning similarity kernel and time embedding. Table
Table 9: Ablation with time embeddingSince mTANs are fundamentally continuous-time interpolation-based models, we perform an ablation study by comparing mTANs with the IP-nets
Table 10: Comparing interpolation kernels
Table 11: Synthetic Data: Mean Squared Error

[INTRODUCTION]
Irregularly sampled time series occur in application domains including healthcare, climate science, ecology, astronomy, biology and others. It is well understood that irregular sampling poses a significant challenge to machine learning models, which typically assume fully-observed, fixed-size feature representations (Marlin et al., 2012;Yadav et al., 2018). While recurrent neural networks (RNNs) have been widely used to model such data because of their ability to handle variable length sequences, basic RNNs assume regular spacing between observation times as well as alignment of the time points where observations occur for different variables (i.e., fully-observed vectors). In practice, both of these assumptions can fail to hold for real-world sparse and irregularly observed time series. To respond to these challenges, there has been significant progress over the last decade on building and adapting machine learning models that can better capture the structure of irregularly sampled multivariate time series (Li & Marlin, 2015;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018;Shukla & Marlin, 2019;Rubanova et al., 2019).
In this work, we introduce a new model for multivariate, sparse and irregularly sampled time series that we refer to as Multi-Time Attention networks or mTANs. mTANs are fundamentally continuous-time, interpolation-based models. Their primary innovations are the inclusion of a learned continuous-time embedding mechanism coupled with a time attention mechanism that replaces the use of a fixed similarity kernel when forming representation from continuous time inputs. This gives mTANs more representational flexibility than previous interpolation-based models (Shukla & Marlin, 2019).
Our approach re-represents an irregularly sampled time series at a fixed set of reference points. The proposed time attention mechanism uses reference time points as queries and the observed time points as keys. We propose an encoder-decoder framework for end-to-end learning using an mTAN module to interface with given multivariate, sparse and irregularly sampled time series inputs. The encoder takes the irregularly sampled time series as input and produces a fixed-length latent representation over a set of reference points, while the decoder uses the latent representations to produce reconstructions conditioned on the set of observed time points. Learning uses established methods for variational autoencoders (Rezende et al., 2014;Kingma & Welling, 2014).
The main contributions of the mTAN model framework are: (1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. (2) It uses a temporally distributed latent representation to better capture local structure in time series data. (3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.

[RELATED WORK]
An irregularly sampled time series is a time series with irregular time intervals between observations. In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records (Marlin et al., 2012;Yadav et al., 2018), climate science (Schulz & Stattegger, 1997), ecology (Clark & Bjørnstad, 2004), biology (Ruf, 1999), and astronomy (Scargle, 1982). It is well understood that such data cause significant issues for standard supervised machine learning models that typically assume fully observed, fixed-size feature representations (Marlin et al., 2012).
A basic approach to dealing with irregular sampling is fixed temporal discretization. For example, Marlin et al. (2012) and Lipton et al. (2016) discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.
The alternative to temporal discretization is to construct models with the ability to directly use an irregularly sampled time series as input. Che et al. (2018) present several methods based on gated recurrent unit networks (GRUs, Chung et al. (2014)), including an approach that takes as input a sequence consisting of observed values, missing data indicators, and time intervals since the last observation. Pham et al. (2017) proposed to capture time irregularity by modifying the forget gate of an LSTM (Hochreiter & Schmidhuber, 1997), while Neil et al. (2016) introduced a new time gate that regulates access to the hidden and cell state of the LSTM. While these approaches allow the network to handle event-based sequences with irregularly spaced vector-valued observations, they do not support learning directly from vectors that are partially observed, which commonly occurs in the multivariate setting because of lack of alignment of observation times across different variables.
Another line of work has looked at using observations from the future as well as from the past for interpolation. Yoon et al. (2019) and Yoon et al. (2018) presented an approach based on the multi-directional RNN (M-RNN) that can leverage observations from the relative past and future of a given time point. Shukla & Marlin (2019) proposed the interpolation-prediction network framework, consisting of several semi-parametric RBF interpolation layers that interpolate multivariate, sparse, and irregularly sampled input time series against a set of reference time points while taking into account all observed data in a time series. Horn et al. (2020) proposed a set function-based approach for classifying time-series with irregularly sampled and unaligned observation.
Chen et al. (2018) proposed a variational auto-encoder model (Kingma & Welling, 2014;Rezende et al., 2014) for continuous time data based on the use of a neural network decoder combined with a latent ordinary differential equation (ODE) model. They model time series data via a latent continuous-time function that is defined via a neural network representation of its gradient field. Building on this, Rubanova et al. (2019) proposed a latent ODE model that uses an ODE-RNN model as the encoder. ODE-RNNs use neural ODEs to model the hidden state dynamics and an RNN to update the hidden state in the presence of a new observation. De Brouwer et al. (2019) proposed GRU-ODE-Bayes, a continuous-time version of the Gated Recurrent Unit (Chung et al., 2014). Instead of the encoder-decoder architecture where the ODE is decoupled from the input processing, GRU-ODE-Bayes provides a tighter integration by interleaving the ODE and the input processing steps.
Several recent approaches have also used attention mechanisms to model irregularly sampled time series (Song et al., 2018;Tan et al., 2020;Zhang et al., 2019) as well as medical concepts (Peng et al., 2019;Cai et al., 2018). Most of these approaches are similar to Vaswani et al. (2017) where they replace the positional encoding with an encoding of time and model sequences using self-attention.
However, instead of adding the time encoding to the input representation as in Vaswani et al. (2017), they concatenate it with the input representation. These methods use a fixed time encoding similar to the positional encoding of Vaswani et al. (2017). Xu et al. (2019) learn a functional time representation and concatenate it with the input event embedding to model time-event interactions.
Like Xu et al. (2019) and Kazemi et al. (2019), our proposed method learns a time representation. However, instead of concatenating it with the input embedding, our model learns to attend to observations at different time points by computing a similarity weighting using only the time embedding. Our proposed model uses the time embedding as both the queries and keys in the attention formulation. It learns an interpolation over the query time points by attending to the observed values at key time points. Our proposed method is thus similar to kernel-based interpolation, but learning the time attention based similarity kernel gives our model more flexibility compared to methods like that of Shukla & Marlin (2019) that use similarity kernels with fixed functional forms. Another important difference relative to many of these previous methods is that our proposed approach attends only to the observed data dimensions at each time point and hence does not require a separate imputation step to handle vector valued observations with an arbitrary collection of dimensions missing at any given time point.

[THE MULTI-TIME ATTENTION MODULE]
In this section, we present the proposed Multi-Time Attention Module (mTAN). The role of this module is to re-represent a sparse and irregularly sampled time series in a fixed-dimensional space. This module uses multiple continuous-time embeddings and attention-based interpolation. We begin by presenting notation followed by the time embedding and attention components.
Notation: In the case of a supervised learning task, we let D = {(s n , y n )|n = 1, ..., N } represent a data set containing N data cases. An individual data case consists of a single target value y n (discrete for classification), as well as a D-dimensional, sparse and irregularly sampled multivariate time series s n . Different dimensions d of the multivariate time series can have observations at different times, as well as different total numbers of observations L dn . Thus, we represent time series d for data case n as a tuple s dn = (t dn , x dn ) where t dn = [t 1dn , ..., t L dn dn ] is the list of time points at which observations are defined and x dn = [x 1dn , ..., x L dn dn ] is the corresponding list of observed values. In the case of an unsupervised task such as interpolation, each data case consists of a multivariate time series s n only. We drop the data case index n for brevity when the context is clear. Time Embedding: Time attention module is based on embedding continuous time points into a vector space. We generalize the notion of a positional encoding used in transformer-based models to continuous time. Time attention networks simultaneously leverage H embedding functions φ h (t), each outputting a representation of size d r . Dimension i of embedding h is defined as follows:
φ h (t)[i] = ω 0h • t + α 0h , if i = 0 sin(ω ih • t + α ih ), if 0 < i < d r (1)
where the ω ih 's and α ih 's are learnable parameters. The periodic terms can capture periodicity in time series data. In this case, ω ih and α ih represent the frequency and phase of the sine function. The linear term, on the other hand, can capture non-periodic patterns dependent on the progression of time. For a given difference ∆, φ h (t + ∆) can be represented as a linear function of φ h (t).
Learning the periodic time embedding functions is equivalent to using a one-layer fully connected network with a sine function non-linearity to map the time values into a higher dimensional space. By contrast, the positional encoding used in transformer models is defined only for discrete positions. We note that our time embedding functions subsume positional encodings when evaluated at discrete positions.
Multi-Time Attention: The time embedding component described above takes a continuous time point and embeds it into H different d r -dimensional spaces. In this section, we describe how we leverage time embeddings to produce a continuous-time embedding module for sparse and irregularly sampled time series. This multi-time attention embedding module mTAN(t, s) takes as input a query time point t and a set of keys and values in the form of a D-dimensional multivariate sparse and irregularly sampled time series s (as defined in the notation section above), and returns a J- Note that the output at all query points can be computed in parallel. dimensional embedding at time t. This process leverages a continuous-time attention mechanism applied to the H time embeddings. The complete computation is described below. to normalize the dot product to counteract the growth in the dot product magnitude with increase in the dimension d k .
mTAN(t, s)[j] = H h=1 D d=1x hd (t, s) • U hdj (2) x hd (t, s) = L d i=1 κ h (t, t id ) x id (3) κ h (t, t id ) = exp φ h (t)wv T φ h (t id ) T / √ d k L d i =1 exp φ h (t)wv T φ h (t i d ) T / √ d k (4
Learning the time embeddings provides our model with flexibility to learn complex temporal kernel functions κ h (t, t ). The use of multiple simultaneous time embeddings φ h (t) and a final linear combination across time embedding dimensions and data dimensions means that the final output representation function mTAN(t, s) is extremely flexible. Different input dimensions can leverage different time embeddings via learned sparsity patterns in the parameter tensor U . Information from different data dimensions can also be mixed together to create compact reduced dimensional representations. We note that all of the required computations can be parallelized using masking variables to deal with unobserved dimensions, allowing for efficient implementation on a GPU. Discretization: Since the mTAN module defines a continuous function of t given s, it can not be directly incorporated into neural network architectures that expect inputs in the form of fixeddimensional vectors or discrete sequences. However, the mTAN module can easily be adapted to produce such an output representation by materializing its output at a set of reference time points
r = [r 1 , ..., r K ].
In some cases, we may have a fixed set of such points. In other cases, the set of reference time points may need to depend on s itself. In particular, we define the auxiliary function ρ(s) to return the set of time points at which there is an observation on any dimension of s.
Given a collection of reference time points r, we define the discretized mTAN module mTAND(r, s) as mTAND(r, s)[i] = mTAN(r i , s). This module takes as input the set of reference time points r and the time series s and outputs a sequence of mTAN embeddings of length |r|, each of dimension J. The architecture of the mTAND module is shown in Figure 1. The mTAND module can be used to interface sparse and irregularly sampled multivariate time series data with any deep neural network layer type including fully-connected, recurrent, and convolutional layers. In the next section, we describe the construction of a temporal encoder-decoder architecture leveraging the mTAND module, which can be applied to both classification and interpolation tasks.

[ENCODER-DECODER FRAMEWORK]
As described in the last section, we leverage the discretized mTAN module in an encoder-decoder framework as the primary model in this paper, which we refer to as an mTAN network. We develop the encoder-decoder framework within the variational autoencoder (VAE) framework in this section.
The architecture for the model framework is shown in Figure 2.
Model Architecture: As we are modeling time series data, we begin by defining a sequence of latent states z i . Each of these latent states are IID-distributed according to a standard multivariate normal distribution p(z i ). We define the set of latent states z = [z 1 , ..., z K ] at K reference time points.
We define a three-stage decoder. First, the latent states are processed through an RNN decoder module to induce temporal dependencies resulting in a first set of deterministic latent variables h
dec RN N = [h dec 1,RN N , ..., h dec K,RN N ].
Second, the output of the RNN decoder stage and the K time points h dec RN N are provided to the mTAND module along with a set of T query time points t. The mTAND module outputs a sequence of embeddings h dec T AN = [h dec 1,T AN , ..., h dec T,T AN ] of length |t|. Third, the mTAN embeddings are independently decoded using a fully connected decoder f dec () and the result is used to parameterize an output distribution. In this work, we use a diagonal covariance Gaussian distribution with mean given by the final decoded representation and a fixed variance σ 2 . The final generated time series is given byŝ = (t, x) with all data dimensions observed. The full generative process is shown below. We let p θ (x|z, t) define the probability distribution over the values of the time series x given the time points t and the latent variables z. θ represents the parameters of all components of the decoder.
z k ∼ p(z k ) (5) h dec RN N = RNN dec (z)(6)
h dec T AN = mTAND dec (t, h dec RN N )(7)
x id ∼ N (x id ; f dec (h dec i,T AN )[d], σ 2 I)(8)
For an encoder, we simply invert the structure of the generative process. We begin by mapping the input time series s through the mTAND module along with a collection of K reference time points r.
We apply an RNN encoder to the mTAND model that outputs h enc T AN to encode longer-range temporal structure. Finally, we construct a distribution over latent variables at each reference time point using a diagonal Gaussian distribution with mean and variance output by fully connected layers applied to the RNN outputs h enc RN N . The complete encoder architecture is described below. We define q γ (z|r, s) to be the distribution over the latent variables induced by the input time series s and the reference time points r. γ represents all of the parameters in all of the encoder components.
h enc T AN = mTAND enc (r, s) (9) h enc RN N = RNN enc (h enc T AN ) (10) z k ∼ q γ (z k |µ k , σ 2 k ), µ k = f enc µ (h enc k,RN N ), σ 2 k = exp(f enc σ (h enc k,RN N ))(11)
Unsupervised Learning: To learn the parameters of our encoder-decoder model given a data set of sparse and irregularly sampled time series, we follow a slightly modified VAE training approach and maximize a normalized variational lower bound on the log marginal likelihood based on the evidence lower bound or ELBO. The learning objective is defined below where p θ (x jdn |z, t n ) and q γ (z|r, s n ) are defined in the previous section.
L NVAE (θ, γ) = N n=1 1 d L dn E qγ (z|r,sn) [log p θ (x n |z, t n )] − D KL (q γ (z|r, s n )||p(z)) (12
)
D KL (q γ (z|r, s n )||p(z)) = K i=1 D KL (q γ (z i |r, s n )||p(z i )) (13
)
log p θ (x n |z, t n ) = D d=1 L dn j=1 log p θ (x jdn |z, t jdn )(14)
Since irregularly sampled time series can have different numbers of observations across different dimensions as well as across different data cases, it can be helpful to normalize the terms in the standard ELBO objective to avoid the model focusing more on sequences that are longer at the expense of sequences that are shorter. The objective above normalizes the contribution of each data case by the total number of observations it contains. The fact that all data dimensions are not observed at all time points is accounted for in Equation 14. In practice, we use k samples from the variational distribution q γ (z|r, s n ) to compute the learning objective.
Supervised Learning: We can also augment the encoder-decoder model with a supervised learning component that leverages the latent states as a feature extractor. We define this component to be of the form p δ (y n |z) where δ are the model parameters. This leads to an augmented learning objective as shown in Equation 15where the λ term trades off the supervised and unsupervised terms.
L supervised (θ, γ, δ) = L NVAE (θ, γ) + λE qγ (z|r,sn) log p δ (y n |z)(15)
In this work, we focus on classification as an illustrative supervised learning problem. For the classification model p δ (y n |z), we use a GRU followed by a 2-layer fully connected network. We use a small number of samples to approximate the required intractable expectations during both learning and prediction. Predictions are computed by marginalizing over the latent variable as shown below.
y * = arg max y∈Y E qγ (z|r,s) [log p δ (y|z)](16)

[EXPERIMENTS]
In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset).
Additional illustrative results on synthetic data can be found in Appendix A.2.

[DATASETS:]
The PhysioNet Challenge 2012 dataset (Silva et al., 2012) consists of multivariate time series data with 37 variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first 48 hours after admission to ICU. We follow the procedures of Rubanova et al. (2019) and round the observation times to the nearest minute. This leads to 2880 possible measurement times per time series. The data set includes 4000 labeled instances and 4000 unlabeled instances. We use all 8000 instances for interpolation experiments and the 4000 labeled instances for classification experiments. We focus on predicting in-hospital mortality.  2019), we extract 53, 211 records each containing 12 physiological variables. We focus on predicting in-hospital mortality using the first 48 hours of data. 8.1% of the instances have positive labels.
The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of Rubanova et al. (2019) and construct a dataset of 6, 554 sequences with 12 channels and 50 time points. We focus on classifying each time point in the sequence into one of eleven types of activities.
Experimental Protocols: We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80% of the instances, and a test set containing the remaining 20% of instances. We use 20% of the training data for validation. In the interpolation task, we condition on a subset of available points and predict values for rest of the time points. We perform interpolation experiments with a varying percentage of observed points ranging from 50% to 90% of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at rest of the available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).
We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification problems. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for the human activity dataset. We randomly divide each data set into a training set containing 80% of the time series, and a test set containing the remaining 20% of instances. We use 20% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy.
For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix A.4.

[MODELS:]
The model we focus on is the encoder-decoder architecture based on the discretized multitime attention module (mTAND-Full). In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (mTAND-Enc). We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) (Chung et al., 2014) module as the recurrent network throughout. Architecture details can be found in Appendix A.3.
• RNN-Impute: Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples (Che et al., 2018).
• RNN-∆ t : Input is concatenated with masking variable and time interval ∆ t indicating how long the particular variable is missing.
• RNN-Decay: RNN with exponential decay on hidden states (Mozer et al., 2017;Che et al., 2018).
• GRU-D: combining hidden state decay with input decay (Che et al., 2018).
• Phased-LSTM: Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM (Neil et al., 2016) with forward filling to handle partially observed vectors.
• IP-Nets: Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU (Shukla & Marlin, 2019).
• SeFT: Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach (Horn et al., 2020).
• RNN-VAE: A VAE-based model where the encoder and decoder are standard RNN models.
• ODE-RNN: Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation (Rubanova et al., 2019).
• L-ODE-RNN: Latent ODE where the encoder is an RNN and decoder is a neural ODE (Chen et al., 2018).
• L-ODE-ODE: Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE (Rubanova et al., 2019).
Physionet Experiments: Table 1 compares the performance of all methods on the interpolation task where we observe 50% − 90% of the values in the test instances. As we can see, the proposed method (mTAND-Full) consistently and substantially outperforms all of the previous approaches across all of the settings of observed time points. We note that in this experiment, different columns correspond to different setting (for example, in the case of 70%, we condition on 70% of data and predict the rest of the data; i.e., 30%) and, hence the results across columns are not comparable. We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver (Chen et al., 2018;Rubanova et al., 2019). These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.  3 shows the ablation results by substituting fixed positional encoding (Vaswani et al., 2017) in place of learnable time embedding defined in Equation 1in mTAND-Full model on PhysioNet and MIMIC-III dataset for classification task. We report the average AUC score over 5 runs. As we can see from Table 3, learning the time embedding improves AUC score by 1% as compared to using fixed positional encodings.  (Shukla & Marlin, 2019). IP-Nets use several semiparametric RBF interpolation layers, followed by a GRU to model irregularly sampled time series. In this framework, we replace the RBK kernel with a learnable similarity kernel using mTAND module, the corresponding model is mTAND-Enc. Table 4 compares the performance of the two methods on classification task on PhysioNet, MIMIC-III and Human Activity dataset. We report the average AUC score over 5 runs. Table 4 shows that learning the similarity kernel using mTAND module performs as well or better than using a fixed RBF kernel. 

[A.2 SYNTHETIC INTERPOLATION EXPERIMENTS]
To demonstrate the capabilities of our model on the interpolation task, we generate a synthetic dataset consisting of 1000 trajectories each of 100 time points sampled over t ∈ [0, 1]. We fix 10 reference points and use RBF kernel with a fixed bandwidth of 100 for constructing local interpolations at 100 time points over [0,1]. The values at the reference points are drawn from a standard normal distribution.
We randomly sample 20 observations from each trajectory to simulate a sparse and irregularly sampled multivariate time series. We use 80% of the data for training and 20% for testing. At test time, encoder conditions on 20 irregularly sampled time points and the decoder generates interpolations on all 100 time points. Figure 3 illustrates the interpolation results on the test set for the Multi-Time Attention Network and Latent ODE model with ODE encoder (Rubanova et al., 2019). For both the models, we draw 100 samples from the approximate posterior distribution. As we can see from Figure 3, the ODE interpolations are much smoother and haven't been able to capture the local structure as well as mTANS.

[OBSERVED DATA]
Ground truth Reconstructions  Table 5 compares the proposed model with best performing baseline Latent-ODE with ODE encoder (L-ODE-ODE) on reconstruction and interpolation task. For both the tasks, we condition on the 20 irregularly sampled time points and reconstruct the input points (reconstruction) and the whole set of 100 time points (interpolation). We report the mean squared error on test set.

[A.3 ARCHITECTURE DETAILS]
Multi-Time Attention Network (mTAND-Full): In our proposed encoder-decoder framework (Figure 2), we use bi-directional GRU as the recurrent model in both encoder and decoder. In encoder, we use a 2 layer fully connected network with 50 hidden units and ReLU activations to map the RNN hidden state at each reference point to mean and variance. Similarly in decoder, mTAN embeddings are independently decoded using a 2 layer fully connected network with 50 hidden units and ReLU activations, and the result is used to parameterize the output distribution. For classification tasks, we use a separate GRU layer on top of the latent states followed by a 2-layer fully connected layer with 300 units and ReLU activations to output the class probabilities.
Multi-Time Attention Encoder (mTAND-Enc): As we show in the experiments, the proposed mTAN module can standalone be used for classification tasks. The mTAND-Enc consists of Multi-Time attention module followed by GRU to extract the final hidden state which is then passed to a 2-layer fully connected layer to output the class probabilities.

[ACKNOWLEDGEMENTS]
Research reported in this paper was partially supported by the National Institutes of Health under award numbers 5U01CA229445 and 1P41EB028242.

[]
mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.
Human Activity Experiments: Table 2 shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets.
Additional Experiments: In Appendix A.2, we demonstrate the effectiveness of learning temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.
We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix A.1. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix A.1 shows that learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.

[DISCUSSION AND CONCLUSIONS]
In this paper, we have presented the Multi-Time Attention (mTAN) module for learning from sparse and irregularly sampled data along with a VAE-based encoder-decoder model leveraging this module.
Our results show that the resulting model performs as well or better than a range of baseline and state-of-the-art models on both the interpolation and classification tasks, while offering training times that are one to two orders of magnitude faster than previous state of the art methods. While in this work we have focused on a VAE-based encoder-decoder architecture, the proposed mTAN module can be used to provide an interface between sparse and irregularly sampled time series and many different types of deep neural network architectures including GAN-based models. Composing the mTAN module with convolutional networks instead of recurrent architectures may also provide further computational enhancements due to improved parallelism.
Loss Function: For computing the evidence lower bound (ELBO) during training, we use negative log-likelihood with fixed variance as the reconstruction loss. For all the datasets, we use a fixed variance of 0.01. For computing ELBO, we use 5 samples for interpolation task and 1 sample for classification tasks. We use cross entropy loss for classification. For the classification tasks, we tune the λ parameter in the supervised learning loss function (Equation 15). We achieved best performance using λ as 100 and 5 for Physionet, MIMIC-III respectively. For human activity dataset, we achieved best results without using the regulaizer or ELBO component. We found that KL annealing with coeff 0.99 improved the performance of interpolation and classification tasks on Physionet.

[A.4 HYPERPARAMETERS]
Baselines: For Physionet and Human Activity dataset, we use the reported hyperparameters for RNN baselines as well as ODE models from Rubanova et al. (2019). For MIMIC-III dataset, we independently tune the hyperparameters of the baseline models on the validation set. We search for GRU hidden units, latent dimension, number of hidden units in fully connected network for ODE function in recognition and generative model over the range {20, 32, 64, 128, 256}. For ODEs, we also searched the number of layers in fully connected network in the range {1, 2, 3}.  In this section, we visualize the attention weights learned by our proposed model. We experiment using synthetic dataset (described in A.2) which consists of univariate time series. Figure 4 shows the attention weights learned by the encoder mTAND module. The input shown in the figure is the irregularly sampled time points and the edges show how the output at reference points attends to the values on the input time points. The final output can be computed by substituting the attention weights in Equation 3. We rescale each feature to be between 0 and 1 for Physionet and MIMIC-III dataset. We also rescale the time to be in [0, 1] for all datasets. In case of MIMIC-III dataset, for the time series missing entirely, we follow the preprocessing steps of Shukla & Marlin (2019) and assign the starting point (time t=0) value of the time series to the global mean for that variable.

[A.6.2 SOURCE CODE]
The code for reproducing the results in this paper is available at https://github.com/ reml-lab/mTAN.

[A.6.3 COMPUTING INFRASTRUCTURE]
All experiments were run on a Nvidia Titan X GPU.","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1"
Multi-Time Attention Networks for Irregularly Sampled Time Series,4c0J6lwQ4_.json,"This article tackles the analysis of irregular samples time series. The approach is mainly based on interpolation. Thus, the authors can del with both supervised and unsupervised problems.
The architecture is made of a sinusoid attention layer, a VAE layer that lead to a fixed size set of landmark in the latent space and a RNN decoder.
For the supervised task, the authors add a classification loss.

They obtain impressive results on the interpolation task and interesting results on the classification task.

* In an interpolation problem, we would like to consider a robust baseline as a linear interpolation or an AR-like modeling. Even if I must admit that the authors already propose a lot of comparisons with models from the -recent- litterature, this would give us a meaningful MSE result to compare other approaches.

* notations should be improved (and/or completed with a schema). The model is really very difficult to understand in this version of the article.

* Results are impressive but I don't get which part of the architecture lead to such a performance
","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1   

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Architecture of the mTAND module. It takes irregularly sampled time points and corresponding values as keys and values and produces a fixed dimensional representation at the query time points. The attention blocks (ATT) perform a scaled dot product attention over the observed values using the time embedding of the query and key time points. Equation 3 and 4 defines this operation.Note that the output at all query points can be computed in parallel.
Table 2: Figure 2 :2Figure 2: Architecture of the proposed encoder-decoder framework mTAND-Full. The classifier is required only for performing classification tasks. The mTAND module is shown in Figure 1.
Table 3: Figure 3 :3Figure 3: Interpolations on the synthetic interpolation dataset. The columns represent 3 different examples. First row: Ground truth trajectories with observed points, second row: reconstructions on the complete range t ∈ [0, 1] using the proposed model mTAN, third row: reconstructions on the complete range t ∈ [0, 1] using the Latent ODE model with ODE encoder.
Table 4: As shown in Equation 2, dimension j of the mTAN embedding mTAN(t, s)[j] is given by a linear combination of intermediate univariate continuous-time functionsx hd (t, s). There is one such function defined for each input data dimension d and each time embedding h. The parameters U hdj are learnable linear combination weights.As shown in Equation3, the structure of the intermediate continuous-time functionx hd (t, s) is essentially a kernel smoother applied to the d th dimension of the time series. However, the interpolation weights κ h (t, t id ) are defined based on a time attention mechanism that leverages time embeddings, as shown in Equation4. As we can see, the same time embedding function φ h (t) is applied for all data dimensions. The form of the attention mechanism is a softmax function over the observed time points t id for dimension d. The activation within the softmax is a scaled inner product between the time embedding φ h (t) of the query time point t and the time embedding φ h (t id ) of the observed time point, the key. The parameters w and v are each d r × d k matrices where d k ≤ d r . We use a scaling factor 1
Table 5: 13.8% of examples are in the positive class.
Table 6: Interpolation performance versus percent observed time points on PhysioNet
Table 7: compares predictive performance on the PhysioNet mortality prediction task. The full Multi-Time Attention network model (mTAND-Full) and the classifier based only on the Multi-Time Attention network encoder (mTAND-Enc) achieve significantly improved performance relative to the current state-of-the-art methods (ODE-RNN and L-ODE-ODE) and other baseline methods.
Table 8: Table 2 compares the predictive performance of the models on the mortality prediction task on MIMIC-III. The Multi-Time Attention network-based encoder-decoder framework (mTAND-Full) achieves better performance than the recent IP-Net and SeFT model as well as all of the RNN baseline models. While ODE-RNN and L-ODE-ODE both have slightly better A APPENDIX A.1 ABLATION STUDY In this section, we perform ablation experiments to show the performance gain achieved by learning similarity kernel and time embedding. Table
Table 9: Ablation with time embeddingSince mTANs are fundamentally continuous-time interpolation-based models, we perform an ablation study by comparing mTANs with the IP-nets
Table 10: Comparing interpolation kernels
Table 11: Synthetic Data: Mean Squared Error

[INTRODUCTION]
Irregularly sampled time series occur in application domains including healthcare, climate science, ecology, astronomy, biology and others. It is well understood that irregular sampling poses a significant challenge to machine learning models, which typically assume fully-observed, fixed-size feature representations (Marlin et al., 2012;Yadav et al., 2018). While recurrent neural networks (RNNs) have been widely used to model such data because of their ability to handle variable length sequences, basic RNNs assume regular spacing between observation times as well as alignment of the time points where observations occur for different variables (i.e., fully-observed vectors). In practice, both of these assumptions can fail to hold for real-world sparse and irregularly observed time series. To respond to these challenges, there has been significant progress over the last decade on building and adapting machine learning models that can better capture the structure of irregularly sampled multivariate time series (Li & Marlin, 2015;Lipton et al., 2016;Futoma et al., 2017;Che et al., 2018;Shukla & Marlin, 2019;Rubanova et al., 2019).
In this work, we introduce a new model for multivariate, sparse and irregularly sampled time series that we refer to as Multi-Time Attention networks or mTANs. mTANs are fundamentally continuous-time, interpolation-based models. Their primary innovations are the inclusion of a learned continuous-time embedding mechanism coupled with a time attention mechanism that replaces the use of a fixed similarity kernel when forming representation from continuous time inputs. This gives mTANs more representational flexibility than previous interpolation-based models (Shukla & Marlin, 2019).
Our approach re-represents an irregularly sampled time series at a fixed set of reference points. The proposed time attention mechanism uses reference time points as queries and the observed time points as keys. We propose an encoder-decoder framework for end-to-end learning using an mTAN module to interface with given multivariate, sparse and irregularly sampled time series inputs. The encoder takes the irregularly sampled time series as input and produces a fixed-length latent representation over a set of reference points, while the decoder uses the latent representations to produce reconstructions conditioned on the set of observed time points. Learning uses established methods for variational autoencoders (Rezende et al., 2014;Kingma & Welling, 2014).
The main contributions of the mTAN model framework are: (1) It provides a flexible approach to modeling multivariate, sparse and irregularly sampled time series data (including irregularly sampled time series of partially observed vectors) by leveraging a time attention mechanism to learn temporal similarity from data instead of using fixed kernels. (2) It uses a temporally distributed latent representation to better capture local structure in time series data. (3) It provides interpolation and classification performance that is as good as current state-of-the-art methods or better, while providing significantly reduced training times.

[RELATED WORK]
An irregularly sampled time series is a time series with irregular time intervals between observations. In the multivariate setting, there can also be a lack of alignment across different variables within the same multivariate time series. Finally, when gaps between observation times are large, the time series is also considered to be sparse. Such data occur in electronic health records (Marlin et al., 2012;Yadav et al., 2018), climate science (Schulz & Stattegger, 1997), ecology (Clark & Bjørnstad, 2004), biology (Ruf, 1999), and astronomy (Scargle, 1982). It is well understood that such data cause significant issues for standard supervised machine learning models that typically assume fully observed, fixed-size feature representations (Marlin et al., 2012).
A basic approach to dealing with irregular sampling is fixed temporal discretization. For example, Marlin et al. (2012) and Lipton et al. (2016) discretize continuous-time observations into hour-long bins. This has the advantage of simplicity, but requires ad-hoc handling of bins with more than one observation and results in missing data when bins are empty.
The alternative to temporal discretization is to construct models with the ability to directly use an irregularly sampled time series as input. Che et al. (2018) present several methods based on gated recurrent unit networks (GRUs, Chung et al. (2014)), including an approach that takes as input a sequence consisting of observed values, missing data indicators, and time intervals since the last observation. Pham et al. (2017) proposed to capture time irregularity by modifying the forget gate of an LSTM (Hochreiter & Schmidhuber, 1997), while Neil et al. (2016) introduced a new time gate that regulates access to the hidden and cell state of the LSTM. While these approaches allow the network to handle event-based sequences with irregularly spaced vector-valued observations, they do not support learning directly from vectors that are partially observed, which commonly occurs in the multivariate setting because of lack of alignment of observation times across different variables.
Another line of work has looked at using observations from the future as well as from the past for interpolation. Yoon et al. (2019) and Yoon et al. (2018) presented an approach based on the multi-directional RNN (M-RNN) that can leverage observations from the relative past and future of a given time point. Shukla & Marlin (2019) proposed the interpolation-prediction network framework, consisting of several semi-parametric RBF interpolation layers that interpolate multivariate, sparse, and irregularly sampled input time series against a set of reference time points while taking into account all observed data in a time series. Horn et al. (2020) proposed a set function-based approach for classifying time-series with irregularly sampled and unaligned observation.
Chen et al. (2018) proposed a variational auto-encoder model (Kingma & Welling, 2014;Rezende et al., 2014) for continuous time data based on the use of a neural network decoder combined with a latent ordinary differential equation (ODE) model. They model time series data via a latent continuous-time function that is defined via a neural network representation of its gradient field. Building on this, Rubanova et al. (2019) proposed a latent ODE model that uses an ODE-RNN model as the encoder. ODE-RNNs use neural ODEs to model the hidden state dynamics and an RNN to update the hidden state in the presence of a new observation. De Brouwer et al. (2019) proposed GRU-ODE-Bayes, a continuous-time version of the Gated Recurrent Unit (Chung et al., 2014). Instead of the encoder-decoder architecture where the ODE is decoupled from the input processing, GRU-ODE-Bayes provides a tighter integration by interleaving the ODE and the input processing steps.
Several recent approaches have also used attention mechanisms to model irregularly sampled time series (Song et al., 2018;Tan et al., 2020;Zhang et al., 2019) as well as medical concepts (Peng et al., 2019;Cai et al., 2018). Most of these approaches are similar to Vaswani et al. (2017) where they replace the positional encoding with an encoding of time and model sequences using self-attention.
However, instead of adding the time encoding to the input representation as in Vaswani et al. (2017), they concatenate it with the input representation. These methods use a fixed time encoding similar to the positional encoding of Vaswani et al. (2017). Xu et al. (2019) learn a functional time representation and concatenate it with the input event embedding to model time-event interactions.
Like Xu et al. (2019) and Kazemi et al. (2019), our proposed method learns a time representation. However, instead of concatenating it with the input embedding, our model learns to attend to observations at different time points by computing a similarity weighting using only the time embedding. Our proposed model uses the time embedding as both the queries and keys in the attention formulation. It learns an interpolation over the query time points by attending to the observed values at key time points. Our proposed method is thus similar to kernel-based interpolation, but learning the time attention based similarity kernel gives our model more flexibility compared to methods like that of Shukla & Marlin (2019) that use similarity kernels with fixed functional forms. Another important difference relative to many of these previous methods is that our proposed approach attends only to the observed data dimensions at each time point and hence does not require a separate imputation step to handle vector valued observations with an arbitrary collection of dimensions missing at any given time point.

[THE MULTI-TIME ATTENTION MODULE]
In this section, we present the proposed Multi-Time Attention Module (mTAN). The role of this module is to re-represent a sparse and irregularly sampled time series in a fixed-dimensional space. This module uses multiple continuous-time embeddings and attention-based interpolation. We begin by presenting notation followed by the time embedding and attention components.
Notation: In the case of a supervised learning task, we let D = {(s n , y n )|n = 1, ..., N } represent a data set containing N data cases. An individual data case consists of a single target value y n (discrete for classification), as well as a D-dimensional, sparse and irregularly sampled multivariate time series s n . Different dimensions d of the multivariate time series can have observations at different times, as well as different total numbers of observations L dn . Thus, we represent time series d for data case n as a tuple s dn = (t dn , x dn ) where t dn = [t 1dn , ..., t L dn dn ] is the list of time points at which observations are defined and x dn = [x 1dn , ..., x L dn dn ] is the corresponding list of observed values. In the case of an unsupervised task such as interpolation, each data case consists of a multivariate time series s n only. We drop the data case index n for brevity when the context is clear. Time Embedding: Time attention module is based on embedding continuous time points into a vector space. We generalize the notion of a positional encoding used in transformer-based models to continuous time. Time attention networks simultaneously leverage H embedding functions φ h (t), each outputting a representation of size d r . Dimension i of embedding h is defined as follows:
φ h (t)[i] = ω 0h • t + α 0h , if i = 0 sin(ω ih • t + α ih ), if 0 < i < d r (1)
where the ω ih 's and α ih 's are learnable parameters. The periodic terms can capture periodicity in time series data. In this case, ω ih and α ih represent the frequency and phase of the sine function. The linear term, on the other hand, can capture non-periodic patterns dependent on the progression of time. For a given difference ∆, φ h (t + ∆) can be represented as a linear function of φ h (t).
Learning the periodic time embedding functions is equivalent to using a one-layer fully connected network with a sine function non-linearity to map the time values into a higher dimensional space. By contrast, the positional encoding used in transformer models is defined only for discrete positions. We note that our time embedding functions subsume positional encodings when evaluated at discrete positions.
Multi-Time Attention: The time embedding component described above takes a continuous time point and embeds it into H different d r -dimensional spaces. In this section, we describe how we leverage time embeddings to produce a continuous-time embedding module for sparse and irregularly sampled time series. This multi-time attention embedding module mTAN(t, s) takes as input a query time point t and a set of keys and values in the form of a D-dimensional multivariate sparse and irregularly sampled time series s (as defined in the notation section above), and returns a J- Note that the output at all query points can be computed in parallel. dimensional embedding at time t. This process leverages a continuous-time attention mechanism applied to the H time embeddings. The complete computation is described below. to normalize the dot product to counteract the growth in the dot product magnitude with increase in the dimension d k .
mTAN(t, s)[j] = H h=1 D d=1x hd (t, s) • U hdj (2) x hd (t, s) = L d i=1 κ h (t, t id ) x id (3) κ h (t, t id ) = exp φ h (t)wv T φ h (t id ) T / √ d k L d i =1 exp φ h (t)wv T φ h (t i d ) T / √ d k (4
Learning the time embeddings provides our model with flexibility to learn complex temporal kernel functions κ h (t, t ). The use of multiple simultaneous time embeddings φ h (t) and a final linear combination across time embedding dimensions and data dimensions means that the final output representation function mTAN(t, s) is extremely flexible. Different input dimensions can leverage different time embeddings via learned sparsity patterns in the parameter tensor U . Information from different data dimensions can also be mixed together to create compact reduced dimensional representations. We note that all of the required computations can be parallelized using masking variables to deal with unobserved dimensions, allowing for efficient implementation on a GPU. Discretization: Since the mTAN module defines a continuous function of t given s, it can not be directly incorporated into neural network architectures that expect inputs in the form of fixeddimensional vectors or discrete sequences. However, the mTAN module can easily be adapted to produce such an output representation by materializing its output at a set of reference time points
r = [r 1 , ..., r K ].
In some cases, we may have a fixed set of such points. In other cases, the set of reference time points may need to depend on s itself. In particular, we define the auxiliary function ρ(s) to return the set of time points at which there is an observation on any dimension of s.
Given a collection of reference time points r, we define the discretized mTAN module mTAND(r, s) as mTAND(r, s)[i] = mTAN(r i , s). This module takes as input the set of reference time points r and the time series s and outputs a sequence of mTAN embeddings of length |r|, each of dimension J. The architecture of the mTAND module is shown in Figure 1. The mTAND module can be used to interface sparse and irregularly sampled multivariate time series data with any deep neural network layer type including fully-connected, recurrent, and convolutional layers. In the next section, we describe the construction of a temporal encoder-decoder architecture leveraging the mTAND module, which can be applied to both classification and interpolation tasks.

[ENCODER-DECODER FRAMEWORK]
As described in the last section, we leverage the discretized mTAN module in an encoder-decoder framework as the primary model in this paper, which we refer to as an mTAN network. We develop the encoder-decoder framework within the variational autoencoder (VAE) framework in this section.
The architecture for the model framework is shown in Figure 2.
Model Architecture: As we are modeling time series data, we begin by defining a sequence of latent states z i . Each of these latent states are IID-distributed according to a standard multivariate normal distribution p(z i ). We define the set of latent states z = [z 1 , ..., z K ] at K reference time points.
We define a three-stage decoder. First, the latent states are processed through an RNN decoder module to induce temporal dependencies resulting in a first set of deterministic latent variables h
dec RN N = [h dec 1,RN N , ..., h dec K,RN N ].
Second, the output of the RNN decoder stage and the K time points h dec RN N are provided to the mTAND module along with a set of T query time points t. The mTAND module outputs a sequence of embeddings h dec T AN = [h dec 1,T AN , ..., h dec T,T AN ] of length |t|. Third, the mTAN embeddings are independently decoded using a fully connected decoder f dec () and the result is used to parameterize an output distribution. In this work, we use a diagonal covariance Gaussian distribution with mean given by the final decoded representation and a fixed variance σ 2 . The final generated time series is given byŝ = (t, x) with all data dimensions observed. The full generative process is shown below. We let p θ (x|z, t) define the probability distribution over the values of the time series x given the time points t and the latent variables z. θ represents the parameters of all components of the decoder.
z k ∼ p(z k ) (5) h dec RN N = RNN dec (z)(6)
h dec T AN = mTAND dec (t, h dec RN N )(7)
x id ∼ N (x id ; f dec (h dec i,T AN )[d], σ 2 I)(8)
For an encoder, we simply invert the structure of the generative process. We begin by mapping the input time series s through the mTAND module along with a collection of K reference time points r.
We apply an RNN encoder to the mTAND model that outputs h enc T AN to encode longer-range temporal structure. Finally, we construct a distribution over latent variables at each reference time point using a diagonal Gaussian distribution with mean and variance output by fully connected layers applied to the RNN outputs h enc RN N . The complete encoder architecture is described below. We define q γ (z|r, s) to be the distribution over the latent variables induced by the input time series s and the reference time points r. γ represents all of the parameters in all of the encoder components.
h enc T AN = mTAND enc (r, s) (9) h enc RN N = RNN enc (h enc T AN ) (10) z k ∼ q γ (z k |µ k , σ 2 k ), µ k = f enc µ (h enc k,RN N ), σ 2 k = exp(f enc σ (h enc k,RN N ))(11)
Unsupervised Learning: To learn the parameters of our encoder-decoder model given a data set of sparse and irregularly sampled time series, we follow a slightly modified VAE training approach and maximize a normalized variational lower bound on the log marginal likelihood based on the evidence lower bound or ELBO. The learning objective is defined below where p θ (x jdn |z, t n ) and q γ (z|r, s n ) are defined in the previous section.
L NVAE (θ, γ) = N n=1 1 d L dn E qγ (z|r,sn) [log p θ (x n |z, t n )] − D KL (q γ (z|r, s n )||p(z)) (12
)
D KL (q γ (z|r, s n )||p(z)) = K i=1 D KL (q γ (z i |r, s n )||p(z i )) (13
)
log p θ (x n |z, t n ) = D d=1 L dn j=1 log p θ (x jdn |z, t jdn )(14)
Since irregularly sampled time series can have different numbers of observations across different dimensions as well as across different data cases, it can be helpful to normalize the terms in the standard ELBO objective to avoid the model focusing more on sequences that are longer at the expense of sequences that are shorter. The objective above normalizes the contribution of each data case by the total number of observations it contains. The fact that all data dimensions are not observed at all time points is accounted for in Equation 14. In practice, we use k samples from the variational distribution q γ (z|r, s n ) to compute the learning objective.
Supervised Learning: We can also augment the encoder-decoder model with a supervised learning component that leverages the latent states as a feature extractor. We define this component to be of the form p δ (y n |z) where δ are the model parameters. This leads to an augmented learning objective as shown in Equation 15where the λ term trades off the supervised and unsupervised terms.
L supervised (θ, γ, δ) = L NVAE (θ, γ) + λE qγ (z|r,sn) log p δ (y n |z)(15)
In this work, we focus on classification as an illustrative supervised learning problem. For the classification model p δ (y n |z), we use a GRU followed by a 2-layer fully connected network. We use a small number of samples to approximate the required intractable expectations during both learning and prediction. Predictions are computed by marginalizing over the latent variable as shown below.
y * = arg max y∈Y E qγ (z|r,s) [log p δ (y|z)](16)

[EXPERIMENTS]
In this section, we present interpolation and classification experiments using a range of models and three real-world data sets (Physionet Challenge 2012, MIMIC-III, and a Human Activity dataset).
Additional illustrative results on synthetic data can be found in Appendix A.2.

[DATASETS:]
The PhysioNet Challenge 2012 dataset (Silva et al., 2012) consists of multivariate time series data with 37 variables extracted from intensive care unit (ICU) records. Each record contains sparse and irregularly spaced measurements from the first 48 hours after admission to ICU. We follow the procedures of Rubanova et al. (2019) and round the observation times to the nearest minute. This leads to 2880 possible measurement times per time series. The data set includes 4000 labeled instances and 4000 unlabeled instances. We use all 8000 instances for interpolation experiments and the 4000 labeled instances for classification experiments. We focus on predicting in-hospital mortality.  2019), we extract 53, 211 records each containing 12 physiological variables. We focus on predicting in-hospital mortality using the first 48 hours of data. 8.1% of the instances have positive labels.
The human activity dataset consists of 3D positions of the waist, chest and ankles collected from five individuals performing various activities including walking, sitting, lying, standing, etc. We follow the data preprocessing steps of Rubanova et al. (2019) and construct a dataset of 6, 554 sequences with 12 channels and 50 time points. We focus on classifying each time point in the sequence into one of eleven types of activities.
Experimental Protocols: We conduct interpolation experiments using the 8000 data cases in the PhysioNet data set. We randomly divide the data set into a training set containing 80% of the instances, and a test set containing the remaining 20% of instances. We use 20% of the training data for validation. In the interpolation task, we condition on a subset of available points and predict values for rest of the time points. We perform interpolation experiments with a varying percentage of observed points ranging from 50% to 90% of the available points. At test time, the values of observed points are conditioned on and each model is used to infer the values at rest of the available time points in the test instance. We repeat each experiment five times using different random seeds to initialize the model parameters. We assess performance using mean squared error (MSE).
We use the labeled data in all three data sets to conduct classification experiments. The PhysioNet and MIMIC III problems are whole time series classification problems. Note that for the human activity dataset, we classify each time point in the time series. We treat this as a smoothing problem and condition on all available observations when producing the classification at each time-point (similar to labeling in a CRF). We use bidirectional RNNs as the RNN-based baselines for the human activity dataset. We randomly divide each data set into a training set containing 80% of the time series, and a test set containing the remaining 20% of instances. We use 20% of the training set for validation. We repeat each experiment five times using different random seeds to initialize the model parameters. Due to class imbalance in the Physionet and MIMIC-III data sets, we assess classification performance using area under the ROC curve (the AUC score). For the Human Activity dataset, we evaluate models using accuracy.
For both interpolation and prediction, we select hyper-parameters on the held-out validation set using grid search, and then apply the best trained model to the test set. The hyper-parameter ranges searched for each model/dataset/task are fully described in Appendix A.4.

[MODELS:]
The model we focus on is the encoder-decoder architecture based on the discretized multitime attention module (mTAND-Full). In the classification experiments, the hidden state at the last observed point is passed to a two-layer binary classification module for all models. For each data set, the structure of this classifier is the same for all models. For the proposed model, the sequence of latent states is first passed through a GRU and then the final hidden state is passed through the same classification module. For the classification task only, we consider an ablation of the full model that uses the proposed mTAND encoder, which consists of our mTAND module followed by a GRU to extract a final hidden state, which is then passed to the classification module (mTAND-Enc). We compare to several deep learning models that expand on recurrent networks to accommodate irregular sampling. We also compare to several encoder-decoder approaches. The full list of model variants is briefly described below. We use a Gated Recurrent Unit (GRU) (Chung et al., 2014) module as the recurrent network throughout. Architecture details can be found in Appendix A.3.
• RNN-Impute: Missing observations replaced with weighted average of last observed measurement within that time series and global mean of the variable across training examples (Che et al., 2018).
• RNN-∆ t : Input is concatenated with masking variable and time interval ∆ t indicating how long the particular variable is missing.
• RNN-Decay: RNN with exponential decay on hidden states (Mozer et al., 2017;Che et al., 2018).
• GRU-D: combining hidden state decay with input decay (Che et al., 2018).
• Phased-LSTM: Captures time irregularity by a time gate that regulates access to the hidden and cell state of the LSTM (Neil et al., 2016) with forward filling to handle partially observed vectors.
• IP-Nets: Interpolation prediction networks, which use several semi-parametric RBF interpolation layers, followed by a GRU (Shukla & Marlin, 2019).
• SeFT: Uses a set function based approach where all the observations are modeled individually before pooling them together using an attention based approach (Horn et al., 2020).
• RNN-VAE: A VAE-based model where the encoder and decoder are standard RNN models.
• ODE-RNN: Uses neural ODEs to model hidden state dynamics and an RNN to update the hidden state in presence of a new observation (Rubanova et al., 2019).
• L-ODE-RNN: Latent ODE where the encoder is an RNN and decoder is a neural ODE (Chen et al., 2018).
• L-ODE-ODE: Latent ODE where the encoder is an ODE-RNN and decoder is a neural ODE (Rubanova et al., 2019).
Physionet Experiments: Table 1 compares the performance of all methods on the interpolation task where we observe 50% − 90% of the values in the test instances. As we can see, the proposed method (mTAND-Full) consistently and substantially outperforms all of the previous approaches across all of the settings of observed time points. We note that in this experiment, different columns correspond to different setting (for example, in the case of 70%, we condition on 70% of data and predict the rest of the data; i.e., 30%) and, hence the results across columns are not comparable. We also report the time per epoch in minutes for all the methods. We note that the ODE-based models require substantially more run time than other methods due to the required use of an ODE solver (Chen et al., 2018;Rubanova et al., 2019). These methods also require taking the union of all observation time points in a batch, which further slows down the training process. As we can see, the proposed full Multi-Time Attention network (mTAND-Full) is over 85 times faster than ODE-RNN and over 100 times faster than L-ODE-ODE, the best-performing ODE-based models.  3 shows the ablation results by substituting fixed positional encoding (Vaswani et al., 2017) in place of learnable time embedding defined in Equation 1in mTAND-Full model on PhysioNet and MIMIC-III dataset for classification task. We report the average AUC score over 5 runs. As we can see from Table 3, learning the time embedding improves AUC score by 1% as compared to using fixed positional encodings.  (Shukla & Marlin, 2019). IP-Nets use several semiparametric RBF interpolation layers, followed by a GRU to model irregularly sampled time series. In this framework, we replace the RBK kernel with a learnable similarity kernel using mTAND module, the corresponding model is mTAND-Enc. Table 4 compares the performance of the two methods on classification task on PhysioNet, MIMIC-III and Human Activity dataset. We report the average AUC score over 5 runs. Table 4 shows that learning the similarity kernel using mTAND module performs as well or better than using a fixed RBF kernel. 

[A.2 SYNTHETIC INTERPOLATION EXPERIMENTS]
To demonstrate the capabilities of our model on the interpolation task, we generate a synthetic dataset consisting of 1000 trajectories each of 100 time points sampled over t ∈ [0, 1]. We fix 10 reference points and use RBF kernel with a fixed bandwidth of 100 for constructing local interpolations at 100 time points over [0,1]. The values at the reference points are drawn from a standard normal distribution.
We randomly sample 20 observations from each trajectory to simulate a sparse and irregularly sampled multivariate time series. We use 80% of the data for training and 20% for testing. At test time, encoder conditions on 20 irregularly sampled time points and the decoder generates interpolations on all 100 time points. Figure 3 illustrates the interpolation results on the test set for the Multi-Time Attention Network and Latent ODE model with ODE encoder (Rubanova et al., 2019). For both the models, we draw 100 samples from the approximate posterior distribution. As we can see from Figure 3, the ODE interpolations are much smoother and haven't been able to capture the local structure as well as mTANS.

[OBSERVED DATA]
Ground truth Reconstructions  Table 5 compares the proposed model with best performing baseline Latent-ODE with ODE encoder (L-ODE-ODE) on reconstruction and interpolation task. For both the tasks, we condition on the 20 irregularly sampled time points and reconstruct the input points (reconstruction) and the whole set of 100 time points (interpolation). We report the mean squared error on test set.

[A.3 ARCHITECTURE DETAILS]
Multi-Time Attention Network (mTAND-Full): In our proposed encoder-decoder framework (Figure 2), we use bi-directional GRU as the recurrent model in both encoder and decoder. In encoder, we use a 2 layer fully connected network with 50 hidden units and ReLU activations to map the RNN hidden state at each reference point to mean and variance. Similarly in decoder, mTAN embeddings are independently decoded using a 2 layer fully connected network with 50 hidden units and ReLU activations, and the result is used to parameterize the output distribution. For classification tasks, we use a separate GRU layer on top of the latent states followed by a 2-layer fully connected layer with 300 units and ReLU activations to output the class probabilities.
Multi-Time Attention Encoder (mTAND-Enc): As we show in the experiments, the proposed mTAN module can standalone be used for classification tasks. The mTAND-Enc consists of Multi-Time attention module followed by GRU to extract the final hidden state which is then passed to a 2-layer fully connected layer to output the class probabilities.

[ACKNOWLEDGEMENTS]
Research reported in this paper was partially supported by the National Institutes of Health under award numbers 5U01CA229445 and 1P41EB028242.

[]
mean AUC than mTAND-Full, the differences are not statistically significant. Further, as shown on the PhysioNet classification problem, mTAND-Full is more than an order of magnitude faster than the ODE-based methods.
Human Activity Experiments: Table 2 shows that the mTAND-based classifiers achieve significantly better performance than the baseline models on this prediction task, followed by ODE-based models and IP-Nets.
Additional Experiments: In Appendix A.2, we demonstrate the effectiveness of learning temporally distributed latent representations with mTANs on a synthetic dataset. We show that mTANs are able to capture local structure in the time series better than latent ODE-based methods that encode to a single time point. This property of mTANs helps to improve the interpolation performance in terms of mean squared error.
We also perform ablation experiments to show the performance gain achieved by learning similarity kernels and time embeddings in Appendix A.1. In particular, we show that learning the time embedding improves classification performance compared to using fixed positional encodings. We also demonstrate the effectiveness of learning the similarity kernel by comparing to an approach that uses fixed RBF kernels. Appendix A.1 shows that learning the similarity kernel using the mTAND module performs as well as or better than using a fixed RBF kernel.

[DISCUSSION AND CONCLUSIONS]
In this paper, we have presented the Multi-Time Attention (mTAN) module for learning from sparse and irregularly sampled data along with a VAE-based encoder-decoder model leveraging this module.
Our results show that the resulting model performs as well or better than a range of baseline and state-of-the-art models on both the interpolation and classification tasks, while offering training times that are one to two orders of magnitude faster than previous state of the art methods. While in this work we have focused on a VAE-based encoder-decoder architecture, the proposed mTAN module can be used to provide an interface between sparse and irregularly sampled time series and many different types of deep neural network architectures including GAN-based models. Composing the mTAN module with convolutional networks instead of recurrent architectures may also provide further computational enhancements due to improved parallelism.
Loss Function: For computing the evidence lower bound (ELBO) during training, we use negative log-likelihood with fixed variance as the reconstruction loss. For all the datasets, we use a fixed variance of 0.01. For computing ELBO, we use 5 samples for interpolation task and 1 sample for classification tasks. We use cross entropy loss for classification. For the classification tasks, we tune the λ parameter in the supervised learning loss function (Equation 15). We achieved best performance using λ as 100 and 5 for Physionet, MIMIC-III respectively. For human activity dataset, we achieved best results without using the regulaizer or ELBO component. We found that KL annealing with coeff 0.99 improved the performance of interpolation and classification tasks on Physionet.

[A.4 HYPERPARAMETERS]
Baselines: For Physionet and Human Activity dataset, we use the reported hyperparameters for RNN baselines as well as ODE models from Rubanova et al. (2019). For MIMIC-III dataset, we independently tune the hyperparameters of the baseline models on the validation set. We search for GRU hidden units, latent dimension, number of hidden units in fully connected network for ODE function in recognition and generative model over the range {20, 32, 64, 128, 256}. For ODEs, we also searched the number of layers in fully connected network in the range {1, 2, 3}.  In this section, we visualize the attention weights learned by our proposed model. We experiment using synthetic dataset (described in A.2) which consists of univariate time series. Figure 4 shows the attention weights learned by the encoder mTAND module. The input shown in the figure is the irregularly sampled time points and the edges show how the output at reference points attends to the values on the input time points. The final output can be computed by substituting the attention weights in Equation 3. We rescale each feature to be between 0 and 1 for Physionet and MIMIC-III dataset. We also rescale the time to be in [0, 1] for all datasets. In case of MIMIC-III dataset, for the time series missing entirely, we follow the preprocessing steps of Shukla & Marlin (2019) and assign the starting point (time t=0) value of the time series to the global mean for that variable.

[A.6.2 SOURCE CODE]
The code for reproducing the results in this paper is available at https://github.com/ reml-lab/mTAN.

[A.6.3 COMPUTING INFRASTRUCTURE]
All experiments were run on a Nvidia Titan X GPU.","[TITLE]
MULTI-TIME ATTENTION NETWORKS FOR IRREGULARLY SAMPLED TIME SERIES

[ABSTRACT]
Irregular sampling occurs in many time series modeling applications where it presents a significant challenge to standard deep learning models. This work is motivated by the analysis of physiological time series data in electronic health records, which are sparse, irregularly sampled, and multivariate. In this paper, we propose a new deep learning framework for this setting that we call Multi-Time Attention Networks. Multi-Time Attention Networks learn an embedding of continuous time values and use an attention mechanism to produce a fixed-length representation of a time series containing a variable number of observations. We investigate the performance of this framework on interpolation and classification tasks using multiple datasets. Our results show that the proposed approach performs as well or better than a range of baseline and recently proposed models while offering significantly faster training times than current state-of-the-art methods. 1"
Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks,BkgXHTNtvS.json,"This paper analyzes the existence of descent paths from any initial point to the global minimum for the two-layer ReLU network and gives a better characterization of the network width that guarantees the descent path property. Concretely, the paper shows that there exists poor local minima under the case of $n > m+2d-2$ by constructing concrete examples of datasets.

To show the global convergence property of the optimization method, this kind of landscape analysis is very important. Basically, I like this paper and I think it makes a certain contribution to this line of researches. However, I did not verify the proof.

A few questions:
- I am not sure why the authors say that ""it was not known whether the descent path property holds for $m \in (2n/d, n)$"" by citing [Soudry and Hoffer(2017)]. I think [Soudry and Hoffer(2017)] does not mention the descent path property. Is this my misunderstanding?
- The theory is limited to 2-layer ReLU. Can it be extended to deep networks?
- The datasets producing poor local minima seem quite artificial. Does this theory hold for a more natural setting (e.g., assume a true distribution or function having preferable properties)?

Typos:
- In abstruct: exit -> exist
- Section 1.2: $m < n - 2d$ -> $m < n - 2d + 2$.
- After Corollary 1: Note that 1 does not ... -> Note that Corollary 1 does not 

-----
Update:
I thank the authors for the response. My concerns have been well addressed and my review stands. I would like to keep the score.","[TITLE]
BOUNDS ON OVER-PARAMETERIZATION FOR GUARANTEED EXISTENCE OF DESCENT PATHS IN SHALLOW RELU NETWORKS

[ABSTRACT]
We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let m and d be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size n ≥ m + 2d − 2. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for n ≤ m, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for ""existence of descent paths"" in the loss landscape.

[CAPTIONS]
Table 1: (B) For any b ∈ B, βb lies on a facet of P A . We denote this facet by S A (b). (p5) (A) For any a ∈ A, S B (a) is a (d − 1)-dimensional simplex. (B) For any b ∈ B, S A (b) is a (d − 1)-dimensional simplex. (p6) (A) For any a ∈ A, there exist scalars α 1 , . . . , α d ∈ (0, 1) such that a = d i=1 α i s i (a), where s 1 (a), . . . , s d (a) are the vertices of simplex S B (a). (B) For any b ∈ B, there exist scalars α 1 , . . . , α d ∈ (0, 1) such that b = d i=1 α i s i (b), where s 1 (b), . . . , s d (b) are the vertices of simplex S A (b). (p7) (A) For any pair a and a of distinct points in A, we have S B (a) = S B (a ). Moreover, letting H a be the hyperplane that contains S B (a), a and a lie on opposite sides of H a , for all a ∈ A Ã with a = a. (B) For any pair b and b of distinct points in B, we have S A (b) = S A (b ). Moreover, letting H b be the hyperplane that contains S A (b), b and b lie on opposite sides of H b , for all b ∈ B B with b = b.
Table 2: Figure 2 :2Figure 2: A (2, t, 0)-configuration with t = 8. The blue dots show the points in A and red crosses are the points in B.
Table 3: Figure 3 :3Figure 3: Illustration of data points X 1 , . . . , X n for d = 3 and m = 12.
Table 4: consider the facet S B (a) defined in Property (p4), and let s 1 (a), . . . , s d−1 (a) be the vertices of S B (a) (as in Property (
Table 5: )•For each b ∈ B, consider the facet S A (b) and let s 1 (b), . . . , s d−1 (b) be the vertices of S A (b).
Table 6: Figure 4 :4Figure 4: Different types of cupped minima in terms of differentiability, discussed in Appendix A.
Table 7: Figure 5 :5Figure 5: An illustration of the points in equation 24-equation 27 for d = 3 and t = 4. The red crosses indicate the points in A Ã and the blue dots correspond to the points in B B .
Table 8: Properties (p4)-(p7): We only prove Part (B) for each of these properties; as similar proofs work also for Part (A)'s. Moreover, because of the rotational symmetry of A and B in the first two coordinates, it suffices to prove of the Properties (p4)-(p7) only for b 0 .
Table 9: )We proceed to verify equation 65-equation 72. Eq. equation 65 follows from equation 9. Recall the definitions X A − = ξu d and X B − = −ξu d . Then, equation 10 implies equation 69 and equation 71. Since X s1(b) , . . . , X s d−1 (b) define a boundary of the (d − 1)-dimensional convex set Q A , andH passes through X s1(b) , . . . , X s d−1 (b)
Table 10: Forequation 66, it follows from Property (p4) that b and the origin, 0, lie on opposite sides of hyperplane H. Consequently, −X b and u d also lie on opposite sides of hyperplaneH. Therefore, w T b X b and w T b u d have a same sing. It then follows from equation 10 that w T b X b > 0, and equation 66 follows.For equation 72, it follows from Property (p7) that for any b ∈ B B with b = b, X b and X b lie on opposite sides ofH. Eq. equation 66 then implies that w T b X b < 0. This establishes equation 72, and completes the proof of Claim 1.
Table 11: where last equality in due to equation 81. Combing equation 79 and equation 82, we obtain equation 78 for w r = w b . A similar argument implies equation 78 for w r = w a , a ∈ A. This completes the proof of Claim 2. Back to the proof of Lemma 3, for r = 1, . . . , m, let r min i=1,...,d−1 min γ

[INTRODUCTION]
We consider shallow neural networks of the form shown in Fig. 1. The network comprises a hidden layer and an input layer of widths m and d, respectively; and is to be trained over a training set of size n. Our results concern the slightly over-parameterized regime where n ≈ m. We study the existence of poor local minima that have positive curvature in the empirical squared loss landscape.
It is well-known that poor local minima exist in the loss landscape of shallow networks of arbitrary width. In fact, in a shallow network with ReLU activation functions, it is easy to construct training sets whose empirical loss landscape has high plateaus. 1 It is however not fully understood that under what conditions poor local minima may have positive curvature. This paper presents results that improve this understanding.
Non-existence of spurious local minima is closely connected to the so called descent path property: a loss landscape is said to have the descent path property if starting from any initial point there is a path of descent loss to a global minimum. From optimization perspective, the descent path property favors descent optimization algorithms like the pure gradient descent (GD) method. For SGD as well, non-existence of poor local minima is known to be a favorable property for guaranteed convergence (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016). The descent path property is shown to be satisfied in over-parameterized shallow networks with sufficiently large widths (Venturi et al., 2018). The results we present in this work, tighten the existing bounds on the over-parameterization required to guarantee this property.

[BACKGROUND]
Over the past few years, deep neural networks have achieved tremendous performance in various artificial intelligence applications such as computer vision, reinforcement learning, and natural language processing, etc. Despite their remarkable success in practice, theoretical aspects of this success remain a mystery. It has long been an open problem why simple local search algorithms for training deep neural networks, like stochastic gradient descent (SGD), typically converge to local minima with low training error despite the highly non-convex behavior of empirical loss. It has been observed, e.g., in (Choromanska et al., 2015), that these methods may get stuck in poor local minima (i.e., local minima with empirical loss much larger than the global optimum) for small networks, while the problem fades away as the number of parameters grows larger. Such observations are often explained by studying the loss landscape in over-parameterized regime where the number of parameters in the network exceeds the training sample size.
Recently, several attempts have been made to characterize properties of squared loss landscape by conditioning on the layers' dimensions and sample size. Soudry and Hoffer (2017) showed that weights of a neural network can be adjusted such that the empirical loss is zero almost surely if m > 4 n/(2d − 2) ≈ (2n)/d. This result is consistent with experimental observations that neural networks can fit training data if the number of parameters (here approximately 2n) is greater than the sample size. They also proved for normally distributed input that as n goes to infinity, the ratio between the volume of poor flat local minima regions to the volume of flat global minima fades exponentially if d =Ω( √ n) and m =Ω(n/d). Safran and Shamir (2016) showed that if the number of neurons in the hidden layer is Ω(n rank(X) ) (where X is the matrix containing all input), then with high probability, random initialization of weights will put them in a region of parameter space at which the loss surface has a basin-like structure, i.e., every local minimum in that region is global. In another work (Safran and Shamir, 2017), the same authors provide a computer-assisted proof to show that spurious local minima are common in the expected loss landscape of shallow under-parameterized (small-width) networks. Xie et al. (2016) showed that if the input data is drawn uniformly at random from the unit sphere, and if m =Ω(n β ) and d =Ω(n β ) with β ∈ (0, 1) being the decay exponent of the smallest eigenvalue of a kernel matrix, then every critical point is a global minimum.  proved that for any continuous activation function and under the assumption that data samples are distinct, there exist no poor local minima with positive curvature if m ≥ n . In the same spirit, Venturi et al. (2018) showed that for any continuous activation function, there is always a descent path to an optimal solution with zero loss in the empirical loss landscape if m ≥ n.
Several works have proposed similar results in other settings and under different assumptions. Soudry and Carmon (2016) showed that in a network of leaky ReLU activation functions with randomized perturbation of slopes, all differentiable local minima are global minima if m ≥ n/d. Kawaguchi (2016) proved that in shallow networks with linear activation functions, every local minimum is a global minimum and all the saddle points are strict in the sense that they have a direction of strictly negative curvature. Soltanolkotabi et al. (2019) showed that the same result carries over to quadratic activation functions under the assumption that the last layer comprises at east d positive and d negative weights. Du and Lee (2018) established similar results for quadratic activation functions, assuming m ≥ √ 2n. For deep neural networks with linear activation functions, Freeman and Bruna (2016) showed that all local minima are global minima if there is a hidden layer whose number of neurons exceeds the minimum of the widths of input and output layers. For deep neural networks with analytical activation functions, Nguyen and Hein (2017) proved a similar property under the assumptions that the number of neurons in some hidden layer is greater than sample size and the network has a pyramidal structure.
Such studies on the properties of loss landscape do not only provide insights into the complication of training, but are also beneficial for proving performance guarantees for some local search algorithms. For the class of loss functions whose landscape satisfy the properties of: a) all local minima are global, b) all saddle points are strict, it has been shown in several works (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016) that perturbed gradient descent converges to global optima in polynomial time. Another direction of research concerns the convergence of specific optimization algorithm such as Figure 1: Architecture of the shallow network considered in this paper. The network has a single hidden layer of m neurons with ReLU activation functions, and a neuron with linear activation function in its output layer. pure gradient descent and SGD without assuming such properties for the loss landscape Allen-Zhu et al., 2018;.

[OUR CONTRIBUTIONS]
We study the amount of over-parameterization required for guaranteed existence of descent paths to zero loss in the empirical loss landscape. Previous works suggest that zero loss is always possible for m ≥ 2n/d (Soudry and Hoffer, 2017). On the other hand, the best existing bound for guaranteed existence of descent paths to this zero loss requires m ≥ n neurons in the hidden layer (Venturi et al., 2018). Prior to the present work, it was not known whether the ""descent path property"" holds for m < n. Even for m ∈ (2n/d, n), where zero empirical risk is known to be achievable (Soudry and Hoffer, 2017), the existence of descent paths was in question. In this work, we tighten this gap and prove that there are training sets, under which in any network of width m ≤ n − 2d + 2, there exist initial weights that have no descent path to global minima. We do this by showing that the loss landscape, in this regime, admits poor local minima with positive curvature. We also provide evidences and make conjectures that these results carry over to networks of width m = n − 4, which, if true, provides a sharp characterization of the over-parameterization required for guaranteed existence of descent paths. We also wish to point that unlike most previous works, we do not restrict to differentiable local minima; for a simple argument shows that local minima with positive curvature cannot be differentiable if m > n/d (cf. Appendix A).

[OUTLINE]
We continue by discussing details of the system model and introducing our key definitions in Section 2. We then present, in Section 3, the main results of the paper. Proof of the main results are then given in Section 4. We finally discuss implications and possible extensions of our results in Section 5 along with a number of open problems and directions for future research.

[MODEL]
We consider shallow networks of the form shown in Fig. 1. The network takes d-dimensional inputs denoted by X. There is a single hidden layer comprising m neurons with ReLU activation function. For simplicity of our proofs, we only consider even values of m. We denote the input weights of r-th neuron by a d-dimensional vector w r , for r = 1, . . . , m. We then let w ∈ R md be the vector representation of all weights in the first layer.
The output layer has a single neuron, whose activation function is linear with an m-dimensional weight vector denoted by v. The network outputs a scalarŷ(w, v) = m r=1 v r w T r X1(w T r X ≥ 0). We fix a training set (X 1 , y 1 ), . . . , (X n , y n ) of size n, and consider the landscape of empirical squared loss function:
F (w, v) n i=1 ŷ i (w, v) − y i 2 .
(1)

[PROPERTIES OF THE LANDSCAPE]
We first provide a formal definition for the descent path property, which is a necessary condition for guaranteed performance of descent optimization algorithms.
Definition 1 (Descent path property). Consider a continuous function f : R d → R and let f * = inf x∈R d f (x) be its infimum. We say that f has the descent path property if for any x ∈ R d , there exists a continuous curve with γ : [0, 1] → R d such that γ(0) = x, f γ(1) = f * , and f γ(t) is a non-increasing function of t.
The descent path property is a necessary condition for any descent optimization algorithm to provably find a global minimum from all initial conditions. It was shown in Venturi et al. (2018) that the empirical loss landscape of a shallow neural network with ReLU activation and squared loss has the descent path property if the size of training data is no larger than the width of the hidden layer, i.e., n ≤ m. We now characterize a class of local minima of specific form in the following definition.
Definition 2 (Cupped minima). Given a function f : R d → R, we call x ∈ R d a cupped minimum of f if there are , δ > 0 such that for any y in the δ-neighborhood of x, we have f (y) ≥ f (x)+ y−x 2 . By a sub-optimal cupped minimum we mean a cupped minimum that is not a global minimum.
Note that every cupped minimum is a local minimum, but not every local minimum is cupped (e.g., flat local minima are not curved downwards, and hence are not cupped). Also note that a function is not necessarily differentiable at its cupped minima. We study cupped minima of the loss function in equation 1. Note however that for any α > 0, F (αw, v/α) = F (w, v). Therefore, F (•, •) has no cupped minima if both arguments are taken as variables. For that matter, when talking about cupped minima of F , we fix a v and consider F (•, v) as a function of its first argument. Interestingly, existence of cupped minima for F (•, v) leads to violation of descent path property for F (•, •) over both arguments, as shown in the following lemma. The proof is given in Appendix B.
Lemma 1. Consider a shallow network with loss function F in equation 1, and a pair of weights (w, v). Suppose that w is a sub-optimal cupped minimum of F (•, v), and that w r = 0, for r = 1, . . . , m. Then, F (•, •) has no descent path w(t), v(t) , initiated at (w, v), to its global minima.

[MAIN RESULTS]
The following theorem and corollary state the main results of the paper.
Theorem 1. For any d ≥ 4, m ≥ 8 + 4 3/(d − 3) , and n ≥ m + 2d − 2, there exists a training set of size n such that the empirical loss function F of a shallow neural network of width m has the following property. For any m-dimensional vector v, with m/2 number of positive and m/2 number of negative entries, F (•, v) has exponentially many sub-optimal cupped minima.
The proof is constructive and is given in Section 4. In particular, we devise a training sequence (X 1 , y 1 ), . . . , (X n , y n ) such that for weights (w, v) at the cupped minima, we have
v r w r = 1/ √ m, for r = 1, . . . , m. Moreover, X i ≤ 1, |y i | ≤ 2, and |e i | = 1 for i = 1, . . . , n (cf. Remark 2).
According to Theorem 1, there are training sequences tailored to give rise to sub-optimal cupped minima. However, we wish to point that the existence of such cupped minima does not stem from measure-zero incidents like placement of several data points on a low dimensional plane. In fact, in view of Lemma 1, any path that starts from a cupped minimum and end up in a global minimum would have an uphill climb of at least , for some > 0. Since the loss surface is a continuous function of (X i , y i ), a small perturbation of (X i , y i )'s leads continuously to a small change in F . Therefore, for small enough perturbations of (X i , y i ), any path to the set of global minima would still witness a positive uphill-climb. Hence, the descent path property remains out of order, even when the training data is slightly perturbed. Based on the above intuition, we can establish the following corollary 2 , Corollary 1. For any d ≥ 4, m ≥ 8 + 4 3/(d − 3) , and n ≥ m + 2d − 2; and when the inputs X and labels y are randomly drawn from independent normal distributions, there is a non-zero probability that F (•, •) does not have the descent path property. Venturi et al. (2018) that n ≤ m is sufficient for the descent path property to hold. In contrast, Corollary 1 show that if n ≥ m + 2d − 2, then the descent path property is not necessarily in effect. This leave a gap of size 2d − 2 for the edge of over-parameterization required to guarantee the descent path property. We believe that this edge lies sharp at n = m. We conjecture a stronger version of Theorem 1, that cupped minima can emerge for training data sizes as small as m = n − 4. Conjecture 1. Statement of Theorem 1 holds for all d ≥ 4, m ≥ 2d + 4, and n ≥ m + 4.

[IT WAS SHOWN IN]
See Remark 1 for insights into the possibility of this conjecture.

[PROOF OF THE MAIN RESULT]
In this section, we present the proof of Theorem 1 organized in a sequence of four subsections. We first present some preliminaries in Subsection 4.1. In Subsection 4.2, we introduce a geometric structure called ""(d, t, k)-configuration"", based on which we construct, in Subsection 4.3, the training set that gives rise to cupped minima in the loss landscape. Finally, in Subsection 4.4, we prove the existence of cupped mimima in the devised setting. In order to provide intuitions on the loss landscape at cupped minima and motivate our construction of the training set in Subsection 4.3, we make a short note on different types of cupped minima in Appendix A. We also defer the proofs of some lemmas from this section to appendices for improved readability.

[PRELIMINARIES]
Consider weights (w, v) and let (w , v ) be another set of weights such that for r = 1, . . . , m, v r and v r have the same sign and w r = (v r /v r )w r . Then, F (w, v) = F (w , v ). Moreover, it is no difficult to see that w is a cupped minimum of F (•, v) if and only if w is a cupped minimum of F (•, v ). For this reason, it suffices to prove existence of cupped minima for a fixed vector v. Note also that where w r 's are distinct, any permutation of the order of neurons would give rise to a new cupped minimum. Hence, existence of a cupped minimum for F (•, v) implies existence of exponentially many cupped minima for F (•, v).
We denote by e i =ŷ i (w, v) − y i the estimation error for input X i . We let u d = [0, . . . , 0, 1] T be a d-dimensional vector with the last entry equal to one and all other entries equal to zero. For a region P ⊆ R d , we denote its interior and and its convex-hull by int(P) and Conv(P), respectively. Assuming differentiability of F at w, the partial derivatives of the loss function with respect to w r , r = 1, . . . , m, is as follows
∇ wr F (w, v) = v r n i=1 e i X i 1 w T r X i ≥ 0 . (2)

[A GEOMETRIC CONFIGURATION]
We introduce a geometric structure for sets of points in R d . This configuration will be used in Subsection 4.3 to construct a landscape with cupped minima.
Definition 3 ((d, t, k)-Configuration). Given integers d, t, k ≥ 0 and disjoint sets A,Ã, B, andB of points in R d , we say that A,Ã, B,B forms a (d, t, k)-configuration if the following properties are satisfied:
(p1) Each of A and B consists of t points, and each ofÃ andB consists of k points.
(p2) (A) The convex hull of A Ã forms a polytope P A that has exactly t + k vertices. Equivalently, no point in A Ã is a convex combination of other points in A Ã . (B) Similarly, the convex hull of B B forms a polytope P B that has exactly t + k vertices.
(p3) We have 0 ∈ int(P A ) and 0 ∈ int(P B ).
(p4) There exists a constant β ∈ (0, 1) such that (A) For any a ∈ A, βa lies on a facet of P B . We denote this facet by S B (a). (p8) Consider a 2t × 2t matrix M whose rows and columns are associated to points p ∈ A B and q ∈ A B, and whose entries are as follows
M pq = d p, H q , if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A, 0, otherwise,(3)
where H q is the hyperplane define in Property (p7), and d(•, •) is the euclidean distance.
The property requires M to be full-rank.
Among the above properties, the most difficult of all is Property (p4), and the requirement that it involves the same β for all points in A B. In fact, elimination of Property (p4) gives rise to trivial configurations. 3 In the two dimensional space, for any t ≥ 4 there exists a (2, t, 0)-configuration of the form illustrated in Fig. 2. In the following proposition, we generalize this observation to higher dimensions. Proposition 1. For any d ≥ 2 and t ≥ 4, there exists a (d, t, d − 2)-configuration.
The proof is constructive, and is given in Appendix C. We conjecture that there also exist (d, t, 0)configurations.
Conjecture 2. For any d ≥ 2 and t ≥ 2d, there exists a (d, t, 0)-configuration. Remark 1. Using a configuration given by Conjecture 2 instead of the configuration from Proposition 1 in the construction and the proof that follow, we obtain a proof for Conjecture 1. In this view, establishing Conjecture 2 would also resolve Conjecture 1.

[CONSTRUCTING A LANDSCAPE WITH CUPPED MINIMA]
Here we present a set of training data (X 1 , y 1 ), . . . , (X n , y n ) and a set of wights (w 1 , v 1 ), . . . , (w m , v m ) such that the empirical loss surface corresponding to (X 1 , y 1 ), . . . , (X n , y n ) has a sub-optimal cupped minimum at (w 1 , v 1 ), . . . , (w m , v m ). Without loss of generality, we assume n = m + 2d − 2. Extension to larger values of n is straightforward via replication.
Let A,Ã, B,B be a (d − 1, m/2, d − 3)-configuration in the (d − 1)
-dimensional space, as in Proposition 1. In view of Property (p3), let 0 > 0 be such that P A and P B contain the 0neighborhood of 0. Let
ξ 1 0 a∈A Ã a + b∈B B b + n + 1 β . (4
)
We proceed by introducing the data points X 1 , . . . , X n . An illustration of these data points in the three dimensional space is shown in Fig. 3.  Data points X: We consider a total number of n = m + 2d − 2 data points as follows.
• For each a ∈ A Ã , we consider a new data point X a as follows.
Let [z 1 , . . . , z d−1 ] ∈ R d−1
be the representation of a in the Cartesian coordinates. We let
X a = z 1 , . . . , z d−1 , 1 . • For each b ∈ B B , we consider a new data point X b as follows. Let [z 1 , . . . , z d−1 ] ∈ R d−1 be the representation of b in the Cartesian coordinates. We let X b = − z 1 , . . . , z d−1 , 1 .
• We consider two extra points X A + , X A − , X B + , and X B − as follows. We let
X A − ξu d , X B −
−ξu d , and
X A + X A − − a∈A Ã X a + 1/β − 1 u d (5) X B + X B − + b∈B B X b + 1/β − 1 u d (6
)
where u d = [0, . . . , 0, 1] T , and ξ and β are defined in equation 4 and Property (p4), respectively.
Weights at cupped minimum: We associate each of m neurons to a point p in A B, in a one-one manner; and denote the vector of input weights and the output weight of that neuron by w p and v p , respectively. These weights are chosen as follows:  p6)). We let w a be the unique vector such that w a = 1 and We let w b be the unique vector such that w b = 1 and
• For each a ∈ A, we let v a = −1/ √ m. • For each b ∈ B, we let v b = 1/ √ m. • For each a ∈ A,
w T a X si(a) = 0, i = 1, . . . , d − 1,(7)
w T a u d < 0. (8
w T b X si(b) = 0, i = 1, . . . , d − 1,(9)
w T b u d > 0. (10
)
Labels y: Having determined the data points X and the weights (w, v), the outputŷ(w, v) of the network is determined for all input X. In the following, we choose the true labels y to obtain a desired error e =ŷ − y for each input data. In particular:
• For each a ∈ A Ã , we associate to X a a label y a so that e a ŷ a (w, v) − y a = 1.
• For each b ∈ B B , we associate to X b a label y b so that e b ŷ b (w, v) − y b = −1.
• We choose the labels associated to X A + , X A − , X B + , and X B − such that e A + = e B + = 1 and e A − = e B − = −1.
This completes the description of the training set. As shown in in Soudry and Carmon (2016), there exist weights that achieve zero loss if m > 4 n/(2d − 2) . In our case, n = m + 2d − 2, and its easy to check that m > 4 (m + 2d − 2)/(2d − 2) for all d ≥ 4 and m ≥ 8 + 4 3/(d − 3) . It follows that in our setting the global optimum has zero loss, showing that the above (w, v) is sub-optimal. Remark 2. In the above construction, the norms of inputs vectors may be very large. If we scale down the inputs such that X i ≤ 1/ √ m for i = 1, . . . , n, and modify the corresponding labels y i such that e i remains unchanged, then a same set of weights will still be a cupped minimum for the landscape defined in terms of new (X i , y i )'s. Moreover, for this setting, it is easy to check that w r = 1, e i = 1, and |y i | ≤ 2.

[PROVING THE CUPPED]
MINIMA PROPERTY Let δ min |w T r X i | X i w T r X i = 0, r = 1, . . . , m, i = 1, . . . , n .(11)
Then, δ > 0. For any θ ∈ R md with θ = 1 and for any t ∈ [0, δ], let
F θ (t) = F (w + θt, v).(12)
We show that there is an > 0 such that for any unit-norm θ and any t ∈ [0, δ],
F θ (t) ≥ F θ (0) + t 2 . (13
)
Lemma 2. For any θ ∈ R md with θ = 1, F θ (•) is a quadratic and convex function over
[0, δ].
The proof is give in Appendix D and relies on the fact that neuron activations do not alter at w + θt for t ∈ [0, δ]. Consider now the following m-dimensional subspace of R md
H w         α 1 w 1 . . . α m w m    α 1 , . . . , α m ∈ R      . (14
)
For any θ ∈ R md , let θ and θ ⊥ be the orthogonal projections of θ on H w and H ⊥ w , respectively. Then, θ = θ + θ ⊥ . In order to establish equation 13, we need lower bounds on F θ (0) and F θ , which we derive in the next two lemmas. Lemma 3. There exists µ > 0 such that for any θ ∈ R md with θ = 1, we have
dF θ (t) d + t t=0 ≥ µ θ ⊥ . (15
)
The proof is given in Appendix E, and relies in a subtle way on the choice of data points in subsection 4.3. We now bound the curvature of F θ . Lemma 4. There exist constants η 1 , η 2 > 0 such that for any θ ∈ R md with θ = 1, and for any t ∈ (0, δ),
d 2 F θ (t) dt 2 ≥ max 2η 1 θ 2 − 2η 2 θ ⊥ , 0 . (16
)
The proof is given in Appendix F, and relies on Property (p8) of the underlying configuration.
Consider now the second order polynomial p(x) = η 1 δ 1 − x 2 − η 2 δx − µx, where µ, η 1 , and η 2 are the constants in Lemmas 3 and 4. Since p(0) > 0 and p(1) < 0, the polynomial p has exactly one root in the interval (0, 1), which we denote by x 0 . Let
µx 0 /δ. (17
)
Lemma 5. For any θ ∈ R md with θ = 1, and any t ∈ [0, δ], we have
F θ (t) ≥ F θ (0) + t 2 .
This lemma is a simple consequence of Lemmas 3 and 4, and its proof is given in Appendix G. It follows from Lemma 5 that for any w in the δ-neighborhood of w, we have F (w , v) ≥ F (w, v) + w − w 2 . This shows that w is a cupped minimum for F (•, v), and completes the proof of Theorem 1.

[DISCUSSION]
The guaranteed existence of descent paths in shallow networks of ReLU neurons was previously established (Venturi et al., 2018), given an over-parameterization of m ≥ n (where m and n are the number of neurons and the size of training data, respectively). This left an uncertainty gap of 2n/d < m < n, where zero empirical risk is known to be achievable (for m ≥ 2n/d) (Soudry and Hoffer, 2017), but the existence of descent paths was in question. In this work, we have tightened this uncertainty gap to n − 2d + 2 < m < n, by proving that for any m ≤ n − 2d + 2, there are input data and initial weights for which a descent path does not exist. This conclusion we reach by establishing the existence of cupped minima for m ≤ n − 2d + 2, and for the right choice of input data.
Compared to similar existing results for other activation functions, our results suggest that the edge m ≈ n of over-parameterization required for elimination of sub-optimal cupped minima in ReLU networks is much higher than that of networks with quadratic activation functions, m ≈ √ 2n (Du and Lee, 2018), and linear activation functions, m ≈ n/d (Kawaguchi, 2016), and is almost as high as general continuous activation functions of any form, m ≤ n (Venturi et al., 2018).
Non-existence of spurious local minima and the decent path property favor the convergence of decent optimization methods like GD. However, for different variants of noisy GD, like SGD and Langevin dynamics, it is quite common for the empirical loss to fluctuate during the training. Nevertheless, for theoretical analysis purposes it usually helps to take the noise away, for example by tending the step size to zero. The resulting GD, which always follows a descent path, is usually easier to analyze and can also help to study the SGD dynamics. On the other hand, from a practical view, convergence of SGD in local-min-free landscapes is well-studied.
Aside from addressing Conjecture 2, there remain several open problems, which we review next. As an important direction for future research, it would be interesting if one could obtain bounds on the probability of existence of cupped minima over random data sets, underneath the edge of over-parameterization. In particular, we showed in Corollary 1 that this probability is non-zero, however we gave no clue on either the size or scaling of this probability. Another class of problems concerns basins of local minima, and how they affect dynamics of first order optimization algorithms. As a step toward this goal, one might characterize the true over-parameterization regime in which the basins of poor local minima have considerable volume. Among other directions are extensions of our results to deep ReLU networks, shallow non-ReLU networks, and shallow ReLU networks under loss functions more general than the squared loss.

[APPENDICES A DIFFERENT TYPES OF CUPPED MINIMA IN TERMS OF DIFFERENTIABILITY]
Here we discuss different types of cupped minima and provide elementary intuitions on the loss landscape at cupped minima.
We first characterize curvature of the loss function at differentiable points. For r = 1, . . . , m, let J r be an n × n diagonal matrix whose (i, i) entry equals 1 w T r X i ≥ 0 . Let G be an n × md matrix of the form:
G = v 1 J 1 X T • • • v m J m X T , (18
)
where X is a d × n matrix that has X i in its i-th column. If F is differentiable at (w, v), its gradient is ∇ w F (w, v) = G T [e 1 , . . . , e m ] T , where e i =ŷ i − y i is the output error for input X i . Moreover, if F is differentiable at (w, v), its Hessian with respect to w equals
∇ 2 w F (w, v) = G T G. (19
)
We classify cupped minima into three categories in terms of differentiability. Specifically, for a cupped minimum w of F (•, v), we consider three cases:
Type 1) F is differentiable at w.
Type 2) F (w + θt, v) as a function of t is non-differentiable at t = 0, for all θ ∈ R md .
Type 3) There are θ 1 , θ 2 ∈ R md such that F (w+θ 1 t, v) is differentiable at t = 0, while F (w+θ 2 t, v) is non-differentiable at t = 0.
Fig. 4 illustrates examples of loss surface at the above three types of cupped minima. We now argue that the first two types are not possible in the loss landscape of shallow networks.
If F (•, v) is differentiable at w, its Hessian given in equation 19 equals G T G. Since G is an n × md matrix, assuming md > n, G T G would have zero eigenvalues. Therefore, w cannot be a cupped minimum of F (•, v). It follows that there exists no differentiable cupped minimum (nor saddle point) if md > n.
For non-differentiable points, note that F (w + wt, v) as a function of t is a differentiable quadratic function. This is because the output scales proportionally with w. Therefore, cupped minima of the second type are not possible, as well. In the same spirit, it can be shown that F (w + θt, v) is differentiable as a function of t, if θ ∈ R md belongs to the m dimensional subspace H w defined in equation 14.
For the above reasons, all cupped minima, if any, are of the third type. Therefore in the construction of Subsection 4.3, we introduce a training set and a pair of weights (w, v) such that w is a cupped minimum of F (•, v), and F (w + θt, v) is differentiable in t only for θ ∈ H w .

[B PROOF OF LEMMA 1]
If v has a zero entry, v r = 0 for some r, then F (•, v) is a constant function with respect to w r , and thereby has no cupped minima. Therefore, we assume that v has no zero entries. Let
t 0 = inf t | ∃r, v r (t) = 0 .
For t > 0, we definew
(t) =    v 1 (t)/v 1 × w 1 (t) . . . v m (t)/v m × w m (t)    . (20
)
Let t 1 = inf t |w(t) = w . We show that t 1 < t 0 . If t 0 < ∞, then continuity of v(t) implies that v r (t 0 ) = 0, for some r ≤ m. Therefore,w r (t 0 ) = 0 = w r . It then follows from the continuity ofw r (•) that there is an > 0 such thatw r (t 0 − ) = w r . Consequently, t 1 < t 0 .
The inequality t 1 < t 0 implies that there is an > 0 such thatw r (t 1 + ) = w, and for any t ∈ [0, t 1 + ], we have sgn v r (t) = sgn(v r ), r = 1, . . . , m. Therefore, for any t ∈ [0, w, v). This shows that w(t), v(t) cannot be a descent path for F (•, •), and completes the proof of Lemma 1.
t 1 + ], we have F w(t), v = F w(t), v(t) . Since w is a cupped minimum of F (•, v), there is an s ∈ [t 1 , t 1 + ] such that F w(t), v > F (

[C PROOF OF PROPOSITION 1]
For i = 3, . . . , d, let
c i 1 i − 1 + 2 cos(π/t) . (21
)
Fix a constantc
d i=3 1 + c i = d i=3 i + 2 cos(π/t) i − 1 + 2 cos(π/t) = d + 2 cos(π/t) 2 + 2 cos(π/t) . (22
)
Let γ be a uniform random variable
γ ∼ unif cos(π/t) − cos(2π/t) 4c , cos(π/t) − cos(2π/t) 2c . (23
)
We take a sample from the above distribution and fix a γ for the rest of the proof.
We now introduce the points in the configuration. For i = 0, . . . , t − 1, we consider points a i ∈ A and b i ∈ B as follows
a i = cos 2π(i − 1/2) t , sin 2π(i − 1/2) t , γc 3 , γc 4 , . . . , γ d c d ,(24)
b i = cos 2πi t , sin 2πi t , −γc 3 , −γc 4 , . . . , −γ d c d . (25
)
For i = 3, . . . , d, we consider pointsã i ∈Ã andb i ∈B as follows
a i = 0, . . . , 0 i−1 , −1, c i+1 , . . . , c d , (26
) b i = 0, . . . , 0 i−1 , 1, −c i+1 , . . . , −c d . (27
)
Fig. 5 shows an illustration of these points for d = 3.
We proceed by verifying Properties (p1)-(p8).
Property (p1): Property (p1) is straightforward from the above construction.
Property (p2): Considering only the first two coordinates, it is easy to see that no point of A is a convex combination of other points in A Ã . Also note that for i = 3, . . . , d,ã i is the only point Property (p3): Let
α i = 1 γt , i = 1, . . . , t,(28)
α i = (c 3 + 1) • • • (c i−1 + 1)c i , i = 3, . . . , d.(29)
A simple induction on j shows that
j−1 i=3α i =α j c j − 1. (30
)
for j = 4, . . . , d. We show that
1 t−1 i=0 α i + d i=3α i t−1 i=0 α i a i + d i=3α iã i = 0. (31
)
For the first two coordinates, equation 31 is easy. Let a i j andã i j be the j-th entries of a i andã i , respectively. For the j-th coordinate, j = 3, . . . , d, we then have
t−1 i=0 α i a i j + d i=3α iã i j = t × 1 tγ × γc j + c j j−1 i=3α i −α j = c j + c j α j c j − 1 −α j = 0,
where the second equality is due to equation 30. This establishes equation 31. It then follows from equation 31 that 0 is a convex combination of points in A Ã . Consequently, 0 ∈ int(P A ). A similar argument shows that 0 ∈ int(P B ). Properties (p4) and (p5): For the j-th coordinate, j = 3, . . . , d, we have
a 0 j + a 1 j + γ d i=3ã i j = γc j + γc j + γ j−1 i=3 c j − γ = γ (j − 1)c j − 1 = γ (j − 1) 1 j − 1 + 2 cos(π/t) − 1 = −2γ cos(π/t) j − 1 + 2 cos(π/t) = − 2 cos(π/t) γc j = 2 cos(π/t) b 0 j ,(32)
where b 0 j is the j-th entry of b 0 defined in equation 25. Similarly, for the first two coordinates, we have:
a 0 1 + a 1 1 + γ d i=3ã i 1 = cos(π/t) + cos(π/t) = 2 cos(π/t) b 0 1 , a 0 2 + a 1 2 + γ d i=3ã i 2 = sin(π/t) − sin(π/t) = 0 = 2 cos(π/t) b 0 2 .(33)
It then follows from equation 32 and equation 33 that
b 0 = 1 2 cos(π/t) a 0 + a 1 + γ d i=3ã i .(34)
Let
β 2 cos(π/t) (d − 2)γ + 2 . (35
)
Then, from equation 34,
βb 0 = 1 (d − 2)γ + 2 a 0 + a 1 + γ d i=3ã i .(36)
Therefore, βb 0 is a convex combination of a 0 , a 1 ,ã 3 , . . . ,ã d , and therefore lies on the simplex S that has a 0 , a 1 ,ã 3 , . . . ,ã d as its vertices. Next, we show that S is a facet of P A .
Consider a vector z ∈ R d with entries
z 1 = (c − 1)γ + 1 cos(π/t) , z 2 = 0, z i = − d j=i+1 (c j + 1), i = 3, . . . , d − 1, z d = −1.(37)
Then, a simple backward induction, with base case i = d, shows that for i = 3, . . . , d, we have
d j=i+1 c j z j = z i + 1. In the same vein, d j=3 c j z j = −c + 1.(38)
It follows that for i = 3, . . . , d,
z Tãi = d j=i+1 z j c j − z i = 1.(39)
Moreover, for i = 0, 1,
z T a i = d j=3 z j γc j + z 1 cos(π/t) = γ(−c + 1) + z 1 cos(π/t) = −γ(c − 1) + (c − 1)γ + 1 cos(π/t) cos(π/t) = 1,(40)
where the second equality is due to equation 38. Let H be the hyperplane that passes through a 0 , a 1 ,ã 3 , . . . ,ã d . It follows from equation 39 and equation 40 that for any p ∈ a 0 , a 1 ,ã 3 , . . . ,ã d , we have z T p = 1. Therefore, z is orthogonal to H. Consequently,
H = x ∈ R d | z T x = 1 .(41)
For i = 2, . . . , t − 1, we have
z T a i = z T a i − a 0 + z T a 0 = z T a i − a 0 + 1 = z 1 cos(2π(i − 1/2)/t) − cos(π/t) + 1 < 1,(42)
where the second equality is due to equation 40, and the inequality is because z 1 > 0 and cos(2π(i − 1/2)/t) − cos(π/t) < 0. It follows from equation 41 and equation 42 that all points of A\{a 0 , a 1 } lie on one side of H, while all points ofÃ {a 0 , a 1 } lie on H. Then, H is a tangent hyperplane to P A . Thus, the simplex S is a facet of P A . This completes the proofs of Properties (p4) and (p5). Moreover, from the definition H b in Property (p7), we have
H b 0 = H.(43)
Property (p6): Since t ≥ 4, we have 2 cos(π/2) > 1. Moreover, recall from equation 23 that γ < 1. Property (p6) then follows from equation 34 and the fact that S is a facet of P A .
Property (p7): For i = 0, . . . , t − 1,
z T b i = z T (b i − a 0 ) + z T a 0 = z 1 cos(2πi/t) − cos(π/t) − 2 d j=3 z j γc j + 1 = z 1 cos(2πi/t) − cos(π/t) + 2γ(c − 1) + 1.(44)
where the second equality is due to equation 40 and definitions of a 0 and b i , and the third equality is from equation 38. It follows that
z T b 0 = z 1 1 − cos(π/t) + 2γ(c − 1) + 1 > 1,(45)
where the inequality is because the first two terms on the left hand side of the inequality are positive.
In the same vein, for i = 1, . . . , t − 1
z T b i = z 1 cos(2πi/t) − cos(π/t) + 2γ(c − 1) + 1 < z 1 cos(2π/t) − cos(π/t) + 2γc + 1 < cos(2π/t) − cos(π/t) + 2γc + 1 ≤ cos(2π/t) − cos(π/t) + cos(π/t) − cos(2π/t) + 1 = 1,(46)
where the second inequality follows from the definition of z 1 in equation 37 and the fact that z 1 > 1, and the third inequality is from the definition of γ in equation 23 and the fact that 2γc ≤ D PROOF OF LEMMA 2
Consider a block representation of θ as follows
θ =    θ 1 . . . θ m    ,(54)
where each θ r is a d-dimensional vector.
It follows from the definition of δ that for any t ∈ (0, δ), if w T r X i > 0, then (w r + θ r t) T X i > 0; and if w T r X i < 0, then (w r + θ r t) T X i < 0. Therefore, for r = 1, . . . , m and i = 1, . . . , n, and for any t ∈ [0, δ) ,
1 (w r + θ r t) T X i ≥ 0 = 1 w T r X i > 0 + 1 w T r X i = 0, θ T r X i ≥ 0(55)
Consequently, for any t ∈ [0, δ] and i = 1, . . . , n, we havê
y i w + θt, v = m r=1 v r w T r X i + tθ T r X i 1 (w r + θ r t) T x i ≥ 0 = m r=1 v r w T r X i + tθ T r X i 1 w T r X i > 0 + 1 w T r X i = 0, θ T r X i ≥ 0 .(56)
It follows thatŷ i w + θt, v is a linear function of t over the interval t ∈ [0, δ]. Therefore, F θ (t) = n i=1 ŷ i (w + θt, v) − y i 2 is a quadratic and convex function of t, for t ∈ [0, δ].

[E PROOF OF LEMMA 3]
We first characterize active neurons for different inputs. For two subsets S 1 , S 2 ⊂ R d we let S 1 \S 2 = S 1 S c 2 . Recall the definition of s i (a) and s i (b) from Property (p6). Claim 1. For any a ∈ A, we have
w T a X si(a) = 0, i = 1, . . . , d − 1,(57)
w T a X a > 0,(58)
w T a X b > 0, b ∈ B B \ s 1 (a), . . . , s d−1 (a) ,(59)
w T a X B + > 0,(60)
w T a X B − > 0,(61)
w T a X A + < 0,(62)
w T a X A − < 0,(63)
w T a X a < 0, a ∈ A Ã \ {a}.(64)
Similarly, for any b ∈ B, we have
w T b X si(b) = 0, i = 1, . . . , d − 1,(65)
w T b X b > 0,(66)
w T b X a > 0, a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) ,(67)
w T b X A + > 0,(68)
w T b X A − > 0,(69)
w T b X B + < 0,(70)
w T b X B − < 0,(71)
w T b X b < 0, b ∈ B B \{b}. (72
)
Proof of Claim 1. Fix a b ∈ B. We begin by introducing some notations. Let H be the (d − 2)dimensional hyperplane in the (d − 1)-dimensional space that passes through s 1 (b), . . . , s d−1 (b). In the same spirit, letH be the (d − 1)-dimensional subspace in the d-dimensional space that passes through X s1(b) , . . . , X s d−1 (b) , equivalentlyH is the subspace orthogonal to w b . We denote the convex hull of X s1(b) , . . . , X s d−1 (b) by C B Conv X s1(b) , . . . , X s d−1 (b) . Similarly, we let
Q A conv X a | a ∈ A Ã and Q B Conv X b | b ∈ B B .
Before presenting the proofs of properties equation 57-equation 72, we review make some easy observations. Recall the definition of 0 from the paragraph proceeding equation 4. Let B d−1 0 be the intersection of the 0 -ball centered at 0 with the orthogonal space of u d . Then, from the definition of , we have
B d−1 0 + u d ⊂ Q A , B d−1 0 − u d ⊂ Q B . (73
)
For x ∈ R d , let π(x) be the projection of x on the span of first d − 1 coordinates, i.e., the orthogonal space of u d . Then,
π ( X A + ) ξ + 1/β − n − 1 = a∈A Ã (X a − 1) ξ + 1/β − n − 1 = a∈A Ã a ξ + 1/β − n − 1 < a∈A Ã a ξ − n < a∈A Ã a a∈A Ã a / 0 = 0 ,(74)
where the first equality is from the definition of X A + and the second inequality follows from the definition of ξ in equation 4. Therefore, π(X
A + )/ ξ + 1/β − n − 1 ∈ B d−1 0 . Consequently, 1 ξ + 1/β − n − 1 X A + = π(X A + ) ξ + 1/β − n − 1 + u T d X A + u d ξ + 1/β − n − 1 = π(X A + ) ξ + 1/β − n − 1 + ξ + 1/β − n − 1 u d ξ + 1/β − n − 1 = π(X A + ) ξ + 1/β − n − 1 + u d ∈ B d−1 0 + u d ∈ Q A ,(75)
where the first equality is orthogonal decomposition of X A + , and last inclusion follows from equation 73. In the same vein, we can show that , then all points in Q A lie on a same side ofH. In other words, either we have w T b x ≥ 0, for all x ∈ Q A ; or we have w T b x ≤ 0, for all x ∈ Q A . In view of Property (p3), u d ∈ int(Q A ). It then follows from equation 10 that for any x ∈ Q A , we have w T b x ≥ 0. Consequently, for any x ∈ Q A \H, we have w T b x > 0. In particular,
− 1 ξ + n + 1 − 1/β X B + ∈ Q A . (76
w T b X a > 0, a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) , w T b X A + > 0, −w T b X B + > 0,
where the first inequality is because Property (p5) implies that X a ∈H for a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) , and the last two inequalities are due to equation 75 and equation 76, respectively. This establishes equation 67, equation 68, and equation 70. In light of Claim 1, it is easy to see for r = 1, . . . , m and i = 1, . . . , n that if w T r X i = 0, then
v r e i = 1 √ m .(77)
In our next claim, we examine a linear combination of data points for which a particular neuron is active.
Claim 2. For r = 1, . . . , m, there exist constants γ r 1 , . . . , γ r m such that
n i=1 γ r i e i X i 1 w T r X i = 0 + n i=1 e i X i 1 w T r X i > 0 = 0. (78
)
Proof of Claim 2. Fix a b ∈ B. We prove the claim for the neuron associated to b. It follows from Claim 1 that
n i=1 e i X i 1 w T b X i > 0 = e b X b + a∈(A Ã )\ s1(b),...,s d−1 (b) e a X a + e A + X A + + e A − X A − = −X b + a∈(A Ã )\ s1(b),...,s d−1 (b) X a + X A + − X A − = −X b + a∈(A Ã )\ s1(b),...,s d−1 (b) X a −   a∈A Ã X a − (1/β − 1) u d   = −X b − d−1 i=1 X si(b) + (1/β − 1) u d ,(79)
where the second equality is due to the definitions of e a , e b , e A + , and e A − , and the third equality is from the definitions of X A + in equation 5.
On the other hand, it follows from Property (p6) that there exist scalars α 1 , . . . , α 
d ∈ (0, 1) such that b = d−1 i=1 α i s i (b). Therefore, from the definition of X b , − X b + u d = d−1 i=1 α i X si(b) − u d (80) Moreover, Property (p4) implies that d−1 i=1 α i = 1/β. Then, from equation 80, −X b = d−1 i=1 α i X si(b) − d−1 i=1 α i − 1 u d = d−1 i=1 α i X si(b) − (1/β − 1) u d . (81
) For i = 1, . . . , d − 1, let γ i = 1 − α i . Then, Claim 1 implies that n i=1 γ i e i X i 1 w T b X i = 0 = d−1 i=1 γ i e i X si(b) = d−1 i=1 (1 − α i )X si(b) = d−1 i=1 X si(b) − d−1 i=1 α i X si(b) = d−1 i=1 X si(b) + X b − 1 β − 1 u d ,(82)
r i , (1 − γ r i ) ,(83)
for the constant γ r i defined in Claim 2. It follows that r > 0, for r = 1, . . . , m. For any r ≤ m, X s1(r) , . . . , X s d−1 (r) are linearly independent and, by definition, are all orthogonal to w r . Therefore, there exists a constant r > 0 such that for any
ζ r ∈ R d with ζ r = 1, we have max i=1,...,d−1 |ζ T r X si(r) | ≥ r ζ ⊥ r , where ζ ⊥ r is the projection of ζ r on the null-space of w r . Consequently, for any ζ r ∈ R d with ζ r = 1, max i=1,...,n |ζ T r X i | × 1 w T r X i = 0 ≥ r ζ ⊥ r .
In particular, considering the block-vector representation of θ in equation 54, we obtain for r = 1, . . . , m, max i=1,...,n
|θ T r X i | × 1 w T r X i = 0 ≥ max i=1,...,n r θ ⊥ r .(84)
Let µ min r r r / √ m. Then, µ > 0. It then follows from Claim 2 that, for r = 1, . . . , m,
v r n i=1 e i θ T X i 1 w T r X i = 0, θ T r X i ≥ 0 + v r n i=1 e i θ T r X i 1 w T r X i > 0 = v r n i=1 e i θ T r X i 1 w T r X i = 0, θ T r X i ≥ 0 − v r n i=1 γ r i e i θ T r X i 1 w T r X i = 0 = v r n i=1 e i θ T r X i 1(θ T r X i ≥ 0) − γ r i 1 w T r X i = 0 = 1 √ m n i=1 θ T r X i 1(θ T r X i ≥ 0) − γ r i 1 w T r X i = 0 = 1 √ m n i=1 θ T r X i × 1(θ T r X i > 0) − γ r i × 1 w T r X i = 0 ≥ 1 √ m r n i=1 θ T r X i 1 w T r X i = 0 ≥ r √ m max i=1,...,n θ T r X i 1 w T r X i = 0 ≥ r √ m r θ ⊥ r ≥ µ θ ⊥ r ,(85)
where the third equality is due to equation 77, the fourth equality is because θ T r X i and 1(θ T r X i > 0) − γ r i have always the same sign, the first inequality is by definition of r in equation 83, the third inequality follows from equation 84, and the last inequality is from the definition of µ.
On the other hand, equation 2 implies that dF θ (t)
d + t t=0 = lim t↓0 m r=1 v r θ T r n i=1 e i X i 1 (w r + θ r t) T X i ≥ 0 = m r=1 v r n i=1 θ T r e i X i 1 w T r X i = 0, θ T r X i ≥ 0 + 1 w T r X i > 0 = m r=1 v r n i=1 e i θ T X i 1 w T r X i = 0, θ T r X i ≥ 0 + v r n i=1 e i θ T r X i 1 w T r X i > 0 ≥ m r=1 µ θ ⊥ r ≥ µ θ ⊥ (86)
where the second equality is due to equation 55 and the first inequality follows from equation 85. This completes the proof of Lemma 3.

[F PROOF OF LEMMA 4]
We begin by a claim. Given a q ∈ A B, recall the definition of hyperplane H q from Property (p7).
Claim 3. For any pair of points p, q ∈ A B, we have
w T q X p = 1 1 + d(0, H q ) 2 d p, H q . (87
)
where the third equality is due to equation 19. On the other hand, since θ = θ + θ ⊥ , we have
Gθ = Gθ + Gθ ⊥ .(96)
In the following claim, we elaborate on Gθ . Claim 4. There exists a constant η 1 > 0 such that Gθ 2 ≥ 2η 1 θ 2 , for all θ ∈ R d .
Proof of Claim 4. Recall that θ is the projection of θ on subspace H w defined in equation 14. Then, there exist constants α 1 . . . , α m such that
θ =    α 1 w 1 . . . α m w m    . (97
)
Let α be the vector representation of α 1 , . . . , α m . Then,
θ 2 = m i=1 α 2 i w i 2 = m i=1 α 2 i = α 2 . (98
)
Consider the n × m matrixG = v 1 J 1 X T w 1 • • • v m J m X T w m .(99)
Then, from the definition of matrix G in equation 18,
Gθ = v 1 J 1 X T • • • v m J m X T    α 1 w 1 . . . α m w m    = v 1 J 1 X T w 1 • • • v m J m X T w m α =Gα.(100)
Each column ofG corresponds to a neuron, and thereby is associated to a point in A B. In the same vein, every row ofG is associated to an input vector. By removing some rows ofG, we devise a matrixM so that each row ofM is associated to an input X p for p ∈ A B. Therefore,M is an m × m matrix, whose rows and columns are associated to the points in A B. It follows thatM α is a vector obtained by removing some entries from vectorGα. As a result, M α ≤ G α .
In the following, we capitalize on Property (p8) to show thatM is full-rank.
For q ∈ A B, let
γ q v q 1 + d(0, H q )
.
For p, q ∈ A B, the entry in row p and column q ofM equals
M pq =G pq = v q X T p w q 1 w T q X p > 0 + 1 w T q X p = 0, θ T q X p ≥ 0 = v q X T p w q 1 w T q X p > 0 = v q 1 + d(0, H q ) d p, H q 1 w T q X p > 0 = γ q d p, H q 1 w T q X p > 0 . (102
)
where the first equality is from the definition ofM , the second equality follows from the definitions ofG and J q in equation 99 and equation 93, the third equality is because X T p w q 1 w T q X p = 0 = 0, and the fourth equality is due to Claim 3. Then, Claim 1 implies that for any p, q ∈ A B, M pq = γ q d p, H q , if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A, 0, otherwise.
Compared to matrix M defined in equation 3, each column q ofM equals the column q of M multiplied by a non-zero constant γ q . In view of Property (p8), M is full-rank. It follows thatM is full-rank, as well.
Let σ be the smallest singular value ofM . SinceM is full-rank, we have σ > 0. Moreover, M α ≥ σ α .
Then, Gθ 2 = G α 2 ≥ M α 2 ≥ σ 2 α 2 = σ 2 θ 2 , (105) where the equations are due to equation 100, equation 101, equation 104, and equation 98, respectively. Claim 4 then follows for η 1 = σ 2 /2.
Back to the proof of Lemma 4, we denote by σ max the largest singular value of G. Let η 2 σ 2 max . Then,
Gθ 2 = Gθ + Gθ ⊥ 2 = Gθ 2 + Gθ ⊥ 2 + 2 Gθ T Gθ ⊥ ≥ Gθ 2 − 2 Gθ × Gθ ⊥ ≥ Gθ 2 − 2σ max θ × σ max θ ⊥ ≥ Gθ 2 − 2σ 2 max θ ⊥ = Gθ 2 − 2η 2 θ ⊥ ≥ 2η 1 θ 2 − 2η 2 θ ⊥ ,(106)
where the second inequality is from the definition of σ max , the third inequality is because θ ≤ θ = 1, the last equality is by the definition of η 2 , and the last inequality follows from Claim 4. This completes the proof of Lemma 4.

[G PROOF OF LEMMA 5]
Fix a θ ∈ R md with θ = 1. Recall the definition of x 0 from the paragraph proceeding equation 17. If θ ⊥ ≥ x 0 , then for any t ∈ [0, δ],
F θ (t) − F θ (0) ≥ F θ (0)t ≥ µ θ ⊥ t ≥ µx 0 t = δt ≥ t 2 ,
where the first inequality is from convexity of F θ in Lemma 2, the second inequality is due to Lemma 3, the equality is by the definition of in equation 17, and the last inequality is because t ≤ δ.
On the other hand, if θ ⊥ < x 0 , then for any t ∈ [0, δ], F θ (t) − F θ (0) = F θ (0)t + 1 2
F θ t 2 ≥ 1 2 F θ t 2 ≥ η 1 θ 2 − η 2 θ ⊥ t 2 = η 1 1 − θ ⊥ 2 − η 2 θ ⊥ t 2 < η 1 (1 − x 2 0 ) − η 2 x 0 t 2 = µx 0 δ t 2 = t 2 ,
where the first equality is because F θ is quadratic (c.f. Lemma 2), the first inequality follows from Lemma 3, the second inequality is due to Lemma 4, the third inequality is because θ ⊥ < x 0 , and the last two equalities are due to p(x 0 ) = 0 and the definition of in equation 17, respectively. Combining the above two cases, we obtain Lemma 5.

[]
cos(π/t) − cos(2π/t). It then follows from equation 41, equation 45, and equation 46 that b 0 and b i lie on opposite sides of H, for i = 1, . . . , t − 1.
On the other hand, sinceb i = −ã i , for i = 3, . . . , d, it follows from equation 39 that z Tbi = −z Tãi = −1 < 1. Therefore, for i = 1, . . . , 3,b i and b 0 lie on opposite sides of H. This completes the proof of Property (p7).
Property (p8): We have
where the first equality is due to equation 43 and equation 41, and the second equality follows from the equality in equation 45. Similarly, from equation 42, we have for i = 0, . . . , t − 1
For i, j ∈ {0, . . . , t−1}, letm a i ,b j cos 2π(i−j −1/2)/t − cos(π/t) andm b i ,a j cos 2π(i− j + 1/2)/t − cos(π/t). Then, it follows from equation 48 and rotational symmetry of A and B in the first two coordinates that for i, j ∈ {0, . . . , t − 1},
Note that for any p, q ∈ A B,m p,q is a constant independent of the value of γ. LetM be a 2t × 2t matrix, with entrieŝ
for p, q ∈ A B. Then, all entries ofM are constants independent of γ. Letλ 1 , . . . ,λ 2t be the eigenvalues ofm. It follows thatλ 1 , . . . ,λ 2t are also constants independent of γ.
Consider the matrix M defined in equation 3. It follows from equation 47 and equation 49 that for any p, q ∈ A B,
Consider the order a 0 , . . . , a t−1 , b 0 , . . . , b t−1 on the elements of A B. Then,
where I is the 2t×2t identity matrix and the second equality is from the definition of z 1 in equation 37.
Denote the eigenvalues of M by λ 1 , . . . , λ 2t . Then, from elementary linear algebra,
for i = 1, . . . , 2t. Therefore, there is at most one value of γ for which λ i = 0. Then, in view of equation 23, we have Pr(λ i = 0) = 0, over the random choice of γ. Thus, with probability one, M has no zero eigenvalues and is thereby full-rank. As an immediate consequence, M is full-rank for suitable choice of γ. This establishes Property (p8) and completes the proof of Proposition 1.
Proof of Claim 3. In the (d − 1)-dimensional space, let ω be the unit normal vector of H q . Recall from Property (p6) that s 1 (q), . . . , s d−1 (q) are located on H q . Let,
Without loss of generality suppose that q ∈ A. Letω be the lifting of ω from the (d − 1)-dimensional space to the d-dimensional space by appending ω by a new coordinate with zero coefficient, i.e.,ω is a d-dimensional vector withω
Then, we have z = 1 and z T u
where the second equality is because s i (q) ∈ B B for q ∈ A. It follows from equation 91 and the definition of w q in equation 7 and equation 8 that w q = z. Therefore,
where the second equality is from the definition of z in equation 90, and the last equality is due to equation 88 and equation 89. This completes the proof of Claim 3.
We now proceed to the proof of Lemma 4. Fix an arbitrary θ ∈ R d with θ = 1. Without loss of generality 4 assume that F (•, •) is differentiable at w + δθ/2, v . For r = 1, . . . , m, let J r be a diagonal matrix whose (i, i) entry, for i = 1, . . . , n, equals 1 (w r + θ r t)
where the equality is due to equation 55. Recall the definition of matrix G in equation 18:
Then, for any t ∈ (0, δ),","[TITLE]
BOUNDS ON OVER-PARAMETERIZATION FOR GUARANTEED EXISTENCE OF DESCENT PATHS IN SHALLOW RELU NETWORKS

[ABSTRACT]
We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let m and d be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size n ≥ m + 2d − 2. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for n ≤ m, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for ""existence of descent paths"" in the loss landscape."
Bounds on Over-Parameterization for Guaranteed Existence of Descent Paths in Shallow ReLU Networks,BkgXHTNtvS.json,"This paper studies the landscape properties of over-parameterized two-layer neural networks, and proposes a network width lower bound for guaranteed existence of descent paths that is tighter than existing results. In particular, the authors prove that if the network width m \leq n - 2d, then there exist training data sets and initial weights such that the square loss on the neural network has no descent path connecting the initial weights and global minima.

Overall, I think this paper is of good quality. The presentation is clear and the logic is easy to follow. I did some high-level check and the theoretical analysis seems reasonable. However, I have the following questions:

1. As is stated in Corollary 1 and discussed below Theorem 1, the training sample sets that lead to suboptimal capped minima are not of measure-zero. However, it seems that no rigorous proof is provided for this result. Perhaps I have missed something, but is Corollary 1 straight forward given Theorem 1 and Lemma 1? How large is the probability? It would be better if the authors could provide a more rigorous proof.

2. A recent line of work has shown convergence of gradient-based algorithms for over-parameterized neural networks. It would be interesting if the authors could provide more comparison between this paper and the results of these works:

- Simon S. Du, Jason D. Lee, Haochuan Li, Liwei Wang, Xiyu Zhai, Gradient Descent Finds Global Minima of Deep Neural Networks
- Zeyuan Allen-Zhu, Yuanzhi Li, Zhao Song, A Convergence Theory for Deep Learning via Over-Parameterization
- Difan Zou, Yuan Cao, Dongruo Zhou, Quanquan Gu, Stochastic Gradient Descent Optimizes Over-parameterized Deep ReLU Networks
","[TITLE]
BOUNDS ON OVER-PARAMETERIZATION FOR GUARANTEED EXISTENCE OF DESCENT PATHS IN SHALLOW RELU NETWORKS

[ABSTRACT]
We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let m and d be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size n ≥ m + 2d − 2. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for n ≤ m, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for ""existence of descent paths"" in the loss landscape.

[CAPTIONS]
Table 1: (B) For any b ∈ B, βb lies on a facet of P A . We denote this facet by S A (b). (p5) (A) For any a ∈ A, S B (a) is a (d − 1)-dimensional simplex. (B) For any b ∈ B, S A (b) is a (d − 1)-dimensional simplex. (p6) (A) For any a ∈ A, there exist scalars α 1 , . . . , α d ∈ (0, 1) such that a = d i=1 α i s i (a), where s 1 (a), . . . , s d (a) are the vertices of simplex S B (a). (B) For any b ∈ B, there exist scalars α 1 , . . . , α d ∈ (0, 1) such that b = d i=1 α i s i (b), where s 1 (b), . . . , s d (b) are the vertices of simplex S A (b). (p7) (A) For any pair a and a of distinct points in A, we have S B (a) = S B (a ). Moreover, letting H a be the hyperplane that contains S B (a), a and a lie on opposite sides of H a , for all a ∈ A Ã with a = a. (B) For any pair b and b of distinct points in B, we have S A (b) = S A (b ). Moreover, letting H b be the hyperplane that contains S A (b), b and b lie on opposite sides of H b , for all b ∈ B B with b = b.
Table 2: Figure 2 :2Figure 2: A (2, t, 0)-configuration with t = 8. The blue dots show the points in A and red crosses are the points in B.
Table 3: Figure 3 :3Figure 3: Illustration of data points X 1 , . . . , X n for d = 3 and m = 12.
Table 4: consider the facet S B (a) defined in Property (p4), and let s 1 (a), . . . , s d−1 (a) be the vertices of S B (a) (as in Property (
Table 5: )•For each b ∈ B, consider the facet S A (b) and let s 1 (b), . . . , s d−1 (b) be the vertices of S A (b).
Table 6: Figure 4 :4Figure 4: Different types of cupped minima in terms of differentiability, discussed in Appendix A.
Table 7: Figure 5 :5Figure 5: An illustration of the points in equation 24-equation 27 for d = 3 and t = 4. The red crosses indicate the points in A Ã and the blue dots correspond to the points in B B .
Table 8: Properties (p4)-(p7): We only prove Part (B) for each of these properties; as similar proofs work also for Part (A)'s. Moreover, because of the rotational symmetry of A and B in the first two coordinates, it suffices to prove of the Properties (p4)-(p7) only for b 0 .
Table 9: )We proceed to verify equation 65-equation 72. Eq. equation 65 follows from equation 9. Recall the definitions X A − = ξu d and X B − = −ξu d . Then, equation 10 implies equation 69 and equation 71. Since X s1(b) , . . . , X s d−1 (b) define a boundary of the (d − 1)-dimensional convex set Q A , andH passes through X s1(b) , . . . , X s d−1 (b)
Table 10: Forequation 66, it follows from Property (p4) that b and the origin, 0, lie on opposite sides of hyperplane H. Consequently, −X b and u d also lie on opposite sides of hyperplaneH. Therefore, w T b X b and w T b u d have a same sing. It then follows from equation 10 that w T b X b > 0, and equation 66 follows.For equation 72, it follows from Property (p7) that for any b ∈ B B with b = b, X b and X b lie on opposite sides ofH. Eq. equation 66 then implies that w T b X b < 0. This establishes equation 72, and completes the proof of Claim 1.
Table 11: where last equality in due to equation 81. Combing equation 79 and equation 82, we obtain equation 78 for w r = w b . A similar argument implies equation 78 for w r = w a , a ∈ A. This completes the proof of Claim 2. Back to the proof of Lemma 3, for r = 1, . . . , m, let r min i=1,...,d−1 min γ

[INTRODUCTION]
We consider shallow neural networks of the form shown in Fig. 1. The network comprises a hidden layer and an input layer of widths m and d, respectively; and is to be trained over a training set of size n. Our results concern the slightly over-parameterized regime where n ≈ m. We study the existence of poor local minima that have positive curvature in the empirical squared loss landscape.
It is well-known that poor local minima exist in the loss landscape of shallow networks of arbitrary width. In fact, in a shallow network with ReLU activation functions, it is easy to construct training sets whose empirical loss landscape has high plateaus. 1 It is however not fully understood that under what conditions poor local minima may have positive curvature. This paper presents results that improve this understanding.
Non-existence of spurious local minima is closely connected to the so called descent path property: a loss landscape is said to have the descent path property if starting from any initial point there is a path of descent loss to a global minimum. From optimization perspective, the descent path property favors descent optimization algorithms like the pure gradient descent (GD) method. For SGD as well, non-existence of poor local minima is known to be a favorable property for guaranteed convergence (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016). The descent path property is shown to be satisfied in over-parameterized shallow networks with sufficiently large widths (Venturi et al., 2018). The results we present in this work, tighten the existing bounds on the over-parameterization required to guarantee this property.

[BACKGROUND]
Over the past few years, deep neural networks have achieved tremendous performance in various artificial intelligence applications such as computer vision, reinforcement learning, and natural language processing, etc. Despite their remarkable success in practice, theoretical aspects of this success remain a mystery. It has long been an open problem why simple local search algorithms for training deep neural networks, like stochastic gradient descent (SGD), typically converge to local minima with low training error despite the highly non-convex behavior of empirical loss. It has been observed, e.g., in (Choromanska et al., 2015), that these methods may get stuck in poor local minima (i.e., local minima with empirical loss much larger than the global optimum) for small networks, while the problem fades away as the number of parameters grows larger. Such observations are often explained by studying the loss landscape in over-parameterized regime where the number of parameters in the network exceeds the training sample size.
Recently, several attempts have been made to characterize properties of squared loss landscape by conditioning on the layers' dimensions and sample size. Soudry and Hoffer (2017) showed that weights of a neural network can be adjusted such that the empirical loss is zero almost surely if m > 4 n/(2d − 2) ≈ (2n)/d. This result is consistent with experimental observations that neural networks can fit training data if the number of parameters (here approximately 2n) is greater than the sample size. They also proved for normally distributed input that as n goes to infinity, the ratio between the volume of poor flat local minima regions to the volume of flat global minima fades exponentially if d =Ω( √ n) and m =Ω(n/d). Safran and Shamir (2016) showed that if the number of neurons in the hidden layer is Ω(n rank(X) ) (where X is the matrix containing all input), then with high probability, random initialization of weights will put them in a region of parameter space at which the loss surface has a basin-like structure, i.e., every local minimum in that region is global. In another work (Safran and Shamir, 2017), the same authors provide a computer-assisted proof to show that spurious local minima are common in the expected loss landscape of shallow under-parameterized (small-width) networks. Xie et al. (2016) showed that if the input data is drawn uniformly at random from the unit sphere, and if m =Ω(n β ) and d =Ω(n β ) with β ∈ (0, 1) being the decay exponent of the smallest eigenvalue of a kernel matrix, then every critical point is a global minimum.  proved that for any continuous activation function and under the assumption that data samples are distinct, there exist no poor local minima with positive curvature if m ≥ n . In the same spirit, Venturi et al. (2018) showed that for any continuous activation function, there is always a descent path to an optimal solution with zero loss in the empirical loss landscape if m ≥ n.
Several works have proposed similar results in other settings and under different assumptions. Soudry and Carmon (2016) showed that in a network of leaky ReLU activation functions with randomized perturbation of slopes, all differentiable local minima are global minima if m ≥ n/d. Kawaguchi (2016) proved that in shallow networks with linear activation functions, every local minimum is a global minimum and all the saddle points are strict in the sense that they have a direction of strictly negative curvature. Soltanolkotabi et al. (2019) showed that the same result carries over to quadratic activation functions under the assumption that the last layer comprises at east d positive and d negative weights. Du and Lee (2018) established similar results for quadratic activation functions, assuming m ≥ √ 2n. For deep neural networks with linear activation functions, Freeman and Bruna (2016) showed that all local minima are global minima if there is a hidden layer whose number of neurons exceeds the minimum of the widths of input and output layers. For deep neural networks with analytical activation functions, Nguyen and Hein (2017) proved a similar property under the assumptions that the number of neurons in some hidden layer is greater than sample size and the network has a pyramidal structure.
Such studies on the properties of loss landscape do not only provide insights into the complication of training, but are also beneficial for proving performance guarantees for some local search algorithms. For the class of loss functions whose landscape satisfy the properties of: a) all local minima are global, b) all saddle points are strict, it has been shown in several works (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016) that perturbed gradient descent converges to global optima in polynomial time. Another direction of research concerns the convergence of specific optimization algorithm such as Figure 1: Architecture of the shallow network considered in this paper. The network has a single hidden layer of m neurons with ReLU activation functions, and a neuron with linear activation function in its output layer. pure gradient descent and SGD without assuming such properties for the loss landscape Allen-Zhu et al., 2018;.

[OUR CONTRIBUTIONS]
We study the amount of over-parameterization required for guaranteed existence of descent paths to zero loss in the empirical loss landscape. Previous works suggest that zero loss is always possible for m ≥ 2n/d (Soudry and Hoffer, 2017). On the other hand, the best existing bound for guaranteed existence of descent paths to this zero loss requires m ≥ n neurons in the hidden layer (Venturi et al., 2018). Prior to the present work, it was not known whether the ""descent path property"" holds for m < n. Even for m ∈ (2n/d, n), where zero empirical risk is known to be achievable (Soudry and Hoffer, 2017), the existence of descent paths was in question. In this work, we tighten this gap and prove that there are training sets, under which in any network of width m ≤ n − 2d + 2, there exist initial weights that have no descent path to global minima. We do this by showing that the loss landscape, in this regime, admits poor local minima with positive curvature. We also provide evidences and make conjectures that these results carry over to networks of width m = n − 4, which, if true, provides a sharp characterization of the over-parameterization required for guaranteed existence of descent paths. We also wish to point that unlike most previous works, we do not restrict to differentiable local minima; for a simple argument shows that local minima with positive curvature cannot be differentiable if m > n/d (cf. Appendix A).

[OUTLINE]
We continue by discussing details of the system model and introducing our key definitions in Section 2. We then present, in Section 3, the main results of the paper. Proof of the main results are then given in Section 4. We finally discuss implications and possible extensions of our results in Section 5 along with a number of open problems and directions for future research.

[MODEL]
We consider shallow networks of the form shown in Fig. 1. The network takes d-dimensional inputs denoted by X. There is a single hidden layer comprising m neurons with ReLU activation function. For simplicity of our proofs, we only consider even values of m. We denote the input weights of r-th neuron by a d-dimensional vector w r , for r = 1, . . . , m. We then let w ∈ R md be the vector representation of all weights in the first layer.
The output layer has a single neuron, whose activation function is linear with an m-dimensional weight vector denoted by v. The network outputs a scalarŷ(w, v) = m r=1 v r w T r X1(w T r X ≥ 0). We fix a training set (X 1 , y 1 ), . . . , (X n , y n ) of size n, and consider the landscape of empirical squared loss function:
F (w, v) n i=1 ŷ i (w, v) − y i 2 .
(1)

[PROPERTIES OF THE LANDSCAPE]
We first provide a formal definition for the descent path property, which is a necessary condition for guaranteed performance of descent optimization algorithms.
Definition 1 (Descent path property). Consider a continuous function f : R d → R and let f * = inf x∈R d f (x) be its infimum. We say that f has the descent path property if for any x ∈ R d , there exists a continuous curve with γ : [0, 1] → R d such that γ(0) = x, f γ(1) = f * , and f γ(t) is a non-increasing function of t.
The descent path property is a necessary condition for any descent optimization algorithm to provably find a global minimum from all initial conditions. It was shown in Venturi et al. (2018) that the empirical loss landscape of a shallow neural network with ReLU activation and squared loss has the descent path property if the size of training data is no larger than the width of the hidden layer, i.e., n ≤ m. We now characterize a class of local minima of specific form in the following definition.
Definition 2 (Cupped minima). Given a function f : R d → R, we call x ∈ R d a cupped minimum of f if there are , δ > 0 such that for any y in the δ-neighborhood of x, we have f (y) ≥ f (x)+ y−x 2 . By a sub-optimal cupped minimum we mean a cupped minimum that is not a global minimum.
Note that every cupped minimum is a local minimum, but not every local minimum is cupped (e.g., flat local minima are not curved downwards, and hence are not cupped). Also note that a function is not necessarily differentiable at its cupped minima. We study cupped minima of the loss function in equation 1. Note however that for any α > 0, F (αw, v/α) = F (w, v). Therefore, F (•, •) has no cupped minima if both arguments are taken as variables. For that matter, when talking about cupped minima of F , we fix a v and consider F (•, v) as a function of its first argument. Interestingly, existence of cupped minima for F (•, v) leads to violation of descent path property for F (•, •) over both arguments, as shown in the following lemma. The proof is given in Appendix B.
Lemma 1. Consider a shallow network with loss function F in equation 1, and a pair of weights (w, v). Suppose that w is a sub-optimal cupped minimum of F (•, v), and that w r = 0, for r = 1, . . . , m. Then, F (•, •) has no descent path w(t), v(t) , initiated at (w, v), to its global minima.

[MAIN RESULTS]
The following theorem and corollary state the main results of the paper.
Theorem 1. For any d ≥ 4, m ≥ 8 + 4 3/(d − 3) , and n ≥ m + 2d − 2, there exists a training set of size n such that the empirical loss function F of a shallow neural network of width m has the following property. For any m-dimensional vector v, with m/2 number of positive and m/2 number of negative entries, F (•, v) has exponentially many sub-optimal cupped minima.
The proof is constructive and is given in Section 4. In particular, we devise a training sequence (X 1 , y 1 ), . . . , (X n , y n ) such that for weights (w, v) at the cupped minima, we have
v r w r = 1/ √ m, for r = 1, . . . , m. Moreover, X i ≤ 1, |y i | ≤ 2, and |e i | = 1 for i = 1, . . . , n (cf. Remark 2).
According to Theorem 1, there are training sequences tailored to give rise to sub-optimal cupped minima. However, we wish to point that the existence of such cupped minima does not stem from measure-zero incidents like placement of several data points on a low dimensional plane. In fact, in view of Lemma 1, any path that starts from a cupped minimum and end up in a global minimum would have an uphill climb of at least , for some > 0. Since the loss surface is a continuous function of (X i , y i ), a small perturbation of (X i , y i )'s leads continuously to a small change in F . Therefore, for small enough perturbations of (X i , y i ), any path to the set of global minima would still witness a positive uphill-climb. Hence, the descent path property remains out of order, even when the training data is slightly perturbed. Based on the above intuition, we can establish the following corollary 2 , Corollary 1. For any d ≥ 4, m ≥ 8 + 4 3/(d − 3) , and n ≥ m + 2d − 2; and when the inputs X and labels y are randomly drawn from independent normal distributions, there is a non-zero probability that F (•, •) does not have the descent path property. Venturi et al. (2018) that n ≤ m is sufficient for the descent path property to hold. In contrast, Corollary 1 show that if n ≥ m + 2d − 2, then the descent path property is not necessarily in effect. This leave a gap of size 2d − 2 for the edge of over-parameterization required to guarantee the descent path property. We believe that this edge lies sharp at n = m. We conjecture a stronger version of Theorem 1, that cupped minima can emerge for training data sizes as small as m = n − 4. Conjecture 1. Statement of Theorem 1 holds for all d ≥ 4, m ≥ 2d + 4, and n ≥ m + 4.

[IT WAS SHOWN IN]
See Remark 1 for insights into the possibility of this conjecture.

[PROOF OF THE MAIN RESULT]
In this section, we present the proof of Theorem 1 organized in a sequence of four subsections. We first present some preliminaries in Subsection 4.1. In Subsection 4.2, we introduce a geometric structure called ""(d, t, k)-configuration"", based on which we construct, in Subsection 4.3, the training set that gives rise to cupped minima in the loss landscape. Finally, in Subsection 4.4, we prove the existence of cupped mimima in the devised setting. In order to provide intuitions on the loss landscape at cupped minima and motivate our construction of the training set in Subsection 4.3, we make a short note on different types of cupped minima in Appendix A. We also defer the proofs of some lemmas from this section to appendices for improved readability.

[PRELIMINARIES]
Consider weights (w, v) and let (w , v ) be another set of weights such that for r = 1, . . . , m, v r and v r have the same sign and w r = (v r /v r )w r . Then, F (w, v) = F (w , v ). Moreover, it is no difficult to see that w is a cupped minimum of F (•, v) if and only if w is a cupped minimum of F (•, v ). For this reason, it suffices to prove existence of cupped minima for a fixed vector v. Note also that where w r 's are distinct, any permutation of the order of neurons would give rise to a new cupped minimum. Hence, existence of a cupped minimum for F (•, v) implies existence of exponentially many cupped minima for F (•, v).
We denote by e i =ŷ i (w, v) − y i the estimation error for input X i . We let u d = [0, . . . , 0, 1] T be a d-dimensional vector with the last entry equal to one and all other entries equal to zero. For a region P ⊆ R d , we denote its interior and and its convex-hull by int(P) and Conv(P), respectively. Assuming differentiability of F at w, the partial derivatives of the loss function with respect to w r , r = 1, . . . , m, is as follows
∇ wr F (w, v) = v r n i=1 e i X i 1 w T r X i ≥ 0 . (2)

[A GEOMETRIC CONFIGURATION]
We introduce a geometric structure for sets of points in R d . This configuration will be used in Subsection 4.3 to construct a landscape with cupped minima.
Definition 3 ((d, t, k)-Configuration). Given integers d, t, k ≥ 0 and disjoint sets A,Ã, B, andB of points in R d , we say that A,Ã, B,B forms a (d, t, k)-configuration if the following properties are satisfied:
(p1) Each of A and B consists of t points, and each ofÃ andB consists of k points.
(p2) (A) The convex hull of A Ã forms a polytope P A that has exactly t + k vertices. Equivalently, no point in A Ã is a convex combination of other points in A Ã . (B) Similarly, the convex hull of B B forms a polytope P B that has exactly t + k vertices.
(p3) We have 0 ∈ int(P A ) and 0 ∈ int(P B ).
(p4) There exists a constant β ∈ (0, 1) such that (A) For any a ∈ A, βa lies on a facet of P B . We denote this facet by S B (a). (p8) Consider a 2t × 2t matrix M whose rows and columns are associated to points p ∈ A B and q ∈ A B, and whose entries are as follows
M pq = d p, H q , if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A, 0, otherwise,(3)
where H q is the hyperplane define in Property (p7), and d(•, •) is the euclidean distance.
The property requires M to be full-rank.
Among the above properties, the most difficult of all is Property (p4), and the requirement that it involves the same β for all points in A B. In fact, elimination of Property (p4) gives rise to trivial configurations. 3 In the two dimensional space, for any t ≥ 4 there exists a (2, t, 0)-configuration of the form illustrated in Fig. 2. In the following proposition, we generalize this observation to higher dimensions. Proposition 1. For any d ≥ 2 and t ≥ 4, there exists a (d, t, d − 2)-configuration.
The proof is constructive, and is given in Appendix C. We conjecture that there also exist (d, t, 0)configurations.
Conjecture 2. For any d ≥ 2 and t ≥ 2d, there exists a (d, t, 0)-configuration. Remark 1. Using a configuration given by Conjecture 2 instead of the configuration from Proposition 1 in the construction and the proof that follow, we obtain a proof for Conjecture 1. In this view, establishing Conjecture 2 would also resolve Conjecture 1.

[CONSTRUCTING A LANDSCAPE WITH CUPPED MINIMA]
Here we present a set of training data (X 1 , y 1 ), . . . , (X n , y n ) and a set of wights (w 1 , v 1 ), . . . , (w m , v m ) such that the empirical loss surface corresponding to (X 1 , y 1 ), . . . , (X n , y n ) has a sub-optimal cupped minimum at (w 1 , v 1 ), . . . , (w m , v m ). Without loss of generality, we assume n = m + 2d − 2. Extension to larger values of n is straightforward via replication.
Let A,Ã, B,B be a (d − 1, m/2, d − 3)-configuration in the (d − 1)
-dimensional space, as in Proposition 1. In view of Property (p3), let 0 > 0 be such that P A and P B contain the 0neighborhood of 0. Let
ξ 1 0 a∈A Ã a + b∈B B b + n + 1 β . (4
)
We proceed by introducing the data points X 1 , . . . , X n . An illustration of these data points in the three dimensional space is shown in Fig. 3.  Data points X: We consider a total number of n = m + 2d − 2 data points as follows.
• For each a ∈ A Ã , we consider a new data point X a as follows.
Let [z 1 , . . . , z d−1 ] ∈ R d−1
be the representation of a in the Cartesian coordinates. We let
X a = z 1 , . . . , z d−1 , 1 . • For each b ∈ B B , we consider a new data point X b as follows. Let [z 1 , . . . , z d−1 ] ∈ R d−1 be the representation of b in the Cartesian coordinates. We let X b = − z 1 , . . . , z d−1 , 1 .
• We consider two extra points X A + , X A − , X B + , and X B − as follows. We let
X A − ξu d , X B −
−ξu d , and
X A + X A − − a∈A Ã X a + 1/β − 1 u d (5) X B + X B − + b∈B B X b + 1/β − 1 u d (6
)
where u d = [0, . . . , 0, 1] T , and ξ and β are defined in equation 4 and Property (p4), respectively.
Weights at cupped minimum: We associate each of m neurons to a point p in A B, in a one-one manner; and denote the vector of input weights and the output weight of that neuron by w p and v p , respectively. These weights are chosen as follows:  p6)). We let w a be the unique vector such that w a = 1 and We let w b be the unique vector such that w b = 1 and
• For each a ∈ A, we let v a = −1/ √ m. • For each b ∈ B, we let v b = 1/ √ m. • For each a ∈ A,
w T a X si(a) = 0, i = 1, . . . , d − 1,(7)
w T a u d < 0. (8
w T b X si(b) = 0, i = 1, . . . , d − 1,(9)
w T b u d > 0. (10
)
Labels y: Having determined the data points X and the weights (w, v), the outputŷ(w, v) of the network is determined for all input X. In the following, we choose the true labels y to obtain a desired error e =ŷ − y for each input data. In particular:
• For each a ∈ A Ã , we associate to X a a label y a so that e a ŷ a (w, v) − y a = 1.
• For each b ∈ B B , we associate to X b a label y b so that e b ŷ b (w, v) − y b = −1.
• We choose the labels associated to X A + , X A − , X B + , and X B − such that e A + = e B + = 1 and e A − = e B − = −1.
This completes the description of the training set. As shown in in Soudry and Carmon (2016), there exist weights that achieve zero loss if m > 4 n/(2d − 2) . In our case, n = m + 2d − 2, and its easy to check that m > 4 (m + 2d − 2)/(2d − 2) for all d ≥ 4 and m ≥ 8 + 4 3/(d − 3) . It follows that in our setting the global optimum has zero loss, showing that the above (w, v) is sub-optimal. Remark 2. In the above construction, the norms of inputs vectors may be very large. If we scale down the inputs such that X i ≤ 1/ √ m for i = 1, . . . , n, and modify the corresponding labels y i such that e i remains unchanged, then a same set of weights will still be a cupped minimum for the landscape defined in terms of new (X i , y i )'s. Moreover, for this setting, it is easy to check that w r = 1, e i = 1, and |y i | ≤ 2.

[PROVING THE CUPPED]
MINIMA PROPERTY Let δ min |w T r X i | X i w T r X i = 0, r = 1, . . . , m, i = 1, . . . , n .(11)
Then, δ > 0. For any θ ∈ R md with θ = 1 and for any t ∈ [0, δ], let
F θ (t) = F (w + θt, v).(12)
We show that there is an > 0 such that for any unit-norm θ and any t ∈ [0, δ],
F θ (t) ≥ F θ (0) + t 2 . (13
)
Lemma 2. For any θ ∈ R md with θ = 1, F θ (•) is a quadratic and convex function over
[0, δ].
The proof is give in Appendix D and relies on the fact that neuron activations do not alter at w + θt for t ∈ [0, δ]. Consider now the following m-dimensional subspace of R md
H w         α 1 w 1 . . . α m w m    α 1 , . . . , α m ∈ R      . (14
)
For any θ ∈ R md , let θ and θ ⊥ be the orthogonal projections of θ on H w and H ⊥ w , respectively. Then, θ = θ + θ ⊥ . In order to establish equation 13, we need lower bounds on F θ (0) and F θ , which we derive in the next two lemmas. Lemma 3. There exists µ > 0 such that for any θ ∈ R md with θ = 1, we have
dF θ (t) d + t t=0 ≥ µ θ ⊥ . (15
)
The proof is given in Appendix E, and relies in a subtle way on the choice of data points in subsection 4.3. We now bound the curvature of F θ . Lemma 4. There exist constants η 1 , η 2 > 0 such that for any θ ∈ R md with θ = 1, and for any t ∈ (0, δ),
d 2 F θ (t) dt 2 ≥ max 2η 1 θ 2 − 2η 2 θ ⊥ , 0 . (16
)
The proof is given in Appendix F, and relies on Property (p8) of the underlying configuration.
Consider now the second order polynomial p(x) = η 1 δ 1 − x 2 − η 2 δx − µx, where µ, η 1 , and η 2 are the constants in Lemmas 3 and 4. Since p(0) > 0 and p(1) < 0, the polynomial p has exactly one root in the interval (0, 1), which we denote by x 0 . Let
µx 0 /δ. (17
)
Lemma 5. For any θ ∈ R md with θ = 1, and any t ∈ [0, δ], we have
F θ (t) ≥ F θ (0) + t 2 .
This lemma is a simple consequence of Lemmas 3 and 4, and its proof is given in Appendix G. It follows from Lemma 5 that for any w in the δ-neighborhood of w, we have F (w , v) ≥ F (w, v) + w − w 2 . This shows that w is a cupped minimum for F (•, v), and completes the proof of Theorem 1.

[DISCUSSION]
The guaranteed existence of descent paths in shallow networks of ReLU neurons was previously established (Venturi et al., 2018), given an over-parameterization of m ≥ n (where m and n are the number of neurons and the size of training data, respectively). This left an uncertainty gap of 2n/d < m < n, where zero empirical risk is known to be achievable (for m ≥ 2n/d) (Soudry and Hoffer, 2017), but the existence of descent paths was in question. In this work, we have tightened this uncertainty gap to n − 2d + 2 < m < n, by proving that for any m ≤ n − 2d + 2, there are input data and initial weights for which a descent path does not exist. This conclusion we reach by establishing the existence of cupped minima for m ≤ n − 2d + 2, and for the right choice of input data.
Compared to similar existing results for other activation functions, our results suggest that the edge m ≈ n of over-parameterization required for elimination of sub-optimal cupped minima in ReLU networks is much higher than that of networks with quadratic activation functions, m ≈ √ 2n (Du and Lee, 2018), and linear activation functions, m ≈ n/d (Kawaguchi, 2016), and is almost as high as general continuous activation functions of any form, m ≤ n (Venturi et al., 2018).
Non-existence of spurious local minima and the decent path property favor the convergence of decent optimization methods like GD. However, for different variants of noisy GD, like SGD and Langevin dynamics, it is quite common for the empirical loss to fluctuate during the training. Nevertheless, for theoretical analysis purposes it usually helps to take the noise away, for example by tending the step size to zero. The resulting GD, which always follows a descent path, is usually easier to analyze and can also help to study the SGD dynamics. On the other hand, from a practical view, convergence of SGD in local-min-free landscapes is well-studied.
Aside from addressing Conjecture 2, there remain several open problems, which we review next. As an important direction for future research, it would be interesting if one could obtain bounds on the probability of existence of cupped minima over random data sets, underneath the edge of over-parameterization. In particular, we showed in Corollary 1 that this probability is non-zero, however we gave no clue on either the size or scaling of this probability. Another class of problems concerns basins of local minima, and how they affect dynamics of first order optimization algorithms. As a step toward this goal, one might characterize the true over-parameterization regime in which the basins of poor local minima have considerable volume. Among other directions are extensions of our results to deep ReLU networks, shallow non-ReLU networks, and shallow ReLU networks under loss functions more general than the squared loss.

[APPENDICES A DIFFERENT TYPES OF CUPPED MINIMA IN TERMS OF DIFFERENTIABILITY]
Here we discuss different types of cupped minima and provide elementary intuitions on the loss landscape at cupped minima.
We first characterize curvature of the loss function at differentiable points. For r = 1, . . . , m, let J r be an n × n diagonal matrix whose (i, i) entry equals 1 w T r X i ≥ 0 . Let G be an n × md matrix of the form:
G = v 1 J 1 X T • • • v m J m X T , (18
)
where X is a d × n matrix that has X i in its i-th column. If F is differentiable at (w, v), its gradient is ∇ w F (w, v) = G T [e 1 , . . . , e m ] T , where e i =ŷ i − y i is the output error for input X i . Moreover, if F is differentiable at (w, v), its Hessian with respect to w equals
∇ 2 w F (w, v) = G T G. (19
)
We classify cupped minima into three categories in terms of differentiability. Specifically, for a cupped minimum w of F (•, v), we consider three cases:
Type 1) F is differentiable at w.
Type 2) F (w + θt, v) as a function of t is non-differentiable at t = 0, for all θ ∈ R md .
Type 3) There are θ 1 , θ 2 ∈ R md such that F (w+θ 1 t, v) is differentiable at t = 0, while F (w+θ 2 t, v) is non-differentiable at t = 0.
Fig. 4 illustrates examples of loss surface at the above three types of cupped minima. We now argue that the first two types are not possible in the loss landscape of shallow networks.
If F (•, v) is differentiable at w, its Hessian given in equation 19 equals G T G. Since G is an n × md matrix, assuming md > n, G T G would have zero eigenvalues. Therefore, w cannot be a cupped minimum of F (•, v). It follows that there exists no differentiable cupped minimum (nor saddle point) if md > n.
For non-differentiable points, note that F (w + wt, v) as a function of t is a differentiable quadratic function. This is because the output scales proportionally with w. Therefore, cupped minima of the second type are not possible, as well. In the same spirit, it can be shown that F (w + θt, v) is differentiable as a function of t, if θ ∈ R md belongs to the m dimensional subspace H w defined in equation 14.
For the above reasons, all cupped minima, if any, are of the third type. Therefore in the construction of Subsection 4.3, we introduce a training set and a pair of weights (w, v) such that w is a cupped minimum of F (•, v), and F (w + θt, v) is differentiable in t only for θ ∈ H w .

[B PROOF OF LEMMA 1]
If v has a zero entry, v r = 0 for some r, then F (•, v) is a constant function with respect to w r , and thereby has no cupped minima. Therefore, we assume that v has no zero entries. Let
t 0 = inf t | ∃r, v r (t) = 0 .
For t > 0, we definew
(t) =    v 1 (t)/v 1 × w 1 (t) . . . v m (t)/v m × w m (t)    . (20
)
Let t 1 = inf t |w(t) = w . We show that t 1 < t 0 . If t 0 < ∞, then continuity of v(t) implies that v r (t 0 ) = 0, for some r ≤ m. Therefore,w r (t 0 ) = 0 = w r . It then follows from the continuity ofw r (•) that there is an > 0 such thatw r (t 0 − ) = w r . Consequently, t 1 < t 0 .
The inequality t 1 < t 0 implies that there is an > 0 such thatw r (t 1 + ) = w, and for any t ∈ [0, t 1 + ], we have sgn v r (t) = sgn(v r ), r = 1, . . . , m. Therefore, for any t ∈ [0, w, v). This shows that w(t), v(t) cannot be a descent path for F (•, •), and completes the proof of Lemma 1.
t 1 + ], we have F w(t), v = F w(t), v(t) . Since w is a cupped minimum of F (•, v), there is an s ∈ [t 1 , t 1 + ] such that F w(t), v > F (

[C PROOF OF PROPOSITION 1]
For i = 3, . . . , d, let
c i 1 i − 1 + 2 cos(π/t) . (21
)
Fix a constantc
d i=3 1 + c i = d i=3 i + 2 cos(π/t) i − 1 + 2 cos(π/t) = d + 2 cos(π/t) 2 + 2 cos(π/t) . (22
)
Let γ be a uniform random variable
γ ∼ unif cos(π/t) − cos(2π/t) 4c , cos(π/t) − cos(2π/t) 2c . (23
)
We take a sample from the above distribution and fix a γ for the rest of the proof.
We now introduce the points in the configuration. For i = 0, . . . , t − 1, we consider points a i ∈ A and b i ∈ B as follows
a i = cos 2π(i − 1/2) t , sin 2π(i − 1/2) t , γc 3 , γc 4 , . . . , γ d c d ,(24)
b i = cos 2πi t , sin 2πi t , −γc 3 , −γc 4 , . . . , −γ d c d . (25
)
For i = 3, . . . , d, we consider pointsã i ∈Ã andb i ∈B as follows
a i = 0, . . . , 0 i−1 , −1, c i+1 , . . . , c d , (26
) b i = 0, . . . , 0 i−1 , 1, −c i+1 , . . . , −c d . (27
)
Fig. 5 shows an illustration of these points for d = 3.
We proceed by verifying Properties (p1)-(p8).
Property (p1): Property (p1) is straightforward from the above construction.
Property (p2): Considering only the first two coordinates, it is easy to see that no point of A is a convex combination of other points in A Ã . Also note that for i = 3, . . . , d,ã i is the only point Property (p3): Let
α i = 1 γt , i = 1, . . . , t,(28)
α i = (c 3 + 1) • • • (c i−1 + 1)c i , i = 3, . . . , d.(29)
A simple induction on j shows that
j−1 i=3α i =α j c j − 1. (30
)
for j = 4, . . . , d. We show that
1 t−1 i=0 α i + d i=3α i t−1 i=0 α i a i + d i=3α iã i = 0. (31
)
For the first two coordinates, equation 31 is easy. Let a i j andã i j be the j-th entries of a i andã i , respectively. For the j-th coordinate, j = 3, . . . , d, we then have
t−1 i=0 α i a i j + d i=3α iã i j = t × 1 tγ × γc j + c j j−1 i=3α i −α j = c j + c j α j c j − 1 −α j = 0,
where the second equality is due to equation 30. This establishes equation 31. It then follows from equation 31 that 0 is a convex combination of points in A Ã . Consequently, 0 ∈ int(P A ). A similar argument shows that 0 ∈ int(P B ). Properties (p4) and (p5): For the j-th coordinate, j = 3, . . . , d, we have
a 0 j + a 1 j + γ d i=3ã i j = γc j + γc j + γ j−1 i=3 c j − γ = γ (j − 1)c j − 1 = γ (j − 1) 1 j − 1 + 2 cos(π/t) − 1 = −2γ cos(π/t) j − 1 + 2 cos(π/t) = − 2 cos(π/t) γc j = 2 cos(π/t) b 0 j ,(32)
where b 0 j is the j-th entry of b 0 defined in equation 25. Similarly, for the first two coordinates, we have:
a 0 1 + a 1 1 + γ d i=3ã i 1 = cos(π/t) + cos(π/t) = 2 cos(π/t) b 0 1 , a 0 2 + a 1 2 + γ d i=3ã i 2 = sin(π/t) − sin(π/t) = 0 = 2 cos(π/t) b 0 2 .(33)
It then follows from equation 32 and equation 33 that
b 0 = 1 2 cos(π/t) a 0 + a 1 + γ d i=3ã i .(34)
Let
β 2 cos(π/t) (d − 2)γ + 2 . (35
)
Then, from equation 34,
βb 0 = 1 (d − 2)γ + 2 a 0 + a 1 + γ d i=3ã i .(36)
Therefore, βb 0 is a convex combination of a 0 , a 1 ,ã 3 , . . . ,ã d , and therefore lies on the simplex S that has a 0 , a 1 ,ã 3 , . . . ,ã d as its vertices. Next, we show that S is a facet of P A .
Consider a vector z ∈ R d with entries
z 1 = (c − 1)γ + 1 cos(π/t) , z 2 = 0, z i = − d j=i+1 (c j + 1), i = 3, . . . , d − 1, z d = −1.(37)
Then, a simple backward induction, with base case i = d, shows that for i = 3, . . . , d, we have
d j=i+1 c j z j = z i + 1. In the same vein, d j=3 c j z j = −c + 1.(38)
It follows that for i = 3, . . . , d,
z Tãi = d j=i+1 z j c j − z i = 1.(39)
Moreover, for i = 0, 1,
z T a i = d j=3 z j γc j + z 1 cos(π/t) = γ(−c + 1) + z 1 cos(π/t) = −γ(c − 1) + (c − 1)γ + 1 cos(π/t) cos(π/t) = 1,(40)
where the second equality is due to equation 38. Let H be the hyperplane that passes through a 0 , a 1 ,ã 3 , . . . ,ã d . It follows from equation 39 and equation 40 that for any p ∈ a 0 , a 1 ,ã 3 , . . . ,ã d , we have z T p = 1. Therefore, z is orthogonal to H. Consequently,
H = x ∈ R d | z T x = 1 .(41)
For i = 2, . . . , t − 1, we have
z T a i = z T a i − a 0 + z T a 0 = z T a i − a 0 + 1 = z 1 cos(2π(i − 1/2)/t) − cos(π/t) + 1 < 1,(42)
where the second equality is due to equation 40, and the inequality is because z 1 > 0 and cos(2π(i − 1/2)/t) − cos(π/t) < 0. It follows from equation 41 and equation 42 that all points of A\{a 0 , a 1 } lie on one side of H, while all points ofÃ {a 0 , a 1 } lie on H. Then, H is a tangent hyperplane to P A . Thus, the simplex S is a facet of P A . This completes the proofs of Properties (p4) and (p5). Moreover, from the definition H b in Property (p7), we have
H b 0 = H.(43)
Property (p6): Since t ≥ 4, we have 2 cos(π/2) > 1. Moreover, recall from equation 23 that γ < 1. Property (p6) then follows from equation 34 and the fact that S is a facet of P A .
Property (p7): For i = 0, . . . , t − 1,
z T b i = z T (b i − a 0 ) + z T a 0 = z 1 cos(2πi/t) − cos(π/t) − 2 d j=3 z j γc j + 1 = z 1 cos(2πi/t) − cos(π/t) + 2γ(c − 1) + 1.(44)
where the second equality is due to equation 40 and definitions of a 0 and b i , and the third equality is from equation 38. It follows that
z T b 0 = z 1 1 − cos(π/t) + 2γ(c − 1) + 1 > 1,(45)
where the inequality is because the first two terms on the left hand side of the inequality are positive.
In the same vein, for i = 1, . . . , t − 1
z T b i = z 1 cos(2πi/t) − cos(π/t) + 2γ(c − 1) + 1 < z 1 cos(2π/t) − cos(π/t) + 2γc + 1 < cos(2π/t) − cos(π/t) + 2γc + 1 ≤ cos(2π/t) − cos(π/t) + cos(π/t) − cos(2π/t) + 1 = 1,(46)
where the second inequality follows from the definition of z 1 in equation 37 and the fact that z 1 > 1, and the third inequality is from the definition of γ in equation 23 and the fact that 2γc ≤ D PROOF OF LEMMA 2
Consider a block representation of θ as follows
θ =    θ 1 . . . θ m    ,(54)
where each θ r is a d-dimensional vector.
It follows from the definition of δ that for any t ∈ (0, δ), if w T r X i > 0, then (w r + θ r t) T X i > 0; and if w T r X i < 0, then (w r + θ r t) T X i < 0. Therefore, for r = 1, . . . , m and i = 1, . . . , n, and for any t ∈ [0, δ) ,
1 (w r + θ r t) T X i ≥ 0 = 1 w T r X i > 0 + 1 w T r X i = 0, θ T r X i ≥ 0(55)
Consequently, for any t ∈ [0, δ] and i = 1, . . . , n, we havê
y i w + θt, v = m r=1 v r w T r X i + tθ T r X i 1 (w r + θ r t) T x i ≥ 0 = m r=1 v r w T r X i + tθ T r X i 1 w T r X i > 0 + 1 w T r X i = 0, θ T r X i ≥ 0 .(56)
It follows thatŷ i w + θt, v is a linear function of t over the interval t ∈ [0, δ]. Therefore, F θ (t) = n i=1 ŷ i (w + θt, v) − y i 2 is a quadratic and convex function of t, for t ∈ [0, δ].

[E PROOF OF LEMMA 3]
We first characterize active neurons for different inputs. For two subsets S 1 , S 2 ⊂ R d we let S 1 \S 2 = S 1 S c 2 . Recall the definition of s i (a) and s i (b) from Property (p6). Claim 1. For any a ∈ A, we have
w T a X si(a) = 0, i = 1, . . . , d − 1,(57)
w T a X a > 0,(58)
w T a X b > 0, b ∈ B B \ s 1 (a), . . . , s d−1 (a) ,(59)
w T a X B + > 0,(60)
w T a X B − > 0,(61)
w T a X A + < 0,(62)
w T a X A − < 0,(63)
w T a X a < 0, a ∈ A Ã \ {a}.(64)
Similarly, for any b ∈ B, we have
w T b X si(b) = 0, i = 1, . . . , d − 1,(65)
w T b X b > 0,(66)
w T b X a > 0, a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) ,(67)
w T b X A + > 0,(68)
w T b X A − > 0,(69)
w T b X B + < 0,(70)
w T b X B − < 0,(71)
w T b X b < 0, b ∈ B B \{b}. (72
)
Proof of Claim 1. Fix a b ∈ B. We begin by introducing some notations. Let H be the (d − 2)dimensional hyperplane in the (d − 1)-dimensional space that passes through s 1 (b), . . . , s d−1 (b). In the same spirit, letH be the (d − 1)-dimensional subspace in the d-dimensional space that passes through X s1(b) , . . . , X s d−1 (b) , equivalentlyH is the subspace orthogonal to w b . We denote the convex hull of X s1(b) , . . . , X s d−1 (b) by C B Conv X s1(b) , . . . , X s d−1 (b) . Similarly, we let
Q A conv X a | a ∈ A Ã and Q B Conv X b | b ∈ B B .
Before presenting the proofs of properties equation 57-equation 72, we review make some easy observations. Recall the definition of 0 from the paragraph proceeding equation 4. Let B d−1 0 be the intersection of the 0 -ball centered at 0 with the orthogonal space of u d . Then, from the definition of , we have
B d−1 0 + u d ⊂ Q A , B d−1 0 − u d ⊂ Q B . (73
)
For x ∈ R d , let π(x) be the projection of x on the span of first d − 1 coordinates, i.e., the orthogonal space of u d . Then,
π ( X A + ) ξ + 1/β − n − 1 = a∈A Ã (X a − 1) ξ + 1/β − n − 1 = a∈A Ã a ξ + 1/β − n − 1 < a∈A Ã a ξ − n < a∈A Ã a a∈A Ã a / 0 = 0 ,(74)
where the first equality is from the definition of X A + and the second inequality follows from the definition of ξ in equation 4. Therefore, π(X
A + )/ ξ + 1/β − n − 1 ∈ B d−1 0 . Consequently, 1 ξ + 1/β − n − 1 X A + = π(X A + ) ξ + 1/β − n − 1 + u T d X A + u d ξ + 1/β − n − 1 = π(X A + ) ξ + 1/β − n − 1 + ξ + 1/β − n − 1 u d ξ + 1/β − n − 1 = π(X A + ) ξ + 1/β − n − 1 + u d ∈ B d−1 0 + u d ∈ Q A ,(75)
where the first equality is orthogonal decomposition of X A + , and last inclusion follows from equation 73. In the same vein, we can show that , then all points in Q A lie on a same side ofH. In other words, either we have w T b x ≥ 0, for all x ∈ Q A ; or we have w T b x ≤ 0, for all x ∈ Q A . In view of Property (p3), u d ∈ int(Q A ). It then follows from equation 10 that for any x ∈ Q A , we have w T b x ≥ 0. Consequently, for any x ∈ Q A \H, we have w T b x > 0. In particular,
− 1 ξ + n + 1 − 1/β X B + ∈ Q A . (76
w T b X a > 0, a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) , w T b X A + > 0, −w T b X B + > 0,
where the first inequality is because Property (p5) implies that X a ∈H for a ∈ A Ã \ s 1 (b), . . . , s d−1 (b) , and the last two inequalities are due to equation 75 and equation 76, respectively. This establishes equation 67, equation 68, and equation 70. In light of Claim 1, it is easy to see for r = 1, . . . , m and i = 1, . . . , n that if w T r X i = 0, then
v r e i = 1 √ m .(77)
In our next claim, we examine a linear combination of data points for which a particular neuron is active.
Claim 2. For r = 1, . . . , m, there exist constants γ r 1 , . . . , γ r m such that
n i=1 γ r i e i X i 1 w T r X i = 0 + n i=1 e i X i 1 w T r X i > 0 = 0. (78
)
Proof of Claim 2. Fix a b ∈ B. We prove the claim for the neuron associated to b. It follows from Claim 1 that
n i=1 e i X i 1 w T b X i > 0 = e b X b + a∈(A Ã )\ s1(b),...,s d−1 (b) e a X a + e A + X A + + e A − X A − = −X b + a∈(A Ã )\ s1(b),...,s d−1 (b) X a + X A + − X A − = −X b + a∈(A Ã )\ s1(b),...,s d−1 (b) X a −   a∈A Ã X a − (1/β − 1) u d   = −X b − d−1 i=1 X si(b) + (1/β − 1) u d ,(79)
where the second equality is due to the definitions of e a , e b , e A + , and e A − , and the third equality is from the definitions of X A + in equation 5.
On the other hand, it follows from Property (p6) that there exist scalars α 1 , . . . , α 
d ∈ (0, 1) such that b = d−1 i=1 α i s i (b). Therefore, from the definition of X b , − X b + u d = d−1 i=1 α i X si(b) − u d (80) Moreover, Property (p4) implies that d−1 i=1 α i = 1/β. Then, from equation 80, −X b = d−1 i=1 α i X si(b) − d−1 i=1 α i − 1 u d = d−1 i=1 α i X si(b) − (1/β − 1) u d . (81
) For i = 1, . . . , d − 1, let γ i = 1 − α i . Then, Claim 1 implies that n i=1 γ i e i X i 1 w T b X i = 0 = d−1 i=1 γ i e i X si(b) = d−1 i=1 (1 − α i )X si(b) = d−1 i=1 X si(b) − d−1 i=1 α i X si(b) = d−1 i=1 X si(b) + X b − 1 β − 1 u d ,(82)
r i , (1 − γ r i ) ,(83)
for the constant γ r i defined in Claim 2. It follows that r > 0, for r = 1, . . . , m. For any r ≤ m, X s1(r) , . . . , X s d−1 (r) are linearly independent and, by definition, are all orthogonal to w r . Therefore, there exists a constant r > 0 such that for any
ζ r ∈ R d with ζ r = 1, we have max i=1,...,d−1 |ζ T r X si(r) | ≥ r ζ ⊥ r , where ζ ⊥ r is the projection of ζ r on the null-space of w r . Consequently, for any ζ r ∈ R d with ζ r = 1, max i=1,...,n |ζ T r X i | × 1 w T r X i = 0 ≥ r ζ ⊥ r .
In particular, considering the block-vector representation of θ in equation 54, we obtain for r = 1, . . . , m, max i=1,...,n
|θ T r X i | × 1 w T r X i = 0 ≥ max i=1,...,n r θ ⊥ r .(84)
Let µ min r r r / √ m. Then, µ > 0. It then follows from Claim 2 that, for r = 1, . . . , m,
v r n i=1 e i θ T X i 1 w T r X i = 0, θ T r X i ≥ 0 + v r n i=1 e i θ T r X i 1 w T r X i > 0 = v r n i=1 e i θ T r X i 1 w T r X i = 0, θ T r X i ≥ 0 − v r n i=1 γ r i e i θ T r X i 1 w T r X i = 0 = v r n i=1 e i θ T r X i 1(θ T r X i ≥ 0) − γ r i 1 w T r X i = 0 = 1 √ m n i=1 θ T r X i 1(θ T r X i ≥ 0) − γ r i 1 w T r X i = 0 = 1 √ m n i=1 θ T r X i × 1(θ T r X i > 0) − γ r i × 1 w T r X i = 0 ≥ 1 √ m r n i=1 θ T r X i 1 w T r X i = 0 ≥ r √ m max i=1,...,n θ T r X i 1 w T r X i = 0 ≥ r √ m r θ ⊥ r ≥ µ θ ⊥ r ,(85)
where the third equality is due to equation 77, the fourth equality is because θ T r X i and 1(θ T r X i > 0) − γ r i have always the same sign, the first inequality is by definition of r in equation 83, the third inequality follows from equation 84, and the last inequality is from the definition of µ.
On the other hand, equation 2 implies that dF θ (t)
d + t t=0 = lim t↓0 m r=1 v r θ T r n i=1 e i X i 1 (w r + θ r t) T X i ≥ 0 = m r=1 v r n i=1 θ T r e i X i 1 w T r X i = 0, θ T r X i ≥ 0 + 1 w T r X i > 0 = m r=1 v r n i=1 e i θ T X i 1 w T r X i = 0, θ T r X i ≥ 0 + v r n i=1 e i θ T r X i 1 w T r X i > 0 ≥ m r=1 µ θ ⊥ r ≥ µ θ ⊥ (86)
where the second equality is due to equation 55 and the first inequality follows from equation 85. This completes the proof of Lemma 3.

[F PROOF OF LEMMA 4]
We begin by a claim. Given a q ∈ A B, recall the definition of hyperplane H q from Property (p7).
Claim 3. For any pair of points p, q ∈ A B, we have
w T q X p = 1 1 + d(0, H q ) 2 d p, H q . (87
)
where the third equality is due to equation 19. On the other hand, since θ = θ + θ ⊥ , we have
Gθ = Gθ + Gθ ⊥ .(96)
In the following claim, we elaborate on Gθ . Claim 4. There exists a constant η 1 > 0 such that Gθ 2 ≥ 2η 1 θ 2 , for all θ ∈ R d .
Proof of Claim 4. Recall that θ is the projection of θ on subspace H w defined in equation 14. Then, there exist constants α 1 . . . , α m such that
θ =    α 1 w 1 . . . α m w m    . (97
)
Let α be the vector representation of α 1 , . . . , α m . Then,
θ 2 = m i=1 α 2 i w i 2 = m i=1 α 2 i = α 2 . (98
)
Consider the n × m matrixG = v 1 J 1 X T w 1 • • • v m J m X T w m .(99)
Then, from the definition of matrix G in equation 18,
Gθ = v 1 J 1 X T • • • v m J m X T    α 1 w 1 . . . α m w m    = v 1 J 1 X T w 1 • • • v m J m X T w m α =Gα.(100)
Each column ofG corresponds to a neuron, and thereby is associated to a point in A B. In the same vein, every row ofG is associated to an input vector. By removing some rows ofG, we devise a matrixM so that each row ofM is associated to an input X p for p ∈ A B. Therefore,M is an m × m matrix, whose rows and columns are associated to the points in A B. It follows thatM α is a vector obtained by removing some entries from vectorGα. As a result, M α ≤ G α .
In the following, we capitalize on Property (p8) to show thatM is full-rank.
For q ∈ A B, let
γ q v q 1 + d(0, H q )
.
For p, q ∈ A B, the entry in row p and column q ofM equals
M pq =G pq = v q X T p w q 1 w T q X p > 0 + 1 w T q X p = 0, θ T q X p ≥ 0 = v q X T p w q 1 w T q X p > 0 = v q 1 + d(0, H q ) d p, H q 1 w T q X p > 0 = γ q d p, H q 1 w T q X p > 0 . (102
)
where the first equality is from the definition ofM , the second equality follows from the definitions ofG and J q in equation 99 and equation 93, the third equality is because X T p w q 1 w T q X p = 0 = 0, and the fourth equality is due to Claim 3. Then, Claim 1 implies that for any p, q ∈ A B, M pq = γ q d p, H q , if p = q, OR p ∈ A and q ∈ B, OR p ∈ B and q ∈ A, 0, otherwise.
Compared to matrix M defined in equation 3, each column q ofM equals the column q of M multiplied by a non-zero constant γ q . In view of Property (p8), M is full-rank. It follows thatM is full-rank, as well.
Let σ be the smallest singular value ofM . SinceM is full-rank, we have σ > 0. Moreover, M α ≥ σ α .
Then, Gθ 2 = G α 2 ≥ M α 2 ≥ σ 2 α 2 = σ 2 θ 2 , (105) where the equations are due to equation 100, equation 101, equation 104, and equation 98, respectively. Claim 4 then follows for η 1 = σ 2 /2.
Back to the proof of Lemma 4, we denote by σ max the largest singular value of G. Let η 2 σ 2 max . Then,
Gθ 2 = Gθ + Gθ ⊥ 2 = Gθ 2 + Gθ ⊥ 2 + 2 Gθ T Gθ ⊥ ≥ Gθ 2 − 2 Gθ × Gθ ⊥ ≥ Gθ 2 − 2σ max θ × σ max θ ⊥ ≥ Gθ 2 − 2σ 2 max θ ⊥ = Gθ 2 − 2η 2 θ ⊥ ≥ 2η 1 θ 2 − 2η 2 θ ⊥ ,(106)
where the second inequality is from the definition of σ max , the third inequality is because θ ≤ θ = 1, the last equality is by the definition of η 2 , and the last inequality follows from Claim 4. This completes the proof of Lemma 4.

[G PROOF OF LEMMA 5]
Fix a θ ∈ R md with θ = 1. Recall the definition of x 0 from the paragraph proceeding equation 17. If θ ⊥ ≥ x 0 , then for any t ∈ [0, δ],
F θ (t) − F θ (0) ≥ F θ (0)t ≥ µ θ ⊥ t ≥ µx 0 t = δt ≥ t 2 ,
where the first inequality is from convexity of F θ in Lemma 2, the second inequality is due to Lemma 3, the equality is by the definition of in equation 17, and the last inequality is because t ≤ δ.
On the other hand, if θ ⊥ < x 0 , then for any t ∈ [0, δ], F θ (t) − F θ (0) = F θ (0)t + 1 2
F θ t 2 ≥ 1 2 F θ t 2 ≥ η 1 θ 2 − η 2 θ ⊥ t 2 = η 1 1 − θ ⊥ 2 − η 2 θ ⊥ t 2 < η 1 (1 − x 2 0 ) − η 2 x 0 t 2 = µx 0 δ t 2 = t 2 ,
where the first equality is because F θ is quadratic (c.f. Lemma 2), the first inequality follows from Lemma 3, the second inequality is due to Lemma 4, the third inequality is because θ ⊥ < x 0 , and the last two equalities are due to p(x 0 ) = 0 and the definition of in equation 17, respectively. Combining the above two cases, we obtain Lemma 5.

[]
cos(π/t) − cos(2π/t). It then follows from equation 41, equation 45, and equation 46 that b 0 and b i lie on opposite sides of H, for i = 1, . . . , t − 1.
On the other hand, sinceb i = −ã i , for i = 3, . . . , d, it follows from equation 39 that z Tbi = −z Tãi = −1 < 1. Therefore, for i = 1, . . . , 3,b i and b 0 lie on opposite sides of H. This completes the proof of Property (p7).
Property (p8): We have
where the first equality is due to equation 43 and equation 41, and the second equality follows from the equality in equation 45. Similarly, from equation 42, we have for i = 0, . . . , t − 1
For i, j ∈ {0, . . . , t−1}, letm a i ,b j cos 2π(i−j −1/2)/t − cos(π/t) andm b i ,a j cos 2π(i− j + 1/2)/t − cos(π/t). Then, it follows from equation 48 and rotational symmetry of A and B in the first two coordinates that for i, j ∈ {0, . . . , t − 1},
Note that for any p, q ∈ A B,m p,q is a constant independent of the value of γ. LetM be a 2t × 2t matrix, with entrieŝ
for p, q ∈ A B. Then, all entries ofM are constants independent of γ. Letλ 1 , . . . ,λ 2t be the eigenvalues ofm. It follows thatλ 1 , . . . ,λ 2t are also constants independent of γ.
Consider the matrix M defined in equation 3. It follows from equation 47 and equation 49 that for any p, q ∈ A B,
Consider the order a 0 , . . . , a t−1 , b 0 , . . . , b t−1 on the elements of A B. Then,
where I is the 2t×2t identity matrix and the second equality is from the definition of z 1 in equation 37.
Denote the eigenvalues of M by λ 1 , . . . , λ 2t . Then, from elementary linear algebra,
for i = 1, . . . , 2t. Therefore, there is at most one value of γ for which λ i = 0. Then, in view of equation 23, we have Pr(λ i = 0) = 0, over the random choice of γ. Thus, with probability one, M has no zero eigenvalues and is thereby full-rank. As an immediate consequence, M is full-rank for suitable choice of γ. This establishes Property (p8) and completes the proof of Proposition 1.
Proof of Claim 3. In the (d − 1)-dimensional space, let ω be the unit normal vector of H q . Recall from Property (p6) that s 1 (q), . . . , s d−1 (q) are located on H q . Let,
Without loss of generality suppose that q ∈ A. Letω be the lifting of ω from the (d − 1)-dimensional space to the d-dimensional space by appending ω by a new coordinate with zero coefficient, i.e.,ω is a d-dimensional vector withω
Then, we have z = 1 and z T u
where the second equality is because s i (q) ∈ B B for q ∈ A. It follows from equation 91 and the definition of w q in equation 7 and equation 8 that w q = z. Therefore,
where the second equality is from the definition of z in equation 90, and the last equality is due to equation 88 and equation 89. This completes the proof of Claim 3.
We now proceed to the proof of Lemma 4. Fix an arbitrary θ ∈ R d with θ = 1. Without loss of generality 4 assume that F (•, •) is differentiable at w + δθ/2, v . For r = 1, . . . , m, let J r be a diagonal matrix whose (i, i) entry, for i = 1, . . . , n, equals 1 (w r + θ r t)
where the equality is due to equation 55. Recall the definition of matrix G in equation 18:
Then, for any t ∈ (0, δ),","[TITLE]
BOUNDS ON OVER-PARAMETERIZATION FOR GUARANTEED EXISTENCE OF DESCENT PATHS IN SHALLOW RELU NETWORKS

[ABSTRACT]
We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let m and d be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size n ≥ m + 2d − 2. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for n ≤ m, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for ""existence of descent paths"" in the loss landscape."
The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets,BJlNs0VYPB.json,"This paper attempts an in depth study of the lottery ticket hypothesis. The lottery ticket hypothesis holds that sparse sub-networks exist inside dense large models and that the sparse sub-networks achieve at least as good an accuracy as the underlying large model. These sub-networks are discovered by training and iteratively pruning the dense model. This paper investigates the epoch at which pruning should occur as well as the epoch at which weights should be rewound when retraining. Then, the authors conduct experiments with different pruning strategies (one-shot vs. gradual) in an attempt to find such sparse models (or ""winning tickets"") earlier than they otherwise would have been found.

The experiments conducted by the authors seem to be very extensive, and I think the paper contains useful data to have for researchers interested in better understanding the lottery ticket hypothesis. However, my main issue is with both the originality and significance of this work. This paper gives evidence that winning tickets may be found ""early,"" although their notion of early still involves quite a lot of training. 

Although the paper is interested in addressing the structure of the winning tickets, I really didn't find any of the discussion of structure to give much insight into the lottery ticket hypothesis. Most of the section focuses on analyzing weight magnitude, though I was hoping for something more about the actual structure of the sparse subnetwork -- especially given the title of the paper. Figure 3 is notable, showing that different winning tickets (parameterized by different prune and rewind epochs) can have a large Hamming distance between them. This is very interesting, and I wish the authors had more to say. How is this affected by different initializations? Are these solutions connected on a loss landscape? Is there something invariant about the sparse architecture after symmetries are taken into account? It's not clear to me that Hamming distance alone is enough.

In conclusion, the paper presents a set of nice experiments, but doesn't really shed too much additional light on the scientific nature of the lottery ticket hypothesis.","[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Extended lottery ticket experiments using ResNet20 on CIFAR10. From the baseline ResNet20 training, weight-magnitude based pruning is applied at different epoch (columns, labeled as s) to obtain the sparsified structure. Each structure is then rewound to the weights from another epoch (rows, labeled as v), where v=0 indicates rewind to the initial weight. Lottery tickets emerge much earlier before the full training ends, achieving matching accuracy compared to the conventional winning ticket, i.e., (s=200,v=200).
Table 2: Frankleet al. (2019)  suggests that a lottery ticket found after a baseline training can achieve the accuracy of the baseline model if it is initialized with the weight of the baseline model after a few epochs, which is called rewinding. We extend this exploration toward different structures obtained at different epochs of the baseline training (via magnitude-based weight pruning).
Table 3: Fig. 11Fig.1shows the validation accuracy after retraining of a ResNet20 model on CIFAR10 pruned at different epochs of its baseline training. The same learning rate schedule of 0.1 reduced by 10x at epoch 120 and 160 is used for both the baseline training and retraining, and the total number of epochs is 200. The rows (= v) correspond to the different rewinding epoch, whereas the columns (= s) correspond to the different epoch that we apply one-shot pruning (pruning rate= 80%). For example, the validation accuracy along the lottery ticket configurations of (s = 200, v = 0 ∼ 200) resembles the phenomena of ""rewinding"" observed inFrankle et al. (2019).
Table 4: Figure 2 :2Figure 2: (a) Change of the standard deviation of weights (W std ) in each layer during the training of ResNet20 on CIFAR10. In the beginning of the training, the weights of different layers change in different rate, dominated by the gradient terms. When the training evolves, weight magnitude is primarily determined by the interplay between learning rate and weight decay, resulting in parallel movement of W std . (b) Change of per-layer pruning rate over the epochs. Due to regular shift of W std at the later epochs of training, the per-layer pruning rate converges to a saturating point, emerging stable structure for pruning.
Table 5: Figure 4 :4Figure4: The pseudo-pruned-then-retrained (PPR) models for CIFAR10 ResNet20, which exhibit increasing capability of memorizing the complex patterns over the epochs. Validation accuracy for the baseline training and validation accuracy after pruning without retraining are also included for comparison.
Table 6: Figure 5 :5Figure 5: Left: Memorization capacity (i.e., training accuracy) of the models pruned at 3 different epochs. The models pruned at epoch 120 and 200 show almost identical memorization capacity, whereas the model pruned at epoch 20 exhibits lower memorization capacity. Right: Memorization capacity of models pruned at different epochs (training data size = 15000).
Table 7: Figure 6 :6Figure 6: The results of PPR accuracy check (retraining learning rate=0.01) for (a) ResNet56 on CIFAR10 (sampled at every epoch) and (b) ResNet18 on ImageNet (sampled every 5 epochs).
Table 8: GP: gradual pruning, IP: iterative method, EWT: early winning ticket, WT: winning ticket * : pruning rate (PR). * * : number of pruning epochs + regular retraining epochs † : the delta of accuracy is measured against the baseline accuracy. Bold: highlight of comparison
Table 9: Figure 8 :8Figure 8: Revisiting the extended lottery ticket experiments of Fig. 1, but with the group size (gs) of 8. As before, (s, v) stands for (epoch drawing the sparse structure, epoch for rewinding). In contrast to Fig. 1, the winning tickets found when gs=1 (green color in Fig. 1) disappear as the group size becomes 8.
Table 10: Figure 9 :9Figure9: Impact of structured sparsity on the saturation of pruning accuracy. The x-axis corresponds to the epoch when the model is 80% pruned. The dotted lines correspond to the moving average of the accuracy to show trends. The larger the group size, the later the accuracy of the pruned models converges at.
Table 11: Figure 10 :10Figure 10: Mode connectivity: Lottery tickets of ImageNet-ResNet50 and CIFAR10-ResNet20 drawn from different epochs are linearly interpolated, then the accuracy is measured for each interpolation coefficient. It can be observed that only the early winning tickets are connected to the winning tickets (drawn at the end of training, i.e., epoch 90 for ImageNet and epoch 200 for CIFAR10).
Table 12: Memorization capacity (i.e., train accuracy) comparison between 1-shot and 4-shot pruning.
Table 13: One-shot and gradual pruning at different epochs for ResNet20 and ResNet56 on CIFAR10. To clarify randomness effect, the average accuracy (%) over 10 different runs is reported. ±0.07) 91.20 (±0.28) 91.49 (±0.27) 91.37 (±0.24) 91.63 (±0.19) 91.63 (±0.21) ResNet56 92.94 (±0.28) 93.60 (±0.18) 94.07 (±0.16) 94.24 (±0.19) 94.29 (±0.17) 94.34 (±0.14)
Table 14: One-shot and gradual pruning for ResNet18 and ResNet50 on ImageNet.
Table 15: Performance gain by our gradual pruning strategy on CIFAR-10 and ImageNet.
Table 16: Memorization capacity comparison for different group size.

[INTRODUCTION]
Deep Neural Networks (DNNs) achieve superior accuracy in a wide spectrum of applications through the use of very large and deep models (Goodfellow et al. (2016)). These high-capacity but complex models, however, pose a tremendous challenge for their deployment, particularly in resource-constrained edge environments. Over the years, many techniques have been developed to compress the models to a compact counterpart to alleviate the computational costs. Among these techniques, pruning less important parameters to obtain a compact sub-network has emerged to be a popular and efficient approach (Cun et al. (1990); Han et al. (2015)). In search of these sub-networks, new intuitions are also built up for understanding the DNN working mechanism, one example of which is the recently proposed ""lottery ticket hypothesis"" by Frankle & Carbin (2018).
The lottery ticket hypothesis states that, once a sub-network is found to match the accuracy of the original neural network, the sub-network (i.e., lottery ticket) together with its initialized weights can be trained in isolation and still achieve accuracy comparable to the original network within a similar number of iterations. This conjecture intrigues discussions on a series of topics, such as the importance of initialization scheme (Liu et al. (2018), Zhou et al. (2019)), the role of over-parameterization in training (Frankle et al. (2019)) and even the transferability of the ""winning ticket"" (S. Morcos et al. (2019)). However, all these discussions start from the point that the winning ticket has been obtained after painfully long iterative pruning procedures, which often take up to thousands of epochs. When and how a winning ticket can be found in the course of pruning procedures has not been studied; most of the prior works use a traditional way of repeated cycles of pruning and retraining, which makes such study less practical.
In this work, we provide insights to find the winning tickets early. We start with an interesting observation that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule finishes. This leads to our in-depth investigation of the weight magnitude; we discover that a model saturates early but not too early under stochastic gradient descent (SGD) optimization. To understand this characteristic of the winning lottery tickets, we conjecture that pruning a premature model causes the loss of capability in learning complex patterns, leading to accuracy degradation. We confirm this conjecture with the empirical evidence as well as the quantitative analysis based on the memorization capacity. Using this analysis framework, we further provide a reasoning behind the success of the gradual pruning over the one-shot pruning. Based on these insights, we identify the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate obtained within only 22% of the total epochs for iterative pruning. This promising outcome not only sheds light on understanding the optimization behavior of the pruned models but also enables performance gain for fast training of the pruned models.

[RELATED WORK]
The lottery ticket hypothesis was first proposed in Frankle & Carbin (2018) where the presence of a trainable subnetwork that achieves compelling final accuracy with the inherited initial values for the un-pruned connections is demonstrated. This paper also argued that the initial weight of the original network is essential for maintaining good accuracy when the model is sparsified. This claim has been extended to the over-parameterized neural networks on the larger datasets in Frankle et al. (2019) with the notion of ""rewinding""; the authors claim that rewinding of the weights not to the initial values but to the values after a few epochs can stabilize the accuracy of the winning lottery tickets. As a follow-up work, Zhou et al. (2019) studied the critical components of the lottery tickets such as zeros, signs and the super-mask. Also, S. Morcos et al. (2019) investigated the transferability of winning tickets obtained in one dataset to the network of a similar structure for the other datasets. But none of these focused on the structure of the winning lottery tickets; it involves several repetitions of the full training schedule for iterative pruning and retraining, often taking hundreds to thousands of epochs. In this work, we demonstrate that many winning tickets can be found in the early stage of the baseline training schedule, drastically reducing the computational effort to obtain them.
Most work on the lottery ticket hypothesis, including this work, rely on magnitude-based weight pruning for identifying unimportant weights to be pruned (usually via global sorting). Such an intuitive method was first proposed in Han et al. (2015) and became popular. Afterwards, more complex pruning methodologies have been presented to improve pruning performance, such as having different pruning criteria , Wen et al. (2016), ) or different pruning granularity (Mao et al. (2017), Molchanov et al. (2019)). While these attempts offer insights on training pruned models, there is little discussion about the interesting interplay between the pruning criteria and the structure of a model pruned by it. In this work, we reveal that the pruned structure obtained based on the weight magnitude has tangible impact on the final accuracy, and further propose a way to quantitatively distinguish good structures for pruning.
There have been various strategies to apply pruning to the neural networks. Iterative pruning by Han et al. (2015) involves several repetition of pruning (with gradual increase of the pruning rate) and retraining. One can increase the repetition cycles arbitrarily large to achieve good accuracy at high pruning rate; Frankle & Carbin (2018) employed iterative pruning with hundreds to thousands of pruning epochs to match the baseline accuracy for the challenging neural networks. On the other hand, gradual pruning introduced by Zhu & Gupta (2017) determines the pruning rate and frequency via a polynomial equation as a function of the starting and ending epochs as well as the target pruning rate. Although it provides a systematic pruning schedule, there is lack of discussion about when to start the gradual pruning. Lastly, Lee et al. (2018) proposes a method of pruning weights at initialization. This method takes most advantage in performance since a pruned model can be obtained without any expensive retraining procedures. However, its effectiveness has not been demonstrated for the challenging neural networks on large datasets such as ImageNet. In contrast, we propose a mechanism to identify the winning lottery ticket early in the course of baseline training so that we can avoid costly iterative pruning while maintaining the baseline accuracy.

[STRUCTURE OF EARLY WINNING TICKETS]
In this section, we extend the lottery ticket hypothesis by Frankle & Carbin (2018); Frankle et al. (2019) to discuss the early winning tickets. The lottery ticket hypothesis can be summarized as: for a given network of f (x; w 0 m 0 ) with the initial weight w 0 and the mask of all ones m 0 , there exists a winning lottery ticket m f where |m f |/|m 0 | = 1 − p% (p is the pruning rate) and training of f (x; w v m f ) for 0 < v f achieves test accuracy comparable to the baseline f (x; w f m 0 ).
There are two main components of a winning lottery ticket: the sparsified structure m f and the weight that initializes it, w v . In the previous work, m f has been obtained only after expensive iterative pruning. We characterize the structure of the early lottery ticket m s where s f , then propose a strategy for finding it early.  Interestingly, Fig. 1 further demonstrates that the winning tickets emerge at much earlier epochs of the baseline training; the lottery ticket configurations of (s ≥ 100, v ≥ 5) achieve almost the same accuracy as the accuracy of (s = 200, v = 200). The accuracy then gradually decreases as s ≤ 80. This result implies two important aspects: 1) a winning ticket can be found in the middle of the baseline training so that one can avoid expensive iterative pruning used in the prior work, and 2) the winning ticket, however, does not emerge arbitrarily early in the process of training. In the following sections, we investigate the characteristics of these early winning tickets. In particular, we focus on their structure, as the weight initialization is not the major factor provided a proper rewinding.

[ANALYSIS ON WEIGHT MAGNITUDE]
As the first step of understanding the characteristics of the early winning tickets, we focus on the important quantity of pruning, weight magnitude. At pruning, we determine the structure of the sparsified model based on the rank of the weight magnitude (via global sorting). Therefore, the change in the weight magnitude during training has large impact on the lottery ticket structure.
In Fig. 2a, we measure the standard deviation of weight (W std ) for each layer in the course of training ResNet20 on CIFAR10 (note: the mean of weight is typically near zero). The same learning rate schedule is used as above. The first thing to note is that the change in W std has strong correlation with the learning rate change. In particular, different layers show different rate of change in W std when the learning rate is 0.1, but from the second learning rate (after epoch 120), all the weights follow a very similar decreasing trend.
This trend in W std can be understood via steps of stochastic gradient descent. From the typical setting of weight update with momentum and weight decay, we have:
v t+1 = mv t + (λw t + w g,t ), w t+1 = w t − ηv t+1 . (1
)
where w t , w g,t and v t are the weight, gradient and momentum at step t, respectively; m is the momentum factor, η is the learning rate and λ is the weight decay factor. After n steps,
v t+n = m n v t + n k=1 (m n−k w g,t+k−1 ) + λ n k=1 (m n−k w t+k−1 ),(2)
w t+n = (1 − ηλ) n w t − η n k=1 ((1 − ηλ) n−k (mv t+k−1 + w g,t+k−1 )).(3)
From this derivation, we can see that the two factors determine the mode of change in W std . When learning rate is high, the gradient terms play the major role in weight update. On the other hand, if the gradient activity becomes low, e.g., when the learning rate is low and the gradients oscillate around zero, we can further simplify Eq. 3. Assume that m n v t approaches to zero when n is relatively large, w t+k−1 ≈ w t , and
n k=1 ((1 − ηλ) n−k w g,t+k−1 )
and n k=1 (m n−k w g,t+k−1 ) approach to zero as the gradients oscillate around zero, we have v t+i ≈ λwt 1−m . Then w t+n is approximated as,
w t+n ≈ (1 − ηλ) n w t − η n−1 i=0 ((1 − ηλ) i mλw t 1 − m ). (4
) Note that (1 − ηλ) n ≈ 1 − nηλ since ηλ 1 and n−1 i=0 ((1 − ηλ) i ) ≈ n. Thus, w t+n ≈ (1 − nηλ)w t − nηλmw t 1 − m = (1 − nηλ 1 − m )w t .(5)
In other words, when the gradient activity is low, the change in W std is dominated by the both the learning rate and the weight decay. As an example, in Fig. 2a, W std is decreased with the slope following Eq. 5 when learning rate is low; W std of layer 10 is decreased by 1.5e-3 in 782 updates of CIFAR10 (with λ =1e-4 and m = 0.9), confirming the slope from Eq. 5.
These two modes governing the change of weight magnitude are critical for understanding the behavior of pruning.
In particular, the interplay between learning rate and weight decay causes the per-layer pruning rate to converge after enough number of epochs. Fig. 2b shows the pseudo per-layer pruning rate, where we just measure the layer-wise pruning rate without really pruning out the weights in the model, for the same CIFAR10 training experiment. As the figure shows, the per-layer pruning rates saturate around epoch 100, indicating that pruning before that would select the weights based on the premature model.
Moreover, although the weight magnitude is an important factor, we discovered that the ranking of individual weight does not play a critical role in winning lottery tickets. Specifically, we empirically show that there exist many winning lottery tickets that are vastly different in terms of its sparse structure m v . Fig. 3 shows the hamming distance of the sparse structure of the lottery tickets at different configuration (s, v). Note that the two distant lottery tickets (e.g., (s = 200, v = 100) and (s = 100, v = 100)) show large hamming distance of 0.159 (where x/y = 200/100) while achieving the equally good accuracy as shown in Fig. 1, indicating that the ranking of the weights itself can not explain the quality of the structure of the lottery ticket.
Figure 3: The hamming distance of the sparse structure of the lottery tickets at different configuration for ResNet20 on CIFAR10.
x/y denotes the distance between (s = x) and (s = y). A large distance between two lottery tickets with equally good accuracy suggests the existence of many winning tickets.

[UNDERSTANDING IMPACT OF PRUNED STRUCTURE]
The weight magnitude analysis motivates us not to prune a model too early. It is also implied that a distance-based metric might not reveal the winning structure of the model at different epochs. To understand the early winning structures, we further investigate the impact of the sparsified structure on the final accuracy. Considering that a highly pruned network is likely to have limited learning capability, we make a conjecture that the accuracy degradation of a pruned model is due to the loss of capability for learning complex pattern if pruned too early. Recently, Li et al. (2019) reveals that the training behavior of a sufficiently over-parameterized model with non-linearity highly depends on the learning rate schedule, where a model tends to memorize the complex patterns when a small learning rate is applied while learning simple patterns with a large learning rate. To validate this claim in the context of pruning, we construct an experiment where a model is pseudo-pruned at every epoch of the baseline training then retrained with large or small learning rate for just one epoch. For the pseudo-pruned-then-retrained (PPR) model at each baseline epoch, we measure the validation accuracy recovered from the retraining.
Fig. 4 shows the result of this experiment on CIFAR10 ResNet20. When it is retrained with the large retraining learning rate (= 0.1), in just 1-epoch retraining, the PPR models from all the baseline epochs achieve the accuracy matching with the baseline accuracy. This indicates that those pruned models maintain the capability of learning the simple patterns. Whereas, when the small retraining learning rate (= 0.01) is used, the accuracy of the PPR models pruned at later epochs (epoch 100-200) is higher than the accuracy of the models from earlier epochs (epoch 20-60). This reveals that the PPR models from different epoch exhibit varying capability of learning complex patterns. In particular, the accuracy of the PPR models increases until around 100 epoch of the baseline training, then it saturates. Note that this coincides with the epoch when the early winning tickets emerges in Fig. 1.
Based on this observation, we hypothesize that the models pruned at 100 epoch of the baseline training or later will preserve the capability of learning complex patterns. To validate this hypothesis, we conducted the memorization test proposed by Boo et al. (2019), where a model is trained with training data of varying size with the randomized labels.  A model has ""high memorization capacity"" if it achieves high training accuracy for a large data size. Fig. 5 (left) shows the training accuracy of the models pruned at different epochs, measured from 10 independent simulations (the average capacity shown as solid lines, and min and max shown as the shaded regions). As the training data size increases, the memorization capacity decreases. The model pruned too early (i.e., at epoch 20) suffers higher degradation in the memorization capacity compared to the models pruned at later epochs (i.e., at epochs 120 or 200). Also, Fig. 5 (right) shows that those models pruned at later than 100 epochs (i.e., the early winning tickets) exhibit similar memorization capacity. This result not only confirms our conjecture on the impact of pruned structure to the capability of learning complex patterns, but also leads us to employ the PPR accuracy check as a computationally reasonable heuristic to discover the early winning tickets.

[UNDERSTANDING GRADUAL PRUNING]
Gradual pruning is a popular pruning approach that applies pruning gradually over a period of training. For example, gradual pruning proposed by Zhu & Gupta (2017) provides a systematic way to schedule iterative pruning as follows:
s t = s f + (s i − s f ) 1 − t − t 0 n∆t 3 ,(6)
where s f and s i are the final and initial sparsity, and t is the time when pruning is applied. Eq. 6 determines how much sparsity is applied at a certain time step t. But it is still a user-hyper-parameter to decide when t 0 or how often ∆t apply pruning. Based on the insights we discussed in the previous section, we explain why gradual pruning helps to obtain better lottery tickets.
The reasoning behind the gradual pruning is that the model can be changed graciously if the pruning is applied gradually.
In terms of the early winning ticket analysis, there are two factors playing the critical role: 1) by applying low pruning rate in the beginning, the structure found at that pruning level can preserve the memorization capacity better, 2) once pruning is applied, the remaining weights of the pruned model is updated via SGD, granting a chance for the pruned model to adopt its weights toward better accuracy. Thanks to these two factors, a structure with better memorization capacity can be found when the increased pruning rate is applied next time.
Table 1 confirms this explanation using the CIFAR10 ResNet20 example. In this experiment, we perform the memorization test for the 4-step gradual pruning as well as four 1-step pruning at the corresponding epoch for comparison. The gap in memorization capacity is maintained across the different epochs, demonstrating that the memorization capacity is maintained thanks to the gradual application of pruning and the evolution of weights after pruning. This suggests a strategy for gradual pruning where 1) we can use gradual pruning to reduce the loss of capability in learning complex patterns, and 2) by applying a smaller pruning rate in the beginning, we can start pruning early and finding the winning lottery tickets faster. The benefit of this strategy combining gradual pruning with the early winning tickets will be demonstrated in Sec. 4. In this section, we demonstrate our strategy of finding the early winning tickets over popular neural networks on CIFAR10 and ImageNet. The detail experimental setup is described in Appendix A. We perform the lottery ticket experiments of Sec. 3.1 for both one-shot and gradual pruning with the pruning rate of 80%.We also conduct the PPR accuracy check of Sec. 3.3 to predict from which epoch the early winning ticket can be found. By comparing the two results, we demonstrate that the proposed heuristic for finding early winning tickets works robustly across the networks and the datasets. Furthermore, we showcase our gradual pruning strategy by comparing the performance in terms of the accuracy and the required pruning epochs with the existing lottery ticket approaches.

[EXPERIMENTS ON CIFAR10]
Table 2 summarizes the lottery ticket experiments of ResNet20 and ResNet56 on CIFAR10 dataset. In case of one-shot pruning, the winning tickets can be found from epoch 100. In case of gradual pruning, the gradual pruning schedule can start from epoch 75 (which is earlier than the one-shot pruning). Note that the gradual pruning can achieve better accuracy as it can preserve higher memorization capacity as discussed in Sec. 3.4. Fig. 6a shows the pseudo-pruning curve that highlights the presence of the winning tickets from the epoch around 100, which is consistent with the results of the lottery ticket experiment in Table 2. 

[EXPERIMENTS ON IMAGENET]
Table 3 summarizes the lottery ticket experiments on ImageNet dataset, and the predicted results of the early winning tickets from the pseudo-pruning are shown in Fig. 6b. Similar to the CIFAR10 experiments, the results of the lottery ticket experiment matches with the results from the PPR accuracy check (which indicates epoch 45 for early winning tickets), demonstrating the robust behavior of the proposed strategy of finding the early winning tickets. Furthermore, the gradual pruning at the early winning tickets achieves the accuracy near to the baseline (ResNet18: baseline=69.7% vs ours=69.24%, ResNet50: baseline=75.7% vs ours=75.31%), showcasing the superior quality of the winning tickets discovered by the proposed gradual pruning strategy.  

[PERFORMANCE GAIN FROM EARLY WINNING TICKET]
To demonstrate the performance gain from our gradual pruning strategy, the pruning results of the proposed algorithm (GP + EWT) and the previous implementation of iterative pruning (IP) along with the winning ticket (WT) of Frankle & Carbin (2018) are shown for the ResNet variants on CIFAR10 and ImageNet in Table 4. Our GP+EWT algorithm consistently achieves high pruning rate with the number of pruning epochs even lower than the regular retraining epochs for all the models, i.e., 80% pruning with negligible accuracy degradation on ResNet50 for ImageNet. In contrast, the iterative pruning approach (IP+WT) achieves the similar accuracy at the cost of more than 4.5× increase in the total training epochs.

[CONCLUSION]
In this work, we investigate the structure of the winning lottery ticket, which leads to the computationally efficient discovery of the winning lottery tickets. Based on a careful analysis of the characteristics of the structure of the winning lottery tickets, we proposed a computationally reasonable heuristic to identify when the early lottery tickets emerge. Furthermore, we proposed a gradual pruning strategy incorporating the early lottery ticket analysis to achieve high accuracy at large pruning rate. This results in the state-of-the-art accuracy on ResNet50 for 80% pruning only within 22% of the total epochs for iterative pruning.   We try to find the winning tickets early for ResNet20 (baseline accuracy: 92.5%) and ResNet56 (baseline accuracy: 94.25%) on CIFAR10 dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 200 epochs. The initial learning rate for the first 120 epochs is 0.1 and decrease to 0.1× every 40 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.

[A.2 EXPERIMENTS ON IMAGENET]
We further try to find the winning tickets early for ResNet18 (baseline accuracy: 69.7%) and ResNet50 (baseline accuracy: 75.7%) on ImageNet dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 90 epochs. The initial learning rate for the first 30 epochs is 0.1 and decrease to 0.1× every 30 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.
Figure 7: Structured pruning with variable group size. We consider a group of weights along the channel dimension with a varying group-size (gs). In case of gs=1, it is the same as the element-wise pruning.

[B.1 IMPACT OF STRUCTURED SPARSITY ON LOTTERY TICKET HYPOTHESIS]
In this section, we expand our scope of analysis on the characteristics of the early winning tickets toward the sparsity obtained via structured pruning. Structured pruning is a popular method that prunes weights by a group. For example,  proposes channel-pruning, where a group consists of weights along each channel. The main motivation of looking at the structured sparsity is that there is a significant discrepancy in the best pruning rate we can achieve between the element-wise pruning (i.e., group-size=1) and the structured pruning (i.e., group-size=channel). For example, on CIFAR10-VGG19, the element-wise pruning achieves 95% of pruning rate whereas the channel-pruning achieves 70% of pruning rate for the same level of accuracy (Liu et al. (2018)). Although such discrepancy has been observed for quite a while, there is no in-depth investigation to understand why. To find out the reasoning behind it, we analyze the impact of structured sparsity with varying group sizes in the context of the lottery ticket hypothesis, as illustrated in Fig. 7.
First, we revisit the extended lottery ticket experiments of Fig. 1 while applying group sparsity along the channel dimension (instead of pruning individual weights). Fig. 8 shows the results when the group size (gs) is 8. Comparing  it with Fig. 1, there are two notable points; 1) there is > 1% accuracy degradation when the group size is increased from 1 to 8, and 2) when gs = 8, the trend that the tickets drawn at epochs s > 100 achieve the higher accuracy disappears (as highlighted with a red rectangle in the figure). (Note that the two figures achieve the similar accuracy when s < 60 (colored in yellow), but the tickets with high accuracy (colored in green) can only be seen in Fig. 1.) The former observation makes sense as the more regularization on the sparse weight structure would result in the lower accuracy. But the latter observation is quite surprising; it seems that a distinct behavior of the lottery ticket hypothesis (i.e., the opportunity of finding winning tickets given a proper initialization) is disrupted as a group structure is asserted on the sparsity.
We can find a clue on this disrupted behavior by employing the memorization capacity analysis. Table 5 shows the results of memorization capacity experiments on CIFAR10-ResNet20 with an increasing group size gs = {1, 2, 4, 8, 16, 32, 64}.
The memorization capacity degrades significantly as the group size increases. Intuitively, by asserting a large group size, the model's expressivity is degraded and it becomes harder to learn the complex patterns, leading to accuracy loss. Note that we had a similar observation when the tickets are drawn too early (cf.,s < 60 in Fig. 1). From this, we can hypothesize that the winning tickets disappear as the memorization capacity is degraded, which can be observed when the tickets are drawn too early or the group sparsity is forced.
Fig. 9 further demonstrates the impact of structured sparsity on the saturation of pruning accuracy. For different group sizes, the figure shows the accuracy of CIFAR10-ResNet20 80% pruned at varying epochs. It can be observed that the models with the larger group size not only achieve the lower accuracy but also converge at earlier epochs. (E.g., the knee points for gs = 1 and gs = 16 are around 90 and 50 epochs, respectively.) This experimental result supports our claim that the larger group size enforced in the sparse structure results in the earlier convergence of the pruned models that misses the winning tickets. 

[B.2 MODE CONNECTIVITY OF EARLY WINNING TICKETS]
Mode connectivity is a phenomenon that SGD solutions are connected through paths of approximately equal loss (Draxler et al. (2018)). It provides the perspective of how well the models trained via a proposed method can generalize.
In the context of early winning tickets, we for the first time reveal that the lottery tickets drawn early based on PPR are indeed connected from the one obtained by IMP, whereas the tickets chosen too early do not.
For the mode connectivity experiments, we draw three lottery tickets -one drawn too early (i.e., a premature ticket, PT), one drawn based on PPR (i.e., an early winning ticket, EWT), and one at the end of the training schedule (i.e., a winning ticket, WT). Note that all the tickets are trained from the same initial weights and with the same retraining schedule.
Then we linearly interpolate PT and EWT with WT, and plot the test error for different interpolation coefficient (from 0 to 1). Fig. 10 shows the results of the mode connectivity experiments for ImageNet and CIFAR10. For both datasets, the linearly interpolated models between PT and WT depict a barrier, indicating that PT and WT are disconnected. Whereas, the linearly interpolated models between EWT and WT show consistently low test error. Note that such linear mode connectivity is observed even when 1) the weights are sparse (i.e., 80% pruning rate) and 2) the mask distance is large (cf., Fig. 3). This intriguing observation counters some observations of prior work; Draxler et al. (2018) claims that it takes a careful search (instead of a simple linear interpolation) to find a pass connecting the modes, and the existence of such a non-linear pass requires enough number of parameters. Our novel observation of early lottery tickets provide a new aspect of sparse model training; the sparse structures mature early in the training, and they converge to a flat local minimum even if its shape is vastly distant. A deeper investigation about the sparse structure of the lottery tickets and the mode connectivity is an interesting future research topic.

[C EARLY WINNING TICKET ALGORITHMS]
In this section, we describe the detail algorithms for finding early winning tickets via one-shot and gradual pruning.

[]
Algorithm 1: Early Winning Ticket Identification with One-Shot Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then Set the model parameters to θ i • m P P R,t ; break; end t++; end end
Step 2: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end Algorithm 2: Early Winning Ticket Identification with Gradual Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R , gradual pruning period T gr ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then break; end t++; end end
Step 2: Gradual Pruning to refine the EWT structure for i = 0; i < T gr ; i++ do Prune model gradually toward the pruning ratio p according to Eq. (6) and create a mask m gr,i ; Train the pruned model θ gr,i • m gr,i for one epoch; end
Step 3: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end","[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning."
The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets,BJlNs0VYPB.json,"Overview:

The paper is dedicated to conducting an in-depth investigation of the structure of winning lottery tickets. The author provides supporting evidence for the structure of the early winning tickets: 1) lottery tickets emerge when the weight magnitude of a model saturates with SGD optimization. 2) pruning before model saturation may result in accuracy degradation. In the experiment part, they employ the memorization capacity analysis and discover the early wining tickets without expensive iterative pruning. The author also conducts extensive experiments with various ResNet architectures on both CIFAR 10 and ImageNet, achieving state-of-the-art results with only 1/5 of the total epochs for iterative pruning.

Strength Bullets:

1. The experiment organization is complete and convincing. Especially for the figure, it not only clearly shows that lottery tickets emerge much earlier before full training ends, but also shows the effect of rewinding. 
2. The author not reveal interesting observations, but also provides useful guidance. It is a complete logic chain that is also aligned with my intuition. For example, first, the author discusses the memorization capacity of different pruned models at different epochs. Then, they introduce a reasonable gradual pruning technique. Finally, they conduct experiments to confirm it.
3.  The early winning tickets in this paper achieve state-of-the-art results with only 1/5 of the total epochs for iterative pruning.

Weakness Bullets:

1. For lottery tickets, especially for early winning tickets, I think there is a lot of randomnesses. Thus, for the plot like figure 2, figure 5, they need to contain an error bar and the curve should be the average of tens of experiments. It will be more convincing if it decouples the randomness from the real patterns.
2. The description and organization of section 4 need to be more clear. For example, an algorithm pseudo code will definitely give readers a much more clear understanding of the early winning tickets finding strategy.



Question the authors don't answer which confuses me more:
Need more convincing analysis about the indicator - Hamming Distance 

Just as the comment I posted after Review 1, we would like to see more analysis about Hamming Distance between different winning tickets. The author mentioned in the following way:

''However, as we demonstrate in Fig. 3 of our paper, the mask-distance does not well characterize the winning tickets. E.g., the lottery tickets drawn at Epoch 120 and 200 have a mask distance of 0.082 in Fig. 3, which is much larger than the mask distance between Epoch 180 and 200. Whereas, all three tickets achieve comparably high accuracy as shown in Fig. 1, implying a shallow correlation between the accuracy and the mask distance''

As far as I know, this observation only tells us that the structure of the lottery tickets changes, which are drawn from 120 epochs to 180 epochs (although the maintain a similar accuracy). However, we can not conclude that the mask distance is not a reliable measure. As mention by the [EB] paper provided by the authors, you can use the rate of the distance change to indicate the early winning tickets. In this way, we can find winning tickets much earlier than authors' work.

To better address this question, I suppose the authors need to provide more analysis of the mask distance indicator. I think all three reviewers would like to see the results. A good indicator for early winning tickets is very important, otherwise authors' notion of early still involves quite a lot of training.

Recommendation:

Due to the unsolved important question, here is a weak reject.","[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Extended lottery ticket experiments using ResNet20 on CIFAR10. From the baseline ResNet20 training, weight-magnitude based pruning is applied at different epoch (columns, labeled as s) to obtain the sparsified structure. Each structure is then rewound to the weights from another epoch (rows, labeled as v), where v=0 indicates rewind to the initial weight. Lottery tickets emerge much earlier before the full training ends, achieving matching accuracy compared to the conventional winning ticket, i.e., (s=200,v=200).
Table 2: Frankleet al. (2019)  suggests that a lottery ticket found after a baseline training can achieve the accuracy of the baseline model if it is initialized with the weight of the baseline model after a few epochs, which is called rewinding. We extend this exploration toward different structures obtained at different epochs of the baseline training (via magnitude-based weight pruning).
Table 3: Fig. 11Fig.1shows the validation accuracy after retraining of a ResNet20 model on CIFAR10 pruned at different epochs of its baseline training. The same learning rate schedule of 0.1 reduced by 10x at epoch 120 and 160 is used for both the baseline training and retraining, and the total number of epochs is 200. The rows (= v) correspond to the different rewinding epoch, whereas the columns (= s) correspond to the different epoch that we apply one-shot pruning (pruning rate= 80%). For example, the validation accuracy along the lottery ticket configurations of (s = 200, v = 0 ∼ 200) resembles the phenomena of ""rewinding"" observed inFrankle et al. (2019).
Table 4: Figure 2 :2Figure 2: (a) Change of the standard deviation of weights (W std ) in each layer during the training of ResNet20 on CIFAR10. In the beginning of the training, the weights of different layers change in different rate, dominated by the gradient terms. When the training evolves, weight magnitude is primarily determined by the interplay between learning rate and weight decay, resulting in parallel movement of W std . (b) Change of per-layer pruning rate over the epochs. Due to regular shift of W std at the later epochs of training, the per-layer pruning rate converges to a saturating point, emerging stable structure for pruning.
Table 5: Figure 4 :4Figure4: The pseudo-pruned-then-retrained (PPR) models for CIFAR10 ResNet20, which exhibit increasing capability of memorizing the complex patterns over the epochs. Validation accuracy for the baseline training and validation accuracy after pruning without retraining are also included for comparison.
Table 6: Figure 5 :5Figure 5: Left: Memorization capacity (i.e., training accuracy) of the models pruned at 3 different epochs. The models pruned at epoch 120 and 200 show almost identical memorization capacity, whereas the model pruned at epoch 20 exhibits lower memorization capacity. Right: Memorization capacity of models pruned at different epochs (training data size = 15000).
Table 7: Figure 6 :6Figure 6: The results of PPR accuracy check (retraining learning rate=0.01) for (a) ResNet56 on CIFAR10 (sampled at every epoch) and (b) ResNet18 on ImageNet (sampled every 5 epochs).
Table 8: GP: gradual pruning, IP: iterative method, EWT: early winning ticket, WT: winning ticket * : pruning rate (PR). * * : number of pruning epochs + regular retraining epochs † : the delta of accuracy is measured against the baseline accuracy. Bold: highlight of comparison
Table 9: Figure 8 :8Figure 8: Revisiting the extended lottery ticket experiments of Fig. 1, but with the group size (gs) of 8. As before, (s, v) stands for (epoch drawing the sparse structure, epoch for rewinding). In contrast to Fig. 1, the winning tickets found when gs=1 (green color in Fig. 1) disappear as the group size becomes 8.
Table 10: Figure 9 :9Figure9: Impact of structured sparsity on the saturation of pruning accuracy. The x-axis corresponds to the epoch when the model is 80% pruned. The dotted lines correspond to the moving average of the accuracy to show trends. The larger the group size, the later the accuracy of the pruned models converges at.
Table 11: Figure 10 :10Figure 10: Mode connectivity: Lottery tickets of ImageNet-ResNet50 and CIFAR10-ResNet20 drawn from different epochs are linearly interpolated, then the accuracy is measured for each interpolation coefficient. It can be observed that only the early winning tickets are connected to the winning tickets (drawn at the end of training, i.e., epoch 90 for ImageNet and epoch 200 for CIFAR10).
Table 12: Memorization capacity (i.e., train accuracy) comparison between 1-shot and 4-shot pruning.
Table 13: One-shot and gradual pruning at different epochs for ResNet20 and ResNet56 on CIFAR10. To clarify randomness effect, the average accuracy (%) over 10 different runs is reported. ±0.07) 91.20 (±0.28) 91.49 (±0.27) 91.37 (±0.24) 91.63 (±0.19) 91.63 (±0.21) ResNet56 92.94 (±0.28) 93.60 (±0.18) 94.07 (±0.16) 94.24 (±0.19) 94.29 (±0.17) 94.34 (±0.14)
Table 14: One-shot and gradual pruning for ResNet18 and ResNet50 on ImageNet.
Table 15: Performance gain by our gradual pruning strategy on CIFAR-10 and ImageNet.
Table 16: Memorization capacity comparison for different group size.

[INTRODUCTION]
Deep Neural Networks (DNNs) achieve superior accuracy in a wide spectrum of applications through the use of very large and deep models (Goodfellow et al. (2016)). These high-capacity but complex models, however, pose a tremendous challenge for their deployment, particularly in resource-constrained edge environments. Over the years, many techniques have been developed to compress the models to a compact counterpart to alleviate the computational costs. Among these techniques, pruning less important parameters to obtain a compact sub-network has emerged to be a popular and efficient approach (Cun et al. (1990); Han et al. (2015)). In search of these sub-networks, new intuitions are also built up for understanding the DNN working mechanism, one example of which is the recently proposed ""lottery ticket hypothesis"" by Frankle & Carbin (2018).
The lottery ticket hypothesis states that, once a sub-network is found to match the accuracy of the original neural network, the sub-network (i.e., lottery ticket) together with its initialized weights can be trained in isolation and still achieve accuracy comparable to the original network within a similar number of iterations. This conjecture intrigues discussions on a series of topics, such as the importance of initialization scheme (Liu et al. (2018), Zhou et al. (2019)), the role of over-parameterization in training (Frankle et al. (2019)) and even the transferability of the ""winning ticket"" (S. Morcos et al. (2019)). However, all these discussions start from the point that the winning ticket has been obtained after painfully long iterative pruning procedures, which often take up to thousands of epochs. When and how a winning ticket can be found in the course of pruning procedures has not been studied; most of the prior works use a traditional way of repeated cycles of pruning and retraining, which makes such study less practical.
In this work, we provide insights to find the winning tickets early. We start with an interesting observation that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule finishes. This leads to our in-depth investigation of the weight magnitude; we discover that a model saturates early but not too early under stochastic gradient descent (SGD) optimization. To understand this characteristic of the winning lottery tickets, we conjecture that pruning a premature model causes the loss of capability in learning complex patterns, leading to accuracy degradation. We confirm this conjecture with the empirical evidence as well as the quantitative analysis based on the memorization capacity. Using this analysis framework, we further provide a reasoning behind the success of the gradual pruning over the one-shot pruning. Based on these insights, we identify the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate obtained within only 22% of the total epochs for iterative pruning. This promising outcome not only sheds light on understanding the optimization behavior of the pruned models but also enables performance gain for fast training of the pruned models.

[RELATED WORK]
The lottery ticket hypothesis was first proposed in Frankle & Carbin (2018) where the presence of a trainable subnetwork that achieves compelling final accuracy with the inherited initial values for the un-pruned connections is demonstrated. This paper also argued that the initial weight of the original network is essential for maintaining good accuracy when the model is sparsified. This claim has been extended to the over-parameterized neural networks on the larger datasets in Frankle et al. (2019) with the notion of ""rewinding""; the authors claim that rewinding of the weights not to the initial values but to the values after a few epochs can stabilize the accuracy of the winning lottery tickets. As a follow-up work, Zhou et al. (2019) studied the critical components of the lottery tickets such as zeros, signs and the super-mask. Also, S. Morcos et al. (2019) investigated the transferability of winning tickets obtained in one dataset to the network of a similar structure for the other datasets. But none of these focused on the structure of the winning lottery tickets; it involves several repetitions of the full training schedule for iterative pruning and retraining, often taking hundreds to thousands of epochs. In this work, we demonstrate that many winning tickets can be found in the early stage of the baseline training schedule, drastically reducing the computational effort to obtain them.
Most work on the lottery ticket hypothesis, including this work, rely on magnitude-based weight pruning for identifying unimportant weights to be pruned (usually via global sorting). Such an intuitive method was first proposed in Han et al. (2015) and became popular. Afterwards, more complex pruning methodologies have been presented to improve pruning performance, such as having different pruning criteria , Wen et al. (2016), ) or different pruning granularity (Mao et al. (2017), Molchanov et al. (2019)). While these attempts offer insights on training pruned models, there is little discussion about the interesting interplay between the pruning criteria and the structure of a model pruned by it. In this work, we reveal that the pruned structure obtained based on the weight magnitude has tangible impact on the final accuracy, and further propose a way to quantitatively distinguish good structures for pruning.
There have been various strategies to apply pruning to the neural networks. Iterative pruning by Han et al. (2015) involves several repetition of pruning (with gradual increase of the pruning rate) and retraining. One can increase the repetition cycles arbitrarily large to achieve good accuracy at high pruning rate; Frankle & Carbin (2018) employed iterative pruning with hundreds to thousands of pruning epochs to match the baseline accuracy for the challenging neural networks. On the other hand, gradual pruning introduced by Zhu & Gupta (2017) determines the pruning rate and frequency via a polynomial equation as a function of the starting and ending epochs as well as the target pruning rate. Although it provides a systematic pruning schedule, there is lack of discussion about when to start the gradual pruning. Lastly, Lee et al. (2018) proposes a method of pruning weights at initialization. This method takes most advantage in performance since a pruned model can be obtained without any expensive retraining procedures. However, its effectiveness has not been demonstrated for the challenging neural networks on large datasets such as ImageNet. In contrast, we propose a mechanism to identify the winning lottery ticket early in the course of baseline training so that we can avoid costly iterative pruning while maintaining the baseline accuracy.

[STRUCTURE OF EARLY WINNING TICKETS]
In this section, we extend the lottery ticket hypothesis by Frankle & Carbin (2018); Frankle et al. (2019) to discuss the early winning tickets. The lottery ticket hypothesis can be summarized as: for a given network of f (x; w 0 m 0 ) with the initial weight w 0 and the mask of all ones m 0 , there exists a winning lottery ticket m f where |m f |/|m 0 | = 1 − p% (p is the pruning rate) and training of f (x; w v m f ) for 0 < v f achieves test accuracy comparable to the baseline f (x; w f m 0 ).
There are two main components of a winning lottery ticket: the sparsified structure m f and the weight that initializes it, w v . In the previous work, m f has been obtained only after expensive iterative pruning. We characterize the structure of the early lottery ticket m s where s f , then propose a strategy for finding it early.  Interestingly, Fig. 1 further demonstrates that the winning tickets emerge at much earlier epochs of the baseline training; the lottery ticket configurations of (s ≥ 100, v ≥ 5) achieve almost the same accuracy as the accuracy of (s = 200, v = 200). The accuracy then gradually decreases as s ≤ 80. This result implies two important aspects: 1) a winning ticket can be found in the middle of the baseline training so that one can avoid expensive iterative pruning used in the prior work, and 2) the winning ticket, however, does not emerge arbitrarily early in the process of training. In the following sections, we investigate the characteristics of these early winning tickets. In particular, we focus on their structure, as the weight initialization is not the major factor provided a proper rewinding.

[ANALYSIS ON WEIGHT MAGNITUDE]
As the first step of understanding the characteristics of the early winning tickets, we focus on the important quantity of pruning, weight magnitude. At pruning, we determine the structure of the sparsified model based on the rank of the weight magnitude (via global sorting). Therefore, the change in the weight magnitude during training has large impact on the lottery ticket structure.
In Fig. 2a, we measure the standard deviation of weight (W std ) for each layer in the course of training ResNet20 on CIFAR10 (note: the mean of weight is typically near zero). The same learning rate schedule is used as above. The first thing to note is that the change in W std has strong correlation with the learning rate change. In particular, different layers show different rate of change in W std when the learning rate is 0.1, but from the second learning rate (after epoch 120), all the weights follow a very similar decreasing trend.
This trend in W std can be understood via steps of stochastic gradient descent. From the typical setting of weight update with momentum and weight decay, we have:
v t+1 = mv t + (λw t + w g,t ), w t+1 = w t − ηv t+1 . (1
)
where w t , w g,t and v t are the weight, gradient and momentum at step t, respectively; m is the momentum factor, η is the learning rate and λ is the weight decay factor. After n steps,
v t+n = m n v t + n k=1 (m n−k w g,t+k−1 ) + λ n k=1 (m n−k w t+k−1 ),(2)
w t+n = (1 − ηλ) n w t − η n k=1 ((1 − ηλ) n−k (mv t+k−1 + w g,t+k−1 )).(3)
From this derivation, we can see that the two factors determine the mode of change in W std . When learning rate is high, the gradient terms play the major role in weight update. On the other hand, if the gradient activity becomes low, e.g., when the learning rate is low and the gradients oscillate around zero, we can further simplify Eq. 3. Assume that m n v t approaches to zero when n is relatively large, w t+k−1 ≈ w t , and
n k=1 ((1 − ηλ) n−k w g,t+k−1 )
and n k=1 (m n−k w g,t+k−1 ) approach to zero as the gradients oscillate around zero, we have v t+i ≈ λwt 1−m . Then w t+n is approximated as,
w t+n ≈ (1 − ηλ) n w t − η n−1 i=0 ((1 − ηλ) i mλw t 1 − m ). (4
) Note that (1 − ηλ) n ≈ 1 − nηλ since ηλ 1 and n−1 i=0 ((1 − ηλ) i ) ≈ n. Thus, w t+n ≈ (1 − nηλ)w t − nηλmw t 1 − m = (1 − nηλ 1 − m )w t .(5)
In other words, when the gradient activity is low, the change in W std is dominated by the both the learning rate and the weight decay. As an example, in Fig. 2a, W std is decreased with the slope following Eq. 5 when learning rate is low; W std of layer 10 is decreased by 1.5e-3 in 782 updates of CIFAR10 (with λ =1e-4 and m = 0.9), confirming the slope from Eq. 5.
These two modes governing the change of weight magnitude are critical for understanding the behavior of pruning.
In particular, the interplay between learning rate and weight decay causes the per-layer pruning rate to converge after enough number of epochs. Fig. 2b shows the pseudo per-layer pruning rate, where we just measure the layer-wise pruning rate without really pruning out the weights in the model, for the same CIFAR10 training experiment. As the figure shows, the per-layer pruning rates saturate around epoch 100, indicating that pruning before that would select the weights based on the premature model.
Moreover, although the weight magnitude is an important factor, we discovered that the ranking of individual weight does not play a critical role in winning lottery tickets. Specifically, we empirically show that there exist many winning lottery tickets that are vastly different in terms of its sparse structure m v . Fig. 3 shows the hamming distance of the sparse structure of the lottery tickets at different configuration (s, v). Note that the two distant lottery tickets (e.g., (s = 200, v = 100) and (s = 100, v = 100)) show large hamming distance of 0.159 (where x/y = 200/100) while achieving the equally good accuracy as shown in Fig. 1, indicating that the ranking of the weights itself can not explain the quality of the structure of the lottery ticket.
Figure 3: The hamming distance of the sparse structure of the lottery tickets at different configuration for ResNet20 on CIFAR10.
x/y denotes the distance between (s = x) and (s = y). A large distance between two lottery tickets with equally good accuracy suggests the existence of many winning tickets.

[UNDERSTANDING IMPACT OF PRUNED STRUCTURE]
The weight magnitude analysis motivates us not to prune a model too early. It is also implied that a distance-based metric might not reveal the winning structure of the model at different epochs. To understand the early winning structures, we further investigate the impact of the sparsified structure on the final accuracy. Considering that a highly pruned network is likely to have limited learning capability, we make a conjecture that the accuracy degradation of a pruned model is due to the loss of capability for learning complex pattern if pruned too early. Recently, Li et al. (2019) reveals that the training behavior of a sufficiently over-parameterized model with non-linearity highly depends on the learning rate schedule, where a model tends to memorize the complex patterns when a small learning rate is applied while learning simple patterns with a large learning rate. To validate this claim in the context of pruning, we construct an experiment where a model is pseudo-pruned at every epoch of the baseline training then retrained with large or small learning rate for just one epoch. For the pseudo-pruned-then-retrained (PPR) model at each baseline epoch, we measure the validation accuracy recovered from the retraining.
Fig. 4 shows the result of this experiment on CIFAR10 ResNet20. When it is retrained with the large retraining learning rate (= 0.1), in just 1-epoch retraining, the PPR models from all the baseline epochs achieve the accuracy matching with the baseline accuracy. This indicates that those pruned models maintain the capability of learning the simple patterns. Whereas, when the small retraining learning rate (= 0.01) is used, the accuracy of the PPR models pruned at later epochs (epoch 100-200) is higher than the accuracy of the models from earlier epochs (epoch 20-60). This reveals that the PPR models from different epoch exhibit varying capability of learning complex patterns. In particular, the accuracy of the PPR models increases until around 100 epoch of the baseline training, then it saturates. Note that this coincides with the epoch when the early winning tickets emerges in Fig. 1.
Based on this observation, we hypothesize that the models pruned at 100 epoch of the baseline training or later will preserve the capability of learning complex patterns. To validate this hypothesis, we conducted the memorization test proposed by Boo et al. (2019), where a model is trained with training data of varying size with the randomized labels.  A model has ""high memorization capacity"" if it achieves high training accuracy for a large data size. Fig. 5 (left) shows the training accuracy of the models pruned at different epochs, measured from 10 independent simulations (the average capacity shown as solid lines, and min and max shown as the shaded regions). As the training data size increases, the memorization capacity decreases. The model pruned too early (i.e., at epoch 20) suffers higher degradation in the memorization capacity compared to the models pruned at later epochs (i.e., at epochs 120 or 200). Also, Fig. 5 (right) shows that those models pruned at later than 100 epochs (i.e., the early winning tickets) exhibit similar memorization capacity. This result not only confirms our conjecture on the impact of pruned structure to the capability of learning complex patterns, but also leads us to employ the PPR accuracy check as a computationally reasonable heuristic to discover the early winning tickets.

[UNDERSTANDING GRADUAL PRUNING]
Gradual pruning is a popular pruning approach that applies pruning gradually over a period of training. For example, gradual pruning proposed by Zhu & Gupta (2017) provides a systematic way to schedule iterative pruning as follows:
s t = s f + (s i − s f ) 1 − t − t 0 n∆t 3 ,(6)
where s f and s i are the final and initial sparsity, and t is the time when pruning is applied. Eq. 6 determines how much sparsity is applied at a certain time step t. But it is still a user-hyper-parameter to decide when t 0 or how often ∆t apply pruning. Based on the insights we discussed in the previous section, we explain why gradual pruning helps to obtain better lottery tickets.
The reasoning behind the gradual pruning is that the model can be changed graciously if the pruning is applied gradually.
In terms of the early winning ticket analysis, there are two factors playing the critical role: 1) by applying low pruning rate in the beginning, the structure found at that pruning level can preserve the memorization capacity better, 2) once pruning is applied, the remaining weights of the pruned model is updated via SGD, granting a chance for the pruned model to adopt its weights toward better accuracy. Thanks to these two factors, a structure with better memorization capacity can be found when the increased pruning rate is applied next time.
Table 1 confirms this explanation using the CIFAR10 ResNet20 example. In this experiment, we perform the memorization test for the 4-step gradual pruning as well as four 1-step pruning at the corresponding epoch for comparison. The gap in memorization capacity is maintained across the different epochs, demonstrating that the memorization capacity is maintained thanks to the gradual application of pruning and the evolution of weights after pruning. This suggests a strategy for gradual pruning where 1) we can use gradual pruning to reduce the loss of capability in learning complex patterns, and 2) by applying a smaller pruning rate in the beginning, we can start pruning early and finding the winning lottery tickets faster. The benefit of this strategy combining gradual pruning with the early winning tickets will be demonstrated in Sec. 4. In this section, we demonstrate our strategy of finding the early winning tickets over popular neural networks on CIFAR10 and ImageNet. The detail experimental setup is described in Appendix A. We perform the lottery ticket experiments of Sec. 3.1 for both one-shot and gradual pruning with the pruning rate of 80%.We also conduct the PPR accuracy check of Sec. 3.3 to predict from which epoch the early winning ticket can be found. By comparing the two results, we demonstrate that the proposed heuristic for finding early winning tickets works robustly across the networks and the datasets. Furthermore, we showcase our gradual pruning strategy by comparing the performance in terms of the accuracy and the required pruning epochs with the existing lottery ticket approaches.

[EXPERIMENTS ON CIFAR10]
Table 2 summarizes the lottery ticket experiments of ResNet20 and ResNet56 on CIFAR10 dataset. In case of one-shot pruning, the winning tickets can be found from epoch 100. In case of gradual pruning, the gradual pruning schedule can start from epoch 75 (which is earlier than the one-shot pruning). Note that the gradual pruning can achieve better accuracy as it can preserve higher memorization capacity as discussed in Sec. 3.4. Fig. 6a shows the pseudo-pruning curve that highlights the presence of the winning tickets from the epoch around 100, which is consistent with the results of the lottery ticket experiment in Table 2. 

[EXPERIMENTS ON IMAGENET]
Table 3 summarizes the lottery ticket experiments on ImageNet dataset, and the predicted results of the early winning tickets from the pseudo-pruning are shown in Fig. 6b. Similar to the CIFAR10 experiments, the results of the lottery ticket experiment matches with the results from the PPR accuracy check (which indicates epoch 45 for early winning tickets), demonstrating the robust behavior of the proposed strategy of finding the early winning tickets. Furthermore, the gradual pruning at the early winning tickets achieves the accuracy near to the baseline (ResNet18: baseline=69.7% vs ours=69.24%, ResNet50: baseline=75.7% vs ours=75.31%), showcasing the superior quality of the winning tickets discovered by the proposed gradual pruning strategy.  

[PERFORMANCE GAIN FROM EARLY WINNING TICKET]
To demonstrate the performance gain from our gradual pruning strategy, the pruning results of the proposed algorithm (GP + EWT) and the previous implementation of iterative pruning (IP) along with the winning ticket (WT) of Frankle & Carbin (2018) are shown for the ResNet variants on CIFAR10 and ImageNet in Table 4. Our GP+EWT algorithm consistently achieves high pruning rate with the number of pruning epochs even lower than the regular retraining epochs for all the models, i.e., 80% pruning with negligible accuracy degradation on ResNet50 for ImageNet. In contrast, the iterative pruning approach (IP+WT) achieves the similar accuracy at the cost of more than 4.5× increase in the total training epochs.

[CONCLUSION]
In this work, we investigate the structure of the winning lottery ticket, which leads to the computationally efficient discovery of the winning lottery tickets. Based on a careful analysis of the characteristics of the structure of the winning lottery tickets, we proposed a computationally reasonable heuristic to identify when the early lottery tickets emerge. Furthermore, we proposed a gradual pruning strategy incorporating the early lottery ticket analysis to achieve high accuracy at large pruning rate. This results in the state-of-the-art accuracy on ResNet50 for 80% pruning only within 22% of the total epochs for iterative pruning.   We try to find the winning tickets early for ResNet20 (baseline accuracy: 92.5%) and ResNet56 (baseline accuracy: 94.25%) on CIFAR10 dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 200 epochs. The initial learning rate for the first 120 epochs is 0.1 and decrease to 0.1× every 40 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.

[A.2 EXPERIMENTS ON IMAGENET]
We further try to find the winning tickets early for ResNet18 (baseline accuracy: 69.7%) and ResNet50 (baseline accuracy: 75.7%) on ImageNet dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 90 epochs. The initial learning rate for the first 30 epochs is 0.1 and decrease to 0.1× every 30 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.
Figure 7: Structured pruning with variable group size. We consider a group of weights along the channel dimension with a varying group-size (gs). In case of gs=1, it is the same as the element-wise pruning.

[B.1 IMPACT OF STRUCTURED SPARSITY ON LOTTERY TICKET HYPOTHESIS]
In this section, we expand our scope of analysis on the characteristics of the early winning tickets toward the sparsity obtained via structured pruning. Structured pruning is a popular method that prunes weights by a group. For example,  proposes channel-pruning, where a group consists of weights along each channel. The main motivation of looking at the structured sparsity is that there is a significant discrepancy in the best pruning rate we can achieve between the element-wise pruning (i.e., group-size=1) and the structured pruning (i.e., group-size=channel). For example, on CIFAR10-VGG19, the element-wise pruning achieves 95% of pruning rate whereas the channel-pruning achieves 70% of pruning rate for the same level of accuracy (Liu et al. (2018)). Although such discrepancy has been observed for quite a while, there is no in-depth investigation to understand why. To find out the reasoning behind it, we analyze the impact of structured sparsity with varying group sizes in the context of the lottery ticket hypothesis, as illustrated in Fig. 7.
First, we revisit the extended lottery ticket experiments of Fig. 1 while applying group sparsity along the channel dimension (instead of pruning individual weights). Fig. 8 shows the results when the group size (gs) is 8. Comparing  it with Fig. 1, there are two notable points; 1) there is > 1% accuracy degradation when the group size is increased from 1 to 8, and 2) when gs = 8, the trend that the tickets drawn at epochs s > 100 achieve the higher accuracy disappears (as highlighted with a red rectangle in the figure). (Note that the two figures achieve the similar accuracy when s < 60 (colored in yellow), but the tickets with high accuracy (colored in green) can only be seen in Fig. 1.) The former observation makes sense as the more regularization on the sparse weight structure would result in the lower accuracy. But the latter observation is quite surprising; it seems that a distinct behavior of the lottery ticket hypothesis (i.e., the opportunity of finding winning tickets given a proper initialization) is disrupted as a group structure is asserted on the sparsity.
We can find a clue on this disrupted behavior by employing the memorization capacity analysis. Table 5 shows the results of memorization capacity experiments on CIFAR10-ResNet20 with an increasing group size gs = {1, 2, 4, 8, 16, 32, 64}.
The memorization capacity degrades significantly as the group size increases. Intuitively, by asserting a large group size, the model's expressivity is degraded and it becomes harder to learn the complex patterns, leading to accuracy loss. Note that we had a similar observation when the tickets are drawn too early (cf.,s < 60 in Fig. 1). From this, we can hypothesize that the winning tickets disappear as the memorization capacity is degraded, which can be observed when the tickets are drawn too early or the group sparsity is forced.
Fig. 9 further demonstrates the impact of structured sparsity on the saturation of pruning accuracy. For different group sizes, the figure shows the accuracy of CIFAR10-ResNet20 80% pruned at varying epochs. It can be observed that the models with the larger group size not only achieve the lower accuracy but also converge at earlier epochs. (E.g., the knee points for gs = 1 and gs = 16 are around 90 and 50 epochs, respectively.) This experimental result supports our claim that the larger group size enforced in the sparse structure results in the earlier convergence of the pruned models that misses the winning tickets. 

[B.2 MODE CONNECTIVITY OF EARLY WINNING TICKETS]
Mode connectivity is a phenomenon that SGD solutions are connected through paths of approximately equal loss (Draxler et al. (2018)). It provides the perspective of how well the models trained via a proposed method can generalize.
In the context of early winning tickets, we for the first time reveal that the lottery tickets drawn early based on PPR are indeed connected from the one obtained by IMP, whereas the tickets chosen too early do not.
For the mode connectivity experiments, we draw three lottery tickets -one drawn too early (i.e., a premature ticket, PT), one drawn based on PPR (i.e., an early winning ticket, EWT), and one at the end of the training schedule (i.e., a winning ticket, WT). Note that all the tickets are trained from the same initial weights and with the same retraining schedule.
Then we linearly interpolate PT and EWT with WT, and plot the test error for different interpolation coefficient (from 0 to 1). Fig. 10 shows the results of the mode connectivity experiments for ImageNet and CIFAR10. For both datasets, the linearly interpolated models between PT and WT depict a barrier, indicating that PT and WT are disconnected. Whereas, the linearly interpolated models between EWT and WT show consistently low test error. Note that such linear mode connectivity is observed even when 1) the weights are sparse (i.e., 80% pruning rate) and 2) the mask distance is large (cf., Fig. 3). This intriguing observation counters some observations of prior work; Draxler et al. (2018) claims that it takes a careful search (instead of a simple linear interpolation) to find a pass connecting the modes, and the existence of such a non-linear pass requires enough number of parameters. Our novel observation of early lottery tickets provide a new aspect of sparse model training; the sparse structures mature early in the training, and they converge to a flat local minimum even if its shape is vastly distant. A deeper investigation about the sparse structure of the lottery tickets and the mode connectivity is an interesting future research topic.

[C EARLY WINNING TICKET ALGORITHMS]
In this section, we describe the detail algorithms for finding early winning tickets via one-shot and gradual pruning.

[]
Algorithm 1: Early Winning Ticket Identification with One-Shot Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then Set the model parameters to θ i • m P P R,t ; break; end t++; end end
Step 2: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end Algorithm 2: Early Winning Ticket Identification with Gradual Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R , gradual pruning period T gr ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then break; end t++; end end
Step 2: Gradual Pruning to refine the EWT structure for i = 0; i < T gr ; i++ do Prune model gradually toward the pruning ratio p according to Eq. (6) and create a mask m gr,i ; Train the pruned model θ gr,i • m gr,i for one epoch; end
Step 3: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end","[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning."
The Sooner The Better: Investigating Structure of Early Winning Lottery Tickets,BJlNs0VYPB.json,"This paper carefully observes the behavior of weight magnitudes during training, finding the is a stage of saturation that is closely related to the winning lottery tickets drawing. Based on this observation the authors hypothesize that we can draw lottery tickets early but too early pruning can irreversibly hurt the learning capability for complex pattern. To remedy this and draw the tickets as soon as possible, the authors propose to adopt gradual pruning, which 1) can start early without hurting the learning capability too much; 2) avoid computation-heavy iterative pruning in previous works.

Questions:

1. Overall I am very happy with the interesting observations and analysis of the dynamics of weight magnitudes and how it can be related to the early winning lottery tickets drawing. But how valuable is it for practical use? In practice, we cannot know in advance when to start (gradual) pruning.

2. In Fig.1, what does it mean if we perform weight-magnitude based pruning at 10th epoch but rewind the weight to the 20th epoch? Is there a baseline network that is normally trained straight to the end and to which we rewind all pruned models?

3. I am not quite convinced by the experiment of Fig. 4 and argument at the bottom half of page 5. I buy the intuition that pruning too early might irreversibly hurt the capability of learning complex pattern. But I have trouble understanding how the experiment of Fig. 4 supports this intuition. The curve of retraining with smaller LR (0.01) has the save trend as the baseline and retraining with larger LR (0.1). Retraining only one epoch can hardly convince me of its relationship with learning capability. Also, for the experiment in Fig. 5, to validate the proposed hypothesis, it's more valuable to provide results around the claimed turning point, i.e. around 100 epoch instead of suddenly jumping from 20 epoch to 120 epoch.

4. In Tab. 2, the results of ResNet56 with gradual pruning is not presented. In Tab. 3, the results of ResNet50 with one-shot pruning is not presented. It would be better to have these results for clear comparison.

Overall, I love the empirical observation of weight magnitudes and think it would help the community to understand lottery tickets and training process of deep models.

Update:
The response from the authors addressed some of my questions and more experiments were added per my suggestions. However, also considering the authors' response to R#1 and R#3, I don't think it's strong enough for me to raise my score. Therefore I will keep my current score.","[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Extended lottery ticket experiments using ResNet20 on CIFAR10. From the baseline ResNet20 training, weight-magnitude based pruning is applied at different epoch (columns, labeled as s) to obtain the sparsified structure. Each structure is then rewound to the weights from another epoch (rows, labeled as v), where v=0 indicates rewind to the initial weight. Lottery tickets emerge much earlier before the full training ends, achieving matching accuracy compared to the conventional winning ticket, i.e., (s=200,v=200).
Table 2: Frankleet al. (2019)  suggests that a lottery ticket found after a baseline training can achieve the accuracy of the baseline model if it is initialized with the weight of the baseline model after a few epochs, which is called rewinding. We extend this exploration toward different structures obtained at different epochs of the baseline training (via magnitude-based weight pruning).
Table 3: Fig. 11Fig.1shows the validation accuracy after retraining of a ResNet20 model on CIFAR10 pruned at different epochs of its baseline training. The same learning rate schedule of 0.1 reduced by 10x at epoch 120 and 160 is used for both the baseline training and retraining, and the total number of epochs is 200. The rows (= v) correspond to the different rewinding epoch, whereas the columns (= s) correspond to the different epoch that we apply one-shot pruning (pruning rate= 80%). For example, the validation accuracy along the lottery ticket configurations of (s = 200, v = 0 ∼ 200) resembles the phenomena of ""rewinding"" observed inFrankle et al. (2019).
Table 4: Figure 2 :2Figure 2: (a) Change of the standard deviation of weights (W std ) in each layer during the training of ResNet20 on CIFAR10. In the beginning of the training, the weights of different layers change in different rate, dominated by the gradient terms. When the training evolves, weight magnitude is primarily determined by the interplay between learning rate and weight decay, resulting in parallel movement of W std . (b) Change of per-layer pruning rate over the epochs. Due to regular shift of W std at the later epochs of training, the per-layer pruning rate converges to a saturating point, emerging stable structure for pruning.
Table 5: Figure 4 :4Figure4: The pseudo-pruned-then-retrained (PPR) models for CIFAR10 ResNet20, which exhibit increasing capability of memorizing the complex patterns over the epochs. Validation accuracy for the baseline training and validation accuracy after pruning without retraining are also included for comparison.
Table 6: Figure 5 :5Figure 5: Left: Memorization capacity (i.e., training accuracy) of the models pruned at 3 different epochs. The models pruned at epoch 120 and 200 show almost identical memorization capacity, whereas the model pruned at epoch 20 exhibits lower memorization capacity. Right: Memorization capacity of models pruned at different epochs (training data size = 15000).
Table 7: Figure 6 :6Figure 6: The results of PPR accuracy check (retraining learning rate=0.01) for (a) ResNet56 on CIFAR10 (sampled at every epoch) and (b) ResNet18 on ImageNet (sampled every 5 epochs).
Table 8: GP: gradual pruning, IP: iterative method, EWT: early winning ticket, WT: winning ticket * : pruning rate (PR). * * : number of pruning epochs + regular retraining epochs † : the delta of accuracy is measured against the baseline accuracy. Bold: highlight of comparison
Table 9: Figure 8 :8Figure 8: Revisiting the extended lottery ticket experiments of Fig. 1, but with the group size (gs) of 8. As before, (s, v) stands for (epoch drawing the sparse structure, epoch for rewinding). In contrast to Fig. 1, the winning tickets found when gs=1 (green color in Fig. 1) disappear as the group size becomes 8.
Table 10: Figure 9 :9Figure9: Impact of structured sparsity on the saturation of pruning accuracy. The x-axis corresponds to the epoch when the model is 80% pruned. The dotted lines correspond to the moving average of the accuracy to show trends. The larger the group size, the later the accuracy of the pruned models converges at.
Table 11: Figure 10 :10Figure 10: Mode connectivity: Lottery tickets of ImageNet-ResNet50 and CIFAR10-ResNet20 drawn from different epochs are linearly interpolated, then the accuracy is measured for each interpolation coefficient. It can be observed that only the early winning tickets are connected to the winning tickets (drawn at the end of training, i.e., epoch 90 for ImageNet and epoch 200 for CIFAR10).
Table 12: Memorization capacity (i.e., train accuracy) comparison between 1-shot and 4-shot pruning.
Table 13: One-shot and gradual pruning at different epochs for ResNet20 and ResNet56 on CIFAR10. To clarify randomness effect, the average accuracy (%) over 10 different runs is reported. ±0.07) 91.20 (±0.28) 91.49 (±0.27) 91.37 (±0.24) 91.63 (±0.19) 91.63 (±0.21) ResNet56 92.94 (±0.28) 93.60 (±0.18) 94.07 (±0.16) 94.24 (±0.19) 94.29 (±0.17) 94.34 (±0.14)
Table 14: One-shot and gradual pruning for ResNet18 and ResNet50 on ImageNet.
Table 15: Performance gain by our gradual pruning strategy on CIFAR-10 and ImageNet.
Table 16: Memorization capacity comparison for different group size.

[INTRODUCTION]
Deep Neural Networks (DNNs) achieve superior accuracy in a wide spectrum of applications through the use of very large and deep models (Goodfellow et al. (2016)). These high-capacity but complex models, however, pose a tremendous challenge for their deployment, particularly in resource-constrained edge environments. Over the years, many techniques have been developed to compress the models to a compact counterpart to alleviate the computational costs. Among these techniques, pruning less important parameters to obtain a compact sub-network has emerged to be a popular and efficient approach (Cun et al. (1990); Han et al. (2015)). In search of these sub-networks, new intuitions are also built up for understanding the DNN working mechanism, one example of which is the recently proposed ""lottery ticket hypothesis"" by Frankle & Carbin (2018).
The lottery ticket hypothesis states that, once a sub-network is found to match the accuracy of the original neural network, the sub-network (i.e., lottery ticket) together with its initialized weights can be trained in isolation and still achieve accuracy comparable to the original network within a similar number of iterations. This conjecture intrigues discussions on a series of topics, such as the importance of initialization scheme (Liu et al. (2018), Zhou et al. (2019)), the role of over-parameterization in training (Frankle et al. (2019)) and even the transferability of the ""winning ticket"" (S. Morcos et al. (2019)). However, all these discussions start from the point that the winning ticket has been obtained after painfully long iterative pruning procedures, which often take up to thousands of epochs. When and how a winning ticket can be found in the course of pruning procedures has not been studied; most of the prior works use a traditional way of repeated cycles of pruning and retraining, which makes such study less practical.
In this work, we provide insights to find the winning tickets early. We start with an interesting observation that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule finishes. This leads to our in-depth investigation of the weight magnitude; we discover that a model saturates early but not too early under stochastic gradient descent (SGD) optimization. To understand this characteristic of the winning lottery tickets, we conjecture that pruning a premature model causes the loss of capability in learning complex patterns, leading to accuracy degradation. We confirm this conjecture with the empirical evidence as well as the quantitative analysis based on the memorization capacity. Using this analysis framework, we further provide a reasoning behind the success of the gradual pruning over the one-shot pruning. Based on these insights, we identify the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate obtained within only 22% of the total epochs for iterative pruning. This promising outcome not only sheds light on understanding the optimization behavior of the pruned models but also enables performance gain for fast training of the pruned models.

[RELATED WORK]
The lottery ticket hypothesis was first proposed in Frankle & Carbin (2018) where the presence of a trainable subnetwork that achieves compelling final accuracy with the inherited initial values for the un-pruned connections is demonstrated. This paper also argued that the initial weight of the original network is essential for maintaining good accuracy when the model is sparsified. This claim has been extended to the over-parameterized neural networks on the larger datasets in Frankle et al. (2019) with the notion of ""rewinding""; the authors claim that rewinding of the weights not to the initial values but to the values after a few epochs can stabilize the accuracy of the winning lottery tickets. As a follow-up work, Zhou et al. (2019) studied the critical components of the lottery tickets such as zeros, signs and the super-mask. Also, S. Morcos et al. (2019) investigated the transferability of winning tickets obtained in one dataset to the network of a similar structure for the other datasets. But none of these focused on the structure of the winning lottery tickets; it involves several repetitions of the full training schedule for iterative pruning and retraining, often taking hundreds to thousands of epochs. In this work, we demonstrate that many winning tickets can be found in the early stage of the baseline training schedule, drastically reducing the computational effort to obtain them.
Most work on the lottery ticket hypothesis, including this work, rely on magnitude-based weight pruning for identifying unimportant weights to be pruned (usually via global sorting). Such an intuitive method was first proposed in Han et al. (2015) and became popular. Afterwards, more complex pruning methodologies have been presented to improve pruning performance, such as having different pruning criteria , Wen et al. (2016), ) or different pruning granularity (Mao et al. (2017), Molchanov et al. (2019)). While these attempts offer insights on training pruned models, there is little discussion about the interesting interplay between the pruning criteria and the structure of a model pruned by it. In this work, we reveal that the pruned structure obtained based on the weight magnitude has tangible impact on the final accuracy, and further propose a way to quantitatively distinguish good structures for pruning.
There have been various strategies to apply pruning to the neural networks. Iterative pruning by Han et al. (2015) involves several repetition of pruning (with gradual increase of the pruning rate) and retraining. One can increase the repetition cycles arbitrarily large to achieve good accuracy at high pruning rate; Frankle & Carbin (2018) employed iterative pruning with hundreds to thousands of pruning epochs to match the baseline accuracy for the challenging neural networks. On the other hand, gradual pruning introduced by Zhu & Gupta (2017) determines the pruning rate and frequency via a polynomial equation as a function of the starting and ending epochs as well as the target pruning rate. Although it provides a systematic pruning schedule, there is lack of discussion about when to start the gradual pruning. Lastly, Lee et al. (2018) proposes a method of pruning weights at initialization. This method takes most advantage in performance since a pruned model can be obtained without any expensive retraining procedures. However, its effectiveness has not been demonstrated for the challenging neural networks on large datasets such as ImageNet. In contrast, we propose a mechanism to identify the winning lottery ticket early in the course of baseline training so that we can avoid costly iterative pruning while maintaining the baseline accuracy.

[STRUCTURE OF EARLY WINNING TICKETS]
In this section, we extend the lottery ticket hypothesis by Frankle & Carbin (2018); Frankle et al. (2019) to discuss the early winning tickets. The lottery ticket hypothesis can be summarized as: for a given network of f (x; w 0 m 0 ) with the initial weight w 0 and the mask of all ones m 0 , there exists a winning lottery ticket m f where |m f |/|m 0 | = 1 − p% (p is the pruning rate) and training of f (x; w v m f ) for 0 < v f achieves test accuracy comparable to the baseline f (x; w f m 0 ).
There are two main components of a winning lottery ticket: the sparsified structure m f and the weight that initializes it, w v . In the previous work, m f has been obtained only after expensive iterative pruning. We characterize the structure of the early lottery ticket m s where s f , then propose a strategy for finding it early.  Interestingly, Fig. 1 further demonstrates that the winning tickets emerge at much earlier epochs of the baseline training; the lottery ticket configurations of (s ≥ 100, v ≥ 5) achieve almost the same accuracy as the accuracy of (s = 200, v = 200). The accuracy then gradually decreases as s ≤ 80. This result implies two important aspects: 1) a winning ticket can be found in the middle of the baseline training so that one can avoid expensive iterative pruning used in the prior work, and 2) the winning ticket, however, does not emerge arbitrarily early in the process of training. In the following sections, we investigate the characteristics of these early winning tickets. In particular, we focus on their structure, as the weight initialization is not the major factor provided a proper rewinding.

[ANALYSIS ON WEIGHT MAGNITUDE]
As the first step of understanding the characteristics of the early winning tickets, we focus on the important quantity of pruning, weight magnitude. At pruning, we determine the structure of the sparsified model based on the rank of the weight magnitude (via global sorting). Therefore, the change in the weight magnitude during training has large impact on the lottery ticket structure.
In Fig. 2a, we measure the standard deviation of weight (W std ) for each layer in the course of training ResNet20 on CIFAR10 (note: the mean of weight is typically near zero). The same learning rate schedule is used as above. The first thing to note is that the change in W std has strong correlation with the learning rate change. In particular, different layers show different rate of change in W std when the learning rate is 0.1, but from the second learning rate (after epoch 120), all the weights follow a very similar decreasing trend.
This trend in W std can be understood via steps of stochastic gradient descent. From the typical setting of weight update with momentum and weight decay, we have:
v t+1 = mv t + (λw t + w g,t ), w t+1 = w t − ηv t+1 . (1
)
where w t , w g,t and v t are the weight, gradient and momentum at step t, respectively; m is the momentum factor, η is the learning rate and λ is the weight decay factor. After n steps,
v t+n = m n v t + n k=1 (m n−k w g,t+k−1 ) + λ n k=1 (m n−k w t+k−1 ),(2)
w t+n = (1 − ηλ) n w t − η n k=1 ((1 − ηλ) n−k (mv t+k−1 + w g,t+k−1 )).(3)
From this derivation, we can see that the two factors determine the mode of change in W std . When learning rate is high, the gradient terms play the major role in weight update. On the other hand, if the gradient activity becomes low, e.g., when the learning rate is low and the gradients oscillate around zero, we can further simplify Eq. 3. Assume that m n v t approaches to zero when n is relatively large, w t+k−1 ≈ w t , and
n k=1 ((1 − ηλ) n−k w g,t+k−1 )
and n k=1 (m n−k w g,t+k−1 ) approach to zero as the gradients oscillate around zero, we have v t+i ≈ λwt 1−m . Then w t+n is approximated as,
w t+n ≈ (1 − ηλ) n w t − η n−1 i=0 ((1 − ηλ) i mλw t 1 − m ). (4
) Note that (1 − ηλ) n ≈ 1 − nηλ since ηλ 1 and n−1 i=0 ((1 − ηλ) i ) ≈ n. Thus, w t+n ≈ (1 − nηλ)w t − nηλmw t 1 − m = (1 − nηλ 1 − m )w t .(5)
In other words, when the gradient activity is low, the change in W std is dominated by the both the learning rate and the weight decay. As an example, in Fig. 2a, W std is decreased with the slope following Eq. 5 when learning rate is low; W std of layer 10 is decreased by 1.5e-3 in 782 updates of CIFAR10 (with λ =1e-4 and m = 0.9), confirming the slope from Eq. 5.
These two modes governing the change of weight magnitude are critical for understanding the behavior of pruning.
In particular, the interplay between learning rate and weight decay causes the per-layer pruning rate to converge after enough number of epochs. Fig. 2b shows the pseudo per-layer pruning rate, where we just measure the layer-wise pruning rate without really pruning out the weights in the model, for the same CIFAR10 training experiment. As the figure shows, the per-layer pruning rates saturate around epoch 100, indicating that pruning before that would select the weights based on the premature model.
Moreover, although the weight magnitude is an important factor, we discovered that the ranking of individual weight does not play a critical role in winning lottery tickets. Specifically, we empirically show that there exist many winning lottery tickets that are vastly different in terms of its sparse structure m v . Fig. 3 shows the hamming distance of the sparse structure of the lottery tickets at different configuration (s, v). Note that the two distant lottery tickets (e.g., (s = 200, v = 100) and (s = 100, v = 100)) show large hamming distance of 0.159 (where x/y = 200/100) while achieving the equally good accuracy as shown in Fig. 1, indicating that the ranking of the weights itself can not explain the quality of the structure of the lottery ticket.
Figure 3: The hamming distance of the sparse structure of the lottery tickets at different configuration for ResNet20 on CIFAR10.
x/y denotes the distance between (s = x) and (s = y). A large distance between two lottery tickets with equally good accuracy suggests the existence of many winning tickets.

[UNDERSTANDING IMPACT OF PRUNED STRUCTURE]
The weight magnitude analysis motivates us not to prune a model too early. It is also implied that a distance-based metric might not reveal the winning structure of the model at different epochs. To understand the early winning structures, we further investigate the impact of the sparsified structure on the final accuracy. Considering that a highly pruned network is likely to have limited learning capability, we make a conjecture that the accuracy degradation of a pruned model is due to the loss of capability for learning complex pattern if pruned too early. Recently, Li et al. (2019) reveals that the training behavior of a sufficiently over-parameterized model with non-linearity highly depends on the learning rate schedule, where a model tends to memorize the complex patterns when a small learning rate is applied while learning simple patterns with a large learning rate. To validate this claim in the context of pruning, we construct an experiment where a model is pseudo-pruned at every epoch of the baseline training then retrained with large or small learning rate for just one epoch. For the pseudo-pruned-then-retrained (PPR) model at each baseline epoch, we measure the validation accuracy recovered from the retraining.
Fig. 4 shows the result of this experiment on CIFAR10 ResNet20. When it is retrained with the large retraining learning rate (= 0.1), in just 1-epoch retraining, the PPR models from all the baseline epochs achieve the accuracy matching with the baseline accuracy. This indicates that those pruned models maintain the capability of learning the simple patterns. Whereas, when the small retraining learning rate (= 0.01) is used, the accuracy of the PPR models pruned at later epochs (epoch 100-200) is higher than the accuracy of the models from earlier epochs (epoch 20-60). This reveals that the PPR models from different epoch exhibit varying capability of learning complex patterns. In particular, the accuracy of the PPR models increases until around 100 epoch of the baseline training, then it saturates. Note that this coincides with the epoch when the early winning tickets emerges in Fig. 1.
Based on this observation, we hypothesize that the models pruned at 100 epoch of the baseline training or later will preserve the capability of learning complex patterns. To validate this hypothesis, we conducted the memorization test proposed by Boo et al. (2019), where a model is trained with training data of varying size with the randomized labels.  A model has ""high memorization capacity"" if it achieves high training accuracy for a large data size. Fig. 5 (left) shows the training accuracy of the models pruned at different epochs, measured from 10 independent simulations (the average capacity shown as solid lines, and min and max shown as the shaded regions). As the training data size increases, the memorization capacity decreases. The model pruned too early (i.e., at epoch 20) suffers higher degradation in the memorization capacity compared to the models pruned at later epochs (i.e., at epochs 120 or 200). Also, Fig. 5 (right) shows that those models pruned at later than 100 epochs (i.e., the early winning tickets) exhibit similar memorization capacity. This result not only confirms our conjecture on the impact of pruned structure to the capability of learning complex patterns, but also leads us to employ the PPR accuracy check as a computationally reasonable heuristic to discover the early winning tickets.

[UNDERSTANDING GRADUAL PRUNING]
Gradual pruning is a popular pruning approach that applies pruning gradually over a period of training. For example, gradual pruning proposed by Zhu & Gupta (2017) provides a systematic way to schedule iterative pruning as follows:
s t = s f + (s i − s f ) 1 − t − t 0 n∆t 3 ,(6)
where s f and s i are the final and initial sparsity, and t is the time when pruning is applied. Eq. 6 determines how much sparsity is applied at a certain time step t. But it is still a user-hyper-parameter to decide when t 0 or how often ∆t apply pruning. Based on the insights we discussed in the previous section, we explain why gradual pruning helps to obtain better lottery tickets.
The reasoning behind the gradual pruning is that the model can be changed graciously if the pruning is applied gradually.
In terms of the early winning ticket analysis, there are two factors playing the critical role: 1) by applying low pruning rate in the beginning, the structure found at that pruning level can preserve the memorization capacity better, 2) once pruning is applied, the remaining weights of the pruned model is updated via SGD, granting a chance for the pruned model to adopt its weights toward better accuracy. Thanks to these two factors, a structure with better memorization capacity can be found when the increased pruning rate is applied next time.
Table 1 confirms this explanation using the CIFAR10 ResNet20 example. In this experiment, we perform the memorization test for the 4-step gradual pruning as well as four 1-step pruning at the corresponding epoch for comparison. The gap in memorization capacity is maintained across the different epochs, demonstrating that the memorization capacity is maintained thanks to the gradual application of pruning and the evolution of weights after pruning. This suggests a strategy for gradual pruning where 1) we can use gradual pruning to reduce the loss of capability in learning complex patterns, and 2) by applying a smaller pruning rate in the beginning, we can start pruning early and finding the winning lottery tickets faster. The benefit of this strategy combining gradual pruning with the early winning tickets will be demonstrated in Sec. 4. In this section, we demonstrate our strategy of finding the early winning tickets over popular neural networks on CIFAR10 and ImageNet. The detail experimental setup is described in Appendix A. We perform the lottery ticket experiments of Sec. 3.1 for both one-shot and gradual pruning with the pruning rate of 80%.We also conduct the PPR accuracy check of Sec. 3.3 to predict from which epoch the early winning ticket can be found. By comparing the two results, we demonstrate that the proposed heuristic for finding early winning tickets works robustly across the networks and the datasets. Furthermore, we showcase our gradual pruning strategy by comparing the performance in terms of the accuracy and the required pruning epochs with the existing lottery ticket approaches.

[EXPERIMENTS ON CIFAR10]
Table 2 summarizes the lottery ticket experiments of ResNet20 and ResNet56 on CIFAR10 dataset. In case of one-shot pruning, the winning tickets can be found from epoch 100. In case of gradual pruning, the gradual pruning schedule can start from epoch 75 (which is earlier than the one-shot pruning). Note that the gradual pruning can achieve better accuracy as it can preserve higher memorization capacity as discussed in Sec. 3.4. Fig. 6a shows the pseudo-pruning curve that highlights the presence of the winning tickets from the epoch around 100, which is consistent with the results of the lottery ticket experiment in Table 2. 

[EXPERIMENTS ON IMAGENET]
Table 3 summarizes the lottery ticket experiments on ImageNet dataset, and the predicted results of the early winning tickets from the pseudo-pruning are shown in Fig. 6b. Similar to the CIFAR10 experiments, the results of the lottery ticket experiment matches with the results from the PPR accuracy check (which indicates epoch 45 for early winning tickets), demonstrating the robust behavior of the proposed strategy of finding the early winning tickets. Furthermore, the gradual pruning at the early winning tickets achieves the accuracy near to the baseline (ResNet18: baseline=69.7% vs ours=69.24%, ResNet50: baseline=75.7% vs ours=75.31%), showcasing the superior quality of the winning tickets discovered by the proposed gradual pruning strategy.  

[PERFORMANCE GAIN FROM EARLY WINNING TICKET]
To demonstrate the performance gain from our gradual pruning strategy, the pruning results of the proposed algorithm (GP + EWT) and the previous implementation of iterative pruning (IP) along with the winning ticket (WT) of Frankle & Carbin (2018) are shown for the ResNet variants on CIFAR10 and ImageNet in Table 4. Our GP+EWT algorithm consistently achieves high pruning rate with the number of pruning epochs even lower than the regular retraining epochs for all the models, i.e., 80% pruning with negligible accuracy degradation on ResNet50 for ImageNet. In contrast, the iterative pruning approach (IP+WT) achieves the similar accuracy at the cost of more than 4.5× increase in the total training epochs.

[CONCLUSION]
In this work, we investigate the structure of the winning lottery ticket, which leads to the computationally efficient discovery of the winning lottery tickets. Based on a careful analysis of the characteristics of the structure of the winning lottery tickets, we proposed a computationally reasonable heuristic to identify when the early lottery tickets emerge. Furthermore, we proposed a gradual pruning strategy incorporating the early lottery ticket analysis to achieve high accuracy at large pruning rate. This results in the state-of-the-art accuracy on ResNet50 for 80% pruning only within 22% of the total epochs for iterative pruning.   We try to find the winning tickets early for ResNet20 (baseline accuracy: 92.5%) and ResNet56 (baseline accuracy: 94.25%) on CIFAR10 dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 200 epochs. The initial learning rate for the first 120 epochs is 0.1 and decrease to 0.1× every 40 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.

[A.2 EXPERIMENTS ON IMAGENET]
We further try to find the winning tickets early for ResNet18 (baseline accuracy: 69.7%) and ResNet50 (baseline accuracy: 75.7%) on ImageNet dataset. All the baseline models are trained using SGD optimizer with momentum (m =0.9) and weight decay (λ =1e-4) for 90 epochs. The initial learning rate for the first 30 epochs is 0.1 and decrease to 0.1× every 30 epochs. For the lottery ticket experiments, the retraining schedule is the same as the baseline learning rate schedule. Following convention, we do not prune the first and the last layers of the ResNets.
Figure 7: Structured pruning with variable group size. We consider a group of weights along the channel dimension with a varying group-size (gs). In case of gs=1, it is the same as the element-wise pruning.

[B.1 IMPACT OF STRUCTURED SPARSITY ON LOTTERY TICKET HYPOTHESIS]
In this section, we expand our scope of analysis on the characteristics of the early winning tickets toward the sparsity obtained via structured pruning. Structured pruning is a popular method that prunes weights by a group. For example,  proposes channel-pruning, where a group consists of weights along each channel. The main motivation of looking at the structured sparsity is that there is a significant discrepancy in the best pruning rate we can achieve between the element-wise pruning (i.e., group-size=1) and the structured pruning (i.e., group-size=channel). For example, on CIFAR10-VGG19, the element-wise pruning achieves 95% of pruning rate whereas the channel-pruning achieves 70% of pruning rate for the same level of accuracy (Liu et al. (2018)). Although such discrepancy has been observed for quite a while, there is no in-depth investigation to understand why. To find out the reasoning behind it, we analyze the impact of structured sparsity with varying group sizes in the context of the lottery ticket hypothesis, as illustrated in Fig. 7.
First, we revisit the extended lottery ticket experiments of Fig. 1 while applying group sparsity along the channel dimension (instead of pruning individual weights). Fig. 8 shows the results when the group size (gs) is 8. Comparing  it with Fig. 1, there are two notable points; 1) there is > 1% accuracy degradation when the group size is increased from 1 to 8, and 2) when gs = 8, the trend that the tickets drawn at epochs s > 100 achieve the higher accuracy disappears (as highlighted with a red rectangle in the figure). (Note that the two figures achieve the similar accuracy when s < 60 (colored in yellow), but the tickets with high accuracy (colored in green) can only be seen in Fig. 1.) The former observation makes sense as the more regularization on the sparse weight structure would result in the lower accuracy. But the latter observation is quite surprising; it seems that a distinct behavior of the lottery ticket hypothesis (i.e., the opportunity of finding winning tickets given a proper initialization) is disrupted as a group structure is asserted on the sparsity.
We can find a clue on this disrupted behavior by employing the memorization capacity analysis. Table 5 shows the results of memorization capacity experiments on CIFAR10-ResNet20 with an increasing group size gs = {1, 2, 4, 8, 16, 32, 64}.
The memorization capacity degrades significantly as the group size increases. Intuitively, by asserting a large group size, the model's expressivity is degraded and it becomes harder to learn the complex patterns, leading to accuracy loss. Note that we had a similar observation when the tickets are drawn too early (cf.,s < 60 in Fig. 1). From this, we can hypothesize that the winning tickets disappear as the memorization capacity is degraded, which can be observed when the tickets are drawn too early or the group sparsity is forced.
Fig. 9 further demonstrates the impact of structured sparsity on the saturation of pruning accuracy. For different group sizes, the figure shows the accuracy of CIFAR10-ResNet20 80% pruned at varying epochs. It can be observed that the models with the larger group size not only achieve the lower accuracy but also converge at earlier epochs. (E.g., the knee points for gs = 1 and gs = 16 are around 90 and 50 epochs, respectively.) This experimental result supports our claim that the larger group size enforced in the sparse structure results in the earlier convergence of the pruned models that misses the winning tickets. 

[B.2 MODE CONNECTIVITY OF EARLY WINNING TICKETS]
Mode connectivity is a phenomenon that SGD solutions are connected through paths of approximately equal loss (Draxler et al. (2018)). It provides the perspective of how well the models trained via a proposed method can generalize.
In the context of early winning tickets, we for the first time reveal that the lottery tickets drawn early based on PPR are indeed connected from the one obtained by IMP, whereas the tickets chosen too early do not.
For the mode connectivity experiments, we draw three lottery tickets -one drawn too early (i.e., a premature ticket, PT), one drawn based on PPR (i.e., an early winning ticket, EWT), and one at the end of the training schedule (i.e., a winning ticket, WT). Note that all the tickets are trained from the same initial weights and with the same retraining schedule.
Then we linearly interpolate PT and EWT with WT, and plot the test error for different interpolation coefficient (from 0 to 1). Fig. 10 shows the results of the mode connectivity experiments for ImageNet and CIFAR10. For both datasets, the linearly interpolated models between PT and WT depict a barrier, indicating that PT and WT are disconnected. Whereas, the linearly interpolated models between EWT and WT show consistently low test error. Note that such linear mode connectivity is observed even when 1) the weights are sparse (i.e., 80% pruning rate) and 2) the mask distance is large (cf., Fig. 3). This intriguing observation counters some observations of prior work; Draxler et al. (2018) claims that it takes a careful search (instead of a simple linear interpolation) to find a pass connecting the modes, and the existence of such a non-linear pass requires enough number of parameters. Our novel observation of early lottery tickets provide a new aspect of sparse model training; the sparse structures mature early in the training, and they converge to a flat local minimum even if its shape is vastly distant. A deeper investigation about the sparse structure of the lottery tickets and the mode connectivity is an interesting future research topic.

[C EARLY WINNING TICKET ALGORITHMS]
In this section, we describe the detail algorithms for finding early winning tickets via one-shot and gradual pruning.

[]
Algorithm 1: Early Winning Ticket Identification with One-Shot Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then Set the model parameters to θ i • m P P R,t ; break; end t++; end end
Step 2: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end Algorithm 2: Early Winning Ticket Identification with Gradual Pruning Require: Initial weights θ 0 , learning rate schedule LS := {η(n)}, n = 0, 1, ..., N − 1, pruning ratio p, PPR accuracy checking period T P P R , PPR accuracy saturation detection threshold k P P R , PPR retraining learning rate η P P R , gradual pruning period T gr ; Initialize model with θ 0 , t = 0;
Step 1: Early winning ticket (EWT) identification by tracking PPR accuracy for i = 0; i < N ; i++ do Train the model with learning rate η(i) for one epoch; if i % T P P R = 0 then Save a copy of the model and optimization state; Prune p% of the model parameters and create a mask m P P R,t ; Train the pruned model θ i • m P P R for one epoch and obtain the test accuracy Acc P P R,t ; Restore the model and optimization state with the saved copy; if Acc P P R,t <= Acc P P R,t −1 for t = t, t − 1, ..., t − k P P R + 1 then break; end t++; end end
Step 2: Gradual Pruning to refine the EWT structure for i = 0; i < T gr ; i++ do Prune model gradually toward the pruning ratio p according to Eq. (6) and create a mask m gr,i ; Train the pruned model θ gr,i • m gr,i for one epoch; end
Step 3: Retraining EWT with the same LS for i = 0; i < N ; i++ do Train the pruned model with learning rate η(i) for one epoch; end","[TITLE]
THE SOONER THE BETTER: INVESTIGATING STRUCTURE OF EARLY WINNING LOTTERY TICKETS

[ABSTRACT]
The recent success of the lottery ticket hypothesis by Frankle & Carbin (2018) suggests that small, sparsified neural networks can be trained as long as the network is initialized properly. Several followup discussions on the initialization of the sparsified model have discovered interesting characteristics such as the necessity of rewinding (Frankle et al. (2019)), importance of sign of the initial weights (Zhou et al. ( 2019)), and the transferability of the winning lottery tickets (S. Morcos et al. ( 2019)). In contrast, another essential aspect of the winning ticket, structure of the sparsified model, has been little discussed. To find the lottery ticket, unfortunately, all the prior work still relies on computationally expensive iterative pruning. In this work, we conduct an in-depth investigation of the structure of winning lottery tickets. Interestingly, we discover that there exist many lottery tickets that can achieve equally good accuracy much before the regular training schedule even finishes. We provide insights into the structure of these early winning tickets with supporting evidence. 1) Under stochastic gradient descent optimization, lottery ticket emerges when weight magnitude of a model saturates; 2) Pruning before the saturation of a model causes the loss of capability in learning complex patterns, resulting in the accuracy degradation. We employ the memorization capacity analysis to quantitatively confirm it, and further explain why gradual pruning can achieve better accuracy over the one-shot pruning. Based on these insights, we discover the early winning tickets for various ResNet architectures on both CIFAR10 and ImageNet, achieving state-of-the-art accuracy at a high pruning rate without expensive iterative pruning. In the case of ResNet50 on ImageNet, this comes to the winning ticket of 75.31% Top-1 accuracy at 80% pruning rate in only 22% of the total epochs for iterative pruning."
Assessing the validity of saliency maps for abnormality localization in medical imaging,02X3kfP6W4.json,"This paper concerns the assessment of saliency map validity.  It was shown that GradCAM is superior to other methods in terms of model and parameter randomization. This is a useful results, as the interpretability that saliency mapping enables is becoming more and more important to help visualize why deep networks are making their decisions. However, there was a lack of discussion of these results in this paper - are there any possible explanations for why GradCAM is performing better? Furthermore, the images in the figures hard to see. They should be larger and as much whitespace should be removed between images.","[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: (a) Visualizations from two independently trained InceptionV3 models (b) Visualizations from an InceptionV3 model (top row) and a DenseNet121 model (bottom row) (c) Comparison of dice score differences across saliency methods and architectures (d) Comparison of intra-vs. inter-architecture repeatability using Spearman rank correlation

[INTRODUCTION]
Saliency maps have become a popular approach for post-hoc interpretability of Convolutional Neural Networks (CNNs). (Adebayo et al., 2018) These maps are designed to highlight the salient components of the input images that are important to the model prediction. As a result, many deep learning medical imaging studies have used saliency maps to rationalize model prediction and provide localization. (Rajpurkar et al., 2017;Bien et al., 2018;Mitani et al., 2019) However, the validity of saliency maps has been called into question in a recent study showing that many popular saliency map approaches are not sensitive to model weight or label randomization for models evaluated on several datasets. (Adebayo et al., 2018) In this study, we extend this work by evaluating popular saliency map methods both quantitatively and qualitatively for classification models trained on the RSNA Pneumonia dataset. (Shih et al., 2019) Specifically, we assess the performance of these methods in localizing abnormalities in medical imaging by quantifying overlap with ground truth bounding boxes. Furthermore, we assess the effect of model weight and label randomization on localization performance. Lastly, we empirically study repeatability of the saliency maps, both within the same model architecture and across different model architectures.

[MODEL AND DATA RANDOMIZATION]
The saliency methods examined in our experiments are Gradient Explanation (Simonyan et al., 2013), Smoothgrad Integrated Gradients (IG) (Sundararajan et al., 2017), GradCAM (Selvaraju et al., 2016), XRAI (Kapishnikov et al., 2019), and Smoothgrad (Smilkov et al., 2017). Along with using Spearman rank correlation to compare maps before and after model weight and label randomization, we leverage the ground-truth bounding box coordinates Figure 1: a) Visualization of saliency maps under cascading randomization on InceptionV3 (performance before randomization: AUC=0.98, precision=0.92) (b) Dice scores under cascading randomization (c) Spearman rank correlation under cascading randomization provided in the RSNA Pneumonia dataset to establish a quantitative baseline using the dice metric. To investigate the sensitivity of saliency methods under changes to model parameters, we employ cascading randomization. (Adebayo et al., 2018) We observed that among these saliency techniques, GradCAM degraded with model randomization to a large degree whereas the other methods did not (Fig 1). This is also verified in a label randomization experiment shown in Fig 2(c) wherein we randomly flipped the labels and retrained the model to observe the difference in the dice scores of the saliency maps. In both the tests, it can be observed that gradient explanation, Smoothgrad IG, and XRAI do not degrade significantly under randomization, suggesting an undesirable invariance to model parameters and labels.

[REPEATABILITY AND REPRODUCIBILITY]
We also conducted repeatability tests on these saliency methods by comparing maps from a) models with the same architecture trained independently (intra-architecture repeatability) 

[DISCUSSION AND CONCLUSION]
In this study, we evaluated the performance of several popular saliency methods on the RSNA Pneumonia Detection dataset in regards to their localization capabilities, robustness to model parameter and label randomization, as well as repeatability and reproducibility with model architectures. It was found that GradCAM showed superior sensitivity to model parameter and label randomization, and was highly agnostic to model architecture. In future studies, we will further examine the effect of different model architectures on saliency maps and validate our findings on a separate medical imaging dataset.

[ACKNOWLEDGMENTS]
We would like to thank Julius Adebayo for providing us with the cascading randomization code used in his work. (Adebayo et al., 2018) Research","[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture."
Assessing the validity of saliency maps for abnormality localization in medical imaging,02X3kfP6W4.json,"CNN interpretability methods are used more and more in medical image analysis. The authors present a thourough evaluation of several of these methods (localisation capabilities, robustness to model parameter and label randomisation, repeatability and reproducibility with model architectures) extending the work first proposed by Adebayo et al.. This work is very interesting and was most needed. ","[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: (a) Visualizations from two independently trained InceptionV3 models (b) Visualizations from an InceptionV3 model (top row) and a DenseNet121 model (bottom row) (c) Comparison of dice score differences across saliency methods and architectures (d) Comparison of intra-vs. inter-architecture repeatability using Spearman rank correlation

[INTRODUCTION]
Saliency maps have become a popular approach for post-hoc interpretability of Convolutional Neural Networks (CNNs). (Adebayo et al., 2018) These maps are designed to highlight the salient components of the input images that are important to the model prediction. As a result, many deep learning medical imaging studies have used saliency maps to rationalize model prediction and provide localization. (Rajpurkar et al., 2017;Bien et al., 2018;Mitani et al., 2019) However, the validity of saliency maps has been called into question in a recent study showing that many popular saliency map approaches are not sensitive to model weight or label randomization for models evaluated on several datasets. (Adebayo et al., 2018) In this study, we extend this work by evaluating popular saliency map methods both quantitatively and qualitatively for classification models trained on the RSNA Pneumonia dataset. (Shih et al., 2019) Specifically, we assess the performance of these methods in localizing abnormalities in medical imaging by quantifying overlap with ground truth bounding boxes. Furthermore, we assess the effect of model weight and label randomization on localization performance. Lastly, we empirically study repeatability of the saliency maps, both within the same model architecture and across different model architectures.

[MODEL AND DATA RANDOMIZATION]
The saliency methods examined in our experiments are Gradient Explanation (Simonyan et al., 2013), Smoothgrad Integrated Gradients (IG) (Sundararajan et al., 2017), GradCAM (Selvaraju et al., 2016), XRAI (Kapishnikov et al., 2019), and Smoothgrad (Smilkov et al., 2017). Along with using Spearman rank correlation to compare maps before and after model weight and label randomization, we leverage the ground-truth bounding box coordinates Figure 1: a) Visualization of saliency maps under cascading randomization on InceptionV3 (performance before randomization: AUC=0.98, precision=0.92) (b) Dice scores under cascading randomization (c) Spearman rank correlation under cascading randomization provided in the RSNA Pneumonia dataset to establish a quantitative baseline using the dice metric. To investigate the sensitivity of saliency methods under changes to model parameters, we employ cascading randomization. (Adebayo et al., 2018) We observed that among these saliency techniques, GradCAM degraded with model randomization to a large degree whereas the other methods did not (Fig 1). This is also verified in a label randomization experiment shown in Fig 2(c) wherein we randomly flipped the labels and retrained the model to observe the difference in the dice scores of the saliency maps. In both the tests, it can be observed that gradient explanation, Smoothgrad IG, and XRAI do not degrade significantly under randomization, suggesting an undesirable invariance to model parameters and labels.

[REPEATABILITY AND REPRODUCIBILITY]
We also conducted repeatability tests on these saliency methods by comparing maps from a) models with the same architecture trained independently (intra-architecture repeatability) 

[DISCUSSION AND CONCLUSION]
In this study, we evaluated the performance of several popular saliency methods on the RSNA Pneumonia Detection dataset in regards to their localization capabilities, robustness to model parameter and label randomization, as well as repeatability and reproducibility with model architectures. It was found that GradCAM showed superior sensitivity to model parameter and label randomization, and was highly agnostic to model architecture. In future studies, we will further examine the effect of different model architectures on saliency maps and validate our findings on a separate medical imaging dataset.

[ACKNOWLEDGMENTS]
We would like to thank Julius Adebayo for providing us with the cascading randomization code used in his work. (Adebayo et al., 2018) Research","[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture."
Assessing the validity of saliency maps for abnormality localization in medical imaging,02X3kfP6W4.json,"The paper compares different state-of-the-art approaches for visual interpretability in 2D chest X-ray classification. The comparison was made based on their localization capabilities, robustness to model parameter, label randomization, and repeatability/reproducibility with model architectures. And the abnormality localization is evaluated with the Dice's score.
1. The paper is well-written and well-organized.
2. The submission relates to the application of deep learning in the field of chest X-ray classification, which is highly relevant to the MIDL audience.
3. The proposed method is technically sound.
4. Experimental results support the claim made in the paper.
","[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: (a) Visualizations from two independently trained InceptionV3 models (b) Visualizations from an InceptionV3 model (top row) and a DenseNet121 model (bottom row) (c) Comparison of dice score differences across saliency methods and architectures (d) Comparison of intra-vs. inter-architecture repeatability using Spearman rank correlation

[INTRODUCTION]
Saliency maps have become a popular approach for post-hoc interpretability of Convolutional Neural Networks (CNNs). (Adebayo et al., 2018) These maps are designed to highlight the salient components of the input images that are important to the model prediction. As a result, many deep learning medical imaging studies have used saliency maps to rationalize model prediction and provide localization. (Rajpurkar et al., 2017;Bien et al., 2018;Mitani et al., 2019) However, the validity of saliency maps has been called into question in a recent study showing that many popular saliency map approaches are not sensitive to model weight or label randomization for models evaluated on several datasets. (Adebayo et al., 2018) In this study, we extend this work by evaluating popular saliency map methods both quantitatively and qualitatively for classification models trained on the RSNA Pneumonia dataset. (Shih et al., 2019) Specifically, we assess the performance of these methods in localizing abnormalities in medical imaging by quantifying overlap with ground truth bounding boxes. Furthermore, we assess the effect of model weight and label randomization on localization performance. Lastly, we empirically study repeatability of the saliency maps, both within the same model architecture and across different model architectures.

[MODEL AND DATA RANDOMIZATION]
The saliency methods examined in our experiments are Gradient Explanation (Simonyan et al., 2013), Smoothgrad Integrated Gradients (IG) (Sundararajan et al., 2017), GradCAM (Selvaraju et al., 2016), XRAI (Kapishnikov et al., 2019), and Smoothgrad (Smilkov et al., 2017). Along with using Spearman rank correlation to compare maps before and after model weight and label randomization, we leverage the ground-truth bounding box coordinates Figure 1: a) Visualization of saliency maps under cascading randomization on InceptionV3 (performance before randomization: AUC=0.98, precision=0.92) (b) Dice scores under cascading randomization (c) Spearman rank correlation under cascading randomization provided in the RSNA Pneumonia dataset to establish a quantitative baseline using the dice metric. To investigate the sensitivity of saliency methods under changes to model parameters, we employ cascading randomization. (Adebayo et al., 2018) We observed that among these saliency techniques, GradCAM degraded with model randomization to a large degree whereas the other methods did not (Fig 1). This is also verified in a label randomization experiment shown in Fig 2(c) wherein we randomly flipped the labels and retrained the model to observe the difference in the dice scores of the saliency maps. In both the tests, it can be observed that gradient explanation, Smoothgrad IG, and XRAI do not degrade significantly under randomization, suggesting an undesirable invariance to model parameters and labels.

[REPEATABILITY AND REPRODUCIBILITY]
We also conducted repeatability tests on these saliency methods by comparing maps from a) models with the same architecture trained independently (intra-architecture repeatability) 

[DISCUSSION AND CONCLUSION]
In this study, we evaluated the performance of several popular saliency methods on the RSNA Pneumonia Detection dataset in regards to their localization capabilities, robustness to model parameter and label randomization, as well as repeatability and reproducibility with model architectures. It was found that GradCAM showed superior sensitivity to model parameter and label randomization, and was highly agnostic to model architecture. In future studies, we will further examine the effect of different model architectures on saliency maps and validate our findings on a separate medical imaging dataset.

[ACKNOWLEDGMENTS]
We would like to thank Julius Adebayo for providing us with the cascading randomization code used in his work. (Adebayo et al., 2018) Research","[TITLE]
Assessing the validity of saliency maps for abnormality localization in medical imaging

[ABSTRACT]
Saliency maps have become a widely used method to assess which areas of the input image are most pertinent to the prediction of a trained neural network. However, in the context of medical imaging, there is no study to our knowledge that has examined the efficacy of these techniques and quantified them using overlap with ground truth bounding boxes. In this work, we explored the credibility of the various existing saliency map methods on the RSNA Pneumonia dataset. We found that GradCAM was the most sensitive to model parameter and label randomization, and was highly agnostic to model architecture."
Explanation-Based Attention for Semi-Supervised Deep Active Learning,SyxKiVmedV.json,"This paper presents a novel method to match feature similarities for selecting unlabeled samples for active learning to train a model more label-efficiently.
The major innovation is to utilize Explanation-Based Attention (EBA) mechanism to improve matching feature similarities, which has been proved effective to attribute feature importance in computer vision domains.
The experiments show it outperforms conventional uncertainty-based approaches, especially when classes are imbalanced.

Overall, this paper is a small but focused contribution on active learning and well-written, clear for readers to follow.
The presentation can be improved with a more detailed description of notations (e,g. N_b and V_a are not explained, though it's easy to guess their meanings).
An illustrative figures of workflow mentioned in the section ""summary for the proposed method"" would be a plus.
The paper also enjoys the merit that it has a brief, clear overview of recent AL research to put itself in a broader context.

My major concerns are two-fold:
1) the intuition of utilizing integrated gradients (IG) and pseudo-labels is not super clear to me; 2) experiments should be more extensive.
The authors assume that the way of using IGs as EBA for evaluating sample similarity by multiplying themselves with descriptor matrices can upweight features that ""a) are not class and instance-level discriminative, b) spatially represent features for a plurality of objects in the input. ""
The assumption needs more justification.
For example, a) why to use average pooling function for both gradients and features is reasonable, b) the derivation of R_b is of what properties such that the distribution between training data and validation set are more similar iteratively (so we can believe the set of b-th iteration is better than the set of {b-1}-th). Also, experiments can justify the assumption as well with more visual explanations on why the proposed AL method is better and reasonable.

For the title, I suggest the authors not to use ""explanation-based"" since it is a little bit misleading. Readers may expect the authors use some kinds of explanitions to improve AL. I would say ""Integrated Gradients-based Attention for Deep Active Learning"" would be better.

That being said, I enjoyed reading this paper and would like to see it accepted with better presentation and more justification, experiments.

","[TITLE]
EXPLANATION-BASED ATTENTION FOR SEMI-SUPERVISED DEEP ACTIVE LEARNING

[ABSTRACT]
We introduce an attention mechanism to improve feature extraction for deep active learning (AL) in the semi-supervised setting. The proposed attention mechanism is based on recent methods to visually explain predictions made by DNNs. We apply the proposed explanation-based attention to MNIST and SVHN classification. The conducted experiments show accuracy improvements for the original and class-imbalanced datasets with the same number of training examples and faster long-tail convergence compared to uncertainty-based methods.

[CAPTIONS]
Table 1: Figure 2 :Figure 3 :23Figure 2: MNIST test dataset accuracy for 3 class imbalance ratios: a) 1 (no imbalance), b) 10 and c) 100. Total 9 AL iterations (b = 10) are performed each with P = 250 pool size.
Table 2: SVHN. The dataset train/validation/test split is 500K/104K/26K. A typical 8-layer CNN is used with the following hyperparameters: epochs=35, batch-size=25, lr=0.1, lr-decay=0.1 every 15 epochs, uncert methods and IG EBA use K = 128 and L is 256 for single-scale (before fc1 layer) and 384 for two-scale descriptors (+ layer before conv7). Figure3shows that the gap between random selection and the best EBA-based AL method grows from 2% to more than 12% when the unlabeled training collection has more class imbalance. The gap between full training dataset accuracy increases for larger-scale SVHN as well. This results in even faster convergence for the proposed AL relative to random selection. Accuracies of the uncert methods are closer to each other than for MNIST, which may signal their declining effectiveness for large-scale data. The proposed EBA-based methods outperform all uncertainty-based methods for SVHN in the first AL iterations (up to +2.5%) and later arrive at approximately equal results.5 CONCLUSIONS AND FUTURE WORKWe applied recent image retrieval feature-extraction techniques to deep AL and introduced a novel EBA mechanism to improve feature-similarity matching. First feasibility experiments on MNIST and SVHN datasets showed advantages of EBA to improve density-based AL. Rather than performing AL for the well-picked training datasets, we also considered more realistic and challenging scenarios with class-imbalanced training collections where the proposed method emphasized the importance of additional feature supervision. In future research, EBA could be evaluated with other types of data distortions and biases: within-class bias, adversarial examples, etc. Furthermore, such applications as object detection and image segmentation may benefit more from EBA because multiscale attention can focus on spatially-important features.

[INTRODUCTION]
Deep active learning (AL) minimizes the number of expensive annotations needed to train DNNs by selecting a subset of relevant data points from a large unlabeled dataset (Lewis & Gale, 1994). This subset is annotated and added to the training dataset in a single pool of data points or, more often, in an iterative fashion. The goal is to maximize prediction accuracy while minimizing the product of pool size × number of iterations. A proxy for this goal could be the task of matching feature distributions between the validation and the AL-selected training datasets.
In density-based AL approaches, data selection is typically performed using a simple L 2 -distance metric (Sener & Savarese, 2018). The image retrieval field (Zhou et al., 2017) has advanced much further in this area. For example, recent state-of-the-art image retrieval systems are based on DNNbased feature extraction (Babenko & Lempitsky, 2015) with attention mechanisms (Noh et al., 2017). The latter estimates an attention mask to weight importance of the extracted features and it is trained along with the feature extraction.
Inspired by this, we employ image retrieval techniques and propose a novel attention mechanism for deep AL. Unlike supervised self-attention in (Noh et al., 2017;Vaswani et al., 2017), our attention mechanism is not trained with the model. It relies on recent methods to generate visual explanations and to attribute feature importance values (Sundararajan et al., 2017). We show the effectiveness of such explanation-based attention (EBA) mechanism for AL when combined with multi-scale feature extraction on a number of image classification datasets. We also conduct experiments for distorted class-imbalanced training data which is a more realistic assumption for unlabeled data.

[RELATED WORK]
AL is a well-studied approach to decrease annotation costs in a traditional machine learning pipelines (Settles, 2010). Recently, AL has been applied to data-demanding DNN-based systems in semi-supervised or weakly-supervised settings. Though AL is an attractive direction, existing methods struggle to deal with high-dimensional data e.g. images. We believe this is related to the lack of class and instance-level feature importance information as well as the inability to capture spatially-localized features. To overcome these limitations, we are interested in estimating spatiallymultiscale features and using our EBA mechanism to select only the most discriminative features. softmax Figure 1: Conventional multi-scale feature extraction and the proposed EBA extension (dashed). Wang et al. (2017) proposed to augment the training dataset by labeling the least confident data points and heuristically pseudo-labeling high confidence predictions. We believe the softmax output is not a reliable proxy for the goals of AL i.e. for selecting images using feature distribution matching between validation and train data. Unlike (Wang et al., 2017), we use pseudo labels only to estimate EBA vectors and find similarities between discriminative features. Gal et al. (2017) introduced a measure of uncertainty for approximate Bayesian inference that can be estimated using stochastic forward passes through a DNN with dropout layers. An acquisition function then selects data points with the highest uncertainty which is measured at the output of softmax using several metrics. Recent work (Beluch et al., 2018) extended this method by using an ensemble of networks for uncertainty estimation and achieved superior accuracy.
Sener & Savarese (2018) formulated feature similarity-based selection as a geometric core-set approach which outperforms greedy k-center clustering. Though their method can complement our approach, we are focusing on the novel feature extraction. For instance, they employed a simple L 2 distance similarity measure for the activations of the last fully-connected layer.
The most similar work to ours, by Vodrahalli et al. (2018), uses the gradients as a measure of importance for dataset subsampling and analysis. However, our approach formulates the problem as a multi-scale EBA for AL application and goes beyond a less robust single-step gradient attention. Other related works are online importance sampling methods (Ren et al., 2018) and the influence functions approach in (Koh & Liang, 2017). Online importance sampling upweights samples within the mini-batch during supervised training using gradient similarity while influence functions analyze data point importance using computationally challenging second-order gradient information.

[METHOD]
Pool-based AL. Let (X, y) be an input-label pair. There is a validation dataset {(X v i , y v i )} i∈M of size M and a collection of training pairs {(X i , y i )} i∈N of size N for which, initially, only a small random subset or pool of labels indexed by N 1 is known. The validation dataset approximates the distribution of test data. At every bth iteration the AL algorithm selects a pool of P new labels to be annotated and added to existing training pairs which creates a training dataset indexed by N b .
A DNN Φ(X, θ) is optimized by minimizing a loss function (N b ) −1 i∈N b L(ŷ i , y i ) w.r.t. to model parameters θ. However, the actual task is to minimize validation loss expressed by
M −1 i∈M L(ŷ v i , y v i ).
Therefore, an oracle AL algorithm achieves minimum of validation loss using the smallest b × P product. In this work, we are interested not in finding an oracle acquisition function, but in a method to extract relevant features for such function. We use a low-complexity greedy k-center algorithm to select the data points in the unlabeled training collection which are most similar to the misclassified entries in the validation dataset.
Feature descriptors. Let F j i ∈ R C×H×W , where C, H, and W are the number of channels, the height, and the width, respectively be the output of the jth layer of DNN for input image X i . Then, a feature vector or descriptor of length L can be defined as d i = φ(F i ) ∈ R L×1 , where function φ(•) is a conventional average pooling operation from (Babenko & Lempitsky, 2015). In a multi-scale case, descriptor is a concatenation of multiple feature vectors
d i = [φ j (F j i ), • • • , φ l (F l i )].
A descriptor matrix for the validation dataset V d ∈ R L×M and training dataset S d ∈ R L×N can be calculated using forward passes. Practically, descriptors can be compressed for storage efficiency reasons using PCA, quantization, etc. Then, a match kernel (Lee, 1999), e.g. cosine similarity, can be used to match features in both datasets. Assuming that vectors d i are L 2 -normalized, the cosine-similarity matrix is simply R d = V T d S d . Explanation-based attention. Feature maps F i extracted by Φ(X, θ) and pooled by φ(•) contain features that: a) are not class and instance-level discriminative (in other words, not disentangled), b) spatially represent features for a plurality of objects in the input. We would like to upweight discriminative features that satisfy a) and b) using an attention mechanism. One approach would be to use self-attention (Vaswani et al., 2017) at the cost of modifying network architecture and intervening into the training process. Instead, we propose to use EBA that is generated only for feature selection. The EBA mechanism attributes feature importance values w.r.t. to the output predictions. Unlike a visual explanation task, which estimates importance heatmaps in the input (image) space, we propose to estimate feature importance tensors A i of the internal DNN representations F i . Attention tensors A i can be efficiently calculated using a series of backpropagation passes. Using one of backpropagation-based methods called integrated gradients (IG) from (Sundararajan et al., 2017) ,  A j i can be estimated as
A j i = 1 K K k=1 ∂L(ŷ i (k), y i ) ∂F j i = 1 K K k=1 ∂L(Φ(kX i /K, θ), y i ) ∂F j i ,(1)
where K is the number of steps to approximate the continuous integral by a linear path. Other forms of (1) are possible: from the simplest saliency method for which K = 1 (Simonyan et al., 2014) to more advanced methods with randomly sampled input features (Gudovskiy et al., 2018).
Due to lack of labels y i in (1), we use common pseudo-labeling strategy: y i = 1 arg maxŷi . It is schematically shown in Figure 1. Unlike (Wang et al., 2017), pseudo-labels are used only to calculate similarity without additional hyperparameters rather than to perform a threshold-selected greedy augmentation. The EBA A i can be converted to multi-scale attention vector using the same processing a i = φ(A i ) ∈ R L×1 , which, by analogy, forms validation V a ∈ R L×M and train attention matrices S a ∈ R L×N . The latter processing is implemented in most modern frameworks and, therefore, the complexity to generate A i is only K forward-backward passes.
Summary for the proposed method. A random subset of N 1 training data points is annotated and a DNN Φ(X, θ) optimized for this subset. Then, the AL algorithm iteratively (b = 2, 3 . . .) performs following steps: 1) generates descriptor-attention matrix pairs
(V d , V a ), (S d , S a ), 2) calculates similarity matrix R = R d R a = (V T d S d ) (V T a S a )
, where is element-wise product, 3) selects P relevant data points from the remaining subset using acquisition function arg max i∈N\N b−1 (R(X i ), Φ) and 4) retrains Φ(X, θ) using augmented subset N b .

[EXPERIMENTS]
Our method as well as uncertainty-based methods from (Gal et al., 2017) are applied to the MNIST and SVHN classification. We evaluate AL with the original and distorted training data because unlabeled collection of data points cannot be a-priori perfectly selected. Hence, we introduce a class imbalance which is defined as the ratio of {0 . . . 4} to {5 . . . 9} digits. The following methods have been employed: random sampling, uncertainty-based (uncert), greedy selection using similarity matching without (top-P:none) and with EBA. The latter is estimated by saliency (top-P:grad) or IG (top-P:ig). We rerun experiments 10 times for MNIST and 5 times for SVHN with all-randomized initial parameters. Mean accuracy and standard deviation are reported. DNN parameters are trained from scratch initially and after each AL iteration. Mini-batch size is chosen by cross-validation.
MNIST. The dataset train/val/test split is 50K/10K/10K. The LeNet is used with the following hyperparameters: epochs=50, batch-size=25, lr=0.05, lr-decay=0.1 every 15 epochs, uncert methods and IG EBA use K = 128 passes and L is 20 for single-scale (before fc1 layer) and 90 for multiscale descriptors (all layers are concatenated). Figure 2(a) shows that feature-only matching (top-P:none L20) outperforms random selection by ≈ 1% while EBA (top-P:ig L90) adds another 1% of accuracy when there is no class imbalance. High class imbalance (Figure 2(c)) increases that gap: up to 20% for feature-only matching and 25% with EBA. The highest accuracy is achieved by multi- Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 0.5 1.0 1.5 2.0 2.5 3.0 3. Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 0.5 1.0 1.5 2.0 2.5 3.0 3. Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 scale EBA estimated by IG. EBA-based methods outperform the best uncertainty-based variation ratio (uncert:varMC) approach for all class imbalance settings except the last one where its accuracy is higher by less than 1% when b = 4. This might be related to small-scale MNIST and pseudo-label noise for EBA. To study the effects of pseudo-labeling, we plot true-label configurations (marked by ""Abl"") as well. The accuracy gap between EBA using true-and pseudo-labels is small with no class imbalance, but much larger (up to 25%) when class imbalance ratio is 100 during first AL iterations.","[TITLE]
EXPLANATION-BASED ATTENTION FOR SEMI-SUPERVISED DEEP ACTIVE LEARNING

[ABSTRACT]
We introduce an attention mechanism to improve feature extraction for deep active learning (AL) in the semi-supervised setting. The proposed attention mechanism is based on recent methods to visually explain predictions made by DNNs. We apply the proposed explanation-based attention to MNIST and SVHN classification. The conducted experiments show accuracy improvements for the original and class-imbalanced datasets with the same number of training examples and faster long-tail convergence compared to uncertainty-based methods."
Explanation-Based Attention for Semi-Supervised Deep Active Learning,SyxKiVmedV.json,"The authors consider the setting of deep attention learning. It consists in selecting critical unlabelled data in a semi-labelled dataset, so that once labelled they can improve drastically the accuracy of the model. The approach of the authors consist in training a DNN that computes similarity between data, starting with a limited pool of labelled data points. To do so, they augment iteratively the dataset using a greedy approach. 

The paper is well written, and even I am not not at all a specialist of the field I think I understood the main points of the paper.

The numerical experiments seems strong enough to be convinced by their approach.","[TITLE]
EXPLANATION-BASED ATTENTION FOR SEMI-SUPERVISED DEEP ACTIVE LEARNING

[ABSTRACT]
We introduce an attention mechanism to improve feature extraction for deep active learning (AL) in the semi-supervised setting. The proposed attention mechanism is based on recent methods to visually explain predictions made by DNNs. We apply the proposed explanation-based attention to MNIST and SVHN classification. The conducted experiments show accuracy improvements for the original and class-imbalanced datasets with the same number of training examples and faster long-tail convergence compared to uncertainty-based methods.

[CAPTIONS]
Table 1: Figure 2 :Figure 3 :23Figure 2: MNIST test dataset accuracy for 3 class imbalance ratios: a) 1 (no imbalance), b) 10 and c) 100. Total 9 AL iterations (b = 10) are performed each with P = 250 pool size.
Table 2: SVHN. The dataset train/validation/test split is 500K/104K/26K. A typical 8-layer CNN is used with the following hyperparameters: epochs=35, batch-size=25, lr=0.1, lr-decay=0.1 every 15 epochs, uncert methods and IG EBA use K = 128 and L is 256 for single-scale (before fc1 layer) and 384 for two-scale descriptors (+ layer before conv7). Figure3shows that the gap between random selection and the best EBA-based AL method grows from 2% to more than 12% when the unlabeled training collection has more class imbalance. The gap between full training dataset accuracy increases for larger-scale SVHN as well. This results in even faster convergence for the proposed AL relative to random selection. Accuracies of the uncert methods are closer to each other than for MNIST, which may signal their declining effectiveness for large-scale data. The proposed EBA-based methods outperform all uncertainty-based methods for SVHN in the first AL iterations (up to +2.5%) and later arrive at approximately equal results.5 CONCLUSIONS AND FUTURE WORKWe applied recent image retrieval feature-extraction techniques to deep AL and introduced a novel EBA mechanism to improve feature-similarity matching. First feasibility experiments on MNIST and SVHN datasets showed advantages of EBA to improve density-based AL. Rather than performing AL for the well-picked training datasets, we also considered more realistic and challenging scenarios with class-imbalanced training collections where the proposed method emphasized the importance of additional feature supervision. In future research, EBA could be evaluated with other types of data distortions and biases: within-class bias, adversarial examples, etc. Furthermore, such applications as object detection and image segmentation may benefit more from EBA because multiscale attention can focus on spatially-important features.

[INTRODUCTION]
Deep active learning (AL) minimizes the number of expensive annotations needed to train DNNs by selecting a subset of relevant data points from a large unlabeled dataset (Lewis & Gale, 1994). This subset is annotated and added to the training dataset in a single pool of data points or, more often, in an iterative fashion. The goal is to maximize prediction accuracy while minimizing the product of pool size × number of iterations. A proxy for this goal could be the task of matching feature distributions between the validation and the AL-selected training datasets.
In density-based AL approaches, data selection is typically performed using a simple L 2 -distance metric (Sener & Savarese, 2018). The image retrieval field (Zhou et al., 2017) has advanced much further in this area. For example, recent state-of-the-art image retrieval systems are based on DNNbased feature extraction (Babenko & Lempitsky, 2015) with attention mechanisms (Noh et al., 2017). The latter estimates an attention mask to weight importance of the extracted features and it is trained along with the feature extraction.
Inspired by this, we employ image retrieval techniques and propose a novel attention mechanism for deep AL. Unlike supervised self-attention in (Noh et al., 2017;Vaswani et al., 2017), our attention mechanism is not trained with the model. It relies on recent methods to generate visual explanations and to attribute feature importance values (Sundararajan et al., 2017). We show the effectiveness of such explanation-based attention (EBA) mechanism for AL when combined with multi-scale feature extraction on a number of image classification datasets. We also conduct experiments for distorted class-imbalanced training data which is a more realistic assumption for unlabeled data.

[RELATED WORK]
AL is a well-studied approach to decrease annotation costs in a traditional machine learning pipelines (Settles, 2010). Recently, AL has been applied to data-demanding DNN-based systems in semi-supervised or weakly-supervised settings. Though AL is an attractive direction, existing methods struggle to deal with high-dimensional data e.g. images. We believe this is related to the lack of class and instance-level feature importance information as well as the inability to capture spatially-localized features. To overcome these limitations, we are interested in estimating spatiallymultiscale features and using our EBA mechanism to select only the most discriminative features. softmax Figure 1: Conventional multi-scale feature extraction and the proposed EBA extension (dashed). Wang et al. (2017) proposed to augment the training dataset by labeling the least confident data points and heuristically pseudo-labeling high confidence predictions. We believe the softmax output is not a reliable proxy for the goals of AL i.e. for selecting images using feature distribution matching between validation and train data. Unlike (Wang et al., 2017), we use pseudo labels only to estimate EBA vectors and find similarities between discriminative features. Gal et al. (2017) introduced a measure of uncertainty for approximate Bayesian inference that can be estimated using stochastic forward passes through a DNN with dropout layers. An acquisition function then selects data points with the highest uncertainty which is measured at the output of softmax using several metrics. Recent work (Beluch et al., 2018) extended this method by using an ensemble of networks for uncertainty estimation and achieved superior accuracy.
Sener & Savarese (2018) formulated feature similarity-based selection as a geometric core-set approach which outperforms greedy k-center clustering. Though their method can complement our approach, we are focusing on the novel feature extraction. For instance, they employed a simple L 2 distance similarity measure for the activations of the last fully-connected layer.
The most similar work to ours, by Vodrahalli et al. (2018), uses the gradients as a measure of importance for dataset subsampling and analysis. However, our approach formulates the problem as a multi-scale EBA for AL application and goes beyond a less robust single-step gradient attention. Other related works are online importance sampling methods (Ren et al., 2018) and the influence functions approach in (Koh & Liang, 2017). Online importance sampling upweights samples within the mini-batch during supervised training using gradient similarity while influence functions analyze data point importance using computationally challenging second-order gradient information.

[METHOD]
Pool-based AL. Let (X, y) be an input-label pair. There is a validation dataset {(X v i , y v i )} i∈M of size M and a collection of training pairs {(X i , y i )} i∈N of size N for which, initially, only a small random subset or pool of labels indexed by N 1 is known. The validation dataset approximates the distribution of test data. At every bth iteration the AL algorithm selects a pool of P new labels to be annotated and added to existing training pairs which creates a training dataset indexed by N b .
A DNN Φ(X, θ) is optimized by minimizing a loss function (N b ) −1 i∈N b L(ŷ i , y i ) w.r.t. to model parameters θ. However, the actual task is to minimize validation loss expressed by
M −1 i∈M L(ŷ v i , y v i ).
Therefore, an oracle AL algorithm achieves minimum of validation loss using the smallest b × P product. In this work, we are interested not in finding an oracle acquisition function, but in a method to extract relevant features for such function. We use a low-complexity greedy k-center algorithm to select the data points in the unlabeled training collection which are most similar to the misclassified entries in the validation dataset.
Feature descriptors. Let F j i ∈ R C×H×W , where C, H, and W are the number of channels, the height, and the width, respectively be the output of the jth layer of DNN for input image X i . Then, a feature vector or descriptor of length L can be defined as d i = φ(F i ) ∈ R L×1 , where function φ(•) is a conventional average pooling operation from (Babenko & Lempitsky, 2015). In a multi-scale case, descriptor is a concatenation of multiple feature vectors
d i = [φ j (F j i ), • • • , φ l (F l i )].
A descriptor matrix for the validation dataset V d ∈ R L×M and training dataset S d ∈ R L×N can be calculated using forward passes. Practically, descriptors can be compressed for storage efficiency reasons using PCA, quantization, etc. Then, a match kernel (Lee, 1999), e.g. cosine similarity, can be used to match features in both datasets. Assuming that vectors d i are L 2 -normalized, the cosine-similarity matrix is simply R d = V T d S d . Explanation-based attention. Feature maps F i extracted by Φ(X, θ) and pooled by φ(•) contain features that: a) are not class and instance-level discriminative (in other words, not disentangled), b) spatially represent features for a plurality of objects in the input. We would like to upweight discriminative features that satisfy a) and b) using an attention mechanism. One approach would be to use self-attention (Vaswani et al., 2017) at the cost of modifying network architecture and intervening into the training process. Instead, we propose to use EBA that is generated only for feature selection. The EBA mechanism attributes feature importance values w.r.t. to the output predictions. Unlike a visual explanation task, which estimates importance heatmaps in the input (image) space, we propose to estimate feature importance tensors A i of the internal DNN representations F i . Attention tensors A i can be efficiently calculated using a series of backpropagation passes. Using one of backpropagation-based methods called integrated gradients (IG) from (Sundararajan et al., 2017) ,  A j i can be estimated as
A j i = 1 K K k=1 ∂L(ŷ i (k), y i ) ∂F j i = 1 K K k=1 ∂L(Φ(kX i /K, θ), y i ) ∂F j i ,(1)
where K is the number of steps to approximate the continuous integral by a linear path. Other forms of (1) are possible: from the simplest saliency method for which K = 1 (Simonyan et al., 2014) to more advanced methods with randomly sampled input features (Gudovskiy et al., 2018).
Due to lack of labels y i in (1), we use common pseudo-labeling strategy: y i = 1 arg maxŷi . It is schematically shown in Figure 1. Unlike (Wang et al., 2017), pseudo-labels are used only to calculate similarity without additional hyperparameters rather than to perform a threshold-selected greedy augmentation. The EBA A i can be converted to multi-scale attention vector using the same processing a i = φ(A i ) ∈ R L×1 , which, by analogy, forms validation V a ∈ R L×M and train attention matrices S a ∈ R L×N . The latter processing is implemented in most modern frameworks and, therefore, the complexity to generate A i is only K forward-backward passes.
Summary for the proposed method. A random subset of N 1 training data points is annotated and a DNN Φ(X, θ) optimized for this subset. Then, the AL algorithm iteratively (b = 2, 3 . . .) performs following steps: 1) generates descriptor-attention matrix pairs
(V d , V a ), (S d , S a ), 2) calculates similarity matrix R = R d R a = (V T d S d ) (V T a S a )
, where is element-wise product, 3) selects P relevant data points from the remaining subset using acquisition function arg max i∈N\N b−1 (R(X i ), Φ) and 4) retrains Φ(X, θ) using augmented subset N b .

[EXPERIMENTS]
Our method as well as uncertainty-based methods from (Gal et al., 2017) are applied to the MNIST and SVHN classification. We evaluate AL with the original and distorted training data because unlabeled collection of data points cannot be a-priori perfectly selected. Hence, we introduce a class imbalance which is defined as the ratio of {0 . . . 4} to {5 . . . 9} digits. The following methods have been employed: random sampling, uncertainty-based (uncert), greedy selection using similarity matching without (top-P:none) and with EBA. The latter is estimated by saliency (top-P:grad) or IG (top-P:ig). We rerun experiments 10 times for MNIST and 5 times for SVHN with all-randomized initial parameters. Mean accuracy and standard deviation are reported. DNN parameters are trained from scratch initially and after each AL iteration. Mini-batch size is chosen by cross-validation.
MNIST. The dataset train/val/test split is 50K/10K/10K. The LeNet is used with the following hyperparameters: epochs=50, batch-size=25, lr=0.05, lr-decay=0.1 every 15 epochs, uncert methods and IG EBA use K = 128 passes and L is 20 for single-scale (before fc1 layer) and 90 for multiscale descriptors (all layers are concatenated). Figure 2(a) shows that feature-only matching (top-P:none L20) outperforms random selection by ≈ 1% while EBA (top-P:ig L90) adds another 1% of accuracy when there is no class imbalance. High class imbalance (Figure 2(c)) increases that gap: up to 20% for feature-only matching and 25% with EBA. The highest accuracy is achieved by multi- Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 0.5 1.0 1.5 2.0 2.5 3.0 3. Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 0.5 1.0 1.5 2.0 2.5 3.0 3. Top-1 Accuracy, % full random uncert:varMC uncert:entMC top-P:none_L20 top-P:grad_L20 top-P:ig_L20 top-P:ig_L90 top-P:igAbl_L20 top-P:igAbl_L90 scale EBA estimated by IG. EBA-based methods outperform the best uncertainty-based variation ratio (uncert:varMC) approach for all class imbalance settings except the last one where its accuracy is higher by less than 1% when b = 4. This might be related to small-scale MNIST and pseudo-label noise for EBA. To study the effects of pseudo-labeling, we plot true-label configurations (marked by ""Abl"") as well. The accuracy gap between EBA using true-and pseudo-labels is small with no class imbalance, but much larger (up to 25%) when class imbalance ratio is 100 during first AL iterations.","[TITLE]
EXPLANATION-BASED ATTENTION FOR SEMI-SUPERVISED DEEP ACTIVE LEARNING

[ABSTRACT]
We introduce an attention mechanism to improve feature extraction for deep active learning (AL) in the semi-supervised setting. The proposed attention mechanism is based on recent methods to visually explain predictions made by DNNs. We apply the proposed explanation-based attention to MNIST and SVHN classification. The conducted experiments show accuracy improvements for the original and class-imbalanced datasets with the same number of training examples and faster long-tail convergence compared to uncertainty-based methods."
Why Deep Surgical Models Fail?: Revisiting Surgical Action Triplet Recognition through the Lens of Robustness,zZcCINENgm.json,"The paper focuses on the task of ""surgical action triplet recognition"" and hypothesizes that: the low performance of the existing deep learning methods arises from the ineffectiveness of the extracted features. To support this argument, the study conducted a variety of experiments and provide rich empirical results including: 1. comparing the performances of baseline models using different backbone networks as the feature extractors; and, 2. examining the robustness of the learned deep features by measuring their resistance to the perturbation under adversarial attack training. They claim their contribution as being the first study to explore the reason behind the failure of deep learning models in this task.

The good things about this paper are:
1. it well introduces the topic and made an in-depth analysis of the existing methods;
2. the study conducted generous experiments with relatively fair comparisons and considerable data and results. Some of them, such as the robustness test under adversarial attacks, indeed revels some potential areas of improvement in the existing methods of this task.

The following points may need to be improved:
1. the study only focuses on the task of ""surgical action triplet recognition"" which may not generally reflect all ""deep surgical models"";
2. there can be a variety of reasons that contributes to the poor performance of deep learning models and the unreliability of the extracted deep features may not be the only answer to the question of why deep surgical models fail;
3. the analysis and conclusion in this paper (Section 3.3) tend to be made by intuition rather than based on its experimental data or empirical evidence, and the provided experimental results may not rigorously prove the hypothesis that ""the failure of the existing deep learning model in this task results from the lack of reliability in the deep features"";
4. all the baseline methods used in this study originate from nearly the same groups of authors, which is not very common and may not generally reflect the common problems of this field.","[TITLE]
WHY DEEP SURGICAL MODELS FAIL?: REVISITING SURGICAL ACTION TRIPLET RECOGNITION THROUGH THE LENS OF ROBUSTNESS

[ABSTRACT]
Surgical action triplet recognition provides a better understanding of the surgical scene. This task is of high relevance as it provides the surgeon with contextaware support and safety. The current go-to strategy for improving performance is the development of new network mechanisms. However, the performance of current state-of-the-art techniques is substantially lower than other surgical tasks. Why is this happening? This is the question that we address in this work. We present the first study to understand the failure of existing deep learning models through the lens of robustness and explainability. Firstly, we study current existing models under weak and strong δ −perturbations via an adversarial optimisation scheme. We then analyse the failure modes via feature based explanations. Our study reveals that the key to improving performance and increasing reliability is in the core and spurious attributes. Our work opens the door to more trustworthy and reliable deep learning models in surgical data science. https://yc443.github.io/robustIVT/Minimally Invasive Surgery (MIS) has become the gold standard for several procedures (i.e., cholecystectomy & appendectomy), as it provides better clinical outcomes including reducing blood loss, minimising trauma to the body, causing less post-operative pain and faster recovery (Velanovich, 2000;Wilson et al., 2014). Despite the benefits of MIS, surgeons lose direct vision and touch on the target, which decreases surgeon-patient transparency imposing technical challenges to the surgeon. These challenges have motivated the development of automatic techniques for the analysis of the surgical workflow (

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Visualisation of the surgical action triplet recognition task. We consider the tasks where the instrument (I), verb (V , action), and target (T , anatomical part) seek to be predicted.
Table 2: Figure 2 :2Figure2: Illustration of the main network structure, and how the adversarial perturbation is added to measure robustness.
Table 3: 3. 11DATASET DESCRIPTION AND EVALUATION PROTOCOLDataset Description. We use CholecT45 dataset(Nwoye & Padoy, 2022) to evaluate the robustness of the three SOTA models for the Surgical Action Triplet Recognition task. Specifically, CholecT45 dataset contains 45 videos with annotations including 6 classes of instrument, 10 classes of verb, and 15 classes of target (i.e. C I = 6, C V = 10, C T = 15) generating 900 (6 × 10 × 25) potential combinations for triplet labels. To maximise the clinical utility, we utilise the top-100 combinations of relevant labels, which are selected by removing a large portion of spurious combinations according to class grouping and surgical relevance rating(Nwoye et al., 2022). Each video contains around 2, 000 annotated frames extracted at 1 fps in RGB channels, leading to a total of 90, 489 recorded frames. To remove the redundant information, the frames captured after the laparoscope been taken out of the body are blacked out with value [0, 0, 0]. Evaluation Protocol. The triplet action recognition is evaluated by the average precision (AP) metric. Our models can directly output the predictions of triplet class AP IV T . Instead, AP d where d ∈ {I,V, T, IV, IT } cannot be predicted explicitly. Then we obtain the final predictions of d ∈ {I,V, T, IV, IT } components according to(Nwoye & Padoy, 2022;Nwoye et al., 2022): 
Table 4: where we calculate the probability of class k ∈ {0, 1, ..,C d − 1} under component d; and h d (•) maps the class m from IV T triplet compositions to the class under component d.
Table 5: Figure 3 :3Figure3: The set of figures shows robustness analysis on randomly selected images with a. the visualisation of the Top 15 percent of important features selected by the 2 explanation methods-Grad and IG; b. (/d.) the trends showing the robustness measured on the relevant S r (/irrelevant S r ) features been selected by the 2 explanation methods against the percentage of Top features been defined as relevant; c. the comparison of the robustness across the 4 backbones embedded in Rendezvous baseline.
Table 6: Performance comparison for the task of Triplet recognition. The results are reported in terms of Average Precision (AP%) on the CholecT45 dataset using the official cross-validation split.
Table 7: Heatmaps Comparison under different feature extraction backbones. We displayed four randomly selected images in fold 3 when using the best performed weights trained and validated on folds 1,2,4 and 5.
Table 8: Top 5 predicted Triplet classes in each of the 10 models. The top 5 is assessed by the AP IV T score.
Table 9: Robustness measured on 400 examples (i.e. images) randomly selected from the images in the fold 3 videos with exactly 1 labeled triplet. Top 25 percent of relevant S r or irrelevant S r features are selected from 2 explanation methods Grad and IG. We perform attacks on the selected 25 percent.
Table 10: Image ID: VID08-000188Ground Truth Label: 17:grasper,retract,gallbladder

[]
In that work, authors proposed a 3D interaction space for learning the triplets. In more recent work, the authors of Nwoye et al. (2022) introduced two new models. The first one is a direct extension of Tripnet called Attention Tripnet, where the novelty relies on a spatial attention mechanism. In the same work, the authors introduced another model called Rendezvous (RDV) that highlights a transformer-inspired neural network. We consider the tasks where the instrument (I), verb (V , action), and target (T , anatomical part) seek to be predicted.

[INSTRUMENT]
A commonality of existing surgical action triplet recognition techniques is the development of new mechanisms for improving the network architecture. However and despite the potential improvements, the performance of existing techniques is substantially lower than other tasks in surgical sciences-for example, force estimation and navigation assisted surgery. In this work, we go contrariwise existing techniques, and tackle the surgical action triplet recognition problem from the lens of robustness and explainability.
In the machine learning community there is a substantial increase of interest in understanding the lack of reliability of deep learning models (e.g., Ribeiro et al. (2016); Koh & Liang (2017); Sundararajan et al. (2017); Liu et al. (2019); Yeh et al. (2019); Hsieh et al. (2020)). To understand the lack of reliability of existing deep networks, a popular family of techniques is the so-called feature based explanations via robustness analysis (Simonyan et al., 2013;Zeiler & Fergus, 2014;Plumb et al., 2018;Wong et al., 2021;Singla & Feizi, 2021). Whilst existing techniques have extensively been evaluated for natural images tasks, there are no existing works addressing the complex problems as in action triplet recognition.
Contributions. In this work, we introduce, to the best of our knowledge, the first study to understand the failure of existing deep learning models for surgical action triplet recognition. To do this, we analyse the failures of existing state-of-the-art solutions through the lens of robustness. Specifically, we push to the limit the existing SOTA techniques for surgical action triplet recognition under weak and strong δ −perturbations. We then extensively analyse the failure modes via the evaluation criteria Robustness-S, which analyses the behaviour of the models through feature based explanations. Our study reveals the impact of core and spurious features for more robust models. Our study opens the door to more trustworthy and reliable deep learning models in surgical data science, which is imperative for MIS.

[METHODOLOGY]
We describe two key parts for Surgical action triplet recognition task: i) our experimental settings along with assumptions and ii) how we evaluate robustness via adversarial optimisation. The workflow of our work is displayed in Figure 2.

[SURGICAL ACTION TRIPLET RECOGNITION]
In the surgical action triplet recognition problem, the main task is to recognise the triplet IV T , which is the composition of three components during surgery: instrument (I), verb (V ), and target (T ) in a given RGB image x ∈ R H×W ×3 . Formally, we consider a given set of samples {(x n , y n )} N n=1 with provided labels Y = {0, 1, ..,C IV T − 1} for C IV T = 100 classes. We seek then to predict a function f : X → Y such that f gets a good estimate for the unseen data. That is, a given parameterised deep learning model takes the image x as input, and outputs a set of class-wise presence probabilities, in our case 100 classes, under the IV T composition, Y IV T ∈ R 100 , which we call it the logits of IV T . Since there are three individual components under the triplet composition, within the training network, we also considered the individual component d * ∈ {I,V, T }, each with class number C d * (i.e. C I = 6, C V = 10, C T = 15). The logits of each component, Y d * ∈ R C d * , are computed and used within the network.
In current state-of-the-art (SOTA) deep models (Nwoye et al., 2020;, there is a communal structure divided into three parts: i) the feature extraction backbone; ii) the individual component encoder; and iii) the triplet aggregation decoder that associate the components and output the logits of the IV T triplet. More precisely, the individual component encoder firstly concentrates on the instrument component to output Class Activation Maps (CAMs ∈ R H×W ×C d ) and the logits Y I of the instrument classes; the CAMs are then associated with the verb and target components separately for their logits (Y V and Y T ) to address the instrument-centric nature of the triplet.
The current SOTA techniques for surgical action triplet recognition focus on improving the components ii) & iii). However, the performance is still substantially lower than other surgical tasks. Our intuition behind such behaviour is due to the inherently complex and ambiguous conditions in MIS, which reflects the inability of the models to learn meaningful features. Our work is then based on the following modelling hypothesis.

[HYPOTHESIS 2.1: DEEP FEATURES ARE KEY FOR ROBUSTNESS]
Deep surgical techniques for triplet recognition lacks reliability due to the ineffective features. Therefore, the key to boosting performance, improving trustworthiness and reliability, and understanding failure of deep models is in the deep features. Following previous hypothesis, we address the questions of-why deep triplet recognition models fail? We do that by analysing the feature based explanations via robustness. To do this, we consider the current three SOTA techniques for our study: Tripnet (Nwoye et al., 2020), Attention Tripnet, andRendezvous (Nwoye et al., 2022). Moreover, we extensively investigate the repercussion of deep features using four widely used backbones ResNet-18, ResNet-50 (He et al., 2015), DenseNet-121 (Huang et al., 2016), and Swin Transformer (Liu et al., 2021). In the next section, we detail our strategy for analysing robustness.

[FEATURE BASED EXPLANATIONS VIA ROBUSTNESS]
Our models of the triplet recognition output the logits of triplets composition, we then use it to select our predicted label for the classification result. We define the model from image x to the predicted labelŷ as f : X → Y , where
X ⊂ R H×W ×3 , Y = {0, 1, 2, ...,C IV T − 1}.
For each class m ∈ Y and within each given sample, we seek to recognise core and spurious attributions (Singla & Feizi, 2021;, which definition is as follows.
Core Attributes: they refer to the features that form a part in the object we are detecting. Spurious Attributes: these are the ones that not a part of the object but co-occurs with it.
How We Evaluate Robustness? The body of literature has reported several alternatives for addressing the robustness of deep networks. Our work is motivated by recent findings on perturbation based methods, where even a small perturbation can significantly affect the performance of neural nets. In particular, we consider the setting of adversarial training (Allen-Zhu & Li, 2022;Olah et al., 2018;Engstrom et al., 2019) for robustify a given deep model.
The idea behind adversarial training for robustness is to enforce a given model to maintain its performance under a given perturbation δ . This problem can be seen cast as an optimisation problem over the network parameters θ as:
θ * = arg min θ E (x,y)∼D [L θ (x, y)].(1)
where E[L θ (•)] denotes the expected loss to the parameter θ .
One seeks to the model be resistant to any δ −perturbation. In this work, we follow a generalised adversarial training model, which reads:
Definition 2.1: Adversarial training under δ θ * = arg min θ E (x,y)∼D [max δ∈∆ L θ (x + δ, y)].
The goal is to the models do not change their performance even under the worse (strong) δ .
The machine learning literature has explored different forms of the generalised model in definition equation 2.1. For example, a better sparsity regulariser for the adversarial training as in (Xu et al., 2018). In this work, we adopt the evaluation criteria of that (Hsieh et al., 2020), where one seeks to measure the susceptibility of features to adversarial perturbations. More precisely, we can have an insight of the deep features extracted by our prediction through visualising compact set of relevant features selected by some defined explanation methods on trained models, and measuring the robustness of the models by performing adversarial attacks on the relevant or the irrelevant features.
We denote the set of all features as U, and consider a general set of feature S ⊆ U. Since the feature we are interested are those in the image x, we further denote the subset of S that related to the image as x S . To measure the robustness of the model, we rewrote the generalised model equation 2.1 following the evaluation criteria of that (Hsieh et al., 2020). A model on input x with adversarial perturbation on feature set S then reads:
Definition 2.2: Adversarial δ & Robustness-S ε * x S := {min δ ∥δ∥ p s.t. f (x + δ) ̸ = y, δ S = 0},
where y is the ground truth label of image x; ∥ • ∥ p denotes the adversarial perturbation norm; S = U \ S denotes the complementary set of feature S with δ S = 0 constraining the perturbation only happens on x S . We refer to ε * x S as Robustness-S (Hsieh et al., 2020), or the minimum adversarial perturbation norm on x S . We then denote the relevant features selected by the explanation methods as S r ⊆ U, with the irrelevant features as its complementary set S r = U \ S r . Thus, the robustness on chosen feature sets-S r and S r tested on image x are: 
Robustness-S r = ε * x Sr ; Robustness-S r = ε * x Sr .

[EXPERIMENTAL RESULTS]
In this section, we describe in detail the range of experiments that we conducted to validate our methodology.    In our robustness analysis, the main evaluation criteria is the robustness subject to the selected feature set (S r and S r ) on each backbone using the formula in equation 2.2.
Y d k = max m {Y IV T m }, ∀ m ∈ {0, 1..,C IV T − 1} s.t. h d (m) = k,

[IMPLEMENTATION DETAILS]
We evaluate the model performance based on five-fold cross-validation, where we split 45 full videos into 5 equal folds. The testing set is selected from these 5 folds, and we treat the remaining 4 folds as the training set. Moreover, 5 videos from the 36 training set videos are selected as validation set during training.
The models are trained using the Stochastic Gradient Descent (SGD) optimiser. The feature extraction backbones are initialised with ImageNet pre-trained weights. Both linear and exponential decay of learning rate are used during training, with initial learning rates as {1e −2 , 1e −2 , 1e −2 } for backbone, encoder and decoder parts respectively. We set the batch size as 32, and epoch which performs the best among all recorded epochs up to AP score saturation on validation set in the specified k-fold. To reduce computational load, the input images and corresponding segmentation masks are resized from 256 × 448 to 8 × 14. For fair comparison, we ran all SOTA models (following all suggested protocols from the official repository) under the same conditions and using the official cross-validation split of the CholecT45 dataset (Nwoye & Padoy, 2022).

[EVALUATION ON DOWNSTREAM TASKS]
In this section, we carefully analyse the current SOTA techniques for triplet recognition from the feature based explainability lens.
Results on Triplet Recognition with Cross-Validation. As first part of our analysis, we investigate the performance limitation on current SOTA techniques, and emphasise how such limitation is linked to the lack of reliable features. The results are reported in Table 1. In a closer look at the results, we observe that ResNet-18, in general, performs the worst among the compared backbones. However, we can observe that for one case, component analysis, it performs better than ResNet-50 under Tripnet Attention baseline. The intuition being such behaviour is that the MIS setting relies on ambiguous condition and, in some cases, some frames might contain higher spurious features that are better captured by it. We remark that the mean and standard-deviation in Table 1 are calculated from the 5 folds in each combination of backbone and baseline.
We also observe that ResNet-50 performs better than ResNet-18 due to the deeper feature extraction. The best performance, for both the tasks-component detection and triplet association, is reported by DenseNet-121. The intuition behind the performance gain is that DenseNet-121 somehow mitigates the issue of the limitation of the capability representation. This is because ResNet type networks are limited by the identity shortcut that stabilises training. These results support our modelling hypothesis that the key of performance is the robustness of the deep features.
A key finding in our results is that whilst existing SOTA techniques (Nwoye & Padoy, 2022;Nwoye et al., 2022) are devoted to developing new network mechanisms, one can observe that a substantial performance improvement when improving the feature extraction. Moreover and unlike other surgical tasks, current techniques for triplet recognition are limited in performance. Why is this happening? Our results showed that the key is in the reliable features (linked to robustness); as enforcing more meaningful features, through several backbones, a significant performance improvement over all SOTA techniques is observed.
To further support our previous findings, we also ran a set of experiments using the trending principle of Transformers. More precisely, an non CNN backbone-the tiny Swin Transformer (Swin-T) (Liu et al., 2021) has also been tested on the Rendezvous, which has rather low AP scores on all of the 6 components in oppose to the 3 CNN backbones. This could be led by the shifted windows in the Swin-T, it is true that the shifted windows largely reduced the computational cost, but this could lead to bias feature attribute within bounding boxes, the incoherent spreading can be seen clearly in the visualisation of detected relevant features in Swin-T in Figure 3 (a).
In Table 1 we displayed the average results over all classes but-what behaviour can be observed from the per-class performance? It can be seen from Table 3 that though the best 5 predicted classes are different in each model, the predicted compositions seem clinically sensible supporting our previous discussion. In addition, the top 1 per-class AP score is significantly higher in DenseNet-121 with Rendezvous.
Visualisation Results. To interpret features is far from being trivial. To address this issue, we provide a human-like comparison via heatmaps in Table 2. The implementation of the heatmaps is adapted from (Zhou et al., 2016). The displayed outputs reflect what the model is focusing based on the extracted features. These results support our hypothesis that deep features are the key in making correct predictions over any new network mechanism.
We observed that in the worst performed backbone-Swin-T, the feature been extracted are mostly spread across the images, however, the ones that concentrate on core attributes are not though performed the best. In the best performed DenseNet-121, a reasonable amount of attention are also been paid to spurious attributes; this can be seen more directly in our later discussion on robustness visualisation Figure 3.
The reported probability on the predicted label emphasises again the outstanding performance of DenseNet-121 backbone; in the sense that, the higher the probability for the correct label the better, the lower it is for incorrect prediction the better.
Why Surgical Triplet Recognition Models Fail? Robustness and Interpretability. We further support our findings through the lens of robustness. We use as evaluation criteria Robustness-S r and Robustness-S r with different explanation methods: vanilla gradient (Grad) (Shrikumar et al., 2017) and integrated gradient (IG) (Sundararajan et al., 2017). The results are in Table 4 & Figure 3. 

[COMPARISON BETWEEN DIFFERENT BACKBONES]
In Table 4, we show the robustness results with top 25% attacked features on the average over 400 frames randomly chosen with exactly 1 labeled triplet. On one hand, we observe that the DenseNet-121 backbone consistently outperforms other network architectures on both evaluation criteria Robustness-S r and Robustness-S r . This suggests that DenseNet-121 backbone does capture different explanation characteristics which ignored by other network backbones. On the other hand, our results are supported by the finding in (Hsieh et al., 2020), as IG performs better than Grad; and the attack on relevant features yields lower robustness than perturbing the same percentage of irrelevant features.

[ROBUSTNESS EXPLANATION FOR SPECIFIC IMAGES]
To more objectively evaluate the robustness explanation for specific images, we show: (a) Visualisation of important features, (b) Robustness-S r , (c) Robustness against the percentage of Top features, and (d) Robustness-S r in Figure 3. In Figure 3 (a), we visualise the Top 15% features (with yellow dots) by Grad and IG, respectively, and overlay it on manually labelled region containing instrument (in red) and target (in green). We observe that the best performed backbone (can be seen from the robustness comparison curves in Figure 3 (c)) on the specific image is the one that not only pays attention to core attributes, but also the spurious attribute. In the image VID08-000188, the best performed model is ResNet-18, which shows the ambiguous condition on individual images. In a closer look at Figure 3 (a), a small portion of the most relevant feature extracted by ResNet-18 is  spread not on the close surrounding of the object area. This importance of spurious attribute is further highlighted in image VID18-001156. We observe that DenseNet-121 provides the most robust result highlighting relevant features within the tissue region and across tool tip. The worst performed model-ResNet-18 merely treated the core attributes as relevant.
The relevant role of spurious attributes can be explained by the nature of the triplet, which consists a verb component that is not the physical object. Overall, we observe that reliable deep features are the key for robust models in triplet recognition. Moreover, we observe, unlike existing works of robustness against spurious features, that both core and spurious attributes are key for the prediction.

[CONCLUSION]
We present the first work to understand the failure of existing deep learning models for the task of triplet recognition. We provided an extensive analysis through the lens of robustness. The significance of our work lies on understanding and addressing the key issues associated with the substantially limited in performance of existing techniques. Our work offers a step forward to more trustworthy and reliable models.","[TITLE]
WHY DEEP SURGICAL MODELS FAIL?: REVISITING SURGICAL ACTION TRIPLET RECOGNITION THROUGH THE LENS OF ROBUSTNESS

[ABSTRACT]
Surgical action triplet recognition provides a better understanding of the surgical scene. This task is of high relevance as it provides the surgeon with contextaware support and safety. The current go-to strategy for improving performance is the development of new network mechanisms. However, the performance of current state-of-the-art techniques is substantially lower than other surgical tasks. Why is this happening? This is the question that we address in this work. We present the first study to understand the failure of existing deep learning models through the lens of robustness and explainability. Firstly, we study current existing models under weak and strong δ −perturbations via an adversarial optimisation scheme. We then analyse the failure modes via feature based explanations. Our study reveals that the key to improving performance and increasing reliability is in the core and spurious attributes. Our work opens the door to more trustworthy and reliable deep learning models in surgical data science. https://yc443.github.io/robustIVT/Minimally Invasive Surgery (MIS) has become the gold standard for several procedures (i.e., cholecystectomy & appendectomy), as it provides better clinical outcomes including reducing blood loss, minimising trauma to the body, causing less post-operative pain and faster recovery (Velanovich, 2000;Wilson et al., 2014). Despite the benefits of MIS, surgeons lose direct vision and touch on the target, which decreases surgeon-patient transparency imposing technical challenges to the surgeon. These challenges have motivated the development of automatic techniques for the analysis of the surgical workflow ("
AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,S1x0CnEtvB.json,"Contributions:
	This paper best fits in the literature that explores growing network depth.  The main framework here is to interleave training a shallower network and adding new layers.  This paper (their final algorithm) differs from existing methods in that they: 1) initialize the new layers using standard initialization as opposed to the commonly used zero-init in this literature, 2) grows at a fixed interval , and this interval is short (to avoid the shallower nets being  overly-trained)., 3) uses a large and constant learning rate during the growing phase.  
Empirically, they show competitive results on standard image benchmarks.  
More interestingly (to me), they provide interesting insights to this paradigm of ‘growing networks’.  

Comments/Questions:
Section 2 of the paper describes the proposed method is good details.  

Section 3 of the paper describes the experiments.  Since for now I see the contribution of this paper is mostly empirical, I will give my detailed feedback here.
3.1 (Suboptimum of Network Morphism (NM) )
Table 2 shows NM is worse than training from scratch, and this isn’t fixed by AdamInit.
Table 3 shows c-AutoGrow (in between p-AutoGrow and NM) still does worse than from scratch, pinpoint the problem to converged subnetworks.
3.2  (p-AutoGrow)
Table 3 shows +Constant LR helps, then +RandomInit helps. 
Table 4, 5 shows +Periodic gets the best performance.  
*Suggestion* The found net is Table 4,5  are significantly deeper than those in Table 2,3, also there are no \Delta.  Also, although within this write-up those are the highest numbers, in the broader literature of NAS this doesn’t seem to be that good.  From a quick search, many methods in the Table 1 of [1] seems to give >96% accuracy on CIFAR10, some even close to 98%.  It might be good to at least discuss why this method is limited from achieving that.
I do like the finding that ZeroInit is unnecessary, as reported in the rest of this subsection.  However, it is unsatisfying to me that many past works (as cited by the authors) required this ZeroInit without ever trying RandomInit.  
*Suggestion* I would love to see a more thorough discussion on why GauInit is better than ZeroInit, not just more numbers.  For example, even just text description on why past works found ZeroInit useful, and countering some of those claims would be interesting.  A more controlled experiment rather than training 2 networks by swapping this would be interesting.  ZeroInit is used in more context than just NM.  For example, good flow models like Glow also uses such initialization, for likely a different reason, but I wonder if findings here have any implication for ZeroInit more generally.

3.3 (Many datasets)
Table 6 is a strong result.  One odd thing is how deep the found-net has to be for MNIST.  This actually suggest to me that AutoGrow does not have the ability to stop early when it can.  And in the discussion, the authors argue that by using a better sub-module like in NAS they can do better.  This raises the question why the authors did not choose to use it.  I would believe it if the proposed method has obvious reasons that it can transfer to different architecture, but for now I cannot jump to the conclusion that, say, p-AutoGrow with GauInit will necessarily work when using a different sub-module.  Perhaps, the reason past NM works didn’t use a GauInit was also due to the fact that past sub-modules didn’t work with GauInit.  

3.4 (Scale to ImageNet) It’d be good to add reference results from other papers.  

Minor details:


There are some good contents in this work, but for it to be a strong *empirical* contribution, perhaps it would be more useful to include experiments on other data modality where things are not so well tuned, and show state-of-the-art results.  For it to be a strong *analysis* paper, it should expanded, at least addressing some of the *suggestions* mentioned above. 
Unrelated to my evaluation of this work, reading this makes me think we should (and can) develop theoretical understanding to this paradigm of growing networks.



References:
[1] https://arxiv.org/pdf/1905.13360.pdf

","[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: p-AutoGrow on CIFAR10 (K = 3).The seed net is Basic3ResNet-1-1-1.
Table 2: Figure 3 :Figure 4 :34Figure 3: AutoGrow vs manual search obtained by training many baselines from scratch. x − axis is the number of parameters. Dataset is CIFAR10.
Table 3: Figure 5 :Figure 6 :56Figure6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table3, which is much smoother compared to Figure6(a). Figure6(c)(d) visualize the trajectories of p-AutoGrow with K = 50 and 3. The 2D projection gives limited information to reveal the advantages of p-AutoGrow
Table 4: Figure 7 :7Figure 7: Loss surfaces around minima found by baselines and AutoGrow. Dataset is CIFAR10.
Table 5: Figure 8:The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table10.
Table 6: A simple example of AutoGrow.
Table 7: Comparison with previous works about layer growth.
Table 8: Network Morphism tested on CIFAR10.
Table 9: Ablation study of c-AutoGrow.
Table 10: p-AutoGrow with different seed architecture.
Table 11: p-AutoGrow with different growing interval K.
Table 12: The adaptability of AutoGrow to datasets
Table 13: Scaling up to ImageNet
Table 14: TaoWei, Changhu Wang, and Chang Wen Chen.  Modularized morphing of neural networks. arXiv preprint arXiv:1701.03281, 2017. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pp. 2074-2082, 2016. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.
Table 15: p-AutoGrow under initializers with K = 3.
Table 16: AutoGrow improves accuracy of plain nets.
Table 17: The efficiency of AutoGrow
Table 18: The adaptability of AutoGrow to dataset sizes

[INTRODUCTION]
Layer depth is one of the decisive factors of the success of Deep Neural Networks (DNNs). For example, image classification accuracy keeps improving as the depth of network models grows (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014;Szegedy et al., 2015;He et al., 2016;Huang et al., 2017). Although shallow networks cannot ensure high accuracy, DNNs composed of too many layers may suffer from over-fitting and convergence difficulty in training. How to obtain the optimal depth for a DNN still remains mysterious. For instance, ResNet-152 (He et al., 2016) uses 3, 8, 36 and 3 residual blocks under output sizes of 56 × 56, 28 × 28, 14 × 14 and 7 × 7, respectively, which don't show an obvious quantitative relation. In practice, people usually reply on some heuristic trials and tests to obtain the depth of a network: they first design a DNN with a specific depth and then train and evaluate the network on a given dataset; finally, they change the depth and repeat the procedure until the accuracy meets the requirement. Besides the high computational cost induced by the iteration process, such trial & test iterations must be repeated whenever dataset changes. In this paper, we propose AutoGrow that can automate depth discovery given a layer architecture. We will show that AutoGrow generalizes to different datasets and layer architectures.
There are some previous works which add or morph layers to increase the depth in DNNs. Vg-gNet (Simonyan & Zisserman, 2014) and DropIn (Smith et al., 2016) added new layers into shallower DNNs; Network Morphism (Wei et al., 2016;Chen et al., 2015) morphed each layer to multiple layers to increase the depth meanwhile preserving the function of the shallower net. Table 1 summarizes differences in this work. Their goal was to overcome difficulty of training deeper DNNs or accelerate it. Our goal is to automatically find an optimal depth. Moreover, previous works applied layer growth by once or a few times at pre-defined locations to grow a pre-defined number of layers; in contrast, ours automatically learns the number of new layers and growth locations without limiting growing times. We will summarize more related works in Section 4. Figure 1 illustrates an example of AutoGrow. It starts from the shallowest backbone network and gradually grows sub-modules (A sub-module can be one or more layers, e.g., a residual block); the growth stops once a stopping policy is satisfied. We studied multiple initializers of new layers and multiple growing policies, and surprisingly find that: (1) a random initializer works equally or better than complicated Network Morphism; (2) it is more effective to grow before a shallow net converges. We hypothesize that this is because a converged shallow net is an inadequate initialization for training deeper net, while random initialization can help to escape from a bad starting point.
Motivated by this, we intentionally avoid full convergence during the growing by using (1) random initialization of new layers, (2) a constant large learning rate, and (3) a short growing interval.  (3) We challenge the idea of Network Morphism, as random initialization works equally or better when growing layers. (4) We find that it is beneficial to rapidly grow layers before a shallower net converge, contradicting previous intuition.
2 AutoGrow -A DEPTH GROWING ALGORITHM Algorithm 1 AutoGrow Algorithm.
Input :
A seed shallow network g(X0) composed of M sub-networks F = {fi (•; Wi) : i = 0 . . . M − 1},
where each sub-network has only one sub-module (a dimension reduction sub-module); an epoch interval K to check growing and stopping policies; the number of fine-tuning epochs N after growing.

[INITIALIZATION:]
A Circular Linked List of sub-networks under growing:
subNetList = f0 (•; W0) → • • • → f M −1 (•; W M −1 ) ← −−−−−−−−−−−−−−−−−−−−−−−−−− − ;
The current growing sub-network: growingSubNet = subNetList.head() = f0 (•; W0);
The recent grown sub-network: grownSubNet = None; Process : A trained neural network g(X0) with learned depth.
#
Figure 1 gives an overview of the proposed AutoGrow. In this paper, we use network, sub-networks, sub-modules and layers to describe the architecture hierarchy. A network is composed of a cascade of sub-networks. A sub-network is composed of sub-modules, which typical share the same output size. A sub-module (e.g. a residual block) is an elementary growing block composed of one or a few layers. In this section, we rigorously formulate a generic version of AutoGrow which will be materialized in subsections. A deep convolutional network g(X 0 ) is a cascade of sub-networks by composing functions as g(X
0 ) = l (f M −1 (f M −2 (• • • f 1 (f 0 (X 0 )) • • • )))
, where X 0 is an input image, M is the number of sub-networks, l(•) is a loss function, and X i+1 = f i (X i ) is a sub-network that operates on an input image or a feature tensor X i ∈ R ci×hi×wi . Here, c i is the number of channels, and h i and w i are spatial dimensions. f i (X i ) is a simplified notation of f i (X i ; W i ), where W i is a set of sub-modules' parameters within the i-th sub-network. Thus W = {W i : i = 0 . . . M − 1} denotes the whole set of parameters in the DNN. To facilitate growing, the following properties are supported within a sub-network: (1) the first sub-module usually reduces the size of input feature maps, e.g., using pooling or convolution with a stride; and (2) all sub-modules in a sub-network maintain the same output size. As such, our framework can support popular networks, including VggNet-like plain networks (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2015), ResNets (He et al., 2016) and DenseNets (Huang et al., 2017). In this paper, we select ResNets and VggNet-like nets as representatives of DNNs with and without shortcuts, respectively.
With above notations, Algorithm 1 rigorously describes the AutoGrow algorithm. In brief, AutoGrow starts with the shallowest net where every sub-network has only one sub-module for spatial dimension reduction. AutoGrow loops over all growing sub-networks in order. For each sub-network, AutoGrow stacks a new sub-module. When the new sub-module does not improve the accuracy, the growth in corresponding sub-network will be permanently stopped. The details of our method will be materialized in the following subsections.

[SEED SHALLOW NETWORKS AND SUB-MODULES]
In this paper, in all datasets except ImageNet, we explore growing depth for four types of DNNs: (1) Basic3ResNet: the same ResNet used for CIFAR10 in He et al. (2016) In AutoGrow, the architectures of seed shallow networks and sub-modules are pre-defined. In plain DNNs, a sub-module is a stack of convolution, Batch Normalization and ReLU; in residual DNNs, a sub-module is a residual block. In AutoGrow, a sub-network is a stack of all sub-modules with the same output spatial size. Unlike He et al. (2016) which manually designed the depth, AutoGrow starts from a seed architecture in which each sub-network has only one sub-module and automatically learns the number of sub-modules.
On ImageNet, we apply the same backbones in He et al. (2016) as the seed architectures. A seed architecture has only one sub-module under each output spatial size. For a ResNet using basic residual blocks or bottleneck residual blocks (He et al., 2016), we respectively name it as Basic4ResNet or Bottleneck4ResNet. Plain4Net is also obtained by removing shortcuts in Basic4ResNet.

[SUB-MODULE INITIALIZERS]
Here we explain how to initialize a new sub-module W in initializer(W) mentioned in Algorithm 1. Network Morphism changes DNN architecture meanwhile preserving the loss function via special initialization of new layers, that is, g(X 0 ; W) = g(X 0 ; W ∪ W) ∀X 0 . A residual sub-module shows a nice property: when stacking a residual block and initializing the last Batch Normalization layer as zeros, the function of the shallower net is preserved but the DNN is morphed to a deeper net. Thus, Network Morphism can be easily implemented by this zero initialization (ZeroInit).
In this work, all layers in W are initialized using default randomization, except for a special treatment of the last Batch Normalization layer in a residual sub-module. Besides ZeroInit, we propose a new AdamInit for Network Morphism. In AdamInit, we freeze all parameters except the last Batch Normalization layer in W, and then use Adam optimizer (Kingma & Ba, 2014) to optimize the last Bath Normalization for maximum 10 epochs till the training accuracy of the deeper net is as good as the shallower one. After AdamInit, all parameters are jointly optimized. We view AdamInit as a Network Morphism because the training loss is similar after AdamInit. We empirically find that AdamInit can usually find a solution in less than 3 epochs. We also study random initialization of the last Batch Normalization layer using uniform (UniInit) or Gaussian (GauInit) noises with a standard deviation 1.0. We will show that GauInit obtains the best result, challenging the idea of Network Morphism (Wei et al., 2016;Chen et al., 2015).

[GROWING AND STOPPING POLICIES]
In Algorithm 1, a growing policy refers to meetGrowingPolicy(), which returns true when the network should grow a sub-module. Two growing policies are studied here:
1. Convergent Growth: meetGrowingPolicy() returns true when the improvement of validation accuracy is less than τ in the last K epochs. That is, in Convergent Growth, AutoGrow only grows when current network has converged. This is a similar growing criterion adopted in previous works (Elsken et al., 2017;Cai et al., 2018a;b). 2. Periodic Growth: meetGrowingPolicy() always returns true, that is, the network always grows every K epochs. Therefore, K is also the growing period. In the best practice of AutoGrow, K is small (e.g. K = 3) such that it grows before current network converges.
Our experiments will show that Periodic Growth outperforms Convergent Growth. We hypothesize that a fully converged shallower net is an inadequate initialization to train a deeper net. We will perform experiments to test this hypothesis and visualize optimization trajectory to illustrate it.
A stopping policy denotes meetStoppingPolicy() in Algorithm 1. When Convergent Growth is adopted, meetStoppingPolicy() returns true if a recent growth does not improve validation accuracy more than τ within K epochs. We use a similar stopping policy for Periodic Growth; however, as it can grow rapidly with a small period K (e.g. K = 3) before it converges, we use a larger window size J for stop. Specifically, when Periodic Growth is adopted, meetStoppingPolicy() returns true when the validation accuracy improves less than τ in the last J epochs, where J K.
Hyper-parameters τ , J and K control the operation of AutoGrow and can be easily setup and generalize well. τ denotes the significance of accuracy improvement for classification. We simply set τ = 0.05% in all experiments. J represents how many epochs to wait for an accuracy improvement before stopping the growth of a sub-network. It is more meaningful to consider stopping when the new net is trained to some extent. As such, we set J to the number of epochs T under the largest learning rate when training a baseline. K means how frequently AutoGrow checks the polices. In Convergent Growth, we simply set K = T , which is long enough to ensure convergence. In Periodic Growth, K is set to a small fraction of T to enable fast growth before convergence; more importantly, K = 3 is very robust to all networks and datasets. Therefore, all those hyper-parameters are very robust and strongly correlated to design considerations.

[EXPERIMENTS]
In this paper, we use Basic3ResNet-2-3-2, for instance, to denote a model architecture which contains 2, 3 and 2 sub-modules in the first, second and third sub-networks, respectively. Sometimes we simplify it as 2-3-2 for convenience. AutoGrow always starts from the shallowest depth of 1-1-1 and uses the maximum validation accuracy as the metric to guide growing and stopping. All DNN baselines are trained by SGD with momentum 0.9 using staircase learning rate. The initial learning rate is 0.1 in ResNets and 0.01 in plain networks. On ImageNet, baselines are trained using batch size 256 for 90 epochs, within which learning rate is decayed by 0.1× at epoch 30 and 60. In all other smaller datasets, baselines are trained using batch size 128 for 200 epochs and learning rate is decayed by 0.1× at epoch 100 and 150.
Our early experiments followed prior wisdom by growing layers with Network Morphism (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b), i.e., AutoGrow with ZeroInit (or AdamInit) and Convergent Growth policy; however, it stopped early with very shallow DNNs, failing to find optimal depth. We hypothesize that a converged shallow net with Network Morphism gives a bad initialization to train a deeper neural network. Section 3.1 experimentally test that the hypothesis is valid. To tackle this issue, we intentionally avoid convergence during growing by three simple solutions, which are evaluated in Section 3.2. Finally, Section 3.3 and Section 3.4 include extensive experiments to show the effectiveness of our final AutoGrow.

[SUBOPTIMUM OF NETWORK MORPHISM AND CONVERGENT GROWTH]
In this section, we study Network Morphism itself and its integration into our AutoGrow under Convergent Growth. When studying Network Morphism, we take the following steps: 1) train a shallower ResNet to converge, 2) stack residual blocks on top of each sub-network to morph to a deeper net, 3) use ZeroInit or AdamInit to initialize new layers, and 4) train the deeper net in a standard way. We compare the accuracy difference (""∆"") between Network Morphism and training the deeper net from scratch. Table 2 summaries our results. Network Morphism has a lower accuracy (negative ""∆"") in all the cases, which validates our hypothesis that a converged shallow network with Network Morphism gives a bad initialization to train a deeper net. We visualize the optimization trajectories in Appendix A.0.1 to illustrate the hypothesis.
To further validate our hypothesis, we integrate Network Morphism as the initializer in AutoGrow with Convergent Growth policy. We refer to this version of AutoGrow as c-AutoGrow with ""c-"" denoting ""Convergent."" More specific, we take ZeroInit or AdamInit as sub-module initializer and ""Convergent Growth"" policy in Algorithm 1. To recap, in this setting, AutoGrow trains a shallower net till it converges, then grows a sub-module which is initialized by Network Morphism, and repeats the same process till there is no further accuracy improvement. In every interval of K training epochs (train(g(X 0 ), K) in Algorithm 1), ""staircase"" learning rate is used. The learning rate is reset to 0.1 at the first epoch, and decayed by 0.1× at epoch K 2 and 3K 4 . The results are shown in Table 3 by ""staircase"" rows, which illustrate that c-AutoGrow can grow a DNN multiple times and finally find a depth. However, there are two problems: 1) the final accuracy is lower than training the found net from scratch, as indicated by ""∆"", validating our hypothesis; 2) the depth learning stops too early with a relatively shallower net, while a deeper net beyond the found depth can achieve a higher accuracy as we will show in Table 6. These problems provide a circumstantial evidence of the hypothesis that a converged shallow net with Network Morphism gives a bad initialization. Thus, AutoGrow cannot receive signals to continue growing after a limited number of growths. In Appendix A.0.1, Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3.

[ABLATION STUDY FOR AUTOGROW DESIGN]
Based on the findings in Section 3.1, we propose three simple but effective solutions to further enhance AutoGrow and refer it as p-AutoGrow, with ""p-"" denoting ""Periodic"": (1) Use a large constant learning rate for growing, i.e., 0.1 for residual networks and 0.01 for plain networks. Stochastic gradient descent with a large learning rate intrinsically introduces noises, which help to avoid a full convergence into a bad initialization from a shallower net. Note that staircase learning rate is still used for fine-tuning after discovering the final DNN; (2) Use random initialization (UniInit or GauInit) as noises to escape from an inadequate initialization; (3) Grow rapidly before a shallower net converges by taking Periodic Growth with a small K. p-AutoGrow is our final AutoGrow. In the rest part of this section, we perform ablation study to prove that the three solutions are effective. We start from c-AutoGrow, and incrementally add above solutions one by one and eventually obtain p-AutoGrow. In Table 3, first, we replace the staircase learning rate with a constant learning rate, the accuracy of AutoGrow improves and therefore ""∆"" improves; second, we further replace Network Morphism (ZeroInit or AdamInit) with a random initializer (UniInit or GauInit) and result in a bigger gain. Overall, combining a constant learning rate with GauInit performs the best. Thus, constant learning rate and GauInit are adopted in the remaining experiments, unless we explicitly specify them.  Note that, in this paper, we are more interested in automating depth discovery to find a final DNN (""found net"") with a high accuracy (""accu""). Ideally, the ""found net"" has a minimum depth, a larger depth than which cannot further improve ""accu"". We will show in Figure 3 that AutoGrow discovers a depth approximately satisfying this property. The ""∆"" is a metric to indicate how well shallower nets initialize deeper nets; a negative ""∆"" indicates that weight initialization from a shallower net hurts training of a deeper net; while a positive ""∆"" indicates AutoGrow helps training a deeper net, which is a byproduct of this work.
Finally, we apply the last solution -Periodic Growth, and obtain our final p-AutoGrow. Our ablation study results for p-AutoGrow are summarized in Table 5 and Table 4. Table 5 analyzes the impact of the growing period K. In general, K is a hyper-parameter to trade off speed and accuracy: a smaller K takes a longer learning time but discovers a deeper net, vice versa. Our results validate the preference of a faster growth (i.e. a smaller K). On CIFAR10/CIFAR100, the accuracy reaches plateau/peak at K = 3; further reducing K produces a deeper net while the accuracy gain is marginal/impossible. In the following, we simply select K = 3 for robustness test. More importantly, our quantitative results in Table 5 show that p-AutoGrow finds much deeper nets, overcoming the very-early stop issue in c-AutoGrow in Table 3. That is, Periodic Growth proposed in this work is much more effective than Convergent Growth utilized in previous work.
For sanity check, we perform the ablation study of initializers for p-AutoGrow. The results are in Table 8 in Appendix A.0.3, which further validates our wisdom on selecting GauInit. The motivation of Network Morphism in previous work was to start a deeper net from a loss function that has been well optimized by a shallower net, so as not to restart the deeper net training from scratch (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b). In all our experiments, we find this is sure even with random initialization. Figure 2 plots the convergence curves and learning process for ""42-42-42"" in Table 5. Even with GauInit, the loss and accuracy rapidly recover and no restart is observed. The convergence pattern in the ""Growing"" stage is similar to the ""Fine-tuning"" stage under the same learning rate (the initial learning rate 0.1). Similar results on ImageNet will be shown in Figure 8. Our results challenge the necessity of Network Morphism when growing neural networks.
At last, we perform the ablation study on the initial depth of the seed network. Table 4 demonstrates that a shallowest DNN works as well as a deeper seed. This implies that AutoGrow can appropriately stop regardless of the depth of the seed network. As the focus of this work is on depth automation, we prefer starting with the shallowest seed to avoid a manual search of a seed depth.  The seed net is Basic3ResNet-1-1-1.  

[ADAPTABILITY OF AUTOGROW]
To verify the adaptability of AutoGrow, we use an identical configuration (p-AutoGrow with K = 3) and test over 5 datasets and 4 seed architectures. Table 6 includes the results of all 20 combinations. Figure 3 compares AutoGrow with manual search which is obtained by training many DNNs with different depths from scratch. The results lead to the following conclusions and contributions:  2. For ResNets, a discovered depth ("" "" in Figure 3) falls at the location where accuracy saturates. This means AutoGrow discovers a near-optimal depth: a shallower depth will lose accuracy while a deeper one gains little. The final accuracy of AutoGrow is as good as training the discovered net from scratch as indicated by ""∆"" in Table 6, indicating that initialization from shallower nets does not hurt training of deeper nets. As a byproduct, in plain networks, there are large positive ""∆""s in Table 6. It implies that baselines fail to train very deep plain networks even using Batch Normalization, but AutoGrow enables the training of these networks; In Appendix A.0.3, Table 9 shows the accuracy improvement of plain networks by tuning K, approaching the accuracy of ResNets with the same depth.
3. For robustness and generalization study purpose, we stick to K = 3 in our experiments, however, we can tune K to trade off accuracy and model size. As shown in Figure 3, Auto-Grow discovers smaller DNNs when increasing K from 3 ("" "") to 50 ("" ""). Interestingly, the accuracy of plain networks even increases at K = 50. This implies the possibility of discovering a better accuracy-depth trade-off by tuning K, although we stick to K = 3 for generalizability study and it generalizes well.
4. In Table 6, AutoGrow discovers different depths under different sub-modules. The final accuracy is limited by the sub-module design, not by our AutoGrow. Given a sub-module architecture, our AutoGrow can always find a near-optimal depth. With a better sub-module architecture, such as NASNet cell , AutoGrow can improve accuracy.
Finally, our supposition is that: when the size of dataset is smaller, the optimal depth should be smaller. Under this supposition, we test the effectiveness of AutoGrow by sampling a subset of dataset and verify if AutoGrow can discover a shallower depth. In Appendix A.0.3, Table 11 summarizes the results. As expected, our experiments show that AutoGrow adapts to shallower networks when the datasets are smaller.

[SCALING TO IMAGENET AND EFFICIENCY]
In ImageNet, K = 3 should generalize well, but we explore AutoGrow with K = 2 and K = 5 to obtain an accuracy-depth trade-off line for comparison with human experts. The larger K = 5 enables AutoGrow to obtain a smaller DNN to trade-off accuracy and model size (computation) and the smaller K = 2 achieves higher accuracy. The results are shown in Table 7, which proves that AutoGrow automatically finds a good depth without any tuning. As a byproduct, the accuracy is even higher than training the found net from scratch, indicating that the Periodic Growth in AutoGrow helps training deeper nets. The comparison of AutoGrow and manual depth design (He et al., 2016) is in Figure 4, which shows that AutoGrow achieves better trade-off between accuracy and computation (measured by floating point operations).
In Appendix A.0.3, Table 10 summarizes the breakdown of wall-clock time in AutoGrow. The growing/searching time is as efficient as (often more efficient than) fine-tuning the single discovered DNN. The scalability of AutoGrow comes from its intrinsic features that (1) it grows quickly with a short period K and stops immediately if no improvement is sensed; and (2) the network is small at the beginning of growing. 

[RELATED WORK]
Neural Architecture Search (NAS) (Zoph & Le, 2016) and neural evolution (Miikkulainen et al., 2019;Angeline et al., 1994;Stanley & Miikkulainen, 2002;Liu et al., 2017a;Real et al., 2017) can search network architectures from a gigantic search space. In NAS, the depth of DNNs in the search space is fixed, while AutoGrow learns the depth. Some NAS methods (Bender et al., 2018;Liu et al., 2018b;Cortes et al., 2017) can find DNNs with different depths, however, the maximum depth is pre-defined and shallower nets are obtained by padding zero operations or selecting shallower branches, while our AutoGrow learns the depth in an open domain to find a minimum depth, beyond which no accuracy improvement can be obtained. Moreover, NAS is very computation and memory intensive. To accelerate NAS, one-shot models (Saxena & Verbeek, 2016;Pham et al., 2018;Bender et al., 2018), DARTS (Liu et al., 2018b) andNAS with Transferable Cell (Zoph et al., 2018;Liu et al., 2018a) were proposed. The search time reduces dramatically but is still long from practical perspective. It is still very challenging to deploy these methods to larger datasets such as ImageNet.
In contrast, our AutoGrow can scale up to ImageNet thanks to its short depth learning time, which is as efficient as training a single DNN.
In addition to architecture search which requires to train lots of DNNs from scratch, there are also many studies on learning neural structures within a single training. Structure pruning and growing were proposed for different goals, such as efficient inference (Wen et al., 2016;Li et al., 2016;Lebedev & Lempitsky, 2016;He et al., 2017;Luo et al., 2017;Liu et al., 2017b;Dai et al., 2017;Huang et al., 2018;Gordon et al., 2018;Du et al., 2019), lifelong learning (Yoon et al., 2017 and model adaptation (Feng & Darrell, 2015;Philipp & Carbonell, 2017). However, those works fixed the network depth and limited structure learning within the existing layers. Optimization over a DNN with fixed depth is easier as the skeleton architecture is known. AutoGrow performs in a scenario where the DNN depth is unknown hence we need to seek for the optimal depth. We hypothesize that a converged shallower net may not be an adequate initialization. Figure 5 visualizes and compares the optimization trajectories of Network Morphism and the training from scratch. In this figure, the shallower net is Basic3ResNet-3-3-3 (ResNet-20) and the deeper one is Basic3ResNet-5-5-5 (ResNet-32) in Table 2. The initializer is ZeroInit. The visualization method is extended from Li et al. (2018). Points on the trajectory are evenly sampled every a few epochs. To maximize the variance of trajectory, we use PCA to project from a high dimensional space to a 2D space and use the first two Principle Components (PC) to form the axes in Figure 5. The contours of training loss function and the trajectory are visualized around the final minimum of the deeper net. When projecting a shallower net to a deeper net space, zeros are padded for the parameters not existing in the deeper net. We must note that the loss increase along the trajectory does not truly represent the situation in high dimensional space, as the trajectory is just a projection. It is possible that the loss remains decreasing in the high dimension while it appears in an opposite way in the 2D space. The sharp detour at ""Morphing"" in Figure 5(a) may indicate that the shallower net plausibly converges to a point that the deeper net struggles to escape. In contrast, Figure 5(b) shows that the trajectory of the direct optimization in the deeper space smoothly converges to a better minimum.
Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure 6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table 3, which is much smoother compared to Figure 6(a).   10.
Table 11 summarizes the adaptability of AutoGrow to the sizes of dataset. In each set of experiments, dataset is randomly down-sampled to 100%, 75%, 50% and 25%. For a fair comparison, K is divided by the percentage of dataset such that the number of mini-batches between growths remains    The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table 10.","[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN."
AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,S1x0CnEtvB.json,"This paper's contribution is a method for automatically growing the depth of a neural network during training. It compares several heuristics that may be used to successfully achieve this goal and identifies a set of choices that work well together on multiple datasets.

The paper focuses on CNNs that conform to a popular design pattern where the network is organized into a series of sub-networks, each consisting of a series of sub-modules (sometimes called blocks) operating at the same resolution. To be precise, the proposed method aims to learn the length of each series of sub-modules. A main contribution of the paper is the demonstration that it is not necessary to train a network until convergence before adding new sub-modules as proposed in past work. Instead, it is better to grow the network after training for a short while.

My current decision for this paper is a weak rejection due to the points below. However, I am open to revising my opinion if these points are addressed satisfactorily.

- The growing strategy identified in the paper as a superior alternative seems to be already known and used, at least in the speech recognition community. Seide et al. (2011) called it Discriminative Pre-training, and showed that it outperforms greedy layer-wise pretraining and DBN pre-training. Zeyer et al. (2017) reported that a similar method also enables the training of very deep LSTM networks which is otherwise notoriously hard. In general, the existence of prior work with the same ideas does not preclude acceptance, but the existence of this work needs to be clearly stated early on and the additional value of the current study sufficiently clarified.

- I find it strange that the final networks found by the proposed method usually have the same/similar number of sub-modules per sub-network (Tables 4,5,6) on multiple datasets. The only exceptions appear to be Basic4ResNet/CIFAR100 in Table 6 and about 50% of ImageNet results in Table 7. This regularity suggests that either A) the proposed algorithm prefers to set same number of sub-modules per sub-network due to its design, or B) datasets except ImageNet have an inherent shared property that produces this result. Since option A suggests a bias in the algorithm, this peculiarity of the results needs to be investigated or explained further.

- Figure 3 constitutes the main evidence that Autogrow finds approximately optimal depths as compared to manual searching, but it is not clear how the plot for baselines is obtained. For any given parameter budget, there are multiple baseline networks possible since the sub-networks can have different number of sub-modules (see previous point). This does not appear to be accounted for in Figure 3. Further, when dealing with CNNs, it would be more useful to have computation budget on the x-axis instead of the parameter budget. This would better account for the difference between increasing depth in an earlier sub-network vs. a later one.

- The reported results appear to be for single trials throughout the paper. This does not seem sufficient especially for results in Tables 2 and 3 where many differences are rather small, and so drawing conclusions from these tables would be unscientific.

References:

Seide, Frank, et al. ""Feature engineering in context-dependent deep neural networks for conversational speech transcription."" 2011 IEEE Workshop on Automatic Speech Recognition & Understanding. IEEE, 2011. https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/FeatureEngineeringInCD-DNN-ASRU2011-pub.pdf

Zeyer, Albert, et al. ""A comprehensive study of deep bidirectional LSTM RNNs for acoustic modeling in speech recognition."" 2017 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2017. https://arxiv.org/abs/1606.06871

Update after rebuttal
-----------------------------
I'm sympathetic to the unfortunate situation that the authors are in, since the underlying growing strategy has already been covered by prior work. As I mentioned earlier, a sufficient rewrite of the paper can clearly state what has been done already so as not to take credit from the earlier authors. A revised version of the paper has not been uploaded; I suggest that the authors do so for the future. 

I agree that the focus of this paper is learning the 'optimal' depth by using the growing strategy. But I am not convinced that the technique indeed finds optimal depths based on the regularity of the sub-network depths mentioned in my review. The rebuttal suggests reasons for the obtained regularity, but does not prove that these regular structures obtained are indeed optimal and not an artifact of the algorithm itself. The baselines are also using the same regular architectures, which distorts the overall picture because it is possible that a non-regular architecture provides a better trade-off.

While my rating doesn't change, I do think that the work is in an interesting direction. My final suggestions for the future are:
- Investigate where non-regular architectures (unequal sub-network depths) are in the trade-off between accuracy, flops and parameters.
- Investigate whether the proposed algorithm can be modified to easily find non-regular architectures if they can yield equally good performance as regular ones at similar or lower cost.
","[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: p-AutoGrow on CIFAR10 (K = 3).The seed net is Basic3ResNet-1-1-1.
Table 2: Figure 3 :Figure 4 :34Figure 3: AutoGrow vs manual search obtained by training many baselines from scratch. x − axis is the number of parameters. Dataset is CIFAR10.
Table 3: Figure 5 :Figure 6 :56Figure6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table3, which is much smoother compared to Figure6(a). Figure6(c)(d) visualize the trajectories of p-AutoGrow with K = 50 and 3. The 2D projection gives limited information to reveal the advantages of p-AutoGrow
Table 4: Figure 7 :7Figure 7: Loss surfaces around minima found by baselines and AutoGrow. Dataset is CIFAR10.
Table 5: Figure 8:The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table10.
Table 6: A simple example of AutoGrow.
Table 7: Comparison with previous works about layer growth.
Table 8: Network Morphism tested on CIFAR10.
Table 9: Ablation study of c-AutoGrow.
Table 10: p-AutoGrow with different seed architecture.
Table 11: p-AutoGrow with different growing interval K.
Table 12: The adaptability of AutoGrow to datasets
Table 13: Scaling up to ImageNet
Table 14: TaoWei, Changhu Wang, and Chang Wen Chen.  Modularized morphing of neural networks. arXiv preprint arXiv:1701.03281, 2017. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pp. 2074-2082, 2016. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.
Table 15: p-AutoGrow under initializers with K = 3.
Table 16: AutoGrow improves accuracy of plain nets.
Table 17: The efficiency of AutoGrow
Table 18: The adaptability of AutoGrow to dataset sizes

[INTRODUCTION]
Layer depth is one of the decisive factors of the success of Deep Neural Networks (DNNs). For example, image classification accuracy keeps improving as the depth of network models grows (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014;Szegedy et al., 2015;He et al., 2016;Huang et al., 2017). Although shallow networks cannot ensure high accuracy, DNNs composed of too many layers may suffer from over-fitting and convergence difficulty in training. How to obtain the optimal depth for a DNN still remains mysterious. For instance, ResNet-152 (He et al., 2016) uses 3, 8, 36 and 3 residual blocks under output sizes of 56 × 56, 28 × 28, 14 × 14 and 7 × 7, respectively, which don't show an obvious quantitative relation. In practice, people usually reply on some heuristic trials and tests to obtain the depth of a network: they first design a DNN with a specific depth and then train and evaluate the network on a given dataset; finally, they change the depth and repeat the procedure until the accuracy meets the requirement. Besides the high computational cost induced by the iteration process, such trial & test iterations must be repeated whenever dataset changes. In this paper, we propose AutoGrow that can automate depth discovery given a layer architecture. We will show that AutoGrow generalizes to different datasets and layer architectures.
There are some previous works which add or morph layers to increase the depth in DNNs. Vg-gNet (Simonyan & Zisserman, 2014) and DropIn (Smith et al., 2016) added new layers into shallower DNNs; Network Morphism (Wei et al., 2016;Chen et al., 2015) morphed each layer to multiple layers to increase the depth meanwhile preserving the function of the shallower net. Table 1 summarizes differences in this work. Their goal was to overcome difficulty of training deeper DNNs or accelerate it. Our goal is to automatically find an optimal depth. Moreover, previous works applied layer growth by once or a few times at pre-defined locations to grow a pre-defined number of layers; in contrast, ours automatically learns the number of new layers and growth locations without limiting growing times. We will summarize more related works in Section 4. Figure 1 illustrates an example of AutoGrow. It starts from the shallowest backbone network and gradually grows sub-modules (A sub-module can be one or more layers, e.g., a residual block); the growth stops once a stopping policy is satisfied. We studied multiple initializers of new layers and multiple growing policies, and surprisingly find that: (1) a random initializer works equally or better than complicated Network Morphism; (2) it is more effective to grow before a shallow net converges. We hypothesize that this is because a converged shallow net is an inadequate initialization for training deeper net, while random initialization can help to escape from a bad starting point.
Motivated by this, we intentionally avoid full convergence during the growing by using (1) random initialization of new layers, (2) a constant large learning rate, and (3) a short growing interval.  (3) We challenge the idea of Network Morphism, as random initialization works equally or better when growing layers. (4) We find that it is beneficial to rapidly grow layers before a shallower net converge, contradicting previous intuition.
2 AutoGrow -A DEPTH GROWING ALGORITHM Algorithm 1 AutoGrow Algorithm.
Input :
A seed shallow network g(X0) composed of M sub-networks F = {fi (•; Wi) : i = 0 . . . M − 1},
where each sub-network has only one sub-module (a dimension reduction sub-module); an epoch interval K to check growing and stopping policies; the number of fine-tuning epochs N after growing.

[INITIALIZATION:]
A Circular Linked List of sub-networks under growing:
subNetList = f0 (•; W0) → • • • → f M −1 (•; W M −1 ) ← −−−−−−−−−−−−−−−−−−−−−−−−−− − ;
The current growing sub-network: growingSubNet = subNetList.head() = f0 (•; W0);
The recent grown sub-network: grownSubNet = None; Process : A trained neural network g(X0) with learned depth.
#
Figure 1 gives an overview of the proposed AutoGrow. In this paper, we use network, sub-networks, sub-modules and layers to describe the architecture hierarchy. A network is composed of a cascade of sub-networks. A sub-network is composed of sub-modules, which typical share the same output size. A sub-module (e.g. a residual block) is an elementary growing block composed of one or a few layers. In this section, we rigorously formulate a generic version of AutoGrow which will be materialized in subsections. A deep convolutional network g(X 0 ) is a cascade of sub-networks by composing functions as g(X
0 ) = l (f M −1 (f M −2 (• • • f 1 (f 0 (X 0 )) • • • )))
, where X 0 is an input image, M is the number of sub-networks, l(•) is a loss function, and X i+1 = f i (X i ) is a sub-network that operates on an input image or a feature tensor X i ∈ R ci×hi×wi . Here, c i is the number of channels, and h i and w i are spatial dimensions. f i (X i ) is a simplified notation of f i (X i ; W i ), where W i is a set of sub-modules' parameters within the i-th sub-network. Thus W = {W i : i = 0 . . . M − 1} denotes the whole set of parameters in the DNN. To facilitate growing, the following properties are supported within a sub-network: (1) the first sub-module usually reduces the size of input feature maps, e.g., using pooling or convolution with a stride; and (2) all sub-modules in a sub-network maintain the same output size. As such, our framework can support popular networks, including VggNet-like plain networks (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2015), ResNets (He et al., 2016) and DenseNets (Huang et al., 2017). In this paper, we select ResNets and VggNet-like nets as representatives of DNNs with and without shortcuts, respectively.
With above notations, Algorithm 1 rigorously describes the AutoGrow algorithm. In brief, AutoGrow starts with the shallowest net where every sub-network has only one sub-module for spatial dimension reduction. AutoGrow loops over all growing sub-networks in order. For each sub-network, AutoGrow stacks a new sub-module. When the new sub-module does not improve the accuracy, the growth in corresponding sub-network will be permanently stopped. The details of our method will be materialized in the following subsections.

[SEED SHALLOW NETWORKS AND SUB-MODULES]
In this paper, in all datasets except ImageNet, we explore growing depth for four types of DNNs: (1) Basic3ResNet: the same ResNet used for CIFAR10 in He et al. (2016) In AutoGrow, the architectures of seed shallow networks and sub-modules are pre-defined. In plain DNNs, a sub-module is a stack of convolution, Batch Normalization and ReLU; in residual DNNs, a sub-module is a residual block. In AutoGrow, a sub-network is a stack of all sub-modules with the same output spatial size. Unlike He et al. (2016) which manually designed the depth, AutoGrow starts from a seed architecture in which each sub-network has only one sub-module and automatically learns the number of sub-modules.
On ImageNet, we apply the same backbones in He et al. (2016) as the seed architectures. A seed architecture has only one sub-module under each output spatial size. For a ResNet using basic residual blocks or bottleneck residual blocks (He et al., 2016), we respectively name it as Basic4ResNet or Bottleneck4ResNet. Plain4Net is also obtained by removing shortcuts in Basic4ResNet.

[SUB-MODULE INITIALIZERS]
Here we explain how to initialize a new sub-module W in initializer(W) mentioned in Algorithm 1. Network Morphism changes DNN architecture meanwhile preserving the loss function via special initialization of new layers, that is, g(X 0 ; W) = g(X 0 ; W ∪ W) ∀X 0 . A residual sub-module shows a nice property: when stacking a residual block and initializing the last Batch Normalization layer as zeros, the function of the shallower net is preserved but the DNN is morphed to a deeper net. Thus, Network Morphism can be easily implemented by this zero initialization (ZeroInit).
In this work, all layers in W are initialized using default randomization, except for a special treatment of the last Batch Normalization layer in a residual sub-module. Besides ZeroInit, we propose a new AdamInit for Network Morphism. In AdamInit, we freeze all parameters except the last Batch Normalization layer in W, and then use Adam optimizer (Kingma & Ba, 2014) to optimize the last Bath Normalization for maximum 10 epochs till the training accuracy of the deeper net is as good as the shallower one. After AdamInit, all parameters are jointly optimized. We view AdamInit as a Network Morphism because the training loss is similar after AdamInit. We empirically find that AdamInit can usually find a solution in less than 3 epochs. We also study random initialization of the last Batch Normalization layer using uniform (UniInit) or Gaussian (GauInit) noises with a standard deviation 1.0. We will show that GauInit obtains the best result, challenging the idea of Network Morphism (Wei et al., 2016;Chen et al., 2015).

[GROWING AND STOPPING POLICIES]
In Algorithm 1, a growing policy refers to meetGrowingPolicy(), which returns true when the network should grow a sub-module. Two growing policies are studied here:
1. Convergent Growth: meetGrowingPolicy() returns true when the improvement of validation accuracy is less than τ in the last K epochs. That is, in Convergent Growth, AutoGrow only grows when current network has converged. This is a similar growing criterion adopted in previous works (Elsken et al., 2017;Cai et al., 2018a;b). 2. Periodic Growth: meetGrowingPolicy() always returns true, that is, the network always grows every K epochs. Therefore, K is also the growing period. In the best practice of AutoGrow, K is small (e.g. K = 3) such that it grows before current network converges.
Our experiments will show that Periodic Growth outperforms Convergent Growth. We hypothesize that a fully converged shallower net is an inadequate initialization to train a deeper net. We will perform experiments to test this hypothesis and visualize optimization trajectory to illustrate it.
A stopping policy denotes meetStoppingPolicy() in Algorithm 1. When Convergent Growth is adopted, meetStoppingPolicy() returns true if a recent growth does not improve validation accuracy more than τ within K epochs. We use a similar stopping policy for Periodic Growth; however, as it can grow rapidly with a small period K (e.g. K = 3) before it converges, we use a larger window size J for stop. Specifically, when Periodic Growth is adopted, meetStoppingPolicy() returns true when the validation accuracy improves less than τ in the last J epochs, where J K.
Hyper-parameters τ , J and K control the operation of AutoGrow and can be easily setup and generalize well. τ denotes the significance of accuracy improvement for classification. We simply set τ = 0.05% in all experiments. J represents how many epochs to wait for an accuracy improvement before stopping the growth of a sub-network. It is more meaningful to consider stopping when the new net is trained to some extent. As such, we set J to the number of epochs T under the largest learning rate when training a baseline. K means how frequently AutoGrow checks the polices. In Convergent Growth, we simply set K = T , which is long enough to ensure convergence. In Periodic Growth, K is set to a small fraction of T to enable fast growth before convergence; more importantly, K = 3 is very robust to all networks and datasets. Therefore, all those hyper-parameters are very robust and strongly correlated to design considerations.

[EXPERIMENTS]
In this paper, we use Basic3ResNet-2-3-2, for instance, to denote a model architecture which contains 2, 3 and 2 sub-modules in the first, second and third sub-networks, respectively. Sometimes we simplify it as 2-3-2 for convenience. AutoGrow always starts from the shallowest depth of 1-1-1 and uses the maximum validation accuracy as the metric to guide growing and stopping. All DNN baselines are trained by SGD with momentum 0.9 using staircase learning rate. The initial learning rate is 0.1 in ResNets and 0.01 in plain networks. On ImageNet, baselines are trained using batch size 256 for 90 epochs, within which learning rate is decayed by 0.1× at epoch 30 and 60. In all other smaller datasets, baselines are trained using batch size 128 for 200 epochs and learning rate is decayed by 0.1× at epoch 100 and 150.
Our early experiments followed prior wisdom by growing layers with Network Morphism (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b), i.e., AutoGrow with ZeroInit (or AdamInit) and Convergent Growth policy; however, it stopped early with very shallow DNNs, failing to find optimal depth. We hypothesize that a converged shallow net with Network Morphism gives a bad initialization to train a deeper neural network. Section 3.1 experimentally test that the hypothesis is valid. To tackle this issue, we intentionally avoid convergence during growing by three simple solutions, which are evaluated in Section 3.2. Finally, Section 3.3 and Section 3.4 include extensive experiments to show the effectiveness of our final AutoGrow.

[SUBOPTIMUM OF NETWORK MORPHISM AND CONVERGENT GROWTH]
In this section, we study Network Morphism itself and its integration into our AutoGrow under Convergent Growth. When studying Network Morphism, we take the following steps: 1) train a shallower ResNet to converge, 2) stack residual blocks on top of each sub-network to morph to a deeper net, 3) use ZeroInit or AdamInit to initialize new layers, and 4) train the deeper net in a standard way. We compare the accuracy difference (""∆"") between Network Morphism and training the deeper net from scratch. Table 2 summaries our results. Network Morphism has a lower accuracy (negative ""∆"") in all the cases, which validates our hypothesis that a converged shallow network with Network Morphism gives a bad initialization to train a deeper net. We visualize the optimization trajectories in Appendix A.0.1 to illustrate the hypothesis.
To further validate our hypothesis, we integrate Network Morphism as the initializer in AutoGrow with Convergent Growth policy. We refer to this version of AutoGrow as c-AutoGrow with ""c-"" denoting ""Convergent."" More specific, we take ZeroInit or AdamInit as sub-module initializer and ""Convergent Growth"" policy in Algorithm 1. To recap, in this setting, AutoGrow trains a shallower net till it converges, then grows a sub-module which is initialized by Network Morphism, and repeats the same process till there is no further accuracy improvement. In every interval of K training epochs (train(g(X 0 ), K) in Algorithm 1), ""staircase"" learning rate is used. The learning rate is reset to 0.1 at the first epoch, and decayed by 0.1× at epoch K 2 and 3K 4 . The results are shown in Table 3 by ""staircase"" rows, which illustrate that c-AutoGrow can grow a DNN multiple times and finally find a depth. However, there are two problems: 1) the final accuracy is lower than training the found net from scratch, as indicated by ""∆"", validating our hypothesis; 2) the depth learning stops too early with a relatively shallower net, while a deeper net beyond the found depth can achieve a higher accuracy as we will show in Table 6. These problems provide a circumstantial evidence of the hypothesis that a converged shallow net with Network Morphism gives a bad initialization. Thus, AutoGrow cannot receive signals to continue growing after a limited number of growths. In Appendix A.0.1, Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3.

[ABLATION STUDY FOR AUTOGROW DESIGN]
Based on the findings in Section 3.1, we propose three simple but effective solutions to further enhance AutoGrow and refer it as p-AutoGrow, with ""p-"" denoting ""Periodic"": (1) Use a large constant learning rate for growing, i.e., 0.1 for residual networks and 0.01 for plain networks. Stochastic gradient descent with a large learning rate intrinsically introduces noises, which help to avoid a full convergence into a bad initialization from a shallower net. Note that staircase learning rate is still used for fine-tuning after discovering the final DNN; (2) Use random initialization (UniInit or GauInit) as noises to escape from an inadequate initialization; (3) Grow rapidly before a shallower net converges by taking Periodic Growth with a small K. p-AutoGrow is our final AutoGrow. In the rest part of this section, we perform ablation study to prove that the three solutions are effective. We start from c-AutoGrow, and incrementally add above solutions one by one and eventually obtain p-AutoGrow. In Table 3, first, we replace the staircase learning rate with a constant learning rate, the accuracy of AutoGrow improves and therefore ""∆"" improves; second, we further replace Network Morphism (ZeroInit or AdamInit) with a random initializer (UniInit or GauInit) and result in a bigger gain. Overall, combining a constant learning rate with GauInit performs the best. Thus, constant learning rate and GauInit are adopted in the remaining experiments, unless we explicitly specify them.  Note that, in this paper, we are more interested in automating depth discovery to find a final DNN (""found net"") with a high accuracy (""accu""). Ideally, the ""found net"" has a minimum depth, a larger depth than which cannot further improve ""accu"". We will show in Figure 3 that AutoGrow discovers a depth approximately satisfying this property. The ""∆"" is a metric to indicate how well shallower nets initialize deeper nets; a negative ""∆"" indicates that weight initialization from a shallower net hurts training of a deeper net; while a positive ""∆"" indicates AutoGrow helps training a deeper net, which is a byproduct of this work.
Finally, we apply the last solution -Periodic Growth, and obtain our final p-AutoGrow. Our ablation study results for p-AutoGrow are summarized in Table 5 and Table 4. Table 5 analyzes the impact of the growing period K. In general, K is a hyper-parameter to trade off speed and accuracy: a smaller K takes a longer learning time but discovers a deeper net, vice versa. Our results validate the preference of a faster growth (i.e. a smaller K). On CIFAR10/CIFAR100, the accuracy reaches plateau/peak at K = 3; further reducing K produces a deeper net while the accuracy gain is marginal/impossible. In the following, we simply select K = 3 for robustness test. More importantly, our quantitative results in Table 5 show that p-AutoGrow finds much deeper nets, overcoming the very-early stop issue in c-AutoGrow in Table 3. That is, Periodic Growth proposed in this work is much more effective than Convergent Growth utilized in previous work.
For sanity check, we perform the ablation study of initializers for p-AutoGrow. The results are in Table 8 in Appendix A.0.3, which further validates our wisdom on selecting GauInit. The motivation of Network Morphism in previous work was to start a deeper net from a loss function that has been well optimized by a shallower net, so as not to restart the deeper net training from scratch (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b). In all our experiments, we find this is sure even with random initialization. Figure 2 plots the convergence curves and learning process for ""42-42-42"" in Table 5. Even with GauInit, the loss and accuracy rapidly recover and no restart is observed. The convergence pattern in the ""Growing"" stage is similar to the ""Fine-tuning"" stage under the same learning rate (the initial learning rate 0.1). Similar results on ImageNet will be shown in Figure 8. Our results challenge the necessity of Network Morphism when growing neural networks.
At last, we perform the ablation study on the initial depth of the seed network. Table 4 demonstrates that a shallowest DNN works as well as a deeper seed. This implies that AutoGrow can appropriately stop regardless of the depth of the seed network. As the focus of this work is on depth automation, we prefer starting with the shallowest seed to avoid a manual search of a seed depth.  The seed net is Basic3ResNet-1-1-1.  

[ADAPTABILITY OF AUTOGROW]
To verify the adaptability of AutoGrow, we use an identical configuration (p-AutoGrow with K = 3) and test over 5 datasets and 4 seed architectures. Table 6 includes the results of all 20 combinations. Figure 3 compares AutoGrow with manual search which is obtained by training many DNNs with different depths from scratch. The results lead to the following conclusions and contributions:  2. For ResNets, a discovered depth ("" "" in Figure 3) falls at the location where accuracy saturates. This means AutoGrow discovers a near-optimal depth: a shallower depth will lose accuracy while a deeper one gains little. The final accuracy of AutoGrow is as good as training the discovered net from scratch as indicated by ""∆"" in Table 6, indicating that initialization from shallower nets does not hurt training of deeper nets. As a byproduct, in plain networks, there are large positive ""∆""s in Table 6. It implies that baselines fail to train very deep plain networks even using Batch Normalization, but AutoGrow enables the training of these networks; In Appendix A.0.3, Table 9 shows the accuracy improvement of plain networks by tuning K, approaching the accuracy of ResNets with the same depth.
3. For robustness and generalization study purpose, we stick to K = 3 in our experiments, however, we can tune K to trade off accuracy and model size. As shown in Figure 3, Auto-Grow discovers smaller DNNs when increasing K from 3 ("" "") to 50 ("" ""). Interestingly, the accuracy of plain networks even increases at K = 50. This implies the possibility of discovering a better accuracy-depth trade-off by tuning K, although we stick to K = 3 for generalizability study and it generalizes well.
4. In Table 6, AutoGrow discovers different depths under different sub-modules. The final accuracy is limited by the sub-module design, not by our AutoGrow. Given a sub-module architecture, our AutoGrow can always find a near-optimal depth. With a better sub-module architecture, such as NASNet cell , AutoGrow can improve accuracy.
Finally, our supposition is that: when the size of dataset is smaller, the optimal depth should be smaller. Under this supposition, we test the effectiveness of AutoGrow by sampling a subset of dataset and verify if AutoGrow can discover a shallower depth. In Appendix A.0.3, Table 11 summarizes the results. As expected, our experiments show that AutoGrow adapts to shallower networks when the datasets are smaller.

[SCALING TO IMAGENET AND EFFICIENCY]
In ImageNet, K = 3 should generalize well, but we explore AutoGrow with K = 2 and K = 5 to obtain an accuracy-depth trade-off line for comparison with human experts. The larger K = 5 enables AutoGrow to obtain a smaller DNN to trade-off accuracy and model size (computation) and the smaller K = 2 achieves higher accuracy. The results are shown in Table 7, which proves that AutoGrow automatically finds a good depth without any tuning. As a byproduct, the accuracy is even higher than training the found net from scratch, indicating that the Periodic Growth in AutoGrow helps training deeper nets. The comparison of AutoGrow and manual depth design (He et al., 2016) is in Figure 4, which shows that AutoGrow achieves better trade-off between accuracy and computation (measured by floating point operations).
In Appendix A.0.3, Table 10 summarizes the breakdown of wall-clock time in AutoGrow. The growing/searching time is as efficient as (often more efficient than) fine-tuning the single discovered DNN. The scalability of AutoGrow comes from its intrinsic features that (1) it grows quickly with a short period K and stops immediately if no improvement is sensed; and (2) the network is small at the beginning of growing. 

[RELATED WORK]
Neural Architecture Search (NAS) (Zoph & Le, 2016) and neural evolution (Miikkulainen et al., 2019;Angeline et al., 1994;Stanley & Miikkulainen, 2002;Liu et al., 2017a;Real et al., 2017) can search network architectures from a gigantic search space. In NAS, the depth of DNNs in the search space is fixed, while AutoGrow learns the depth. Some NAS methods (Bender et al., 2018;Liu et al., 2018b;Cortes et al., 2017) can find DNNs with different depths, however, the maximum depth is pre-defined and shallower nets are obtained by padding zero operations or selecting shallower branches, while our AutoGrow learns the depth in an open domain to find a minimum depth, beyond which no accuracy improvement can be obtained. Moreover, NAS is very computation and memory intensive. To accelerate NAS, one-shot models (Saxena & Verbeek, 2016;Pham et al., 2018;Bender et al., 2018), DARTS (Liu et al., 2018b) andNAS with Transferable Cell (Zoph et al., 2018;Liu et al., 2018a) were proposed. The search time reduces dramatically but is still long from practical perspective. It is still very challenging to deploy these methods to larger datasets such as ImageNet.
In contrast, our AutoGrow can scale up to ImageNet thanks to its short depth learning time, which is as efficient as training a single DNN.
In addition to architecture search which requires to train lots of DNNs from scratch, there are also many studies on learning neural structures within a single training. Structure pruning and growing were proposed for different goals, such as efficient inference (Wen et al., 2016;Li et al., 2016;Lebedev & Lempitsky, 2016;He et al., 2017;Luo et al., 2017;Liu et al., 2017b;Dai et al., 2017;Huang et al., 2018;Gordon et al., 2018;Du et al., 2019), lifelong learning (Yoon et al., 2017 and model adaptation (Feng & Darrell, 2015;Philipp & Carbonell, 2017). However, those works fixed the network depth and limited structure learning within the existing layers. Optimization over a DNN with fixed depth is easier as the skeleton architecture is known. AutoGrow performs in a scenario where the DNN depth is unknown hence we need to seek for the optimal depth. We hypothesize that a converged shallower net may not be an adequate initialization. Figure 5 visualizes and compares the optimization trajectories of Network Morphism and the training from scratch. In this figure, the shallower net is Basic3ResNet-3-3-3 (ResNet-20) and the deeper one is Basic3ResNet-5-5-5 (ResNet-32) in Table 2. The initializer is ZeroInit. The visualization method is extended from Li et al. (2018). Points on the trajectory are evenly sampled every a few epochs. To maximize the variance of trajectory, we use PCA to project from a high dimensional space to a 2D space and use the first two Principle Components (PC) to form the axes in Figure 5. The contours of training loss function and the trajectory are visualized around the final minimum of the deeper net. When projecting a shallower net to a deeper net space, zeros are padded for the parameters not existing in the deeper net. We must note that the loss increase along the trajectory does not truly represent the situation in high dimensional space, as the trajectory is just a projection. It is possible that the loss remains decreasing in the high dimension while it appears in an opposite way in the 2D space. The sharp detour at ""Morphing"" in Figure 5(a) may indicate that the shallower net plausibly converges to a point that the deeper net struggles to escape. In contrast, Figure 5(b) shows that the trajectory of the direct optimization in the deeper space smoothly converges to a better minimum.
Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure 6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table 3, which is much smoother compared to Figure 6(a).   10.
Table 11 summarizes the adaptability of AutoGrow to the sizes of dataset. In each set of experiments, dataset is randomly down-sampled to 100%, 75%, 50% and 25%. For a fair comparison, K is divided by the percentage of dataset such that the number of mini-batches between growths remains    The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table 10.","[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN."
AutoGrow: Automatic Layer Growing in Deep Convolutional Networks,S1x0CnEtvB.json,"The paper presents a meta-learning algorithm to automatically detemine the depth of neural network through a policy to add depth if this bring improvement on accuracy.

I have conserved opinion based on the technique being used here is extremely simple, basically is an implementation of naive greedy algorithm in such a scenario, which implies the problem may not be intrinsically hard, or even useful. The paper consists of detailed narrative about how these procedure are conducted, but still, it is really hard for me to find the true merit to appreciate, and why this brings a nontrivial and usefull contribution. The tables, visualization figures also didnot imply too much about whether this is more than overfitting on previous works with hand-chosen depth. ","[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN.

[CAPTIONS]
Table 1: Figure 2 :2Figure 2: p-AutoGrow on CIFAR10 (K = 3).The seed net is Basic3ResNet-1-1-1.
Table 2: Figure 3 :Figure 4 :34Figure 3: AutoGrow vs manual search obtained by training many baselines from scratch. x − axis is the number of parameters. Dataset is CIFAR10.
Table 3: Figure 5 :Figure 6 :56Figure6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table3, which is much smoother compared to Figure6(a). Figure6(c)(d) visualize the trajectories of p-AutoGrow with K = 50 and 3. The 2D projection gives limited information to reveal the advantages of p-AutoGrow
Table 4: Figure 7 :7Figure 7: Loss surfaces around minima found by baselines and AutoGrow. Dataset is CIFAR10.
Table 5: Figure 8:The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table10.
Table 6: A simple example of AutoGrow.
Table 7: Comparison with previous works about layer growth.
Table 8: Network Morphism tested on CIFAR10.
Table 9: Ablation study of c-AutoGrow.
Table 10: p-AutoGrow with different seed architecture.
Table 11: p-AutoGrow with different growing interval K.
Table 12: The adaptability of AutoGrow to datasets
Table 13: Scaling up to ImageNet
Table 14: TaoWei, Changhu Wang, and Chang Wen Chen.  Modularized morphing of neural networks. arXiv preprint arXiv:1701.03281, 2017. Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. In Advances in neural information processing systems, pp. 2074-2082, 2016. Jaehong Yoon, Eunho Yang, Jeongtae Lee, and Sung Ju Hwang. Lifelong learning with dynamically expandable networks. arXiv preprint arXiv:1708.01547, 2017.
Table 15: p-AutoGrow under initializers with K = 3.
Table 16: AutoGrow improves accuracy of plain nets.
Table 17: The efficiency of AutoGrow
Table 18: The adaptability of AutoGrow to dataset sizes

[INTRODUCTION]
Layer depth is one of the decisive factors of the success of Deep Neural Networks (DNNs). For example, image classification accuracy keeps improving as the depth of network models grows (Krizhevsky et al., 2012;Simonyan & Zisserman, 2014;Szegedy et al., 2015;He et al., 2016;Huang et al., 2017). Although shallow networks cannot ensure high accuracy, DNNs composed of too many layers may suffer from over-fitting and convergence difficulty in training. How to obtain the optimal depth for a DNN still remains mysterious. For instance, ResNet-152 (He et al., 2016) uses 3, 8, 36 and 3 residual blocks under output sizes of 56 × 56, 28 × 28, 14 × 14 and 7 × 7, respectively, which don't show an obvious quantitative relation. In practice, people usually reply on some heuristic trials and tests to obtain the depth of a network: they first design a DNN with a specific depth and then train and evaluate the network on a given dataset; finally, they change the depth and repeat the procedure until the accuracy meets the requirement. Besides the high computational cost induced by the iteration process, such trial & test iterations must be repeated whenever dataset changes. In this paper, we propose AutoGrow that can automate depth discovery given a layer architecture. We will show that AutoGrow generalizes to different datasets and layer architectures.
There are some previous works which add or morph layers to increase the depth in DNNs. Vg-gNet (Simonyan & Zisserman, 2014) and DropIn (Smith et al., 2016) added new layers into shallower DNNs; Network Morphism (Wei et al., 2016;Chen et al., 2015) morphed each layer to multiple layers to increase the depth meanwhile preserving the function of the shallower net. Table 1 summarizes differences in this work. Their goal was to overcome difficulty of training deeper DNNs or accelerate it. Our goal is to automatically find an optimal depth. Moreover, previous works applied layer growth by once or a few times at pre-defined locations to grow a pre-defined number of layers; in contrast, ours automatically learns the number of new layers and growth locations without limiting growing times. We will summarize more related works in Section 4. Figure 1 illustrates an example of AutoGrow. It starts from the shallowest backbone network and gradually grows sub-modules (A sub-module can be one or more layers, e.g., a residual block); the growth stops once a stopping policy is satisfied. We studied multiple initializers of new layers and multiple growing policies, and surprisingly find that: (1) a random initializer works equally or better than complicated Network Morphism; (2) it is more effective to grow before a shallow net converges. We hypothesize that this is because a converged shallow net is an inadequate initialization for training deeper net, while random initialization can help to escape from a bad starting point.
Motivated by this, we intentionally avoid full convergence during the growing by using (1) random initialization of new layers, (2) a constant large learning rate, and (3) a short growing interval.  (3) We challenge the idea of Network Morphism, as random initialization works equally or better when growing layers. (4) We find that it is beneficial to rapidly grow layers before a shallower net converge, contradicting previous intuition.
2 AutoGrow -A DEPTH GROWING ALGORITHM Algorithm 1 AutoGrow Algorithm.
Input :
A seed shallow network g(X0) composed of M sub-networks F = {fi (•; Wi) : i = 0 . . . M − 1},
where each sub-network has only one sub-module (a dimension reduction sub-module); an epoch interval K to check growing and stopping policies; the number of fine-tuning epochs N after growing.

[INITIALIZATION:]
A Circular Linked List of sub-networks under growing:
subNetList = f0 (•; W0) → • • • → f M −1 (•; W M −1 ) ← −−−−−−−−−−−−−−−−−−−−−−−−−− − ;
The current growing sub-network: growingSubNet = subNetList.head() = f0 (•; W0);
The recent grown sub-network: grownSubNet = None; Process : A trained neural network g(X0) with learned depth.
#
Figure 1 gives an overview of the proposed AutoGrow. In this paper, we use network, sub-networks, sub-modules and layers to describe the architecture hierarchy. A network is composed of a cascade of sub-networks. A sub-network is composed of sub-modules, which typical share the same output size. A sub-module (e.g. a residual block) is an elementary growing block composed of one or a few layers. In this section, we rigorously formulate a generic version of AutoGrow which will be materialized in subsections. A deep convolutional network g(X 0 ) is a cascade of sub-networks by composing functions as g(X
0 ) = l (f M −1 (f M −2 (• • • f 1 (f 0 (X 0 )) • • • )))
, where X 0 is an input image, M is the number of sub-networks, l(•) is a loss function, and X i+1 = f i (X i ) is a sub-network that operates on an input image or a feature tensor X i ∈ R ci×hi×wi . Here, c i is the number of channels, and h i and w i are spatial dimensions. f i (X i ) is a simplified notation of f i (X i ; W i ), where W i is a set of sub-modules' parameters within the i-th sub-network. Thus W = {W i : i = 0 . . . M − 1} denotes the whole set of parameters in the DNN. To facilitate growing, the following properties are supported within a sub-network: (1) the first sub-module usually reduces the size of input feature maps, e.g., using pooling or convolution with a stride; and (2) all sub-modules in a sub-network maintain the same output size. As such, our framework can support popular networks, including VggNet-like plain networks (Simonyan & Zisserman, 2014), GoogLeNet (Szegedy et al., 2015), ResNets (He et al., 2016) and DenseNets (Huang et al., 2017). In this paper, we select ResNets and VggNet-like nets as representatives of DNNs with and without shortcuts, respectively.
With above notations, Algorithm 1 rigorously describes the AutoGrow algorithm. In brief, AutoGrow starts with the shallowest net where every sub-network has only one sub-module for spatial dimension reduction. AutoGrow loops over all growing sub-networks in order. For each sub-network, AutoGrow stacks a new sub-module. When the new sub-module does not improve the accuracy, the growth in corresponding sub-network will be permanently stopped. The details of our method will be materialized in the following subsections.

[SEED SHALLOW NETWORKS AND SUB-MODULES]
In this paper, in all datasets except ImageNet, we explore growing depth for four types of DNNs: (1) Basic3ResNet: the same ResNet used for CIFAR10 in He et al. (2016) In AutoGrow, the architectures of seed shallow networks and sub-modules are pre-defined. In plain DNNs, a sub-module is a stack of convolution, Batch Normalization and ReLU; in residual DNNs, a sub-module is a residual block. In AutoGrow, a sub-network is a stack of all sub-modules with the same output spatial size. Unlike He et al. (2016) which manually designed the depth, AutoGrow starts from a seed architecture in which each sub-network has only one sub-module and automatically learns the number of sub-modules.
On ImageNet, we apply the same backbones in He et al. (2016) as the seed architectures. A seed architecture has only one sub-module under each output spatial size. For a ResNet using basic residual blocks or bottleneck residual blocks (He et al., 2016), we respectively name it as Basic4ResNet or Bottleneck4ResNet. Plain4Net is also obtained by removing shortcuts in Basic4ResNet.

[SUB-MODULE INITIALIZERS]
Here we explain how to initialize a new sub-module W in initializer(W) mentioned in Algorithm 1. Network Morphism changes DNN architecture meanwhile preserving the loss function via special initialization of new layers, that is, g(X 0 ; W) = g(X 0 ; W ∪ W) ∀X 0 . A residual sub-module shows a nice property: when stacking a residual block and initializing the last Batch Normalization layer as zeros, the function of the shallower net is preserved but the DNN is morphed to a deeper net. Thus, Network Morphism can be easily implemented by this zero initialization (ZeroInit).
In this work, all layers in W are initialized using default randomization, except for a special treatment of the last Batch Normalization layer in a residual sub-module. Besides ZeroInit, we propose a new AdamInit for Network Morphism. In AdamInit, we freeze all parameters except the last Batch Normalization layer in W, and then use Adam optimizer (Kingma & Ba, 2014) to optimize the last Bath Normalization for maximum 10 epochs till the training accuracy of the deeper net is as good as the shallower one. After AdamInit, all parameters are jointly optimized. We view AdamInit as a Network Morphism because the training loss is similar after AdamInit. We empirically find that AdamInit can usually find a solution in less than 3 epochs. We also study random initialization of the last Batch Normalization layer using uniform (UniInit) or Gaussian (GauInit) noises with a standard deviation 1.0. We will show that GauInit obtains the best result, challenging the idea of Network Morphism (Wei et al., 2016;Chen et al., 2015).

[GROWING AND STOPPING POLICIES]
In Algorithm 1, a growing policy refers to meetGrowingPolicy(), which returns true when the network should grow a sub-module. Two growing policies are studied here:
1. Convergent Growth: meetGrowingPolicy() returns true when the improvement of validation accuracy is less than τ in the last K epochs. That is, in Convergent Growth, AutoGrow only grows when current network has converged. This is a similar growing criterion adopted in previous works (Elsken et al., 2017;Cai et al., 2018a;b). 2. Periodic Growth: meetGrowingPolicy() always returns true, that is, the network always grows every K epochs. Therefore, K is also the growing period. In the best practice of AutoGrow, K is small (e.g. K = 3) such that it grows before current network converges.
Our experiments will show that Periodic Growth outperforms Convergent Growth. We hypothesize that a fully converged shallower net is an inadequate initialization to train a deeper net. We will perform experiments to test this hypothesis and visualize optimization trajectory to illustrate it.
A stopping policy denotes meetStoppingPolicy() in Algorithm 1. When Convergent Growth is adopted, meetStoppingPolicy() returns true if a recent growth does not improve validation accuracy more than τ within K epochs. We use a similar stopping policy for Periodic Growth; however, as it can grow rapidly with a small period K (e.g. K = 3) before it converges, we use a larger window size J for stop. Specifically, when Periodic Growth is adopted, meetStoppingPolicy() returns true when the validation accuracy improves less than τ in the last J epochs, where J K.
Hyper-parameters τ , J and K control the operation of AutoGrow and can be easily setup and generalize well. τ denotes the significance of accuracy improvement for classification. We simply set τ = 0.05% in all experiments. J represents how many epochs to wait for an accuracy improvement before stopping the growth of a sub-network. It is more meaningful to consider stopping when the new net is trained to some extent. As such, we set J to the number of epochs T under the largest learning rate when training a baseline. K means how frequently AutoGrow checks the polices. In Convergent Growth, we simply set K = T , which is long enough to ensure convergence. In Periodic Growth, K is set to a small fraction of T to enable fast growth before convergence; more importantly, K = 3 is very robust to all networks and datasets. Therefore, all those hyper-parameters are very robust and strongly correlated to design considerations.

[EXPERIMENTS]
In this paper, we use Basic3ResNet-2-3-2, for instance, to denote a model architecture which contains 2, 3 and 2 sub-modules in the first, second and third sub-networks, respectively. Sometimes we simplify it as 2-3-2 for convenience. AutoGrow always starts from the shallowest depth of 1-1-1 and uses the maximum validation accuracy as the metric to guide growing and stopping. All DNN baselines are trained by SGD with momentum 0.9 using staircase learning rate. The initial learning rate is 0.1 in ResNets and 0.01 in plain networks. On ImageNet, baselines are trained using batch size 256 for 90 epochs, within which learning rate is decayed by 0.1× at epoch 30 and 60. In all other smaller datasets, baselines are trained using batch size 128 for 200 epochs and learning rate is decayed by 0.1× at epoch 100 and 150.
Our early experiments followed prior wisdom by growing layers with Network Morphism (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b), i.e., AutoGrow with ZeroInit (or AdamInit) and Convergent Growth policy; however, it stopped early with very shallow DNNs, failing to find optimal depth. We hypothesize that a converged shallow net with Network Morphism gives a bad initialization to train a deeper neural network. Section 3.1 experimentally test that the hypothesis is valid. To tackle this issue, we intentionally avoid convergence during growing by three simple solutions, which are evaluated in Section 3.2. Finally, Section 3.3 and Section 3.4 include extensive experiments to show the effectiveness of our final AutoGrow.

[SUBOPTIMUM OF NETWORK MORPHISM AND CONVERGENT GROWTH]
In this section, we study Network Morphism itself and its integration into our AutoGrow under Convergent Growth. When studying Network Morphism, we take the following steps: 1) train a shallower ResNet to converge, 2) stack residual blocks on top of each sub-network to morph to a deeper net, 3) use ZeroInit or AdamInit to initialize new layers, and 4) train the deeper net in a standard way. We compare the accuracy difference (""∆"") between Network Morphism and training the deeper net from scratch. Table 2 summaries our results. Network Morphism has a lower accuracy (negative ""∆"") in all the cases, which validates our hypothesis that a converged shallow network with Network Morphism gives a bad initialization to train a deeper net. We visualize the optimization trajectories in Appendix A.0.1 to illustrate the hypothesis.
To further validate our hypothesis, we integrate Network Morphism as the initializer in AutoGrow with Convergent Growth policy. We refer to this version of AutoGrow as c-AutoGrow with ""c-"" denoting ""Convergent."" More specific, we take ZeroInit or AdamInit as sub-module initializer and ""Convergent Growth"" policy in Algorithm 1. To recap, in this setting, AutoGrow trains a shallower net till it converges, then grows a sub-module which is initialized by Network Morphism, and repeats the same process till there is no further accuracy improvement. In every interval of K training epochs (train(g(X 0 ), K) in Algorithm 1), ""staircase"" learning rate is used. The learning rate is reset to 0.1 at the first epoch, and decayed by 0.1× at epoch K 2 and 3K 4 . The results are shown in Table 3 by ""staircase"" rows, which illustrate that c-AutoGrow can grow a DNN multiple times and finally find a depth. However, there are two problems: 1) the final accuracy is lower than training the found net from scratch, as indicated by ""∆"", validating our hypothesis; 2) the depth learning stops too early with a relatively shallower net, while a deeper net beyond the found depth can achieve a higher accuracy as we will show in Table 6. These problems provide a circumstantial evidence of the hypothesis that a converged shallow net with Network Morphism gives a bad initialization. Thus, AutoGrow cannot receive signals to continue growing after a limited number of growths. In Appendix A.0.1, Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3.

[ABLATION STUDY FOR AUTOGROW DESIGN]
Based on the findings in Section 3.1, we propose three simple but effective solutions to further enhance AutoGrow and refer it as p-AutoGrow, with ""p-"" denoting ""Periodic"": (1) Use a large constant learning rate for growing, i.e., 0.1 for residual networks and 0.01 for plain networks. Stochastic gradient descent with a large learning rate intrinsically introduces noises, which help to avoid a full convergence into a bad initialization from a shallower net. Note that staircase learning rate is still used for fine-tuning after discovering the final DNN; (2) Use random initialization (UniInit or GauInit) as noises to escape from an inadequate initialization; (3) Grow rapidly before a shallower net converges by taking Periodic Growth with a small K. p-AutoGrow is our final AutoGrow. In the rest part of this section, we perform ablation study to prove that the three solutions are effective. We start from c-AutoGrow, and incrementally add above solutions one by one and eventually obtain p-AutoGrow. In Table 3, first, we replace the staircase learning rate with a constant learning rate, the accuracy of AutoGrow improves and therefore ""∆"" improves; second, we further replace Network Morphism (ZeroInit or AdamInit) with a random initializer (UniInit or GauInit) and result in a bigger gain. Overall, combining a constant learning rate with GauInit performs the best. Thus, constant learning rate and GauInit are adopted in the remaining experiments, unless we explicitly specify them.  Note that, in this paper, we are more interested in automating depth discovery to find a final DNN (""found net"") with a high accuracy (""accu""). Ideally, the ""found net"" has a minimum depth, a larger depth than which cannot further improve ""accu"". We will show in Figure 3 that AutoGrow discovers a depth approximately satisfying this property. The ""∆"" is a metric to indicate how well shallower nets initialize deeper nets; a negative ""∆"" indicates that weight initialization from a shallower net hurts training of a deeper net; while a positive ""∆"" indicates AutoGrow helps training a deeper net, which is a byproduct of this work.
Finally, we apply the last solution -Periodic Growth, and obtain our final p-AutoGrow. Our ablation study results for p-AutoGrow are summarized in Table 5 and Table 4. Table 5 analyzes the impact of the growing period K. In general, K is a hyper-parameter to trade off speed and accuracy: a smaller K takes a longer learning time but discovers a deeper net, vice versa. Our results validate the preference of a faster growth (i.e. a smaller K). On CIFAR10/CIFAR100, the accuracy reaches plateau/peak at K = 3; further reducing K produces a deeper net while the accuracy gain is marginal/impossible. In the following, we simply select K = 3 for robustness test. More importantly, our quantitative results in Table 5 show that p-AutoGrow finds much deeper nets, overcoming the very-early stop issue in c-AutoGrow in Table 3. That is, Periodic Growth proposed in this work is much more effective than Convergent Growth utilized in previous work.
For sanity check, we perform the ablation study of initializers for p-AutoGrow. The results are in Table 8 in Appendix A.0.3, which further validates our wisdom on selecting GauInit. The motivation of Network Morphism in previous work was to start a deeper net from a loss function that has been well optimized by a shallower net, so as not to restart the deeper net training from scratch (Wei et al., 2016;Chen et al., 2015;Elsken et al., 2017;Cai et al., 2018a;b). In all our experiments, we find this is sure even with random initialization. Figure 2 plots the convergence curves and learning process for ""42-42-42"" in Table 5. Even with GauInit, the loss and accuracy rapidly recover and no restart is observed. The convergence pattern in the ""Growing"" stage is similar to the ""Fine-tuning"" stage under the same learning rate (the initial learning rate 0.1). Similar results on ImageNet will be shown in Figure 8. Our results challenge the necessity of Network Morphism when growing neural networks.
At last, we perform the ablation study on the initial depth of the seed network. Table 4 demonstrates that a shallowest DNN works as well as a deeper seed. This implies that AutoGrow can appropriately stop regardless of the depth of the seed network. As the focus of this work is on depth automation, we prefer starting with the shallowest seed to avoid a manual search of a seed depth.  The seed net is Basic3ResNet-1-1-1.  

[ADAPTABILITY OF AUTOGROW]
To verify the adaptability of AutoGrow, we use an identical configuration (p-AutoGrow with K = 3) and test over 5 datasets and 4 seed architectures. Table 6 includes the results of all 20 combinations. Figure 3 compares AutoGrow with manual search which is obtained by training many DNNs with different depths from scratch. The results lead to the following conclusions and contributions:  2. For ResNets, a discovered depth ("" "" in Figure 3) falls at the location where accuracy saturates. This means AutoGrow discovers a near-optimal depth: a shallower depth will lose accuracy while a deeper one gains little. The final accuracy of AutoGrow is as good as training the discovered net from scratch as indicated by ""∆"" in Table 6, indicating that initialization from shallower nets does not hurt training of deeper nets. As a byproduct, in plain networks, there are large positive ""∆""s in Table 6. It implies that baselines fail to train very deep plain networks even using Batch Normalization, but AutoGrow enables the training of these networks; In Appendix A.0.3, Table 9 shows the accuracy improvement of plain networks by tuning K, approaching the accuracy of ResNets with the same depth.
3. For robustness and generalization study purpose, we stick to K = 3 in our experiments, however, we can tune K to trade off accuracy and model size. As shown in Figure 3, Auto-Grow discovers smaller DNNs when increasing K from 3 ("" "") to 50 ("" ""). Interestingly, the accuracy of plain networks even increases at K = 50. This implies the possibility of discovering a better accuracy-depth trade-off by tuning K, although we stick to K = 3 for generalizability study and it generalizes well.
4. In Table 6, AutoGrow discovers different depths under different sub-modules. The final accuracy is limited by the sub-module design, not by our AutoGrow. Given a sub-module architecture, our AutoGrow can always find a near-optimal depth. With a better sub-module architecture, such as NASNet cell , AutoGrow can improve accuracy.
Finally, our supposition is that: when the size of dataset is smaller, the optimal depth should be smaller. Under this supposition, we test the effectiveness of AutoGrow by sampling a subset of dataset and verify if AutoGrow can discover a shallower depth. In Appendix A.0.3, Table 11 summarizes the results. As expected, our experiments show that AutoGrow adapts to shallower networks when the datasets are smaller.

[SCALING TO IMAGENET AND EFFICIENCY]
In ImageNet, K = 3 should generalize well, but we explore AutoGrow with K = 2 and K = 5 to obtain an accuracy-depth trade-off line for comparison with human experts. The larger K = 5 enables AutoGrow to obtain a smaller DNN to trade-off accuracy and model size (computation) and the smaller K = 2 achieves higher accuracy. The results are shown in Table 7, which proves that AutoGrow automatically finds a good depth without any tuning. As a byproduct, the accuracy is even higher than training the found net from scratch, indicating that the Periodic Growth in AutoGrow helps training deeper nets. The comparison of AutoGrow and manual depth design (He et al., 2016) is in Figure 4, which shows that AutoGrow achieves better trade-off between accuracy and computation (measured by floating point operations).
In Appendix A.0.3, Table 10 summarizes the breakdown of wall-clock time in AutoGrow. The growing/searching time is as efficient as (often more efficient than) fine-tuning the single discovered DNN. The scalability of AutoGrow comes from its intrinsic features that (1) it grows quickly with a short period K and stops immediately if no improvement is sensed; and (2) the network is small at the beginning of growing. 

[RELATED WORK]
Neural Architecture Search (NAS) (Zoph & Le, 2016) and neural evolution (Miikkulainen et al., 2019;Angeline et al., 1994;Stanley & Miikkulainen, 2002;Liu et al., 2017a;Real et al., 2017) can search network architectures from a gigantic search space. In NAS, the depth of DNNs in the search space is fixed, while AutoGrow learns the depth. Some NAS methods (Bender et al., 2018;Liu et al., 2018b;Cortes et al., 2017) can find DNNs with different depths, however, the maximum depth is pre-defined and shallower nets are obtained by padding zero operations or selecting shallower branches, while our AutoGrow learns the depth in an open domain to find a minimum depth, beyond which no accuracy improvement can be obtained. Moreover, NAS is very computation and memory intensive. To accelerate NAS, one-shot models (Saxena & Verbeek, 2016;Pham et al., 2018;Bender et al., 2018), DARTS (Liu et al., 2018b) andNAS with Transferable Cell (Zoph et al., 2018;Liu et al., 2018a) were proposed. The search time reduces dramatically but is still long from practical perspective. It is still very challenging to deploy these methods to larger datasets such as ImageNet.
In contrast, our AutoGrow can scale up to ImageNet thanks to its short depth learning time, which is as efficient as training a single DNN.
In addition to architecture search which requires to train lots of DNNs from scratch, there are also many studies on learning neural structures within a single training. Structure pruning and growing were proposed for different goals, such as efficient inference (Wen et al., 2016;Li et al., 2016;Lebedev & Lempitsky, 2016;He et al., 2017;Luo et al., 2017;Liu et al., 2017b;Dai et al., 2017;Huang et al., 2018;Gordon et al., 2018;Du et al., 2019), lifelong learning (Yoon et al., 2017 and model adaptation (Feng & Darrell, 2015;Philipp & Carbonell, 2017). However, those works fixed the network depth and limited structure learning within the existing layers. Optimization over a DNN with fixed depth is easier as the skeleton architecture is known. AutoGrow performs in a scenario where the DNN depth is unknown hence we need to seek for the optimal depth. We hypothesize that a converged shallower net may not be an adequate initialization. Figure 5 visualizes and compares the optimization trajectories of Network Morphism and the training from scratch. In this figure, the shallower net is Basic3ResNet-3-3-3 (ResNet-20) and the deeper one is Basic3ResNet-5-5-5 (ResNet-32) in Table 2. The initializer is ZeroInit. The visualization method is extended from Li et al. (2018). Points on the trajectory are evenly sampled every a few epochs. To maximize the variance of trajectory, we use PCA to project from a high dimensional space to a 2D space and use the first two Principle Components (PC) to form the axes in Figure 5. The contours of training loss function and the trajectory are visualized around the final minimum of the deeper net. When projecting a shallower net to a deeper net space, zeros are padded for the parameters not existing in the deeper net. We must note that the loss increase along the trajectory does not truly represent the situation in high dimensional space, as the trajectory is just a projection. It is possible that the loss remains decreasing in the high dimension while it appears in an opposite way in the 2D space. The sharp detour at ""Morphing"" in Figure 5(a) may indicate that the shallower net plausibly converges to a point that the deeper net struggles to escape. In contrast, Figure 5(b) shows that the trajectory of the direct optimization in the deeper space smoothly converges to a better minimum.
Figure 6(a) visualizes the trajectory of c-AutoGrow corresponding to row ""2-3-6"" in Table 3. Along the trajectory, there are many trials to detour and escape an initialization from a shallower net. Figure 6(b) visualizes the trajectory corresponding to row ""2-4-3"" in Table 3, which is much smoother compared to Figure 6(a).   10.
Table 11 summarizes the adaptability of AutoGrow to the sizes of dataset. In each set of experiments, dataset is randomly down-sampled to 100%, 75%, 50% and 25%. For a fair comparison, K is divided by the percentage of dataset such that the number of mini-batches between growths remains    The convergence curves and growing process on ImageNet for (a) Basic4ResNet-9-3-6-4 and (b) Plain4Net-6-6-6-6 in Table 10.","[TITLE]
AutoGrow: AUTOMATIC LAYER GROWING IN DEEP CONVOLUTIONAL NETWORKS

[ABSTRACT]
Depth is a key component of Deep Neural Networks (DNNs), however, designing depth is heuristic and requires many human efforts. We propose AutoGrow to automate depth discovery in DNNs: starting from a shallow seed architecture, AutoGrow grows new layers if the growth improves the accuracy; otherwise, stops growing and thus discovers the depth. We propose robust growing and stopping policies to generalize to different network architectures and datasets. Our experiments show that by applying the same policy to different network architectures, AutoGrow can always discover near-optimal depth on various datasets of MNIST, FashionMNIST, SVHN, CIFAR10, CIFAR100 and ImageNet. For example, in terms of accuracy-computation trade-off, AutoGrow discovers a better depth combination in ResNets than human experts. Our AutoGrow is efficient. It discovers depth within similar time of training a single DNN."
Bayesian Time Series Forecasting with Change Point and Anomaly Detection,rJLTTe-0W.json,"Minor comments:
- page 3. “The observation equation and transition equations together (i.e., Equation (1,2,3)) together define “ - one “together” should be removed
- page 4. “From Figure 2, the joint distribution (i.e., the likelihood function ” - there should be additional bracket
- page 7. “We can further integral out αn “ -> integrate out

Major comments:
The paper is well-written. The paper considers structural time-series model with seasonal component and stochastic trend, which allow for change-points and structural breaks.

Such type of parametric models are widely considered in econometric literature, see e.g.
[1] Jalles, João Tovar, Structural Time Series Models and the Kalman Filter: A Concise Review (June 19, 2009). FEUNL Working Paper No. 541. Available at SSRN: https://ssrn.com/abstract=1496864 or http://dx.doi.org/10.2139/ssrn.1496864 
[2] Jacques J. F. Commandeur, Siem Jan Koopman, Marius Ooms. Statistical Software for State Space Methods // May 2011, Volume 41, Issue 1.
[3] Scott, Steven L. and Varian, Hal R., Predicting the Present with Bayesian Structural Time Series (June 28, 2013). Available at SSRN: https://ssrn.com/abstract=2304426 or http://dx.doi.org/10.2139/ssrn.2304426 
[4] Phillip G. Gould, Anne B. Koehler, J. Keith Ord, Ralph D. Snyder, Rob J. Hyndman, Farshid Vahid-Araghi, Forecasting time series with multiple seasonal patterns, In European Journal of Operational Research, Volume 191, Issue 1, 2008, Pages 207-222, ISSN 0377-2217, https://doi.org/10.1016/j.ejor.2007.08.024.
[5] A.C. Harvey, S. Peters. Estimation Procedures for structural time series models // Journal of Forecasting, Vol. 9, 89-108, 1990
[6] A. Harvey, S.J. Koopman, J. Penzer. Messy Time Series: A Unified approach // Advances in Econometrics, Vol. 13, pp. 103-143.

They also use Kalman filter and MCMC-based approaches to sample posterior to estimate hidden components.

There are also non-parametric approaches to extraction of components from quasi-periodic time-series, see e.g.
[7] Artemov A., Burnaev E. Detecting Performance Degradation of Software-Intensive Systems in the Presence of Trends and Long-Range Dependence // 16th International Conference on Data Mining Workshops (ICDMW), IEEE Conference Publications, pp. 29 - 36, 2016. DOI: 10.1109/ICDMW.2016.0013
[8] Alexey Artemov, Evgeny Burnaev and Andrey Lokot. Nonparametric Decomposition of Quasi-periodic Time Series for Change-point Detection // Proc. SPIE 9875, Eighth International Conference on Machine Vision, 987520 (December 8, 2015); 5 P. doi:10.1117/12.2228370;http://dx.doi.org/10.1117/12.2228370

In some of these papers models of structural brakes and change-points are also considered, see e.g. 
- page 118 in [6]
- papers [7, 8]

There were also Bayesian approaches for change-point detection, which are similar to the model of change-point, proposed in the considered paper, e.g.
[9] Ryan Prescott Adams, David J.C. MacKay. Bayesian Online Changepoint Detection // https://arxiv.org/abs/0710.3742
[10] Ryan Turner, Yunus Saatçi, and Carl Edward Rasmussen. Adaptive sequential Bayesian change point detection. In Zaïd Harchaoui, editor, NIPS Workshop on Temporal Segmentation, Whistler, BC, Canada, December 2009.

Thus,
- the paper does not provide comparison with relevant econometric literature on parametric structural time-series models,
- the paper does not provide comparison with relevant advanced change-point detection methods e.g. [7,8,9,10]. The comparison is provided only with very simple methods,
- the proposed model itself looks very similar to what can be found across econometric literature,
- the datasets, used for comparison, are very scarce. There are datasets for anomaly detection in time-series data, which should be used for extensive comparison, e.g. Numenta Anomaly Detection Benchmark.

Therefore, also the paper is well-written, 
- it lacks novelty,
- its topic does not perfectly fit topics of interest for ICLR,
So, I do not recommend this paper to be published.","[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Demostration of Decompositions.
Table 2: Algorithm 3 :3Proposed Algorithm Input: Observed time series y = (y 1 , y 2 , . . . , y n ), seasonality length S, length of time series for forecasting m, number of predictive paths N , change point minimum segment l Output: Change point detection z c , anomaly points z a , forecasting result y future = (y n+1 , y n+1 , . . . , y n+m ) and its distribution or predictive intervals Part I: Initialization; Initialize σ , σ o , σ u , σ r , σ v , σ w all with the empirical standard deviation of y;
Table 3: Figure 3 :3Figure 3: An example of synthesis data (left), and the result after applying our algorithm (right).
Table 4: Figure 4 :4Figure 4: Well-log Data (left). The result of implementing our proposed algorithm (right).
Table 5: Figure 5 :5Figure 5: Internet Traffic Data (top); The result of implementing our proposed algorithm (bottom).
Table 6: Two Categories for Hidden Variables α2, . . . , αn)
Table 7: Comparison of Forecasting in Well-log Data

[INTRODUCTION]
Time series forecasting has a rich and luminous history, and is essentially important in most of business operations nowadays. The main aim of time series forecasting is to carefully collect and rigorously study the past observations of a time series to develop an appropriate model which could describe the inherent structure of the series, in order to generate future values. For instance, the internet companies are interested in the number of daily active users (DAU), say, what is DAU after certain period of time, or when will reach their target DAU goal. Time series forecasting is a fruitful research area with many existing methodologies. The most popular and frequently used time series model might be the Autoregressive Integrated Moving Average (ARIMA) (Box et al., 2015;Zhang, 2003;Cochrane, 2005;Hipel & McLeod, 1994). Taking seasonality into consideration, Box et al. (2015) proposed the Seasonal ARIMA. The Holt-Winters method (Winters, 1960) is also very popular by using exponential smoothing. State space model (Durbin & Koopman, 2012;Scott & Varian, 2014;Brodersen et al., 2015) also attracts much attention, which is a linear function of an underlying Markov process plus additive noise. Exponential Smoothing State Space Model (ETS)  decomposes times series into error, trend, seasonal that change over time. Recently, deep learning is applied for time-series trend learning using LSTM (Tao Lin, 2017), bidirectional dynamic Boltzmann machine (Osogami et al., 2017) is applied for time-series long-term dependency learning, and coherent probabilistic forecast (Taieb et al., 2017) is proposed for a hierarchy or an aggregation-level comprising a set of time series. Orthogonal to these works, this paper focuses on robust ways of time series forecasting in presence of change points and anomalies.
In Internet time series forecasting, Google develops the Bayesian structure time series (BSTS) model (Brodersen et al., 2015;Scott & Varian, 2014) to capture the trend, seasonality, and similar components of the target series. Recently, Facebook proposes the Prophet approach (Taylor & Letham, 2017) based on a decomposable model with interpretable parameters that can be intuitively adjusted by analyst. However, as in the DAU example, some special events like Christmas Holiday or President Election, newly launched apps or features, may cause short period or long-term change of DAU, leading to weird forecasting of those traditional models. The aforementioned special cases are well known as • Anomaly points. The items, events or observations that don't conform to an expected pattern or other items in the dataset, leading to a sudden spike or decrease in the series.
• Change points. A market intervention, such as a new product launch or the onset of an advertising (or ad) campaign, may lead to the level change of the original series.
Time series forecasting without change/anomaly point detection and adjustment may also lead to bizarre forecasting since these models might learn the abrupt changes in the past. There are literatures on detecting anomaly or change points individually, examples can be found in Twitter (2017); Netflix (2017); Barry & Hartigan (1993); Killick & Eckley (2014); twitter (2017). However, the aforementioned change point detection models could not support detection in the presence of seasonality, while the presence of trend/change point is not handled by the anomaly detection models. Most importantly, there is a discrepancy between anomaly/change points detection and adjustment, and commonly used manually adjustment might be a bit arbitrary. Unfortunately, the forecasting gap caused by abnormal and change points, to the best of our knowledge, has not been given full attention and no good solution has been found so far. This paper is strongly motivated by bridging this gap.
In this paper, to overcome the limitations of the most (if not all) current models that the anomaly points and change points are not properly considered, we develop a state space time series forecasting model in the Bayesian framework that can simultaneously detect anomaly and change points and perform forecasting. The learned structure information related to anomaly and change points is automatically incorporated into the forecasting process, which naturally enhances the model prediction based on the feedback of state-space model. To solve the resultant optimization problem, an iterative algorithm based on Bayesian approximate inference with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. The novel model could explicitly capture the structure of change points, anomaly points, trend and seasonality, as also provide the distributions and forecasting intervals due to Bayesian forecasting framework. Both synthetic and real data sets show the better performance of proposed model, in comparison with existing baseline. Moreover, our proposed model outperforms state-of-the-art models in identifying anomaly and change points.
To summarize, our work has the following contributions.
• We proposed a robust 1 Bayesian state-space time series forecasting model that is able to explicitly capture the structures of change points and anomalies (which are generally ignored in most current models), and therefore automatically adapt for forecasting by incorporating the prior information of trend, seasonality, as well as change points and anomalies using state space modeling. Due to the enhancement of model description capability, the results of model prediction and abnormal and change points detection are mutually improved.
• To solve the resultant optimization problem, an effective algorithm based on approximate inference using Markov chain Monte Carlo (MCMC) is proposed with theoretical guaranteed forecasting paths.
• Our proposed method outperforms the state-of-the-art methods in time series forecasting in presence of change points and anomalies, and detects change points and anomalies with high accuracy and low false discovery rate on both tasks, outperforming popular change point and anomaly detection methods. Our method is flexible to capture the structure of time series under various scenarios with any component combinations of trend, seasonality, change points and anomalies. Therefore our method can be applied in many settings in practice.

[MODEL OVERVIEW]
State space time series model (Hangos et al., 2014) has been one of the most popular models in time series analysis. It is capable of fitting complicated time series structure including linear trend and seasonality. However, times series observed in real life are almost all prevailed with outliers. Change points, less in frequency but are still widely observed in real time series analysis. Unfortunately, both structures are ignored in the classic state space time series model. In the section, we aim to address this issue by introducing a novel state space time series model. Let y = (y 1 , y 2 , . . . , y n ) be a sequence of time series observations with length n. The ultimate goal is to forecast (y n+1 , y n+2 , . . .). The accuracy in forecasting lies in a successful decomposition of y into existing components. Apart from the residuals, we assume the time series is composed by trend, seasonality, change points and anomaly points. In a nutshell, we have an additive model with time series = trend + seasonality + change point + anomaly point + residual.
Figure 1 provides a demonstration of desired decomposition of time series. In Figure 1, the left panel shows the observed time series. And it can be decomposed into the remaining five panels. The shift in the change point panel shows where the change point lies. And the spikes in the last panel reveals the anomaly points.
As the classical state space model, we have observation equation and transition equations to model y and hidden variables. We use µ = (µ 1 , µ 2 , . . . , µ n ) to model trend, and use γ = (γ 1 , γ 2 , . . . , γ n ) to model seasonality. We use a binary vector z a = (z a 1 , z a 2 , . . . , z a n ) to indicate anomaly points. Then we have Observation equation:
y t = µ t + γ t + t , if z a t = 0 o t , if z a t = 1
.
(1)
The deviation between the observation y t and its ""mean"" µ t + γ t is modeled by t and o t , depending on the value of z a t . If z a t = 1, then y t is an anomaly point; otherwise it is not. Distinguished from the residues = ( 1 , 2 , . . . , n ), the anomaly is captured by o = (o 1 , o 2 , . . . , o n ) which has relative large magnitude.
The hidden state variable µ and γ have intrinsic structures. There are two transition equations, for trend and seasonality separately Transition Equations: Trend:
µ t = µ t−1 + δ t−1 + u t , if z c t = 0 r t , if z c t = 1 ,(2)
δ t = δ t−1 + v t , Seasonality: γ t = − S−1 s=1 γ t−s + w t .(3)
In Equation (2), δ = (δ 1 , δ 2 , . . . , δ n ) can be viewed as the ""slope"" of the trend, measuring how fast the trend changes over time. The change point component is also incorporated in Equation (2) by a binary vector z c = (z c 1 , z c 2 , . . . , z c n ). If z c t = 1, it means the t-th point is a change point, with µ t differs from µ t−1 + δ t−1 (which can be interpreted as the ""momentum"" from the previous status ) by r t ; otherwise it is not a change point and they differ by u t . We model the change points in a way such that r = (r 1 , r 2 , . . . , r n ) have larger magnitude compared u = (u 1 , u 2 , . . . , u n ). The ""slope"" part δ also has its own noise v = (v 1 , v 2 , . . . , v n ).
A first look on Equation (2) may bring up with the question that it is not presented in an exactly the same way as shown in Figure 1. In Figure 1, the change points component is a step function, and it is one of the five additive components along with trend, seasonality, anomaly points and residuals. Here we model the change point directly into the trend component. Though differing in formulation, they are equivalent to each other. We choose to model in as in Equation ( 2) due to simplicity, and its similarity with the definition of anomaly points in Equation (1).
The seasonality component is presented in Equation ( 3). Here S is the length of one season and w = (w 1 , w 2 , . . . , w n ) is the noise for seasonality. The seasonality component is assumed to have almost zero average in each season.
The observation equation and transition equations (i.e., Equation (1,2,3)) define how y is generated from all the hidden variables including change points and anomaly points. We continue to explore this new model, under a Bayesian framework.

[BAYESIAN FRAMEWORK]
Bayesian methods are widely used in many data analysis fields. It is easy to implement and interpret, and it also has the ability to produce posterior distribution. The Bayesian method on state space time series model has been investigated in Scott & Varian (2014);Brodersen et al. (2015). In this section, we also consider Bayesian framework for our novel state space time series model. We assume all the noises are normally distributed
{ t } n t=1 iid ∼ N (0, σ 2 ), {o t } n t=1 iid ∼ N (0, σ 2 o ), {u t } n t=1 iid ∼ N (0, σ 2 u ), {r t } n t=1 iid ∼ N (0, σ 2 r ), {v t } n t=1 iid ∼ N (0, σ 2 v ), {w t } n t=1 iid ∼ N (0, σ 2 w )
, where σ , σ o , σ u , σ r , σ v , σ w are parameters for standard deviation. As binary vectors, a natural choice is to model anomaly point indicator z a and change point indicator z c to the model them as Bernoulli random variables
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c ),
where p a , p c are probabilities for each point to be an anomaly or change point.
Figure 2: Graphical presentation of our model. Note that y is observed, highlighted by gray background, distinguished from all the remaining ones that are hidden. Among the hidden ones, squares indicate fixed parameters, and circles indicate random variables.
For simplicity, we denote α t = (µ t , δ t , γ t , γ t−1 , . . . , γ t−(S−2) ) to include the main hidden variables (except z a t and z c t ) in the transition equations. All the α t are well defined and can be generated from the previous status, except α 1 . We denote a 1 to be the parameter for α 1 , which can be interpreted as the ""mean"" for α 1 .
With Bayesian framework, we are able to represent our model graphically as in Figure 2. As shown in Figure 2, the only observations are y and all the others are hidden. In this paper, we assume there is no additional information on all the hidden states. If we have some prior information, for example, some points are more likely to be change points, then our model can be easily modified to incorporate such information, by using proper prior.
In Figure 2, we use squares and circles to classify unknown variables. Despite all being unknown, they actually behave differently according to their own functionality. For those in squares, they behave like turning parameters. Once they are initialized or given, those in circles behaves like latent variables. We call the former ""parameters"" and the latter ""latent variable"", as listed in Table 1. Trend and seasonality z = (z a , z c ) Anomaly and change points Parameter a1
The ""mean"" for the initial trend and seasonality p = (pa, pc)
Probabilities for each point to be anomaly or change point σ = (σ , σo, σu, σr, σv, σw) Standard deviation
The discrepancy between these two categories is clearly captured by the joint likelihood function. From Figure 2, the joint distribution (i.e., the likelihood function) can be written down explicitly as
La 1 ,p,σ (y, α, z) (4) = {t:z a t =0} g(yt − µt − γt, σ ) × {t:z a t =1} g(yt − µt − γt, σo) × {t:z c t =0} g(µt − µ t−1 − δ t−1 , σu) × {t:z c t =1} g(µt − µ t−1 − δ t−1 , σr) × n t=1 g(δt − δ t−1 , σv) × n t=1 g(− S−1 s=1 γ t−s , σv) × n i=1 (pa) z a t (1 − pa) 1−z a t (pc) z c t (1 − pc) 1−z c t , where g(x 1 , x 2 ) = 1 √ 2πx2 exp −x 2 1 /(2x2
2 ) is the density function for normal distribution with mean x 1 and standard deviation x 2 . Here we slightly abuse the notation by using µ 0 , δ 0 , γ 0 , γ −1 , . . . , γ 2−S , which are actually the corresponding coordinates of a 1 .
As long with other probabilistic graphical models, our model can also be viewed as a generative model. Given the parameters a 1 , p, σ, we are able to generate time series. We present the generative procedure as follows.
Algorithm 1: Generative Procedure Input: Parameters a 1 , σ = (σ , σ o , σ u , σ r , σ v , σ w ) and p a , p c , length of time series to generate m Output: Time series y = (y 1 , y 2 , . . . , y m ) Generate the indexes where anomalies or change points occur
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c );
Generate all the noises , o, u, r, v, w as independent normal random variables with mean zero and standard deviation σ , σ o , σ u , σ r , σ v , σ w respectively; Generate {α t } m t=1 sequentially by the transition functions in Equation ( 2) and (3); Generate time series {y t } m t=1 by the observation function in Equation (1).

[INFERENCE]
This section is about inferring unknown variables from y, given the Bayesian setting described in the previous section. The main framework here is to sequentially update each hidden variable by fixing the remaining ones. As stated in the previous section, there are two different categories of unknown variables. Different update schemes need to be used due to the difference in their functionality. For the latent variables, we implement Markov chain Monte Carlo (MCMC) for inference. Particular, we use Gibbs sampler. We will elaborate the details of updates in the following sections.

[UPDATES ON TREND AND SEASONALITY]
In this section, we focus on updating α assuming all the other hidden variables are given and fixed. The essence of Gibbs sampler is to obtain posterior distribution p a1,p,σ (α|y, z). This can be achieved by a combination of Kalman filter, Kalman smoothing and the so-called ""fake-path"" trick. We provide some intuitive explanation here and refer the readers to Durbin & Koopman (2012) for detailed implementation.
Kalman filter and Kalman smoothing are classic algorithms in signal processing and pattern recolonization for Bayesian inference. It is well related to other algorithms especially message passing algorithm. Kalman filter collects information forwards to obtain E(α t |y 1 , y 2 , . . . , y t ); while Kalman smoothing distribute information backwards to achieve E(α t |y).
However, the combination of Kalman filter and Kalman smoothing is not enough, as it only gives the the expectations of marginal distributions {E(α t |y)} n t=1 , instead of the joint distribution required for Gibbs sampler. To address this issue, we can use the ""fake-path"" trick described in Brodersen et al. (2015); Durbin & Koopman (2012). The main idea underlying this trick lies on the fact that the covariance structure of p(α t |y) is not dependent on the means. If we are able to obtain the covariance by some other way, then we can add it up with {E(α t |y)} n t=1 to obtain a sample from p(α|y). This trick involves three steps. Note that all the other hidden variables z, p, σ are given.
1. Pick some vectorã 1 , and generate a sequence of time seriesỹ from it by Algorithm 1. In this way, we also observeα. 2. Obtain {E(α t |ỹ)} n t=1 fromỹ by Kalman filter and Kalman smoothing. 3. We use {α t − E(α t |ỹ) + E(α t |y)} n t=1 as our sampling from the conditional distribution.

[CHANGE POINT AND ANOMALY DETECTION]
In this section, we update z by Gibbs sampler, assuming α, a 1 , p, σ are all given and fixed. We need to obtain the conditional distribution p a1,p,σ (z|y, α). Note that in the graphical model described in Section 2, {z a t } n t=1 and {z c t } n i=1 are all Bernoulli random variables and independent of each other. Then the conditional distribution p a1,p,σ (z|y, α) can also be decomposed into product of Bernoulli density functions. In other words, conditioned on y, α, {z a t } n t=1 and {z c t } n i=1 are still independent Bernoulli random variables, but possibly with different success probabilities. Thus, we can take the calculation point by point. For example, for the anomaly detection for the t-th point, we have
z a t = 0 : y t − µ t − γ t ∼ N (0, σ 2 ) z a t = 1 : y t − µ t − γ t ∼ N (0, σ 2 o
). And the prior on z a t is P(z a t = 1) = p a and P(z a t = 0) = p 1 . Let p a t = P(z a t = 1|y, α). Directly calculation leads to
p a t = pa σo exp − (yt−µt−γt) 2 2σ 2 o 1−pa σ exp − (yt−µt−γt) 2 2σ 2 + pa σo exp − (yt−µt−γt) 2 2σ 2 o .(5)
This equality holds for all t = 1, 2, . . . , n. Similarly for change point detection, let p c t = P(z c t = 1|y, α), and we have
p c t = pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r 1−pc σu exp − (µt−µt−1−δt−1) 2 2σ 2 u + pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r .(6)
As mentioned above, all the coordinates in z are still independent Bernoulli random variables conditioned on y, α. Thus, for Gibbs sampler, we can generate z by sampling independently with
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t ).
For change point detection here, we have an additional segment control step. After obtaining {z c t } n t=1 as mentioned above, we need to make sure that the change points detected satisfy some additional requirement on the length of segment among two consecutive change points. This issue arises from the ambiguity between the definitions of change point and anomaly points. For example, consider a time series with value (0, 0, 0, 0, 1, 1, 1, 0, 0, 0). We can view it with two change points, one increases the trend by 1 and the other decreases it by 1. Alternatively, we can also argue the three 1s in this time series are anomalies, though next to each other. One way to address this ambiguity is by defining the minimum length of segment (denoted as ). In this toy example, if we set the minimum length to be 4, then they are anomaly points; if we set it to be 3, then we regard them to be change points. But a more complicated criterion is needed than using minimum length as the time series usually own much more complex structure than this toy example. Consider time series (0, 0, 0, 0, −1, −1, 1, 1, 1, 1) and the minimum time series parameter = 3. It is reasonable to view it with one change point with increment 1, and the two -1s should be regarded as anomalies. As a combination of all these factors, we propose the following segment control method. A default value for the parameter is the length of seasonality, i.e., = S.
Algorithm 2: Segment control on change points Input: change point binary vector z c ,trend µ, standard deviation for outliers σ r , change point minimum segment Output: change point binary vector z c Denote t 1 < t 2 < . . . to be all the indexes such that z c ti = 1; while there exists i such that
|t i+1 − t i | < do Check if |µ ti−1 − µ ti+1+1 | ≤ σ r /2.
If so, exclude both them from change points by setting z c ti = z c ti+1 = 0. Otherwise, randomly exclude one of them by setting the corresponding coordinate in z c to be 0; Update all the indexes of change points in z c . end

[INITIALIZATION AND UPDATES ON PARAMETERS]
The parameters σ, a 1 and p need both initialization and update. We have different initializations and update schemes for each of them.
For all the standard deviations, once we obtain α and z, we update them by taking the empirical standard deviation correspondingly. For σ δ and σ γ , the calculation is straightforward as they only involve δ and γ respectively. For σ , σ o , σ u and σ r , it is a bit more involved due to z. Nevertheless, we can obtain the following update equations for all of them:
σ = {t:z a t =0} (yt − µt − γt) 2 |{t : z a t = 0}| , σo = {t:z a t =1} (yt − µt − γt) 2 |{t : z a t = 1}| , σu = {t:z c t =0} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| ,(7)
σr = {t:z c t =1} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| , σ δ = 1 n n t=1 (δt − δt−1) 2 , σγ = 1 n n t=1 ( S−1 s=0 γt−s) 2 . (8
)
Note that in some iterations, when there is no change point or anomaly detected in z, then the updates above for σ o , σ r are not well-defined. In those cases, we simply let them remain the same.
To initialize σ, we let them all equal to the standard deviation of y.
For a 1 , we initialize it by letting its first coordinate to be equal to the average of y 1 , y 2 , . . . , y S , and all the remaining coordinates to be equal to 0. Since a 1 can be interpreted as the mean vector of α 1 , in this way the trend is initialized to be matched up with average of the first season, and the slope and seasonality are initialized to be equal to 0. We update a 1 by using information of α. We let the first two coordinates (trend and slope) of a 1 to be equal to those of α 1 , and we let the remaining coordinates (seasonality) of a 1 to be equal to those of α S+1 . The reason why we do not let a 1 to be equal to α 1 entirely is due to the consideration on convergence and robustness. Since we initialize the seasonality part in a 1 as 0, it will remain 0 if we let a 1 equals α 1 entirely (due to the mechanism how we update α 1 as described in Section 4.1. We can avoid such trouble via using α S+1 .
For p, we initialize them to be equal to 1/n. If we have additional information on the number of change points or anomaly points, we can initiate them with different values, for example, 0.1/n, or 10/n. We can update p after obtaining z, but we choose not to, also for the sake of robustness. In the early iterations when the algorithm is far from convergence, it is highly possible that z a or z c may turn out to be all 0. If we update p, say, by taking the proportion of change point or anomaly points in z. Then p a or p c might be 0, and it may get stuck in 0 in the remaining iterations.

[FORECASTING]
Once we infer all the latent variables α, z and tune all the parameters p, a 1 , σ, we are able to forecast the future time series y future . From the graphical model described in Section 3, the future forecasting only involves α n instead of the whole α. Note that we assume that there exists no change point and anomaly point in the future. This is reasonable as in most cases we have no additional information on the future time series. Given α n and σ we can use our predictive procedures (i.e., Algorithm 1) to generate future time series y future . We can further integrate out α n to have the posterior predictive distribution as p σ (y future |y).
The forecasting on future time series is not deterministic. There are two sources for the randomness in y future . One comes from the inference of α n (and also σ) from y. Under the Bayesian framework in Section 3, we have a posterior distribution over α n rather than a single point estimation. The second one comes from the forecasting function itself. The forecasting involves intrinsic noise like t , u t , v t and w t . Thus, the predictive density function p σ (y future |y, α n ) will lead to different path even with fixed σ and α n . In this way we are able to obtain distribution and predictive interval for forecasting. We also suggest to take the average of multiple forecasting paths, as the posterior mean for the forecasting.
The average of multiple forecasting paths (denoted asȳ future ), if the number of paths is large enough, always takes the form as a combination of linear trend and seasonality. This can be observed in both our synthesis data (Section 7) and real data analysis (Section 8). This seems to be surprising at the first glance, but makes some sense intuitively. Under our assumption, we have no information on the future, and thus a reliable way to forecast the future is to use the information collected at the end of observed time series, i.e., trend µ n , slope δ n and seasonality structure. Theorem 1 gives mathematical explanation of the linearity ofȳ future , in both mean and standard deviation. Theorem 1. Let N be the number of future time series paths we generate from Algorithm 1). Let m be the number of points we are going to forecast. Denote {y
(1) n+j } m j=1 , {y(2)
n+j } m j=1 , . . . , {y
n+j } m j=1 to be the future paths. Defineȳ future = (ȳ n+1 ,ȳ n+2 , . . . ,ȳ n+m ) to be the average such that
y n+j = 1 N N i=1 y (i) n+j .
Then for all j = 1, 2, . . . , N , we haveȳ n+j as a normal distribution with mean and variance as
E[ȳ n+j ] = µ n + jδ n + γ n−S+(j mod S) Var [ȳ n+j ] = 1 N j(j + 1)σ 2 v /2 + j(σ 2 u + σ 2 w ) + σ 2 .
Consequently, for all j = 1, 2, . . . , m, E[ȳ n+j ] is in a linear form with respect to j, and the standard deviation ofȳ n+j also takes a approximately linear form with respect to j.
Proof. Recall that α n , σ are given and fixed, and we assume there is no change point or anomaly in the future time series. The Equation (2) leads to δ n+j = δ n + j l=1 v n+l , which implies that
µ n+j = µ n + jδ n + j l=1 (j + 1 − l)v n+l + j l=1 u m+l .
For the seasonality part, simple linear algebra together with Equation 3 leads to γ n+j = γ n−S+(j mod S) + j l=1 w n+l . Thus,
y n+j = 1 N N i=1 µ n + jδ n + γ n−S+(j mod S) + j l=1 (j + 1 − l)v (i) n+l + j l=1 u (i) m+l + j l=1 w (i) n+l + (i) n+j .
Due to the independence and Gaussian distribution of all the noises,ȳ n+j is also normally distributed and its means and variance can be calculated accordingly.

[ALGORITHM]
Our proposed method can be divided into three parts: initialization, inference, and forecasting. Section 4 and Section 5 provide detailed explanation and reasoning for each of them. We present a whole picture of our proposed methodology in Algorithm 3. Initialize a 1 such that its first coordinate equals to the average of (y 1 , y 2 , . . . , y S ) and all the remaining S coordinates with 0; Initialize p a and p c by 1/n. Then generate z a and z c as independent Bernoulli random variables with success probability p a and p c respectively;
Part II: Inference; while the likelihood function L a1,p,σ (y, α, z) not converges do Infer α by Kalman filter, Kalman smoothing and ""fake-path"" trick described in Section 4.1;
Update z a and z c by sampling from
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t )
, where the success probability {p a t } n t=1 and {p c t } n t=1 are defined in Equation ( 5) and ( 6);
Segment control on z c by Algorithm 2; Update σ by Equation ( 7) to (8);
Update a 1 such that its first two coordinates equal to the those of α 1 and the remaining (S − 1) coordinates equals to those of α S+1 ;
Calculate the likelihood function L a1,p,σ (y, α, z) given in Equation ( 4); end Part III: Forecasting; With a n and σ, use the generate procedure in Algorithm 1 to generate future time series y future with length m. Repeat the generative procedure to obtain multiple future paths y
(1) future , y
future , . . . , y
future ; Combine all the predictive paths give the distribution for the future time series forecasting. If needed, calculate the point-wise quantile to obtain predictive intervals. Use the point-wise average as our final forecasting result.
It is worth mentioning that our proposed methodology is downward compatible with many simpler state space time series models. By letting p c = 0, we assume there is no change point in the time series. By letting p a = 0, we assume there is no anomaly point in the time series. If both p c and p a are set to be 0, then our model is reduced to the classic state space time series model. Also, the seasonality and slope can be removed from our model, if we know there exists no such structure in the data.

[SIMULATION]
In this section, we study the synthetic data generated from our model. We let S = 7 and provide values for σ and a 1 . The change points and anomaly points are randomly generated. We use our generative procedure (Algorithm 1) to generate time series with total length 500 by fixed parameters. The first 350 points will be used as training set and the remaining 150 points will be used to evaluate the performance of forecasting.
When generating, we let the time series have weekly seasonality with S = 7. For σ we have σ = 0.1, σ u = 0.1, σ v = 0.0004, σ w = 0.01, σ r = 1, σ o = 4. For α 1 we have value for µ as 20, value for δ as 0, and value for seasonality as (1, 2, 4, −1, −3, −2)/10. For p we have p c = 4/350 and p a = 10/350. Despite that, to make sure that at least one change point is in existence, we force z c 330 = 1 and r 330 = 2. That is, for each time series we generate, its 330th point is a change point with the mean shifted up by 3. Also to be consistence with our assumption, we force z c i = z a i = 0, ∀351 ≤ i ≤ 500 so there exists no change point or anomaly point in the testing part. The top panel of Figure 3 shows one example of synthesis data. The blue line marks the separation between training and testing set. The blue dashed line indicates the locations for the change point, while the yellow dots indicate the positions of anomaly points. Also see Figure 3 for illustration on the results returned by implementing our proposed algorithm on the same dataset. The red line gives the fitting results in the first 350 points and forecasting results in the last 150 points. The change points detected are marked with vertical red dotted line, and the anomaly detected are flagged with purple squares. Figure 3 shows that on this dataset, our proposed algorithm yields perfect detection on both change points and anomaly points. In Figure 3, the gray part indicates the 90% predictive interval for forecasting. We run our generative model 100 times to produce 100 different time series, and implement multiply methods on each of them, and aggregate the results together for comparison. We include the following methodologies.  et al., 2008)), and the Prophet R package by Taylor & Letham (2017). We evaluate the performances by mean absolute percentage error (MAPE), mean square error (MSE) and mean absolute error (MAE) on forecasting set. The mathematical definition of these three criterion is given as follows. Let x 1 , x 2 , . . . , x n be the true value andx 1 ,x 2 , . . . ,x n be the estimation or predictive values.
Then we have
MAPE = 1 n n i=1 |x i −x i | x i , MSE = 1 n n i=1 (x i −x i ) 2 , MAE = 1 n n i=1 |x i −x i |.
The comparison of our proposed algorithm and the aforementioned algorithms are included below in Table 2. As we mentioned in Section 6, our algorithm is downward compatible with the cases ignoring the existence of change point or anomaly, by setting p c = 0 or p a = 0. We also run proposed algorithm on the synthetic data with p c = 0 (no change point), or p a = 0 (no anomaly point), or p c = p a = 0 (no change and anomaly point), for the purpose of numeric comparison.
From Table 2 it turns out that our proposed algorithm achieves the best performance compared to other existing methods. Our proposed algorithm also performs better compared with the cases ignoring change point or anomaly point. This is a convincing evidence on the importance of incorporating both change point structure and anomaly point structure when modeling, for time series forecasting.
We also compare our proposed method with other existing change point detection methods and anomaly detection algorithm with respect to the performance of detections. We evaluate the performance by two criterions: True Positive Rate (TPR) and False Positive (FP). TPR measures the percentage of change points or anomalies to be correctly detected. FP count the number of points wrongly detected as change points or anomaly points. The mathematical definitions of TPR and FP are as follows. Let (z 1 , z 2 , . . . , z n ) be the true binary vector for change points or anomalies, and points in total. We split it such that the first 3000 points are used as training set and last 1000 points are used to evaluate the forecasting performance. From Figure 4, it is obvious that there exists no seasonality or slope structure in the dataset. This motivates us not to include these two components in our model. We implement our proposed algorithm without seasonality and slope, and compare the forecasting performance with other methods in Table 5. Our method outperforms BSTS, ARIMA, ETS and Prophet. However in Table 5 the performance can be slightly improved if we ignore the existence of anomaly points by letting p a = 0. This may be caused by model mis-specification as the data may not generated in a way not entirely captured by our model. Nevertheless, the performances of our method considering anomaly points or not, are comparable to each other. In this dataset there is no ground-truth of change point and anomaly point on their locations or even existence. However, from bottom panel of Figure 4, there are some obvious changes in the sequence and they all successfully captured by our algorithm.

[INTERNET TRAFFIC DATA]
Our second real data is an Internet traffic data acquired from a major Tech company (see Figure 5).
It is a daily traffic data, with seasonality S = 7. We use the first 800 observations as training set and evaluate the performance of forecasting on the remaining 265 points. The bottom panel of Figure 5 show the result from implementing our algorithm. We also do the comparison of forecasting performance of our proposed algorithm together with other existing methods, shown in Table 6. We can also see that our algorithm outperforms all the other algorithms with respect to MAPE, MSE and MAE.
Compared to the aforementioned models, our work differs in Bayesian modeling which samples posterior to estimate hidden components given the independent Bernoulli priors of changing point and anomalies.

[CONCLUSION]
We incorporate the change point structure and anomaly point structure into the classic space state time series model. We provide a Bayesian scheme for inference and time series forecasting. We compare the performance of our methodology and state-of-the-art methods on both synthetic data and real datasets. Our method performs the best with respect to forecasting, change point detection, and anomaly detection as well.

[]
(ẑ 1 ,ẑ 2 , . . . ,ẑ n ) are the estimated ones. Then
From the definition, we can see high TPR and low FP means the algorithm has better performance in detection.
The comparison on change point detection is shown in Table 3. We compare our results against three popular change point detection methods: Bayesian Change Point (BCP) (Barry & Hartigan, 1993), Change-Point (CP) (Killick & Eckley, 2014) and Breakout (twitter, 2017). From Table 3 our proposed method outperforms the most of the others by both TPR and FP. We have smaller TPR compared to CP, but we are better in FP.  In Table 4, we also compare the performance of our algorithm on anomaly detection with three existing common anomaly detection methods: the AnomalyDetection package by Twitter (2017), RAD by Netflix (2017) and Tsoutlier by Chen & Liu (1993). The comparison is listed in Table 4. We can see our method also outperforms most of the others with respect to anomaly detection, by both TPR and FP. RAD has slightly better TPR but its FP is much worse compared with ours.

[REAL DATA ANALYSIS]
In this section, we implement our proposed method on real-world datasets. We also compare its performance against other existing time series forecasting methodologies. We consider two datasets, one is a public data called Well-log dataset, and the other is an unpublished internet traffic dataset. The bottom panels of Figure 4 and Figure 5 give the result of our proposed algorithms. The blue line separates the training set and testing set. We use red line to show our fitting and forecasting result, vertical red dashed line to indicate change points and purple dots to indicate anomaly points. The gray part shows 90% predication interval.

[WELL-LOG DATA]
This dataset (Fearnhead & Clifford, 2003;JK & WJ, 1996) was collected when drilling a well. It measures the nuclear magnetic response, which provides geophysical information to analyze the structure of rock surrounding the well. This dataset is public and available online 2 . It has 4050 From Figure 5 our proposed algorithm identifies one change point (the 576th point, indicated by the vertical red dashed line), which can be confirmed that this is exactly the only one change point existing in this time series caused by the change of counting methods, by some external information. Thus, we give the perfect change point detection in this Internet traffic data.
For this Internet traffic dataset, since we have ground-truth for change point, we can compare the performance of change point detection of different methodologies. BCP returns posterior distribution, which peaks in the the 576th point with posterior probability value 0.5. And it also returns with many other points with posterior probability value around 0.1. CP returns 4 change points, where the 576th point (the only true one) is one of them. Breakout returns 8 change points without including the 576th point. To sum up, our proposed method achieves the best change point detection in this real dataset.

[RELATED WORK]
Parametric models are widely considered in econometric literature for time series forecasting, e.g. Jalles (2009), Commandeur et al. (2011), Gould et al. (2008, Harvey & Peters (1990), Harvey et al. (1998). The general procedure of decomposition method (using trend, seasonal and irregular components) for univariate structural time series modeling is discussed in Harvey & Peters (1990); a unified state space framework is proposed to handle any messy time series in Harvey et al. (1998); and the explicit modeling of both additive and multiplicative seasonalities in Gould et al. (2008); Jalles (2009). Although Kalman filter and MCMC-based approaches are used to sample posterior to estimate hidden components, the changing points and anomalies are not considered and processed in the above works. For example, the irregular component considered in Jalles ( 2009) is simply the noises. Commandeur et al. (2011) discusses the statistical software for state-space modeling which is designed for generic time series analytic and modeling, which cannot directly be used when changing point and anomalies are in existence. Our proposed approach shares similarity with the aforementioned papers as we have similar additive structure of components. However we are able to incorporate the change points and anomalies, two common structure widely observed in real data, into our model by using Bernoulli indicators. This is non-trivial, and cannot be handled by the aforementioned papers or their variants.
Non-parametric approaches are used for extraction of components from quasi-periodic time-series, e.g., the ensembles of weak detectors using non-parametric measurement are used in Artemov & Burnaev (2016) to detect change-points and anomalies, and the online decomposition algorithm based on per-component is adopted for change-point detection in Alexey Artemov (2015). Different from the above works, to handle the structural brakes and change-points, this paper presents the parametric approach for modeling anomalies and changing points by fitting them in the state-space framework using approximate inference for forecasting path prediction.
Different Bayesian approaches are proposed for change-point detection, e.g., Adams & MacKay (2007) performs Bayesian change point detection from online inference by generating the distribution estimation of the next unseen datum in the sequence given only data already observed, and the Bayesian Online CPD (BOCPD) algorithm proposed by Turner et al. (2009) performs online prediction using hidden variable given the underlying predictive model (UPM) and the hazard function.","[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection."
Bayesian Time Series Forecasting with Change Point and Anomaly Detection,rJLTTe-0W.json,"

Summary:

This paper develops a state space time series forecasting model in the Bayesian framework, jointly detects anomaly and change points. Integrated with an iterative MCMC method, the authors develop an efficient algorithm and use both synthetic and real data set to demonstrate that their algorithms outperform many other state-of-art algorithms. 

Major comments:
In the beginning of section 3, the authors assume that all the terms that characterize the change-points and anomaly points are normally distributed with mean zero and different variance. However, in classic formulation for change-point or anomaly detection, usually there is also a mean shift other than the variance change. For example, we might assume $r_t \sim N(\theta, \sigma_r^2)$ for some $\theta>0$ to demonstrate the positive mean shift. I believe that this kind of mean shift is more efficient to model the structure of change-point. 

My main concern is with the novelty. The work does not seem to be very novel.

Minor comments:

1. In the end of the page 2, the last panel is the residual, not the spikes. 

2. In page 12, the caption of figure 5 should be (left) and (right), not (top) and (bottom).","[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Demostration of Decompositions.
Table 2: Algorithm 3 :3Proposed Algorithm Input: Observed time series y = (y 1 , y 2 , . . . , y n ), seasonality length S, length of time series for forecasting m, number of predictive paths N , change point minimum segment l Output: Change point detection z c , anomaly points z a , forecasting result y future = (y n+1 , y n+1 , . . . , y n+m ) and its distribution or predictive intervals Part I: Initialization; Initialize σ , σ o , σ u , σ r , σ v , σ w all with the empirical standard deviation of y;
Table 3: Figure 3 :3Figure 3: An example of synthesis data (left), and the result after applying our algorithm (right).
Table 4: Figure 4 :4Figure 4: Well-log Data (left). The result of implementing our proposed algorithm (right).
Table 5: Figure 5 :5Figure 5: Internet Traffic Data (top); The result of implementing our proposed algorithm (bottom).
Table 6: Two Categories for Hidden Variables α2, . . . , αn)
Table 7: Comparison of Forecasting in Well-log Data

[INTRODUCTION]
Time series forecasting has a rich and luminous history, and is essentially important in most of business operations nowadays. The main aim of time series forecasting is to carefully collect and rigorously study the past observations of a time series to develop an appropriate model which could describe the inherent structure of the series, in order to generate future values. For instance, the internet companies are interested in the number of daily active users (DAU), say, what is DAU after certain period of time, or when will reach their target DAU goal. Time series forecasting is a fruitful research area with many existing methodologies. The most popular and frequently used time series model might be the Autoregressive Integrated Moving Average (ARIMA) (Box et al., 2015;Zhang, 2003;Cochrane, 2005;Hipel & McLeod, 1994). Taking seasonality into consideration, Box et al. (2015) proposed the Seasonal ARIMA. The Holt-Winters method (Winters, 1960) is also very popular by using exponential smoothing. State space model (Durbin & Koopman, 2012;Scott & Varian, 2014;Brodersen et al., 2015) also attracts much attention, which is a linear function of an underlying Markov process plus additive noise. Exponential Smoothing State Space Model (ETS)  decomposes times series into error, trend, seasonal that change over time. Recently, deep learning is applied for time-series trend learning using LSTM (Tao Lin, 2017), bidirectional dynamic Boltzmann machine (Osogami et al., 2017) is applied for time-series long-term dependency learning, and coherent probabilistic forecast (Taieb et al., 2017) is proposed for a hierarchy or an aggregation-level comprising a set of time series. Orthogonal to these works, this paper focuses on robust ways of time series forecasting in presence of change points and anomalies.
In Internet time series forecasting, Google develops the Bayesian structure time series (BSTS) model (Brodersen et al., 2015;Scott & Varian, 2014) to capture the trend, seasonality, and similar components of the target series. Recently, Facebook proposes the Prophet approach (Taylor & Letham, 2017) based on a decomposable model with interpretable parameters that can be intuitively adjusted by analyst. However, as in the DAU example, some special events like Christmas Holiday or President Election, newly launched apps or features, may cause short period or long-term change of DAU, leading to weird forecasting of those traditional models. The aforementioned special cases are well known as • Anomaly points. The items, events or observations that don't conform to an expected pattern or other items in the dataset, leading to a sudden spike or decrease in the series.
• Change points. A market intervention, such as a new product launch or the onset of an advertising (or ad) campaign, may lead to the level change of the original series.
Time series forecasting without change/anomaly point detection and adjustment may also lead to bizarre forecasting since these models might learn the abrupt changes in the past. There are literatures on detecting anomaly or change points individually, examples can be found in Twitter (2017); Netflix (2017); Barry & Hartigan (1993); Killick & Eckley (2014); twitter (2017). However, the aforementioned change point detection models could not support detection in the presence of seasonality, while the presence of trend/change point is not handled by the anomaly detection models. Most importantly, there is a discrepancy between anomaly/change points detection and adjustment, and commonly used manually adjustment might be a bit arbitrary. Unfortunately, the forecasting gap caused by abnormal and change points, to the best of our knowledge, has not been given full attention and no good solution has been found so far. This paper is strongly motivated by bridging this gap.
In this paper, to overcome the limitations of the most (if not all) current models that the anomaly points and change points are not properly considered, we develop a state space time series forecasting model in the Bayesian framework that can simultaneously detect anomaly and change points and perform forecasting. The learned structure information related to anomaly and change points is automatically incorporated into the forecasting process, which naturally enhances the model prediction based on the feedback of state-space model. To solve the resultant optimization problem, an iterative algorithm based on Bayesian approximate inference with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. The novel model could explicitly capture the structure of change points, anomaly points, trend and seasonality, as also provide the distributions and forecasting intervals due to Bayesian forecasting framework. Both synthetic and real data sets show the better performance of proposed model, in comparison with existing baseline. Moreover, our proposed model outperforms state-of-the-art models in identifying anomaly and change points.
To summarize, our work has the following contributions.
• We proposed a robust 1 Bayesian state-space time series forecasting model that is able to explicitly capture the structures of change points and anomalies (which are generally ignored in most current models), and therefore automatically adapt for forecasting by incorporating the prior information of trend, seasonality, as well as change points and anomalies using state space modeling. Due to the enhancement of model description capability, the results of model prediction and abnormal and change points detection are mutually improved.
• To solve the resultant optimization problem, an effective algorithm based on approximate inference using Markov chain Monte Carlo (MCMC) is proposed with theoretical guaranteed forecasting paths.
• Our proposed method outperforms the state-of-the-art methods in time series forecasting in presence of change points and anomalies, and detects change points and anomalies with high accuracy and low false discovery rate on both tasks, outperforming popular change point and anomaly detection methods. Our method is flexible to capture the structure of time series under various scenarios with any component combinations of trend, seasonality, change points and anomalies. Therefore our method can be applied in many settings in practice.

[MODEL OVERVIEW]
State space time series model (Hangos et al., 2014) has been one of the most popular models in time series analysis. It is capable of fitting complicated time series structure including linear trend and seasonality. However, times series observed in real life are almost all prevailed with outliers. Change points, less in frequency but are still widely observed in real time series analysis. Unfortunately, both structures are ignored in the classic state space time series model. In the section, we aim to address this issue by introducing a novel state space time series model. Let y = (y 1 , y 2 , . . . , y n ) be a sequence of time series observations with length n. The ultimate goal is to forecast (y n+1 , y n+2 , . . .). The accuracy in forecasting lies in a successful decomposition of y into existing components. Apart from the residuals, we assume the time series is composed by trend, seasonality, change points and anomaly points. In a nutshell, we have an additive model with time series = trend + seasonality + change point + anomaly point + residual.
Figure 1 provides a demonstration of desired decomposition of time series. In Figure 1, the left panel shows the observed time series. And it can be decomposed into the remaining five panels. The shift in the change point panel shows where the change point lies. And the spikes in the last panel reveals the anomaly points.
As the classical state space model, we have observation equation and transition equations to model y and hidden variables. We use µ = (µ 1 , µ 2 , . . . , µ n ) to model trend, and use γ = (γ 1 , γ 2 , . . . , γ n ) to model seasonality. We use a binary vector z a = (z a 1 , z a 2 , . . . , z a n ) to indicate anomaly points. Then we have Observation equation:
y t = µ t + γ t + t , if z a t = 0 o t , if z a t = 1
.
(1)
The deviation between the observation y t and its ""mean"" µ t + γ t is modeled by t and o t , depending on the value of z a t . If z a t = 1, then y t is an anomaly point; otherwise it is not. Distinguished from the residues = ( 1 , 2 , . . . , n ), the anomaly is captured by o = (o 1 , o 2 , . . . , o n ) which has relative large magnitude.
The hidden state variable µ and γ have intrinsic structures. There are two transition equations, for trend and seasonality separately Transition Equations: Trend:
µ t = µ t−1 + δ t−1 + u t , if z c t = 0 r t , if z c t = 1 ,(2)
δ t = δ t−1 + v t , Seasonality: γ t = − S−1 s=1 γ t−s + w t .(3)
In Equation (2), δ = (δ 1 , δ 2 , . . . , δ n ) can be viewed as the ""slope"" of the trend, measuring how fast the trend changes over time. The change point component is also incorporated in Equation (2) by a binary vector z c = (z c 1 , z c 2 , . . . , z c n ). If z c t = 1, it means the t-th point is a change point, with µ t differs from µ t−1 + δ t−1 (which can be interpreted as the ""momentum"" from the previous status ) by r t ; otherwise it is not a change point and they differ by u t . We model the change points in a way such that r = (r 1 , r 2 , . . . , r n ) have larger magnitude compared u = (u 1 , u 2 , . . . , u n ). The ""slope"" part δ also has its own noise v = (v 1 , v 2 , . . . , v n ).
A first look on Equation (2) may bring up with the question that it is not presented in an exactly the same way as shown in Figure 1. In Figure 1, the change points component is a step function, and it is one of the five additive components along with trend, seasonality, anomaly points and residuals. Here we model the change point directly into the trend component. Though differing in formulation, they are equivalent to each other. We choose to model in as in Equation ( 2) due to simplicity, and its similarity with the definition of anomaly points in Equation (1).
The seasonality component is presented in Equation ( 3). Here S is the length of one season and w = (w 1 , w 2 , . . . , w n ) is the noise for seasonality. The seasonality component is assumed to have almost zero average in each season.
The observation equation and transition equations (i.e., Equation (1,2,3)) define how y is generated from all the hidden variables including change points and anomaly points. We continue to explore this new model, under a Bayesian framework.

[BAYESIAN FRAMEWORK]
Bayesian methods are widely used in many data analysis fields. It is easy to implement and interpret, and it also has the ability to produce posterior distribution. The Bayesian method on state space time series model has been investigated in Scott & Varian (2014);Brodersen et al. (2015). In this section, we also consider Bayesian framework for our novel state space time series model. We assume all the noises are normally distributed
{ t } n t=1 iid ∼ N (0, σ 2 ), {o t } n t=1 iid ∼ N (0, σ 2 o ), {u t } n t=1 iid ∼ N (0, σ 2 u ), {r t } n t=1 iid ∼ N (0, σ 2 r ), {v t } n t=1 iid ∼ N (0, σ 2 v ), {w t } n t=1 iid ∼ N (0, σ 2 w )
, where σ , σ o , σ u , σ r , σ v , σ w are parameters for standard deviation. As binary vectors, a natural choice is to model anomaly point indicator z a and change point indicator z c to the model them as Bernoulli random variables
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c ),
where p a , p c are probabilities for each point to be an anomaly or change point.
Figure 2: Graphical presentation of our model. Note that y is observed, highlighted by gray background, distinguished from all the remaining ones that are hidden. Among the hidden ones, squares indicate fixed parameters, and circles indicate random variables.
For simplicity, we denote α t = (µ t , δ t , γ t , γ t−1 , . . . , γ t−(S−2) ) to include the main hidden variables (except z a t and z c t ) in the transition equations. All the α t are well defined and can be generated from the previous status, except α 1 . We denote a 1 to be the parameter for α 1 , which can be interpreted as the ""mean"" for α 1 .
With Bayesian framework, we are able to represent our model graphically as in Figure 2. As shown in Figure 2, the only observations are y and all the others are hidden. In this paper, we assume there is no additional information on all the hidden states. If we have some prior information, for example, some points are more likely to be change points, then our model can be easily modified to incorporate such information, by using proper prior.
In Figure 2, we use squares and circles to classify unknown variables. Despite all being unknown, they actually behave differently according to their own functionality. For those in squares, they behave like turning parameters. Once they are initialized or given, those in circles behaves like latent variables. We call the former ""parameters"" and the latter ""latent variable"", as listed in Table 1. Trend and seasonality z = (z a , z c ) Anomaly and change points Parameter a1
The ""mean"" for the initial trend and seasonality p = (pa, pc)
Probabilities for each point to be anomaly or change point σ = (σ , σo, σu, σr, σv, σw) Standard deviation
The discrepancy between these two categories is clearly captured by the joint likelihood function. From Figure 2, the joint distribution (i.e., the likelihood function) can be written down explicitly as
La 1 ,p,σ (y, α, z) (4) = {t:z a t =0} g(yt − µt − γt, σ ) × {t:z a t =1} g(yt − µt − γt, σo) × {t:z c t =0} g(µt − µ t−1 − δ t−1 , σu) × {t:z c t =1} g(µt − µ t−1 − δ t−1 , σr) × n t=1 g(δt − δ t−1 , σv) × n t=1 g(− S−1 s=1 γ t−s , σv) × n i=1 (pa) z a t (1 − pa) 1−z a t (pc) z c t (1 − pc) 1−z c t , where g(x 1 , x 2 ) = 1 √ 2πx2 exp −x 2 1 /(2x2
2 ) is the density function for normal distribution with mean x 1 and standard deviation x 2 . Here we slightly abuse the notation by using µ 0 , δ 0 , γ 0 , γ −1 , . . . , γ 2−S , which are actually the corresponding coordinates of a 1 .
As long with other probabilistic graphical models, our model can also be viewed as a generative model. Given the parameters a 1 , p, σ, we are able to generate time series. We present the generative procedure as follows.
Algorithm 1: Generative Procedure Input: Parameters a 1 , σ = (σ , σ o , σ u , σ r , σ v , σ w ) and p a , p c , length of time series to generate m Output: Time series y = (y 1 , y 2 , . . . , y m ) Generate the indexes where anomalies or change points occur
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c );
Generate all the noises , o, u, r, v, w as independent normal random variables with mean zero and standard deviation σ , σ o , σ u , σ r , σ v , σ w respectively; Generate {α t } m t=1 sequentially by the transition functions in Equation ( 2) and (3); Generate time series {y t } m t=1 by the observation function in Equation (1).

[INFERENCE]
This section is about inferring unknown variables from y, given the Bayesian setting described in the previous section. The main framework here is to sequentially update each hidden variable by fixing the remaining ones. As stated in the previous section, there are two different categories of unknown variables. Different update schemes need to be used due to the difference in their functionality. For the latent variables, we implement Markov chain Monte Carlo (MCMC) for inference. Particular, we use Gibbs sampler. We will elaborate the details of updates in the following sections.

[UPDATES ON TREND AND SEASONALITY]
In this section, we focus on updating α assuming all the other hidden variables are given and fixed. The essence of Gibbs sampler is to obtain posterior distribution p a1,p,σ (α|y, z). This can be achieved by a combination of Kalman filter, Kalman smoothing and the so-called ""fake-path"" trick. We provide some intuitive explanation here and refer the readers to Durbin & Koopman (2012) for detailed implementation.
Kalman filter and Kalman smoothing are classic algorithms in signal processing and pattern recolonization for Bayesian inference. It is well related to other algorithms especially message passing algorithm. Kalman filter collects information forwards to obtain E(α t |y 1 , y 2 , . . . , y t ); while Kalman smoothing distribute information backwards to achieve E(α t |y).
However, the combination of Kalman filter and Kalman smoothing is not enough, as it only gives the the expectations of marginal distributions {E(α t |y)} n t=1 , instead of the joint distribution required for Gibbs sampler. To address this issue, we can use the ""fake-path"" trick described in Brodersen et al. (2015); Durbin & Koopman (2012). The main idea underlying this trick lies on the fact that the covariance structure of p(α t |y) is not dependent on the means. If we are able to obtain the covariance by some other way, then we can add it up with {E(α t |y)} n t=1 to obtain a sample from p(α|y). This trick involves three steps. Note that all the other hidden variables z, p, σ are given.
1. Pick some vectorã 1 , and generate a sequence of time seriesỹ from it by Algorithm 1. In this way, we also observeα. 2. Obtain {E(α t |ỹ)} n t=1 fromỹ by Kalman filter and Kalman smoothing. 3. We use {α t − E(α t |ỹ) + E(α t |y)} n t=1 as our sampling from the conditional distribution.

[CHANGE POINT AND ANOMALY DETECTION]
In this section, we update z by Gibbs sampler, assuming α, a 1 , p, σ are all given and fixed. We need to obtain the conditional distribution p a1,p,σ (z|y, α). Note that in the graphical model described in Section 2, {z a t } n t=1 and {z c t } n i=1 are all Bernoulli random variables and independent of each other. Then the conditional distribution p a1,p,σ (z|y, α) can also be decomposed into product of Bernoulli density functions. In other words, conditioned on y, α, {z a t } n t=1 and {z c t } n i=1 are still independent Bernoulli random variables, but possibly with different success probabilities. Thus, we can take the calculation point by point. For example, for the anomaly detection for the t-th point, we have
z a t = 0 : y t − µ t − γ t ∼ N (0, σ 2 ) z a t = 1 : y t − µ t − γ t ∼ N (0, σ 2 o
). And the prior on z a t is P(z a t = 1) = p a and P(z a t = 0) = p 1 . Let p a t = P(z a t = 1|y, α). Directly calculation leads to
p a t = pa σo exp − (yt−µt−γt) 2 2σ 2 o 1−pa σ exp − (yt−µt−γt) 2 2σ 2 + pa σo exp − (yt−µt−γt) 2 2σ 2 o .(5)
This equality holds for all t = 1, 2, . . . , n. Similarly for change point detection, let p c t = P(z c t = 1|y, α), and we have
p c t = pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r 1−pc σu exp − (µt−µt−1−δt−1) 2 2σ 2 u + pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r .(6)
As mentioned above, all the coordinates in z are still independent Bernoulli random variables conditioned on y, α. Thus, for Gibbs sampler, we can generate z by sampling independently with
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t ).
For change point detection here, we have an additional segment control step. After obtaining {z c t } n t=1 as mentioned above, we need to make sure that the change points detected satisfy some additional requirement on the length of segment among two consecutive change points. This issue arises from the ambiguity between the definitions of change point and anomaly points. For example, consider a time series with value (0, 0, 0, 0, 1, 1, 1, 0, 0, 0). We can view it with two change points, one increases the trend by 1 and the other decreases it by 1. Alternatively, we can also argue the three 1s in this time series are anomalies, though next to each other. One way to address this ambiguity is by defining the minimum length of segment (denoted as ). In this toy example, if we set the minimum length to be 4, then they are anomaly points; if we set it to be 3, then we regard them to be change points. But a more complicated criterion is needed than using minimum length as the time series usually own much more complex structure than this toy example. Consider time series (0, 0, 0, 0, −1, −1, 1, 1, 1, 1) and the minimum time series parameter = 3. It is reasonable to view it with one change point with increment 1, and the two -1s should be regarded as anomalies. As a combination of all these factors, we propose the following segment control method. A default value for the parameter is the length of seasonality, i.e., = S.
Algorithm 2: Segment control on change points Input: change point binary vector z c ,trend µ, standard deviation for outliers σ r , change point minimum segment Output: change point binary vector z c Denote t 1 < t 2 < . . . to be all the indexes such that z c ti = 1; while there exists i such that
|t i+1 − t i | < do Check if |µ ti−1 − µ ti+1+1 | ≤ σ r /2.
If so, exclude both them from change points by setting z c ti = z c ti+1 = 0. Otherwise, randomly exclude one of them by setting the corresponding coordinate in z c to be 0; Update all the indexes of change points in z c . end

[INITIALIZATION AND UPDATES ON PARAMETERS]
The parameters σ, a 1 and p need both initialization and update. We have different initializations and update schemes for each of them.
For all the standard deviations, once we obtain α and z, we update them by taking the empirical standard deviation correspondingly. For σ δ and σ γ , the calculation is straightforward as they only involve δ and γ respectively. For σ , σ o , σ u and σ r , it is a bit more involved due to z. Nevertheless, we can obtain the following update equations for all of them:
σ = {t:z a t =0} (yt − µt − γt) 2 |{t : z a t = 0}| , σo = {t:z a t =1} (yt − µt − γt) 2 |{t : z a t = 1}| , σu = {t:z c t =0} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| ,(7)
σr = {t:z c t =1} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| , σ δ = 1 n n t=1 (δt − δt−1) 2 , σγ = 1 n n t=1 ( S−1 s=0 γt−s) 2 . (8
)
Note that in some iterations, when there is no change point or anomaly detected in z, then the updates above for σ o , σ r are not well-defined. In those cases, we simply let them remain the same.
To initialize σ, we let them all equal to the standard deviation of y.
For a 1 , we initialize it by letting its first coordinate to be equal to the average of y 1 , y 2 , . . . , y S , and all the remaining coordinates to be equal to 0. Since a 1 can be interpreted as the mean vector of α 1 , in this way the trend is initialized to be matched up with average of the first season, and the slope and seasonality are initialized to be equal to 0. We update a 1 by using information of α. We let the first two coordinates (trend and slope) of a 1 to be equal to those of α 1 , and we let the remaining coordinates (seasonality) of a 1 to be equal to those of α S+1 . The reason why we do not let a 1 to be equal to α 1 entirely is due to the consideration on convergence and robustness. Since we initialize the seasonality part in a 1 as 0, it will remain 0 if we let a 1 equals α 1 entirely (due to the mechanism how we update α 1 as described in Section 4.1. We can avoid such trouble via using α S+1 .
For p, we initialize them to be equal to 1/n. If we have additional information on the number of change points or anomaly points, we can initiate them with different values, for example, 0.1/n, or 10/n. We can update p after obtaining z, but we choose not to, also for the sake of robustness. In the early iterations when the algorithm is far from convergence, it is highly possible that z a or z c may turn out to be all 0. If we update p, say, by taking the proportion of change point or anomaly points in z. Then p a or p c might be 0, and it may get stuck in 0 in the remaining iterations.

[FORECASTING]
Once we infer all the latent variables α, z and tune all the parameters p, a 1 , σ, we are able to forecast the future time series y future . From the graphical model described in Section 3, the future forecasting only involves α n instead of the whole α. Note that we assume that there exists no change point and anomaly point in the future. This is reasonable as in most cases we have no additional information on the future time series. Given α n and σ we can use our predictive procedures (i.e., Algorithm 1) to generate future time series y future . We can further integrate out α n to have the posterior predictive distribution as p σ (y future |y).
The forecasting on future time series is not deterministic. There are two sources for the randomness in y future . One comes from the inference of α n (and also σ) from y. Under the Bayesian framework in Section 3, we have a posterior distribution over α n rather than a single point estimation. The second one comes from the forecasting function itself. The forecasting involves intrinsic noise like t , u t , v t and w t . Thus, the predictive density function p σ (y future |y, α n ) will lead to different path even with fixed σ and α n . In this way we are able to obtain distribution and predictive interval for forecasting. We also suggest to take the average of multiple forecasting paths, as the posterior mean for the forecasting.
The average of multiple forecasting paths (denoted asȳ future ), if the number of paths is large enough, always takes the form as a combination of linear trend and seasonality. This can be observed in both our synthesis data (Section 7) and real data analysis (Section 8). This seems to be surprising at the first glance, but makes some sense intuitively. Under our assumption, we have no information on the future, and thus a reliable way to forecast the future is to use the information collected at the end of observed time series, i.e., trend µ n , slope δ n and seasonality structure. Theorem 1 gives mathematical explanation of the linearity ofȳ future , in both mean and standard deviation. Theorem 1. Let N be the number of future time series paths we generate from Algorithm 1). Let m be the number of points we are going to forecast. Denote {y
(1) n+j } m j=1 , {y(2)
n+j } m j=1 , . . . , {y
n+j } m j=1 to be the future paths. Defineȳ future = (ȳ n+1 ,ȳ n+2 , . . . ,ȳ n+m ) to be the average such that
y n+j = 1 N N i=1 y (i) n+j .
Then for all j = 1, 2, . . . , N , we haveȳ n+j as a normal distribution with mean and variance as
E[ȳ n+j ] = µ n + jδ n + γ n−S+(j mod S) Var [ȳ n+j ] = 1 N j(j + 1)σ 2 v /2 + j(σ 2 u + σ 2 w ) + σ 2 .
Consequently, for all j = 1, 2, . . . , m, E[ȳ n+j ] is in a linear form with respect to j, and the standard deviation ofȳ n+j also takes a approximately linear form with respect to j.
Proof. Recall that α n , σ are given and fixed, and we assume there is no change point or anomaly in the future time series. The Equation (2) leads to δ n+j = δ n + j l=1 v n+l , which implies that
µ n+j = µ n + jδ n + j l=1 (j + 1 − l)v n+l + j l=1 u m+l .
For the seasonality part, simple linear algebra together with Equation 3 leads to γ n+j = γ n−S+(j mod S) + j l=1 w n+l . Thus,
y n+j = 1 N N i=1 µ n + jδ n + γ n−S+(j mod S) + j l=1 (j + 1 − l)v (i) n+l + j l=1 u (i) m+l + j l=1 w (i) n+l + (i) n+j .
Due to the independence and Gaussian distribution of all the noises,ȳ n+j is also normally distributed and its means and variance can be calculated accordingly.

[ALGORITHM]
Our proposed method can be divided into three parts: initialization, inference, and forecasting. Section 4 and Section 5 provide detailed explanation and reasoning for each of them. We present a whole picture of our proposed methodology in Algorithm 3. Initialize a 1 such that its first coordinate equals to the average of (y 1 , y 2 , . . . , y S ) and all the remaining S coordinates with 0; Initialize p a and p c by 1/n. Then generate z a and z c as independent Bernoulli random variables with success probability p a and p c respectively;
Part II: Inference; while the likelihood function L a1,p,σ (y, α, z) not converges do Infer α by Kalman filter, Kalman smoothing and ""fake-path"" trick described in Section 4.1;
Update z a and z c by sampling from
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t )
, where the success probability {p a t } n t=1 and {p c t } n t=1 are defined in Equation ( 5) and ( 6);
Segment control on z c by Algorithm 2; Update σ by Equation ( 7) to (8);
Update a 1 such that its first two coordinates equal to the those of α 1 and the remaining (S − 1) coordinates equals to those of α S+1 ;
Calculate the likelihood function L a1,p,σ (y, α, z) given in Equation ( 4); end Part III: Forecasting; With a n and σ, use the generate procedure in Algorithm 1 to generate future time series y future with length m. Repeat the generative procedure to obtain multiple future paths y
(1) future , y
future , . . . , y
future ; Combine all the predictive paths give the distribution for the future time series forecasting. If needed, calculate the point-wise quantile to obtain predictive intervals. Use the point-wise average as our final forecasting result.
It is worth mentioning that our proposed methodology is downward compatible with many simpler state space time series models. By letting p c = 0, we assume there is no change point in the time series. By letting p a = 0, we assume there is no anomaly point in the time series. If both p c and p a are set to be 0, then our model is reduced to the classic state space time series model. Also, the seasonality and slope can be removed from our model, if we know there exists no such structure in the data.

[SIMULATION]
In this section, we study the synthetic data generated from our model. We let S = 7 and provide values for σ and a 1 . The change points and anomaly points are randomly generated. We use our generative procedure (Algorithm 1) to generate time series with total length 500 by fixed parameters. The first 350 points will be used as training set and the remaining 150 points will be used to evaluate the performance of forecasting.
When generating, we let the time series have weekly seasonality with S = 7. For σ we have σ = 0.1, σ u = 0.1, σ v = 0.0004, σ w = 0.01, σ r = 1, σ o = 4. For α 1 we have value for µ as 20, value for δ as 0, and value for seasonality as (1, 2, 4, −1, −3, −2)/10. For p we have p c = 4/350 and p a = 10/350. Despite that, to make sure that at least one change point is in existence, we force z c 330 = 1 and r 330 = 2. That is, for each time series we generate, its 330th point is a change point with the mean shifted up by 3. Also to be consistence with our assumption, we force z c i = z a i = 0, ∀351 ≤ i ≤ 500 so there exists no change point or anomaly point in the testing part. The top panel of Figure 3 shows one example of synthesis data. The blue line marks the separation between training and testing set. The blue dashed line indicates the locations for the change point, while the yellow dots indicate the positions of anomaly points. Also see Figure 3 for illustration on the results returned by implementing our proposed algorithm on the same dataset. The red line gives the fitting results in the first 350 points and forecasting results in the last 150 points. The change points detected are marked with vertical red dotted line, and the anomaly detected are flagged with purple squares. Figure 3 shows that on this dataset, our proposed algorithm yields perfect detection on both change points and anomaly points. In Figure 3, the gray part indicates the 90% predictive interval for forecasting. We run our generative model 100 times to produce 100 different time series, and implement multiply methods on each of them, and aggregate the results together for comparison. We include the following methodologies.  et al., 2008)), and the Prophet R package by Taylor & Letham (2017). We evaluate the performances by mean absolute percentage error (MAPE), mean square error (MSE) and mean absolute error (MAE) on forecasting set. The mathematical definition of these three criterion is given as follows. Let x 1 , x 2 , . . . , x n be the true value andx 1 ,x 2 , . . . ,x n be the estimation or predictive values.
Then we have
MAPE = 1 n n i=1 |x i −x i | x i , MSE = 1 n n i=1 (x i −x i ) 2 , MAE = 1 n n i=1 |x i −x i |.
The comparison of our proposed algorithm and the aforementioned algorithms are included below in Table 2. As we mentioned in Section 6, our algorithm is downward compatible with the cases ignoring the existence of change point or anomaly, by setting p c = 0 or p a = 0. We also run proposed algorithm on the synthetic data with p c = 0 (no change point), or p a = 0 (no anomaly point), or p c = p a = 0 (no change and anomaly point), for the purpose of numeric comparison.
From Table 2 it turns out that our proposed algorithm achieves the best performance compared to other existing methods. Our proposed algorithm also performs better compared with the cases ignoring change point or anomaly point. This is a convincing evidence on the importance of incorporating both change point structure and anomaly point structure when modeling, for time series forecasting.
We also compare our proposed method with other existing change point detection methods and anomaly detection algorithm with respect to the performance of detections. We evaluate the performance by two criterions: True Positive Rate (TPR) and False Positive (FP). TPR measures the percentage of change points or anomalies to be correctly detected. FP count the number of points wrongly detected as change points or anomaly points. The mathematical definitions of TPR and FP are as follows. Let (z 1 , z 2 , . . . , z n ) be the true binary vector for change points or anomalies, and points in total. We split it such that the first 3000 points are used as training set and last 1000 points are used to evaluate the forecasting performance. From Figure 4, it is obvious that there exists no seasonality or slope structure in the dataset. This motivates us not to include these two components in our model. We implement our proposed algorithm without seasonality and slope, and compare the forecasting performance with other methods in Table 5. Our method outperforms BSTS, ARIMA, ETS and Prophet. However in Table 5 the performance can be slightly improved if we ignore the existence of anomaly points by letting p a = 0. This may be caused by model mis-specification as the data may not generated in a way not entirely captured by our model. Nevertheless, the performances of our method considering anomaly points or not, are comparable to each other. In this dataset there is no ground-truth of change point and anomaly point on their locations or even existence. However, from bottom panel of Figure 4, there are some obvious changes in the sequence and they all successfully captured by our algorithm.

[INTERNET TRAFFIC DATA]
Our second real data is an Internet traffic data acquired from a major Tech company (see Figure 5).
It is a daily traffic data, with seasonality S = 7. We use the first 800 observations as training set and evaluate the performance of forecasting on the remaining 265 points. The bottom panel of Figure 5 show the result from implementing our algorithm. We also do the comparison of forecasting performance of our proposed algorithm together with other existing methods, shown in Table 6. We can also see that our algorithm outperforms all the other algorithms with respect to MAPE, MSE and MAE.
Compared to the aforementioned models, our work differs in Bayesian modeling which samples posterior to estimate hidden components given the independent Bernoulli priors of changing point and anomalies.

[CONCLUSION]
We incorporate the change point structure and anomaly point structure into the classic space state time series model. We provide a Bayesian scheme for inference and time series forecasting. We compare the performance of our methodology and state-of-the-art methods on both synthetic data and real datasets. Our method performs the best with respect to forecasting, change point detection, and anomaly detection as well.

[]
(ẑ 1 ,ẑ 2 , . . . ,ẑ n ) are the estimated ones. Then
From the definition, we can see high TPR and low FP means the algorithm has better performance in detection.
The comparison on change point detection is shown in Table 3. We compare our results against three popular change point detection methods: Bayesian Change Point (BCP) (Barry & Hartigan, 1993), Change-Point (CP) (Killick & Eckley, 2014) and Breakout (twitter, 2017). From Table 3 our proposed method outperforms the most of the others by both TPR and FP. We have smaller TPR compared to CP, but we are better in FP.  In Table 4, we also compare the performance of our algorithm on anomaly detection with three existing common anomaly detection methods: the AnomalyDetection package by Twitter (2017), RAD by Netflix (2017) and Tsoutlier by Chen & Liu (1993). The comparison is listed in Table 4. We can see our method also outperforms most of the others with respect to anomaly detection, by both TPR and FP. RAD has slightly better TPR but its FP is much worse compared with ours.

[REAL DATA ANALYSIS]
In this section, we implement our proposed method on real-world datasets. We also compare its performance against other existing time series forecasting methodologies. We consider two datasets, one is a public data called Well-log dataset, and the other is an unpublished internet traffic dataset. The bottom panels of Figure 4 and Figure 5 give the result of our proposed algorithms. The blue line separates the training set and testing set. We use red line to show our fitting and forecasting result, vertical red dashed line to indicate change points and purple dots to indicate anomaly points. The gray part shows 90% predication interval.

[WELL-LOG DATA]
This dataset (Fearnhead & Clifford, 2003;JK & WJ, 1996) was collected when drilling a well. It measures the nuclear magnetic response, which provides geophysical information to analyze the structure of rock surrounding the well. This dataset is public and available online 2 . It has 4050 From Figure 5 our proposed algorithm identifies one change point (the 576th point, indicated by the vertical red dashed line), which can be confirmed that this is exactly the only one change point existing in this time series caused by the change of counting methods, by some external information. Thus, we give the perfect change point detection in this Internet traffic data.
For this Internet traffic dataset, since we have ground-truth for change point, we can compare the performance of change point detection of different methodologies. BCP returns posterior distribution, which peaks in the the 576th point with posterior probability value 0.5. And it also returns with many other points with posterior probability value around 0.1. CP returns 4 change points, where the 576th point (the only true one) is one of them. Breakout returns 8 change points without including the 576th point. To sum up, our proposed method achieves the best change point detection in this real dataset.

[RELATED WORK]
Parametric models are widely considered in econometric literature for time series forecasting, e.g. Jalles (2009), Commandeur et al. (2011), Gould et al. (2008, Harvey & Peters (1990), Harvey et al. (1998). The general procedure of decomposition method (using trend, seasonal and irregular components) for univariate structural time series modeling is discussed in Harvey & Peters (1990); a unified state space framework is proposed to handle any messy time series in Harvey et al. (1998); and the explicit modeling of both additive and multiplicative seasonalities in Gould et al. (2008); Jalles (2009). Although Kalman filter and MCMC-based approaches are used to sample posterior to estimate hidden components, the changing points and anomalies are not considered and processed in the above works. For example, the irregular component considered in Jalles ( 2009) is simply the noises. Commandeur et al. (2011) discusses the statistical software for state-space modeling which is designed for generic time series analytic and modeling, which cannot directly be used when changing point and anomalies are in existence. Our proposed approach shares similarity with the aforementioned papers as we have similar additive structure of components. However we are able to incorporate the change points and anomalies, two common structure widely observed in real data, into our model by using Bernoulli indicators. This is non-trivial, and cannot be handled by the aforementioned papers or their variants.
Non-parametric approaches are used for extraction of components from quasi-periodic time-series, e.g., the ensembles of weak detectors using non-parametric measurement are used in Artemov & Burnaev (2016) to detect change-points and anomalies, and the online decomposition algorithm based on per-component is adopted for change-point detection in Alexey Artemov (2015). Different from the above works, to handle the structural brakes and change-points, this paper presents the parametric approach for modeling anomalies and changing points by fitting them in the state-space framework using approximate inference for forecasting path prediction.
Different Bayesian approaches are proposed for change-point detection, e.g., Adams & MacKay (2007) performs Bayesian change point detection from online inference by generating the distribution estimation of the next unseen datum in the sequence given only data already observed, and the Bayesian Online CPD (BOCPD) algorithm proposed by Turner et al. (2009) performs online prediction using hidden variable given the underlying predictive model (UPM) and the hazard function.","[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection."
Bayesian Time Series Forecasting with Change Point and Anomaly Detection,rJLTTe-0W.json,"The paper introduces a Bayesian model for timeseries with anomaly and change points besides regular trend and seasonality. It develops algorithms for inference and forecasting. The performance is evaluated and compared against state-of-the-art methods on three data sets: 1) synthetic data obtained from the generative Bayesian model itself; 2) well-log data; 3) internet traffic data.

On the methodological side, this appears to be a solid and significant contribution, although I am not sure how well it is aligned with the scope of ICLR. The introduced model is elegant; the algorithms for inference are non-trivial.

From a practical perspective, one cannot expect this contribution to be ground-breaking, since there has been more than 40 years of work on time series forecasting, change point and anomaly detection. In some situations the methodology proposed here will work better than previous approaches (particularly in the situation where the data comes from the Bayesian model itself - in that case, there clearly is no better approach), in other cases (which the paper might have put less emphasis on), previous approaches will work better. To position this kind of work, I think it is important the authors discuss the limitations of their approach. Some guidelines on when or when not to use it would be valuable. Clearly, these days one cannot introduce methodology in this area and expect it to outperform existing methods under all circumstances (and hence practitioners to always choose it over any other existing method).

What is surprising is that relatively simple approaches like ETS or STL work pretty much equally well (in some cases even better in terms of MSE) than the proposed approach, while more recent approaches - like BSTS - dramatically fail. It would be good if the authors could comment on why this might be the case.

Summary:
+ Methodology appears to be a significant, solid contribution.
- Experiments are not conclusive as to when or when not to choose this approach over existing methods
- writing needs to be improved (large number of grammatical errors and typos, e.g. 'Mehtods')","[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: Demostration of Decompositions.
Table 2: Algorithm 3 :3Proposed Algorithm Input: Observed time series y = (y 1 , y 2 , . . . , y n ), seasonality length S, length of time series for forecasting m, number of predictive paths N , change point minimum segment l Output: Change point detection z c , anomaly points z a , forecasting result y future = (y n+1 , y n+1 , . . . , y n+m ) and its distribution or predictive intervals Part I: Initialization; Initialize σ , σ o , σ u , σ r , σ v , σ w all with the empirical standard deviation of y;
Table 3: Figure 3 :3Figure 3: An example of synthesis data (left), and the result after applying our algorithm (right).
Table 4: Figure 4 :4Figure 4: Well-log Data (left). The result of implementing our proposed algorithm (right).
Table 5: Figure 5 :5Figure 5: Internet Traffic Data (top); The result of implementing our proposed algorithm (bottom).
Table 6: Two Categories for Hidden Variables α2, . . . , αn)
Table 7: Comparison of Forecasting in Well-log Data

[INTRODUCTION]
Time series forecasting has a rich and luminous history, and is essentially important in most of business operations nowadays. The main aim of time series forecasting is to carefully collect and rigorously study the past observations of a time series to develop an appropriate model which could describe the inherent structure of the series, in order to generate future values. For instance, the internet companies are interested in the number of daily active users (DAU), say, what is DAU after certain period of time, or when will reach their target DAU goal. Time series forecasting is a fruitful research area with many existing methodologies. The most popular and frequently used time series model might be the Autoregressive Integrated Moving Average (ARIMA) (Box et al., 2015;Zhang, 2003;Cochrane, 2005;Hipel & McLeod, 1994). Taking seasonality into consideration, Box et al. (2015) proposed the Seasonal ARIMA. The Holt-Winters method (Winters, 1960) is also very popular by using exponential smoothing. State space model (Durbin & Koopman, 2012;Scott & Varian, 2014;Brodersen et al., 2015) also attracts much attention, which is a linear function of an underlying Markov process plus additive noise. Exponential Smoothing State Space Model (ETS)  decomposes times series into error, trend, seasonal that change over time. Recently, deep learning is applied for time-series trend learning using LSTM (Tao Lin, 2017), bidirectional dynamic Boltzmann machine (Osogami et al., 2017) is applied for time-series long-term dependency learning, and coherent probabilistic forecast (Taieb et al., 2017) is proposed for a hierarchy or an aggregation-level comprising a set of time series. Orthogonal to these works, this paper focuses on robust ways of time series forecasting in presence of change points and anomalies.
In Internet time series forecasting, Google develops the Bayesian structure time series (BSTS) model (Brodersen et al., 2015;Scott & Varian, 2014) to capture the trend, seasonality, and similar components of the target series. Recently, Facebook proposes the Prophet approach (Taylor & Letham, 2017) based on a decomposable model with interpretable parameters that can be intuitively adjusted by analyst. However, as in the DAU example, some special events like Christmas Holiday or President Election, newly launched apps or features, may cause short period or long-term change of DAU, leading to weird forecasting of those traditional models. The aforementioned special cases are well known as • Anomaly points. The items, events or observations that don't conform to an expected pattern or other items in the dataset, leading to a sudden spike or decrease in the series.
• Change points. A market intervention, such as a new product launch or the onset of an advertising (or ad) campaign, may lead to the level change of the original series.
Time series forecasting without change/anomaly point detection and adjustment may also lead to bizarre forecasting since these models might learn the abrupt changes in the past. There are literatures on detecting anomaly or change points individually, examples can be found in Twitter (2017); Netflix (2017); Barry & Hartigan (1993); Killick & Eckley (2014); twitter (2017). However, the aforementioned change point detection models could not support detection in the presence of seasonality, while the presence of trend/change point is not handled by the anomaly detection models. Most importantly, there is a discrepancy between anomaly/change points detection and adjustment, and commonly used manually adjustment might be a bit arbitrary. Unfortunately, the forecasting gap caused by abnormal and change points, to the best of our knowledge, has not been given full attention and no good solution has been found so far. This paper is strongly motivated by bridging this gap.
In this paper, to overcome the limitations of the most (if not all) current models that the anomaly points and change points are not properly considered, we develop a state space time series forecasting model in the Bayesian framework that can simultaneously detect anomaly and change points and perform forecasting. The learned structure information related to anomaly and change points is automatically incorporated into the forecasting process, which naturally enhances the model prediction based on the feedback of state-space model. To solve the resultant optimization problem, an iterative algorithm based on Bayesian approximate inference with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. The novel model could explicitly capture the structure of change points, anomaly points, trend and seasonality, as also provide the distributions and forecasting intervals due to Bayesian forecasting framework. Both synthetic and real data sets show the better performance of proposed model, in comparison with existing baseline. Moreover, our proposed model outperforms state-of-the-art models in identifying anomaly and change points.
To summarize, our work has the following contributions.
• We proposed a robust 1 Bayesian state-space time series forecasting model that is able to explicitly capture the structures of change points and anomalies (which are generally ignored in most current models), and therefore automatically adapt for forecasting by incorporating the prior information of trend, seasonality, as well as change points and anomalies using state space modeling. Due to the enhancement of model description capability, the results of model prediction and abnormal and change points detection are mutually improved.
• To solve the resultant optimization problem, an effective algorithm based on approximate inference using Markov chain Monte Carlo (MCMC) is proposed with theoretical guaranteed forecasting paths.
• Our proposed method outperforms the state-of-the-art methods in time series forecasting in presence of change points and anomalies, and detects change points and anomalies with high accuracy and low false discovery rate on both tasks, outperforming popular change point and anomaly detection methods. Our method is flexible to capture the structure of time series under various scenarios with any component combinations of trend, seasonality, change points and anomalies. Therefore our method can be applied in many settings in practice.

[MODEL OVERVIEW]
State space time series model (Hangos et al., 2014) has been one of the most popular models in time series analysis. It is capable of fitting complicated time series structure including linear trend and seasonality. However, times series observed in real life are almost all prevailed with outliers. Change points, less in frequency but are still widely observed in real time series analysis. Unfortunately, both structures are ignored in the classic state space time series model. In the section, we aim to address this issue by introducing a novel state space time series model. Let y = (y 1 , y 2 , . . . , y n ) be a sequence of time series observations with length n. The ultimate goal is to forecast (y n+1 , y n+2 , . . .). The accuracy in forecasting lies in a successful decomposition of y into existing components. Apart from the residuals, we assume the time series is composed by trend, seasonality, change points and anomaly points. In a nutshell, we have an additive model with time series = trend + seasonality + change point + anomaly point + residual.
Figure 1 provides a demonstration of desired decomposition of time series. In Figure 1, the left panel shows the observed time series. And it can be decomposed into the remaining five panels. The shift in the change point panel shows where the change point lies. And the spikes in the last panel reveals the anomaly points.
As the classical state space model, we have observation equation and transition equations to model y and hidden variables. We use µ = (µ 1 , µ 2 , . . . , µ n ) to model trend, and use γ = (γ 1 , γ 2 , . . . , γ n ) to model seasonality. We use a binary vector z a = (z a 1 , z a 2 , . . . , z a n ) to indicate anomaly points. Then we have Observation equation:
y t = µ t + γ t + t , if z a t = 0 o t , if z a t = 1
.
(1)
The deviation between the observation y t and its ""mean"" µ t + γ t is modeled by t and o t , depending on the value of z a t . If z a t = 1, then y t is an anomaly point; otherwise it is not. Distinguished from the residues = ( 1 , 2 , . . . , n ), the anomaly is captured by o = (o 1 , o 2 , . . . , o n ) which has relative large magnitude.
The hidden state variable µ and γ have intrinsic structures. There are two transition equations, for trend and seasonality separately Transition Equations: Trend:
µ t = µ t−1 + δ t−1 + u t , if z c t = 0 r t , if z c t = 1 ,(2)
δ t = δ t−1 + v t , Seasonality: γ t = − S−1 s=1 γ t−s + w t .(3)
In Equation (2), δ = (δ 1 , δ 2 , . . . , δ n ) can be viewed as the ""slope"" of the trend, measuring how fast the trend changes over time. The change point component is also incorporated in Equation (2) by a binary vector z c = (z c 1 , z c 2 , . . . , z c n ). If z c t = 1, it means the t-th point is a change point, with µ t differs from µ t−1 + δ t−1 (which can be interpreted as the ""momentum"" from the previous status ) by r t ; otherwise it is not a change point and they differ by u t . We model the change points in a way such that r = (r 1 , r 2 , . . . , r n ) have larger magnitude compared u = (u 1 , u 2 , . . . , u n ). The ""slope"" part δ also has its own noise v = (v 1 , v 2 , . . . , v n ).
A first look on Equation (2) may bring up with the question that it is not presented in an exactly the same way as shown in Figure 1. In Figure 1, the change points component is a step function, and it is one of the five additive components along with trend, seasonality, anomaly points and residuals. Here we model the change point directly into the trend component. Though differing in formulation, they are equivalent to each other. We choose to model in as in Equation ( 2) due to simplicity, and its similarity with the definition of anomaly points in Equation (1).
The seasonality component is presented in Equation ( 3). Here S is the length of one season and w = (w 1 , w 2 , . . . , w n ) is the noise for seasonality. The seasonality component is assumed to have almost zero average in each season.
The observation equation and transition equations (i.e., Equation (1,2,3)) define how y is generated from all the hidden variables including change points and anomaly points. We continue to explore this new model, under a Bayesian framework.

[BAYESIAN FRAMEWORK]
Bayesian methods are widely used in many data analysis fields. It is easy to implement and interpret, and it also has the ability to produce posterior distribution. The Bayesian method on state space time series model has been investigated in Scott & Varian (2014);Brodersen et al. (2015). In this section, we also consider Bayesian framework for our novel state space time series model. We assume all the noises are normally distributed
{ t } n t=1 iid ∼ N (0, σ 2 ), {o t } n t=1 iid ∼ N (0, σ 2 o ), {u t } n t=1 iid ∼ N (0, σ 2 u ), {r t } n t=1 iid ∼ N (0, σ 2 r ), {v t } n t=1 iid ∼ N (0, σ 2 v ), {w t } n t=1 iid ∼ N (0, σ 2 w )
, where σ , σ o , σ u , σ r , σ v , σ w are parameters for standard deviation. As binary vectors, a natural choice is to model anomaly point indicator z a and change point indicator z c to the model them as Bernoulli random variables
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c ),
where p a , p c are probabilities for each point to be an anomaly or change point.
Figure 2: Graphical presentation of our model. Note that y is observed, highlighted by gray background, distinguished from all the remaining ones that are hidden. Among the hidden ones, squares indicate fixed parameters, and circles indicate random variables.
For simplicity, we denote α t = (µ t , δ t , γ t , γ t−1 , . . . , γ t−(S−2) ) to include the main hidden variables (except z a t and z c t ) in the transition equations. All the α t are well defined and can be generated from the previous status, except α 1 . We denote a 1 to be the parameter for α 1 , which can be interpreted as the ""mean"" for α 1 .
With Bayesian framework, we are able to represent our model graphically as in Figure 2. As shown in Figure 2, the only observations are y and all the others are hidden. In this paper, we assume there is no additional information on all the hidden states. If we have some prior information, for example, some points are more likely to be change points, then our model can be easily modified to incorporate such information, by using proper prior.
In Figure 2, we use squares and circles to classify unknown variables. Despite all being unknown, they actually behave differently according to their own functionality. For those in squares, they behave like turning parameters. Once they are initialized or given, those in circles behaves like latent variables. We call the former ""parameters"" and the latter ""latent variable"", as listed in Table 1. Trend and seasonality z = (z a , z c ) Anomaly and change points Parameter a1
The ""mean"" for the initial trend and seasonality p = (pa, pc)
Probabilities for each point to be anomaly or change point σ = (σ , σo, σu, σr, σv, σw) Standard deviation
The discrepancy between these two categories is clearly captured by the joint likelihood function. From Figure 2, the joint distribution (i.e., the likelihood function) can be written down explicitly as
La 1 ,p,σ (y, α, z) (4) = {t:z a t =0} g(yt − µt − γt, σ ) × {t:z a t =1} g(yt − µt − γt, σo) × {t:z c t =0} g(µt − µ t−1 − δ t−1 , σu) × {t:z c t =1} g(µt − µ t−1 − δ t−1 , σr) × n t=1 g(δt − δ t−1 , σv) × n t=1 g(− S−1 s=1 γ t−s , σv) × n i=1 (pa) z a t (1 − pa) 1−z a t (pc) z c t (1 − pc) 1−z c t , where g(x 1 , x 2 ) = 1 √ 2πx2 exp −x 2 1 /(2x2
2 ) is the density function for normal distribution with mean x 1 and standard deviation x 2 . Here we slightly abuse the notation by using µ 0 , δ 0 , γ 0 , γ −1 , . . . , γ 2−S , which are actually the corresponding coordinates of a 1 .
As long with other probabilistic graphical models, our model can also be viewed as a generative model. Given the parameters a 1 , p, σ, we are able to generate time series. We present the generative procedure as follows.
Algorithm 1: Generative Procedure Input: Parameters a 1 , σ = (σ , σ o , σ u , σ r , σ v , σ w ) and p a , p c , length of time series to generate m Output: Time series y = (y 1 , y 2 , . . . , y m ) Generate the indexes where anomalies or change points occur
{z a t } n t=1 iid ∼ Ber(p a ), {z c t } n t=1 iid ∼ Ber(p c );
Generate all the noises , o, u, r, v, w as independent normal random variables with mean zero and standard deviation σ , σ o , σ u , σ r , σ v , σ w respectively; Generate {α t } m t=1 sequentially by the transition functions in Equation ( 2) and (3); Generate time series {y t } m t=1 by the observation function in Equation (1).

[INFERENCE]
This section is about inferring unknown variables from y, given the Bayesian setting described in the previous section. The main framework here is to sequentially update each hidden variable by fixing the remaining ones. As stated in the previous section, there are two different categories of unknown variables. Different update schemes need to be used due to the difference in their functionality. For the latent variables, we implement Markov chain Monte Carlo (MCMC) for inference. Particular, we use Gibbs sampler. We will elaborate the details of updates in the following sections.

[UPDATES ON TREND AND SEASONALITY]
In this section, we focus on updating α assuming all the other hidden variables are given and fixed. The essence of Gibbs sampler is to obtain posterior distribution p a1,p,σ (α|y, z). This can be achieved by a combination of Kalman filter, Kalman smoothing and the so-called ""fake-path"" trick. We provide some intuitive explanation here and refer the readers to Durbin & Koopman (2012) for detailed implementation.
Kalman filter and Kalman smoothing are classic algorithms in signal processing and pattern recolonization for Bayesian inference. It is well related to other algorithms especially message passing algorithm. Kalman filter collects information forwards to obtain E(α t |y 1 , y 2 , . . . , y t ); while Kalman smoothing distribute information backwards to achieve E(α t |y).
However, the combination of Kalman filter and Kalman smoothing is not enough, as it only gives the the expectations of marginal distributions {E(α t |y)} n t=1 , instead of the joint distribution required for Gibbs sampler. To address this issue, we can use the ""fake-path"" trick described in Brodersen et al. (2015); Durbin & Koopman (2012). The main idea underlying this trick lies on the fact that the covariance structure of p(α t |y) is not dependent on the means. If we are able to obtain the covariance by some other way, then we can add it up with {E(α t |y)} n t=1 to obtain a sample from p(α|y). This trick involves three steps. Note that all the other hidden variables z, p, σ are given.
1. Pick some vectorã 1 , and generate a sequence of time seriesỹ from it by Algorithm 1. In this way, we also observeα. 2. Obtain {E(α t |ỹ)} n t=1 fromỹ by Kalman filter and Kalman smoothing. 3. We use {α t − E(α t |ỹ) + E(α t |y)} n t=1 as our sampling from the conditional distribution.

[CHANGE POINT AND ANOMALY DETECTION]
In this section, we update z by Gibbs sampler, assuming α, a 1 , p, σ are all given and fixed. We need to obtain the conditional distribution p a1,p,σ (z|y, α). Note that in the graphical model described in Section 2, {z a t } n t=1 and {z c t } n i=1 are all Bernoulli random variables and independent of each other. Then the conditional distribution p a1,p,σ (z|y, α) can also be decomposed into product of Bernoulli density functions. In other words, conditioned on y, α, {z a t } n t=1 and {z c t } n i=1 are still independent Bernoulli random variables, but possibly with different success probabilities. Thus, we can take the calculation point by point. For example, for the anomaly detection for the t-th point, we have
z a t = 0 : y t − µ t − γ t ∼ N (0, σ 2 ) z a t = 1 : y t − µ t − γ t ∼ N (0, σ 2 o
). And the prior on z a t is P(z a t = 1) = p a and P(z a t = 0) = p 1 . Let p a t = P(z a t = 1|y, α). Directly calculation leads to
p a t = pa σo exp − (yt−µt−γt) 2 2σ 2 o 1−pa σ exp − (yt−µt−γt) 2 2σ 2 + pa σo exp − (yt−µt−γt) 2 2σ 2 o .(5)
This equality holds for all t = 1, 2, . . . , n. Similarly for change point detection, let p c t = P(z c t = 1|y, α), and we have
p c t = pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r 1−pc σu exp − (µt−µt−1−δt−1) 2 2σ 2 u + pc σr exp − (µt−µt−1−δt−1) 2 2σ 2 r .(6)
As mentioned above, all the coordinates in z are still independent Bernoulli random variables conditioned on y, α. Thus, for Gibbs sampler, we can generate z by sampling independently with
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t ).
For change point detection here, we have an additional segment control step. After obtaining {z c t } n t=1 as mentioned above, we need to make sure that the change points detected satisfy some additional requirement on the length of segment among two consecutive change points. This issue arises from the ambiguity between the definitions of change point and anomaly points. For example, consider a time series with value (0, 0, 0, 0, 1, 1, 1, 0, 0, 0). We can view it with two change points, one increases the trend by 1 and the other decreases it by 1. Alternatively, we can also argue the three 1s in this time series are anomalies, though next to each other. One way to address this ambiguity is by defining the minimum length of segment (denoted as ). In this toy example, if we set the minimum length to be 4, then they are anomaly points; if we set it to be 3, then we regard them to be change points. But a more complicated criterion is needed than using minimum length as the time series usually own much more complex structure than this toy example. Consider time series (0, 0, 0, 0, −1, −1, 1, 1, 1, 1) and the minimum time series parameter = 3. It is reasonable to view it with one change point with increment 1, and the two -1s should be regarded as anomalies. As a combination of all these factors, we propose the following segment control method. A default value for the parameter is the length of seasonality, i.e., = S.
Algorithm 2: Segment control on change points Input: change point binary vector z c ,trend µ, standard deviation for outliers σ r , change point minimum segment Output: change point binary vector z c Denote t 1 < t 2 < . . . to be all the indexes such that z c ti = 1; while there exists i such that
|t i+1 − t i | < do Check if |µ ti−1 − µ ti+1+1 | ≤ σ r /2.
If so, exclude both them from change points by setting z c ti = z c ti+1 = 0. Otherwise, randomly exclude one of them by setting the corresponding coordinate in z c to be 0; Update all the indexes of change points in z c . end

[INITIALIZATION AND UPDATES ON PARAMETERS]
The parameters σ, a 1 and p need both initialization and update. We have different initializations and update schemes for each of them.
For all the standard deviations, once we obtain α and z, we update them by taking the empirical standard deviation correspondingly. For σ δ and σ γ , the calculation is straightforward as they only involve δ and γ respectively. For σ , σ o , σ u and σ r , it is a bit more involved due to z. Nevertheless, we can obtain the following update equations for all of them:
σ = {t:z a t =0} (yt − µt − γt) 2 |{t : z a t = 0}| , σo = {t:z a t =1} (yt − µt − γt) 2 |{t : z a t = 1}| , σu = {t:z c t =0} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| ,(7)
σr = {t:z c t =1} (µt − µt−1 − δt−1) 2 |{t : z c t = 0}| , σ δ = 1 n n t=1 (δt − δt−1) 2 , σγ = 1 n n t=1 ( S−1 s=0 γt−s) 2 . (8
)
Note that in some iterations, when there is no change point or anomaly detected in z, then the updates above for σ o , σ r are not well-defined. In those cases, we simply let them remain the same.
To initialize σ, we let them all equal to the standard deviation of y.
For a 1 , we initialize it by letting its first coordinate to be equal to the average of y 1 , y 2 , . . . , y S , and all the remaining coordinates to be equal to 0. Since a 1 can be interpreted as the mean vector of α 1 , in this way the trend is initialized to be matched up with average of the first season, and the slope and seasonality are initialized to be equal to 0. We update a 1 by using information of α. We let the first two coordinates (trend and slope) of a 1 to be equal to those of α 1 , and we let the remaining coordinates (seasonality) of a 1 to be equal to those of α S+1 . The reason why we do not let a 1 to be equal to α 1 entirely is due to the consideration on convergence and robustness. Since we initialize the seasonality part in a 1 as 0, it will remain 0 if we let a 1 equals α 1 entirely (due to the mechanism how we update α 1 as described in Section 4.1. We can avoid such trouble via using α S+1 .
For p, we initialize them to be equal to 1/n. If we have additional information on the number of change points or anomaly points, we can initiate them with different values, for example, 0.1/n, or 10/n. We can update p after obtaining z, but we choose not to, also for the sake of robustness. In the early iterations when the algorithm is far from convergence, it is highly possible that z a or z c may turn out to be all 0. If we update p, say, by taking the proportion of change point or anomaly points in z. Then p a or p c might be 0, and it may get stuck in 0 in the remaining iterations.

[FORECASTING]
Once we infer all the latent variables α, z and tune all the parameters p, a 1 , σ, we are able to forecast the future time series y future . From the graphical model described in Section 3, the future forecasting only involves α n instead of the whole α. Note that we assume that there exists no change point and anomaly point in the future. This is reasonable as in most cases we have no additional information on the future time series. Given α n and σ we can use our predictive procedures (i.e., Algorithm 1) to generate future time series y future . We can further integrate out α n to have the posterior predictive distribution as p σ (y future |y).
The forecasting on future time series is not deterministic. There are two sources for the randomness in y future . One comes from the inference of α n (and also σ) from y. Under the Bayesian framework in Section 3, we have a posterior distribution over α n rather than a single point estimation. The second one comes from the forecasting function itself. The forecasting involves intrinsic noise like t , u t , v t and w t . Thus, the predictive density function p σ (y future |y, α n ) will lead to different path even with fixed σ and α n . In this way we are able to obtain distribution and predictive interval for forecasting. We also suggest to take the average of multiple forecasting paths, as the posterior mean for the forecasting.
The average of multiple forecasting paths (denoted asȳ future ), if the number of paths is large enough, always takes the form as a combination of linear trend and seasonality. This can be observed in both our synthesis data (Section 7) and real data analysis (Section 8). This seems to be surprising at the first glance, but makes some sense intuitively. Under our assumption, we have no information on the future, and thus a reliable way to forecast the future is to use the information collected at the end of observed time series, i.e., trend µ n , slope δ n and seasonality structure. Theorem 1 gives mathematical explanation of the linearity ofȳ future , in both mean and standard deviation. Theorem 1. Let N be the number of future time series paths we generate from Algorithm 1). Let m be the number of points we are going to forecast. Denote {y
(1) n+j } m j=1 , {y(2)
n+j } m j=1 , . . . , {y
n+j } m j=1 to be the future paths. Defineȳ future = (ȳ n+1 ,ȳ n+2 , . . . ,ȳ n+m ) to be the average such that
y n+j = 1 N N i=1 y (i) n+j .
Then for all j = 1, 2, . . . , N , we haveȳ n+j as a normal distribution with mean and variance as
E[ȳ n+j ] = µ n + jδ n + γ n−S+(j mod S) Var [ȳ n+j ] = 1 N j(j + 1)σ 2 v /2 + j(σ 2 u + σ 2 w ) + σ 2 .
Consequently, for all j = 1, 2, . . . , m, E[ȳ n+j ] is in a linear form with respect to j, and the standard deviation ofȳ n+j also takes a approximately linear form with respect to j.
Proof. Recall that α n , σ are given and fixed, and we assume there is no change point or anomaly in the future time series. The Equation (2) leads to δ n+j = δ n + j l=1 v n+l , which implies that
µ n+j = µ n + jδ n + j l=1 (j + 1 − l)v n+l + j l=1 u m+l .
For the seasonality part, simple linear algebra together with Equation 3 leads to γ n+j = γ n−S+(j mod S) + j l=1 w n+l . Thus,
y n+j = 1 N N i=1 µ n + jδ n + γ n−S+(j mod S) + j l=1 (j + 1 − l)v (i) n+l + j l=1 u (i) m+l + j l=1 w (i) n+l + (i) n+j .
Due to the independence and Gaussian distribution of all the noises,ȳ n+j is also normally distributed and its means and variance can be calculated accordingly.

[ALGORITHM]
Our proposed method can be divided into three parts: initialization, inference, and forecasting. Section 4 and Section 5 provide detailed explanation and reasoning for each of them. We present a whole picture of our proposed methodology in Algorithm 3. Initialize a 1 such that its first coordinate equals to the average of (y 1 , y 2 , . . . , y S ) and all the remaining S coordinates with 0; Initialize p a and p c by 1/n. Then generate z a and z c as independent Bernoulli random variables with success probability p a and p c respectively;
Part II: Inference; while the likelihood function L a1,p,σ (y, α, z) not converges do Infer α by Kalman filter, Kalman smoothing and ""fake-path"" trick described in Section 4.1;
Update z a and z c by sampling from
{z a t } n t=1 ∼ Ber(p a t ), {z c t } n t=1 ∼ Ber(p c t )
, where the success probability {p a t } n t=1 and {p c t } n t=1 are defined in Equation ( 5) and ( 6);
Segment control on z c by Algorithm 2; Update σ by Equation ( 7) to (8);
Update a 1 such that its first two coordinates equal to the those of α 1 and the remaining (S − 1) coordinates equals to those of α S+1 ;
Calculate the likelihood function L a1,p,σ (y, α, z) given in Equation ( 4); end Part III: Forecasting; With a n and σ, use the generate procedure in Algorithm 1 to generate future time series y future with length m. Repeat the generative procedure to obtain multiple future paths y
(1) future , y
future , . . . , y
future ; Combine all the predictive paths give the distribution for the future time series forecasting. If needed, calculate the point-wise quantile to obtain predictive intervals. Use the point-wise average as our final forecasting result.
It is worth mentioning that our proposed methodology is downward compatible with many simpler state space time series models. By letting p c = 0, we assume there is no change point in the time series. By letting p a = 0, we assume there is no anomaly point in the time series. If both p c and p a are set to be 0, then our model is reduced to the classic state space time series model. Also, the seasonality and slope can be removed from our model, if we know there exists no such structure in the data.

[SIMULATION]
In this section, we study the synthetic data generated from our model. We let S = 7 and provide values for σ and a 1 . The change points and anomaly points are randomly generated. We use our generative procedure (Algorithm 1) to generate time series with total length 500 by fixed parameters. The first 350 points will be used as training set and the remaining 150 points will be used to evaluate the performance of forecasting.
When generating, we let the time series have weekly seasonality with S = 7. For σ we have σ = 0.1, σ u = 0.1, σ v = 0.0004, σ w = 0.01, σ r = 1, σ o = 4. For α 1 we have value for µ as 20, value for δ as 0, and value for seasonality as (1, 2, 4, −1, −3, −2)/10. For p we have p c = 4/350 and p a = 10/350. Despite that, to make sure that at least one change point is in existence, we force z c 330 = 1 and r 330 = 2. That is, for each time series we generate, its 330th point is a change point with the mean shifted up by 3. Also to be consistence with our assumption, we force z c i = z a i = 0, ∀351 ≤ i ≤ 500 so there exists no change point or anomaly point in the testing part. The top panel of Figure 3 shows one example of synthesis data. The blue line marks the separation between training and testing set. The blue dashed line indicates the locations for the change point, while the yellow dots indicate the positions of anomaly points. Also see Figure 3 for illustration on the results returned by implementing our proposed algorithm on the same dataset. The red line gives the fitting results in the first 350 points and forecasting results in the last 150 points. The change points detected are marked with vertical red dotted line, and the anomaly detected are flagged with purple squares. Figure 3 shows that on this dataset, our proposed algorithm yields perfect detection on both change points and anomaly points. In Figure 3, the gray part indicates the 90% predictive interval for forecasting. We run our generative model 100 times to produce 100 different time series, and implement multiply methods on each of them, and aggregate the results together for comparison. We include the following methodologies.  et al., 2008)), and the Prophet R package by Taylor & Letham (2017). We evaluate the performances by mean absolute percentage error (MAPE), mean square error (MSE) and mean absolute error (MAE) on forecasting set. The mathematical definition of these three criterion is given as follows. Let x 1 , x 2 , . . . , x n be the true value andx 1 ,x 2 , . . . ,x n be the estimation or predictive values.
Then we have
MAPE = 1 n n i=1 |x i −x i | x i , MSE = 1 n n i=1 (x i −x i ) 2 , MAE = 1 n n i=1 |x i −x i |.
The comparison of our proposed algorithm and the aforementioned algorithms are included below in Table 2. As we mentioned in Section 6, our algorithm is downward compatible with the cases ignoring the existence of change point or anomaly, by setting p c = 0 or p a = 0. We also run proposed algorithm on the synthetic data with p c = 0 (no change point), or p a = 0 (no anomaly point), or p c = p a = 0 (no change and anomaly point), for the purpose of numeric comparison.
From Table 2 it turns out that our proposed algorithm achieves the best performance compared to other existing methods. Our proposed algorithm also performs better compared with the cases ignoring change point or anomaly point. This is a convincing evidence on the importance of incorporating both change point structure and anomaly point structure when modeling, for time series forecasting.
We also compare our proposed method with other existing change point detection methods and anomaly detection algorithm with respect to the performance of detections. We evaluate the performance by two criterions: True Positive Rate (TPR) and False Positive (FP). TPR measures the percentage of change points or anomalies to be correctly detected. FP count the number of points wrongly detected as change points or anomaly points. The mathematical definitions of TPR and FP are as follows. Let (z 1 , z 2 , . . . , z n ) be the true binary vector for change points or anomalies, and points in total. We split it such that the first 3000 points are used as training set and last 1000 points are used to evaluate the forecasting performance. From Figure 4, it is obvious that there exists no seasonality or slope structure in the dataset. This motivates us not to include these two components in our model. We implement our proposed algorithm without seasonality and slope, and compare the forecasting performance with other methods in Table 5. Our method outperforms BSTS, ARIMA, ETS and Prophet. However in Table 5 the performance can be slightly improved if we ignore the existence of anomaly points by letting p a = 0. This may be caused by model mis-specification as the data may not generated in a way not entirely captured by our model. Nevertheless, the performances of our method considering anomaly points or not, are comparable to each other. In this dataset there is no ground-truth of change point and anomaly point on their locations or even existence. However, from bottom panel of Figure 4, there are some obvious changes in the sequence and they all successfully captured by our algorithm.

[INTERNET TRAFFIC DATA]
Our second real data is an Internet traffic data acquired from a major Tech company (see Figure 5).
It is a daily traffic data, with seasonality S = 7. We use the first 800 observations as training set and evaluate the performance of forecasting on the remaining 265 points. The bottom panel of Figure 5 show the result from implementing our algorithm. We also do the comparison of forecasting performance of our proposed algorithm together with other existing methods, shown in Table 6. We can also see that our algorithm outperforms all the other algorithms with respect to MAPE, MSE and MAE.
Compared to the aforementioned models, our work differs in Bayesian modeling which samples posterior to estimate hidden components given the independent Bernoulli priors of changing point and anomalies.

[CONCLUSION]
We incorporate the change point structure and anomaly point structure into the classic space state time series model. We provide a Bayesian scheme for inference and time series forecasting. We compare the performance of our methodology and state-of-the-art methods on both synthetic data and real datasets. Our method performs the best with respect to forecasting, change point detection, and anomaly detection as well.

[]
(ẑ 1 ,ẑ 2 , . . . ,ẑ n ) are the estimated ones. Then
From the definition, we can see high TPR and low FP means the algorithm has better performance in detection.
The comparison on change point detection is shown in Table 3. We compare our results against three popular change point detection methods: Bayesian Change Point (BCP) (Barry & Hartigan, 1993), Change-Point (CP) (Killick & Eckley, 2014) and Breakout (twitter, 2017). From Table 3 our proposed method outperforms the most of the others by both TPR and FP. We have smaller TPR compared to CP, but we are better in FP.  In Table 4, we also compare the performance of our algorithm on anomaly detection with three existing common anomaly detection methods: the AnomalyDetection package by Twitter (2017), RAD by Netflix (2017) and Tsoutlier by Chen & Liu (1993). The comparison is listed in Table 4. We can see our method also outperforms most of the others with respect to anomaly detection, by both TPR and FP. RAD has slightly better TPR but its FP is much worse compared with ours.

[REAL DATA ANALYSIS]
In this section, we implement our proposed method on real-world datasets. We also compare its performance against other existing time series forecasting methodologies. We consider two datasets, one is a public data called Well-log dataset, and the other is an unpublished internet traffic dataset. The bottom panels of Figure 4 and Figure 5 give the result of our proposed algorithms. The blue line separates the training set and testing set. We use red line to show our fitting and forecasting result, vertical red dashed line to indicate change points and purple dots to indicate anomaly points. The gray part shows 90% predication interval.

[WELL-LOG DATA]
This dataset (Fearnhead & Clifford, 2003;JK & WJ, 1996) was collected when drilling a well. It measures the nuclear magnetic response, which provides geophysical information to analyze the structure of rock surrounding the well. This dataset is public and available online 2 . It has 4050 From Figure 5 our proposed algorithm identifies one change point (the 576th point, indicated by the vertical red dashed line), which can be confirmed that this is exactly the only one change point existing in this time series caused by the change of counting methods, by some external information. Thus, we give the perfect change point detection in this Internet traffic data.
For this Internet traffic dataset, since we have ground-truth for change point, we can compare the performance of change point detection of different methodologies. BCP returns posterior distribution, which peaks in the the 576th point with posterior probability value 0.5. And it also returns with many other points with posterior probability value around 0.1. CP returns 4 change points, where the 576th point (the only true one) is one of them. Breakout returns 8 change points without including the 576th point. To sum up, our proposed method achieves the best change point detection in this real dataset.

[RELATED WORK]
Parametric models are widely considered in econometric literature for time series forecasting, e.g. Jalles (2009), Commandeur et al. (2011), Gould et al. (2008, Harvey & Peters (1990), Harvey et al. (1998). The general procedure of decomposition method (using trend, seasonal and irregular components) for univariate structural time series modeling is discussed in Harvey & Peters (1990); a unified state space framework is proposed to handle any messy time series in Harvey et al. (1998); and the explicit modeling of both additive and multiplicative seasonalities in Gould et al. (2008); Jalles (2009). Although Kalman filter and MCMC-based approaches are used to sample posterior to estimate hidden components, the changing points and anomalies are not considered and processed in the above works. For example, the irregular component considered in Jalles ( 2009) is simply the noises. Commandeur et al. (2011) discusses the statistical software for state-space modeling which is designed for generic time series analytic and modeling, which cannot directly be used when changing point and anomalies are in existence. Our proposed approach shares similarity with the aforementioned papers as we have similar additive structure of components. However we are able to incorporate the change points and anomalies, two common structure widely observed in real data, into our model by using Bernoulli indicators. This is non-trivial, and cannot be handled by the aforementioned papers or their variants.
Non-parametric approaches are used for extraction of components from quasi-periodic time-series, e.g., the ensembles of weak detectors using non-parametric measurement are used in Artemov & Burnaev (2016) to detect change-points and anomalies, and the online decomposition algorithm based on per-component is adopted for change-point detection in Alexey Artemov (2015). Different from the above works, to handle the structural brakes and change-points, this paper presents the parametric approach for modeling anomalies and changing points by fitting them in the state-space framework using approximate inference for forecasting path prediction.
Different Bayesian approaches are proposed for change-point detection, e.g., Adams & MacKay (2007) performs Bayesian change point detection from online inference by generating the distribution estimation of the next unseen datum in the sequence given only data already observed, and the Bayesian Online CPD (BOCPD) algorithm proposed by Turner et al. (2009) performs online prediction using hidden variable given the underlying predictive model (UPM) and the hazard function.","[TITLE]
BAYESIAN TIME SERIES FORECASTING WITH CHANGE POINT AND ANOMALY DETECTION

[ABSTRACT]
Time series forecasting plays a crucial role in marketing, finance and many other quantitative fields. A large amount of methodologies has been developed on this topic, including ARIMA, Holt-Winters, etc. However, their performance is easily undermined by the existence of change points and anomaly points, two structures commonly observed in real data, but rarely considered in the aforementioned methods. In this paper, we propose a novel state space time series model, with the capability to capture the structure of change points and anomaly points, as well as trend and seasonality. To infer all the hidden variables, we develop a Bayesian framework, which is able to obtain distributions and forecasting intervals for time series forecasting, with provable theoretical properties. For implementation, an iterative algorithm with Markov chain Monte Carlo (MCMC), Kalman filter and Kalman smoothing is proposed. In both synthetic data and real data applications, our methodology yields a better performance in time series forecasting compared with existing methods, along with more accurate change point detection and anomaly detection."
Deep Anomaly Detection with Outlier Exposure,HyxCxhRcY7.json,"This paper proposes fine-tuning an out-of-distribution detector using an Outlier Exposure (OE) dataset. The novelty is in proposing a model-specific rather than dataset-specific fine-tuning. Their modifications are referred to as Outlier Exposure. OE includes the choice of an OE dataset for fine-tuning and a regularization term evaluated on the OE dataset. It is a comprehensive study that explores multiple datasets and improves dataset-specific baselines.

Suggestions and clarification requests:
- The structure of the writing does not clearly present the novel aspects of the paper as opposed to the previous works. I suggest moving the details of model-specific OE regularization terms to section 3 and review the details of the baseline models. Then present the other set of novelties in proposing OE datasets in a new section before presenting the results. Clearly presenting two sets of novelties in this work and then the results. If constrained in space, I suggest squeezing the discussion, conclusion, and 4.1.
- In the related work section Radford et al., 2016 is references when mentioning GAN. Why not the original reference for GAN?
- Maybe define BPP, BPC, and BPW in the paragraphs on PixelCNN++ and language modeling or add a reference.
- Numbers in Table 3 column MSP should match the numbers in Table 1, right? Or am I missing something?","[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: ROC curve with Tiny Im-ageNet (D in ) and Textures (D test out ).
Table 2: Figure 2 :2Figure 2: OOD scores from PixelCNN++ on images from CIFAR-10 and SVHN.
Table 3: to refine a scraped D OE out auxiliary dataset to be appropriately close to D test in .SV HN CI FA R-10 CI FA R
Table 4: Figure 3 :3Figure 3: Root Mean Square Calibration Error values with temperature tuning and temperature tuning + OE across various datasets.
Table 5: .The Root Mean Square Calibration Error measures the square root of the expected squared difference between confidence and accuracy at a confidence level. It has the formula E C [(P(Y = Y |C = c) − c) 2 ] . A similar formulation which less severely penalizes large confidence-accuracy deviations is the Mean Absolute Value Calibration error, written E C [|P(Y = Y |C = c) − c|]. The MAD Calibration Error is a lower bound of the RMS Calibration Error. To empirically estimate these miscalibration measures, we partition the n samples of S into b bins {B 1 , B 2 , . . . , B b } with approximately 100 samples in each bin. UnlikeGuo et al. (2017), bins are not equally spaced since the distribution of confidence values is not uniform but dynamic. Concretely, the RMS Calibration Error is estimated with the numerically stable formula
Table 6: Figure 4 :4Figure 4: ROC curves with Tiny ImageNet as D in and Textures, Places365, LSUN, and ImageNet as D test out . Figures show the curves corresponding to the maximum softmax probability (MSP) baseline detector and the MSP detector with Outlier Exposure (OE).
Table 7: We use the ImageNet dataset with images from approximately 22 thousand classes as D OE WikiText-2 is a corpus of Wikipedia articles typically used for language modeling. We use WikiText-2 as D OE
Table 8: 8 in Appendix A. Notice that the SVHN classifier with OE can be used to detect new anomalies such as emojis and street view alphabet letters, even though D test OE is a dataset of natural images. Thus, Outlier Exposure helps models to generalize to unseen D test out distributions far better than the baseline. Comparisons between the MSP baseline and the MSP of the natural language classifier fine-tuned with OE. Results are percentages and averaged over 10 runs.Confidence Branch. A recently proposed OOD detection technique(DeVries & Taylor, 2018) involves appending an OOD scoring branch b : X → [0, 1] onto a deep network. Trained with samples from only D in , this branch estimates the network's confidence on any input. The creators of this technique made their code publicly available, so we use their code to train new 40-4 Wide Residual Network classifiers. We fine-tune the confidence branch with Outlier Exposure by adding 0.5E x∼D OE out[log b(x)] to the network's original optimization objective. In Table3, the baseline values are derived from the maximum softmax probabilities produced by the classifier trained with DeVries & Taylor (2018)'s publicly available training code. The confidence branch improves over this MSP detector, and after OE, the confidence branch detects anomalies more effectively. Comparison among the maximum softmax probability, Confidence Branch, and Confidence Branch + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages, and averaged across all D test out datasets.
Table 9: Comparison among the maximum softmax probability (MSP), MSP + GAN, and MSP + GAN + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages and averaged across all D test out datasets.
Table 10: Table 5: OOD detection results with a PixelCNN++ density estimator, and the same estimator after applying OE. The model's bits per pixel (BPP) scores each sample. All results are percentages. Test distributions D test out are described in Appendix A.
Table 11: ; expanded results and D test out descriptions are in Appendix F. In all cases, OE improves over the baseline, and the improvement is especially large for the word-level model.
Table 12: 
Table 13: NLP OOD example detection for the maximum softmax probability (MSP) baseline detector and the MSP detector after fine-tuning with Outlier Exposure (OE). All results are percentages and the result of 10 runs. Values are rounded so that 99.95% rounds to 100%.
Table 14: C TRAINING FROM SCRATCH WITH OUTLIER EXPOSURE USUALLY IMPROVES DETECTION PERFORMANCE Elsewhere we show results for pre-trained networks that are fine-tuned with OE. However, a network trained from scratch which simultaneously trains with OE tends to give superior results. For example, a CIFAR-10 Wide ResNet trained normally obtains a classification error rate of 5.16% and an FPR95 of 34.94%. Fine-tuned, this network has an error rate of 5.27% and an FPR95 of 9.50%. Yet if we instead train the network from scratch and expose it to outliers as it trains, then the error rate is 4.26% and the FPR95 is 6.15%. This architecture corresponds to a 9.50% RMS calibration error with OE fine-tuning, but by training with OE from scratch the RMS calibration error is 6.15%. Compared to fine-tuning, training a network in tandem with OE tends to produce a network with a better error rate, calibration, and OOD detection performance. The reason why we use OE for fine-tuning is because training from scratch requires more time and sometimes more GPU memory than fine-tuning.
Table 15: 
Table 16: OOD detection results on Penn Treebank examples and English Web Treebank outliers. All results are percentages.The D test out datasets come from the English Web Treebank(Bies et al., 2012), which contains text from five different domains: Yahoo! Answers, emails, newsgroups, product reviews, and weblogs. Other NLP D test out datasets we consider do not satisfy the language modeling assumption of continuity in the examples, so we do not evaluate on them.
Table 17: Table 12: Calibration results for the temperature tuned baseline and temperature tuning + OE.
Table 18: Calibration results for the softmax temperature tuning baseline, the same baseline after adding Posterior Rescaling, and temperature tuning + Posterior Rescaling + OE.

[INTRODUCTION]
Machine Learning systems in deployment often encounter data that is unlike the model's training data. This can occur in discovering novel astronomical phenomena, finding unknown diseases, or detecting sensor failure. In these situations, models that can detect anomalies (Liu et al., 2018;Emmott et al., 2013) are capable of correctly flagging unusual examples for human intervention, or carefully proceeding with a more conservative fallback policy.
Behind many machine learning systems are deep learning models (Krizhevsky et al., 2012) which can provide high performance in a variety of applications, so long as the data seen at test time is similar to the training data. However, when there is a distribution mismatch, deep neural network classifiers tend to give high confidence predictions on anomalous test examples . This can invalidate the use of prediction probabilities as calibrated confidence estimates (Guo et al., 2017), and makes detecting anomalous examples doubly important.
Several previous works seek to address these problems by giving deep neural network classifiers a means of assigning anomaly scores to inputs. These scores can then be used for detecting outof-distribution (OOD) examples (Hendrycks & Gimpel, 2017;Lee et al., 2018;Liu et al., 2018). These approaches have been demonstrated to work surprisingly well for complex input spaces, such as images, text, and speech. Moreover, they do not require modeling the full data distribution, but instead can use heuristics for detecting unmodeled phenomena. Several of these methods detect unmodeled phenomena by using representations from only in-distribution data.
In this paper, we investigate a complementary method where we train models to detect unmodeled data by learning cues for whether an input is unmodeled. While it is difficult to model the full data distribution, we can learn effective heuristics for detecting out-of-distribution inputs by exposing the model to OOD examples, thus learning a more conservative concept of the inliers and enabling the detection of novel forms of anomalies. We propose leveraging diverse, realistic datasets for this purpose, with a method we call Outlier Exposure (OE). OE provides a simple and effective way to consistently improve existing methods for OOD detection.
Through numerous experiments, we extensively evaluate the broad applicability of Outlier Exposure. For multiclass neural networks, we provide thorough results on Computer Vision and Natural Language Processing tasks which show that Outlier Exposure can help anomaly detectors generalize to and perform well on unseen distributions of outliers, even on large-scale images. We also demonstrate that Outlier Exposure provides gains over several existing approaches to out-of-distribution detection. Our results also show the flexibility of Outlier Exposure, as we can train various models with different sources of outlier distributions. Additionally, we establish that Outlier Exposure can make density estimates of OOD samples significantly more useful for OOD detection. Finally, we demonstrate that Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. Our code is made publicly available at https://github.com/hendrycks/outlier-exposure.

[RELATED WORK]
Out-of-Distribution Detection with Deep Networks. Hendrycks & Gimpel (2017) demonstrate that a deep, pre-trained classifier has a lower maximum softmax probability on anomalous examples than in-distribution examples, so a classifier can conveniently double as a consistently useful outof-distribution detector. Building on this work, DeVries & Taylor (2018) attach an auxiliary branch onto a pre-trained classifier and derive a new OOD score from this branch. Liang et al. (2018) present a method which can improve performance of OOD detectors that use a softmax distribution. In particular, they make the maximum softmax probability more discriminative between anomalies and in-distribution examples by pre-processing input data with adversarial perturbations (Goodfellow et al., 2015). Unlike in our work, their parameters are tailored to each source of anomalies. Lee et al. (2018) train a classifier concurrently with a GAN (Radford et al., 2016;Goodfellow et al., 2014), and the classifier is trained to have lower confidence on GAN samples. For each testing distribution of anomalies, they tune the classifier and GAN using samples from that out-distribution, as discussed in Appendix B of their work. Unlike Liang et al. (2018); Lee et al. (2018), in this work we train our method without tuning parameters to fit specific types of anomaly test distributions, so our results are not directly comparable with their results. Many other works (de Vries et al., 2016;Subramanya et al., 2017;Malinin & Gales, 2018;Bevandic et al., 2018) also encourage the model to have lower confidence on anomalous examples. Recently, Liu et al. (2018) provide theoretical guarantees for detecting out-of-distribution examples under the assumption that a suitably powerful anomaly detector is available.
Utilizing Auxiliary Datasets. Outlier Exposure uses an auxiliary dataset entirely disjoint from test-time data in order to teach the network better representations for anomaly detection. Goodfellow et al. (2015) train on adversarial examples to increased robustness. Salakhutdinov et al. (2011) pre-train unsupervised deep models on a database of web images for stronger features. Radford et al. (2017) train an unsupervised network on a corpus of Amazon reviews for a month in order to obtain quality sentiment representations. Zeiler & Fergus (2014) find that pre-training a network on the large ImageNet database (Russakovsky et al., 2015) endows the network with general representations that are useful in many fine-tuning applications. Chen & Gupta (2015); Mahajan et al. (2018) show that representations learned from images scraped from the nigh unlimited source of search engines and photo-sharing websites improve object detection performance.

[OUTLIER EXPOSURE]
We consider the task of deciding whether or not a sample is from a learned distribution called D in . Samples from D in are called ""in-distribution,"" and otherwise are said to be ""out-of-distribution"" (OOD) or samples from D out . In real applications, it may be difficult to know the distribution of outliers one will encounter in advance. Thus, we consider the realistic setting where D out is unknown. Given a parametrized OOD detector and an Outlier Exposure (OE) dataset D OE out , disjoint from D test out , we train the model to discover signals and learn heuristics to detect whether a query is sampled from D in or D OE out . We find that these heuristics generalize to unseen distributions D out . Deep parametrized anomaly detectors typically leverage learned representations from an auxiliary task, such as classification or density estimation. Given a model f and the original learning objective L, we can thus formalize Outlier Exposure as minimizing the objective
E (x,y)∼Din [L(f (x), y) + λE x ∼D OE out [L OE (f (x ), f (x), y)]
] over the parameters of f . In cases where labeled data is not available, then y can be ignored.
Outlier Exposure can be applied with many types of data and original tasks. Hence, the specific formulation of L OE is a design choice, and depends on the task at hand and the OOD detector used. For example, when using the maximum softmax probability baseline detector (Hendrycks & Gimpel, 2017), we set L OE to the cross-entropy from f (x ) to the uniform distribution (Lee et al., 2018). When the original objective L is density estimation and labels are not available, we set L OE to a margin ranking loss on the log probabilities f (x ) and f (x).

[EXPERIMENTS]
We evaluate OOD detectors with and without OE on a wide range of datasets. Each evaluation consists of an in-distribution dataset D in used to train an initial model, a dataset of anomalous examples D OE out , and a baseline detector to which we apply OE. We describe the datasets in Section 4.2. The OOD detectors and L OE losses are described on a case-by-case basis.
In the first experiment, we show that OE can help detectors generalize to new text and image anomalies. This is all accomplished without assuming access to the test distribution during training or tuning, unlike much previous work. In the confidence branch experiment, we show that OE is flexible and complements a binary anomaly detector. Then we demonstrate that using synthetic outliers does not work as well as using real and diverse data; previously it was assumed that we need synthetic data or carefully selected close-to-distribution data, but real and diverse data is enough. We conclude with experiments in density estimation. In these experiments we find that a cutting-edge density estimator unexpectedly assigns higher density to out-of-distribution samples than in-distribution samples, and we ameliorate this surprising behavior with Outlier Exposure.  We evaluate out-of-distribution detection methods on their ability to detect OOD points. For this purpose, we treat the OOD examples as the positive class, and we evaluate three metrics: area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPR), and the false positive rate at N % true positive rate (FPRN ). The AUROC and AUPR are holistic metrics that summarize the performance of a detection method across multiple thresholds. The AUROC can be thought of as the probability that an anomalous example is given a higher OOD score than a in-distribution example (Davis & Goadrich, 2006). Thus, a higher AUROC is better, and an uninformative detector has an AUROC of 50%. The AUPR is useful when anomalous examples are infrequent (Manning & Schütze, 1999), as it takes the base rate of anomalies into account. During evaluation with these metrics, the base rate of D test out to D test in test examples in all of our experiments is 1:5.
Whereas the previous two metrics represent the detection performance across various thresholds, the FPRN metric represents performance at one strict threshold. By observing performance at a strict threshold, we can make clear comparisons among strong detectors. The FPRN metric (Liu et al., 2018;Kumar et al., 2016;Balntas et al., 2016) is the probability that an in-distribution example (negative) raises a false alarm when N % of anomalous examples (positive) are detected, so a lower FPRN is better. Capturing nearly all anomalies with few false alarms can be of high practical value.

[IN-DISTRIBUTION DATASETS]
SVHN. The SVHN dataset (Netzer et al., 2011) contains 32 × 32 color images of house numbers. There are ten classes comprised of the digits 0-9. The training set has 604, 388 images, and the test set has 26, 032 images. For preprocessing, we rescale the pixels to be in the interval [0, 1]. CIFAR. The two CIFAR (Krizhevsky & Hinton, 2009) datasets contain 32 × 32 natural color images. CIFAR-10 has ten classes while CIFAR-100 has 100. CIFAR-10 and CIFAR-100 classes are disjoint but have similiarities. For example, CIFAR-10 has ""automobiles"" and ""trucks"" but not CIFAR-100's ""pickup truck"" class. Both have 50, 000 training images and 10, 000 test images. For this and the remaining image datasets, each image is standardized channel-wise. Tiny ImageNet. The Tiny ImageNet dataset (Johnson et al.) is a 200-class subset of the ImageNet (Russakovsky et al., 2015) dataset where images are resized and cropped to 64 × 64 resolution. The dataset's images were cropped using bounding box information so that cropped images contain the target, unlike Downsampled ImageNet (Chrabaszcz et al., 2017). The training set has 100, 000 images and the test set has 10, 000 images. Places365. The Places365 training dataset (Zhou et al., 2017)  For classification tasks on 20 Newsgroups, TREC, and SST, we treat each sentence of WikiText-2 as an individual example, and use simple filters to remove low-quality sentences.

[MULTICLASS CLASSIFICATION]
In what follows, we use Outlier Exposure to enhance the performance of existing OOD detection techniques with multiclass classification as the original task. Throughout the following experiments, we let x ∈ X be a classifier's input and y ∈ Y = {1, 2, . . . , k} be a class. We also represent the classifier with the function f : X → R k , such that for any x, 1 T f (x) = 1 and f (x) 0.

[MAXIMUM SOFTMAX PROBABILITY (MSP).]
Consider the maximum softmax probability baseline (Hendrycks & Gimpel, 2017) which gives an input x the OOD score − max c f c (x). Out-ofdistribution samples are drawn from various unseen distributions (Appendix A). For each task, we test with approximately twice the number of D test out distributions compared to most other papers, and we also test on NLP tasks. The quality of the OOD example scores are judged with the metrics described in Section 4.1. For this multiclass setting, we perform Outlier Exposure by fine-tuning a pre-trained classifier f so that its posterior is more uniform on D OE out samples. Specifically, the fine-
tuning objective is E (x,y)∼Din [− log f y (x)] + λE x∼D OE out [H(U; f (x))]
, where H is the cross entropy and U is the uniform distribution over k classes. When there is class imbalance, we could encourage f (x) to match (P (y = 1), . . . , P (y = k)); yet for the datasets we consider, matching U works well enough. Also, note that training from scratch with OE can result in even better performance than fine-tuning (Appendix C). This approach works on different architectures as well (Appendix D).
Unlike Liang et al. (2018); Lee et al. (2018) and like Hendrycks & Gimpel (2017); DeVries & Taylor (2018), we do not tune our hyperparameters for each D test out distribution, so that D test out is kept unknown like with real-world anomalies. Instead, the λ coefficients were determined early in experimentation with validation D val out distributions described in Appendix A. In particular, we use λ = 0.5 for vision experiments and λ = 1.0 for NLP experiments. Like previous OOD detection methods involving network fine-tuning, we chose λ so that impact on classification accuracy is negligible.
For nearly all of the vision experiments, we train Wide Residual Networks (Zagoruyko & Komodakis, 2016) and then fine-tune network copies with OE for 10 epochs. However we use a pretrained ResNet-18 for Places365. For NLP experiments, we train 2-layer GRUs (Cho et al., 2014) for 5 epochs, then fine-tune network copies with OE for 2 epochs. Networks trained on CIFAR-10 or CIFAR-100 are exposed to images from 80 Million Tiny Images, and the Tiny ImageNet and Places365 classifiers are exposed to ImageNet-22K. NLP classifiers are exposed to WikiText-2. Further architectural and training details are in Appendix B. For all tasks, OE improves average performance by a large margin. Averaged results are shown in Tables 1 and 2. Sample ROC curves are shown in Figures 1 and 4. Detailed results on individual D test out datasets are in Table 7 and Table (Hafner et al., 2018). A method with better success is from Lee et al. (2018). They carefully train a GAN to generate synthetic examples near the classifier's decision boundary. The classifier is encouraged to have a low maximum softmax probability on these synthetic examples. For CIFAR classifiers, they mention that a GAN can be a better source of anomalies than datasets such as SVHN. In contrast, we find that the simpler approach of drawing anomalies from a diverse dataset is sufficient for marked improvements in OOD detection.
We train a 40-4 Wide Residual Network using Lee et al. (2018)'s publicly available code, and use the network's maximum softmax probabilities as our baseline. Another classifier trains concurrently with a GAN so that the classifier assigns GAN-generated examples a high OOD score. We want each D test out to be novel. Consequently we use their code's default hyperparameters, and exactly one model encounters all tested D test out distributions. This is unlike their work since, for each D test out distribution, they train and tune a new network. We do not evaluate on Tiny ImageNet, Places365, nor text, since DCGANs cannot stably generate such images and text reliably. Lastly, we take the network trained in tandem with a GAN and fine-tune it with OE. Table 4 shows the large gains from using OE with a real and diverse dataset over using synthetic samples from a GAN.    (Nalisnick et al., 2019). Consequently, density estimates are another means by which to score anomalies (Zong et al., 2018). We show the ability of OE to improve density estimates on low-probability, outlying data.
PixelCNN++. Autoregressive neural density estimators provide a way to parametrize the probability density of image data. Although sampling from these architectures is slow, they allow for evaluating the probability density with a single forward pass through a CNN, making them promising candidates for OOD detection. We use Pix-elCNN++ (Salimans et al., 2017) as a baseline OOD detector, and we train it on CIFAR-10. The OOD score of example x is the bits per pixel (BPP), defined as nll(x)/num_pixels, where nll is the negative log-likelihood. With this loss we fine-tune for 2 epochs using OE, which we find is sufficient for the training loss to converge. Here OE is implemented with a margin loss over the log-likelihood difference between in-distribution and anomalous examples, so that the loss for a sample x in from D in and point x out from D OE out is max{0, num_pixels + nll(x in ) − nll(x out )}.
Results are shown in Table 5. Notice that PixelCNN++ without OE unexpectedly assigns lower BPP from SVHN images than CIFAR-10 images. For all D test out datasets, OE significantly improves results. Language Modeling. We next explore using OE on language models. We use QRNN (Merity et al., 2018a;b) language models as baseline OOD detectors. For the OOD score, we use bits per character (BPC) or bits per word (BPW), defined as nll(x)/sequence_length, where nll(x) is the negative log-likelihood of the sequence x. Outlier Exposure is implemented by adding the cross entropy to the uniform distribution on tokens from sequences in D OE out as an additional loss term. For D in , we convert the language-modeling version of Penn Treebank, split into sequences of length 70 for backpropagation for word-level models, and 150 for character-level models. We do not train or evaluate with preserved hidden states as in BPTT. This is because retaining hidden states would greatly simplify the task of OOD detection. Accordingly, the OOD detection task is to provide a score for 70-or 150-token sequences in the unseen D test out datasets. We train word-level models for 300 epochs, and character-level models for 50 epochs. We then fine-tune using OE on WikiText-2 for 5 epochs. For the character-level language model, we create a character-level version of WikiText-2 by converting words to lowercase and leaving out characters which do not appear in PTB. OOD detection results for the word-level and character-level language models are shown in 
FPR95 ↓ AUROC ↑ AUPR ↑ D in D

[DISCUSSION]
Extensions to Multilabel Classifiers and the Reject Option. Outlier Exposure can work in more classification regimes than just those considered above. For example, a multilabel classifier trained on CIFAR-10 obtains an 88.8% mean AUROC when using the maximum prediction probability as the OOD score. By training with OE to decrease the classifier's output probabilities on OOD samples, the mean AUROC increases to 97.1%. This is slightly less than the AUROC for a multiclass model tuned with OE. An alternative OOD detection formulation is to give classifiers a ""reject class"" (Bartlett & Wegkamp, 2008). Outlier Exposure is also flexible enough to improve performance in this setting, but we find that even with OE, classifiers with the reject option or multilabel outputs are not as competitive as OOD detectors with multiclass outputs.
Flexibility in Choosing D OE out . Early in experimentation, we found that the choice of D OE out is important for generalization to unseen D test out distributions. For example, adding Gaussian noise to samples from D in to create D OE out does not teach the network to generalize to unseen anomaly distributions for complex D in . Similarly, we found in Section 4.3 that synthetic anomalies do not work as well as real data for D OE out . In contrast, our experiments demonstrate that the large datasets of realistic anomalies described in Section 4.2.2 do generalize to unseen D test out distributions. In addition to size and realism, we found diversity of D OE out to be an important factor. Concretely, a CIFAR-100 classifier with CIFAR-10 as D OE out hardly improves over the baseline. A CIFAR-10 classifier exposed to ten CIFAR-100 outlier classes corresponds to an average AUPR of 78.5%. Exposed to 30 such classes, the classifier's average AUPR becomes 85.1%. Next, 50 classes corresponds to 85.3%, and from thereon additional CIFAR-100 classes barely improve performance. This suggests that dataset diversity is important, not just size. In fact, experiments in this paper often used around 1% of the images in the 80 Million Tiny Images dataset since we only briefly fine-tuned the models. We also found that using only 50,000 examples from this dataset led to a negligible degradation in detection performance. Additionally, D OE out datasets with significantly different statistics can perform similarly. For instance, using the Project Gutenberg dataset in lieu of WikiText-2 for D OE out in the SST experiments gives an average AUROC of 90.1% instead of 89.3%.

[CLOSENESS OF D TEST]
out , D OE out , and D test in . Our experiments show several interesting effects of the closeness of the datasets involved. Firstly, we find that D test out and D OE out need not be close for training with OE to improve performance on D test out . In Appendix A, we observe that an OOD detector for SVHN has its performance improve with Outlier Exposure even though (1) D OE out samples are images of natural scenes rather than digits, and (2) D test out includes unnatural examples such as emojis. We observed the same in our preliminary experiments with MNIST; using 80 Million Tiny Images as D OE out , OE increased the AUPR from 94.2% to 97.0%.
Secondly, we find that the closeness of D OE out to D test in can be an important factor in the success of OE. In the NLP experiments, preprocessing D OE out to be closer to D in improves OOD detection performance significantly. Without preprocessing, the network may discover easy-to-learn cues which reveal whether the input is in-or out-of-distribution, so the OE training objective can be optimized in unintended ways. That results in weaker detectors. In a separate experiment, we use Online Hard Example Mining so that difficult outliers have more weight in Outlier Exposure. Although this improves performance on the hardest anomalies, anomalies without plausible local statistics like noise are detected slightly less effectively than before. Thus hard or close-to-distribution examples do not necessarily teach the detector all valuable heuristics for detecting various forms of anomalies. Real-world applications of OE could use the method of Sun et al. (2018)   OE Improves Calibration. When using classifiers for prediction, it is important that confidence estimates given for the predictions do not misrepresent empirical performance. A calibrated classifier gives confidence probabilities that match the empirical frequency of correctness. That is, if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires.
Existing confidence calibration approaches consider the standard setting where data at test-time is always drawn from D in . We extend this setting to include examples from D test out at test-time since systems should provide calibrated probabilities on both in-and out-of-distribution samples. The classifier should have low-confidence predictions on these OOD examples, since they do not have a class. Building on the temperature tuning method of Guo et al. (2017), we demonstrate that OE can improve calibration performance in this realistic setting. Summary results are shown in Figure 3. Detailed results and a description of the metrics are in Appendix G.

[CONCLUSION]
In this paper, we proposed Outlier Exposure, a simple technique that enhances many current OOD detectors across various settings. It uses out-of-distribution samples to teach a network heuristics to detect new, unmodeled, out-of-distribution examples. We showed that this method is broadly applicable in vision and natural language settings, even for large-scale image tasks. OE can improve model calibration and several previous anomaly detection techniques. Further, OE can teach density estimation models to assign more plausible densities to out-of-distribution samples. Finally, Outlier Exposure is computationally inexpensive, and it can be applied with low overhead to existing systems. In summary, Outlier Exposure is an effective and complementary approach for enhancing out-of-distribution detection systems.

[A EXPANDED MULTICLASS RESULTS]
Expanded mutliclass out-of-distribution detection results are in Table 7 and Table 8   Anomalous Data. For each in-distribution dataset D in , we comprehensively evaluate OOD detectors on artificial and real anomalous distributions D test out following Hendrycks & Gimpel (2017). For each learned distribution D in , the number of test distributions that we compare against is approximately double that of most previous works.
Gaussian anomalies have each dimension i.i.d. sampled from an isotropic Gaussian distribution. Rademacher anomalies are images where each dimension is −1 or 1 with equal probability, so each dimension is sampled from a symmetric Rademacher distribution. Bernoulli images have each pixel sampled from a Bernoulli distribution if the input range is [0, 1]. Blobs data consist in algorithmically generated amorphous shapes with definite edges. Icons-50 is a dataset of icons and emojis (Hendrycks & Dietterich, 2019); icons from the ""Number"" class are removed. Textures is a dataset of describable textural images (Cimpoi et al., 2014). Places365 consists in images for scene recognition rather than object recognition (Zhou et al., 2017). LSUN is another scene understanding dataset with fewer classes than Places365 (Yu et al., 2015). ImageNet anomalous examples are taken from the 800 ImageNet-1K classes disjoint from Tiny ImageNet's 200 classes, and when possible each image is cropped with bounding box information as in Tiny ImageNet. For the Places365 experiment, ImageNet is ImageNet-1K with all 1000 classes. With CIFAR-10 as D in , we use also CIFAR-100 as D test out and vice versa; recall that the CIFAR-10 and CIFAR-100 classes do not overlap. Chars74K is a dataset of photographed characters in various styles; digits and letters such as ""O"" and ""l"" were removed since they can look like numbers. Places69 has images from 69 scene categories not found in the Places365 dataset.
SNLI is a dataset of predicates and hypotheses for natural language inference. We use the hypotheses for D OE out . IMDB is a sentiment classification dataset of movie reviews, with similar statistics to those of SST. Multi30K is a dataset of English-German image descriptions, of which we use the English descriptions. WMT16 is the English portion of the test set from WMT16. Yelp is a dataset of restaurant reviews. English Web Treebank (EWT) consists of five individual datasets: Answers (A), Email (E), Newsgroups (N), Reviews (R), and Weblog (W). Each contains examples from the indicated domain.
Validation Data. For each experiment, we create a set of validation distributions D val out . The first anomalies are uniform noise anomalies where each pixel is sampled from U[0, 1] or U[−1, 1] depending on the input space of the classifier. The remaining D val out validation sources are generated by corrupting in-distribution data, so that the data becomes out-of-distribution. One such source of anomalies is created by taking the pixelwise arithmetic mean of a random pair of in-distribution images. Other anomalies are created by taking the geometric mean of a random pair of in-distribution images. Jigsaw anomalies are created by taking an in-distribution example, partitioning the image into 16 equally sized patches, and permuting those patches. Speckle Noised anomalies are created by applying speckle noise to in-distribution images. RGB Ghosted anomalies involves shifting and reordering the color channels of in-distribution images. Inverted images are anomalies which have some or all of their color channels inverted.

[B ARCHITECTURES AND TRAINING DETAILS]
For CIFAR-10, CIFAR-100, and Tiny ImageNet classification experiments, we use a 40-2 Wide Residual Network (Zagoruyko & Komodakis, 2016). The network trains for 100 epochs with a dropout rate of 0.3. The initial learning rate of 0.1 decays following a cosine learning rate schedule . During fine-tuning of the entire network, we again use a cosine learning rate schedule but with an initial learning rate of 0.001. We use standard flipping and data cropping augmentation, Nesterov momentum, and 2 weight decay with a coefficient of 5 × 10 −4 . SVHN architectures are 16-4 Wide ResNets trained for 20 epochs with an initial learning rate of 0.01 and no data augmentation. For Places365, we use a ResNet-18 pre-trained on Places365. In this Places365 experiment, we tune with Outlier Exposure for 5 epochs, use 512 outlier samples per iteration, and start with a learning rate of 0.0001. Outlier Exposure fine-tuning occurs with each epoch being the length of in-distribution dataset epoch, so that Outlier Exposure completes quickly and does involve reading the entire D OE out dataset. 

[D OE WORKS ON OTHER VISION ARCHITECTURES]
Outlier Exposure also improves vision OOD detection performance for more than just Wide ResNets. Table 9 shows that Outlier Exposure also improves vision OOD detection performance for ""All Convolutional Networks"" (Salimans & Kingma, 2016 

[G CONFIDENCE CALIBRATION]
Models integrated into a decision making process should indicate when they are trustworthy, and such models should not have inordinate confidence in their predictions. In an effort to combat a false sense of certainty from overconfident models, we aim to calibrate model confidence. A model is calibrated if its predicted probabilities match empirical frequencies. Thus if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires. Prior research (Guo et al., 2017;Nguyen & O'Connor, 2015;Kuleshov & Liang, 2015) considers calibrating systems where test-time queries are samples from D in , but systems also encounter samples from D test out and should also ascribe low confidence to these samples. Hence, we use OE to control the confidence on these samples.

[G.1 METRICS]
In order to evaluate a multiclass classifier's calibration, we present three metrics. First we establish context. For input example X ∈ X , let Y ∈ Y = {1, 2, . . . , k} be the ground truth class. Let Y be the model's class prediction, and let C be the corresponding model confidence or prediction probability. Denote the set of prediction-label pairs made by the model with S = {( y 1 , c 1 ), ( y 2 , c 2 ), . . . , ( y n , c n )}. 

[RMS AND MAD CALIBRATION ERROR]
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k 2 .
Along similar lines, the MAD Calibration Error-which is an improper scoring rule due to its use of absolute differences rather than squared differences-is estimated with
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k .
Soft F1 Score. If a classifier makes only a few mistakes, then most examples should have high confidence. But if the classifier gives all predictions high confidence, including its mistakes, then the previous metrics will indicate that the model is calibrated on the vast majority of instances, despite having systematic miscalibration. The Soft F1 score (Pastor-Pellicer et al., 2013;Hendrycks & Gimpel, 2017) is suited for measuring the calibration of a system where there is an acute imbalance between mistaken and correct decisions. Since we treat mistakes a positive examples, we can write the model's confidence that the examples are anomalous with c a = (1 − c 1 , 1 − c 2 , . . . , 1 − c n ). To indicate that an example is positive (mistaken), we use the vector m ∈ {0, 1} n such that m i = 1(y i = y i ) for 1 ≤ i ≤ n. Then the Soft F1 score is c T a m 1 T (c a + m)/2 . 

[G.2 SETUP AND RESULTS]
There are many ways to estimate a classifier's confidence. One way is to bind a logistic regression branch onto the network, so that confidence values are in [0, 1]. Other confidence estimates use the model's logits l ∈ R k , such as the estimate σ(max i l i ) ∈ [0, 1], where σ is the logistic sigmoid.
Another common confidence estimate is max i exp (l i )/ k j=1 exp (l j ) . A modification of this estimate is our baseline. Softmax Temperature Tuning. Guo et al. (2017) show that good calibration can be obtained by including a tuned temperature parameter into the softmax: p(y = i | x) = exp(l i /T )/ k j=1 exp(l j /T ). We tune T to maximize log likelihood on a validation set after the network has been trained on the training set.
Results. In this calibration experiment, the baseline is confidence estimation with softmax temperature tuning. Therefore, we train SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet classifiers with 5000, 5000, 5000, and 10000 training examples held out, respectively. A copy of this classifier is fine-tuned with Outlier Exposure. Then we determine the optimal temperatures of the original and OE-fine-tuned classifiers on the held-out examples. To measure calibration, we take equally many examples from a given in-distribution dataset D test in and OOD dataset D test out . Out-of-distribution points are understood to be incorrectly classified since their label is not in the model's output space, so calibrated models should assign these out-of-distribution points low confidence. Results are in Table 12. Outlier Exposure noticeably improves model calibration.

[G.3 POSTERIOR RESCALING]
While temperature tuning improves calibration, the confidence estimate p(y = i | x) cannot be less than 1/k, k the number of classes. For an out-of-distribution example like Gaussian Noise, a good model should have no confidence in its prediction over k classes. One possibility is to add a reject option, or a (k + 1)st class, which we cover in Section 5. A simpler option we found is to perform an affine transformation of p(y = i | x) ∈ [1/k, 1] with the formula ( p(y = i | x) − 1/k)/(1 − 1/k) ∈ [0, 1]. This simple transformation makes it possible for a network to express no confidence on an out-of-distribution input and improves calibration performance. As Table 13 shows, this simple 0-1 posterior rescaling technique consistently improves calibration, and the model fine-tuned with OE using temperature tuning and posterior rescaling achieved large calibration improvements.   In Figure 4, we show additional PR and ROC Curves using the Tiny ImageNet dataset and various anomalous distributions.

[ACKNOWLEDGMENTS]
We thank NVIDIA for donating GPUs used in this research. This research was supported by a grant from the Future of Life Institute.","[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance."
Deep Anomaly Detection with Outlier Exposure,HyxCxhRcY7.json,"This paper describes how a deep neural network can be fine-tuned to perform outlier detection in addition to its primary objective. For classification, the fine-tuning objective encourages out-of-distribution samples to have a uniform distribution over all class labels. For density estimation, the objective encourages out-of-distribution samples to be ranked as less probability than in-distribution samples. On a variety of image and text datasets, this additional fine-tuning step results in a network that does much better at outlier detection than a naive baseline, sometimes approaching perfect AUROC.

The biggest weakness in this paper is the assumption that we have access to out-of-distribution data, and that we will encounter data from that same distribution in the future. For the typical anomaly detection setting, we expect that anomalies could look like almost anything. For example, in network intrusion detection (a common application of anomaly detection), future attacks are likely to have different characteristics than past attacks, but will still look unusual in some way. The challenge is to define ""normal"" behavior in a way that captures the full range of normal while excluding ""unusual"" examples. This topic has been studied for decades.

Thus, I would not classify this paper as an anomaly detection paper. Instead, it's defining a new task and evaluating performance on that task. The empirical results demonstrate that the optimization succeeds in optimizing the objective it was given. What's missing is the justification for this problem setting -- when is it the case that we need to detect outliers *and* have access to the distribution over outliers?

--------

UPDATE AFTER RESPONSE PERIOD:

My initial read of this paper was incorrect -- the authors do indeed separate the outlier distribution used to train the detector from the outlier distribution used for evaluation. Much of these details are in Appendix A; I suggest that the authors move some of this earlier or more heavily reference Appendix A when describing the methods and introducing the results. I am not well-read in the other work in this area, but this looks like a nice advance.

Based on my read of the related work section (again, having not studied the other papers), it looks like this work fills a slightly different niche from some previous work. In particular, OE is unlikely to be adversarially robust. So this might be a poor choice for finding anomalies that represent malicious behavior (e.g., network intrusion detection, adversarial examples, etc.), but good for finding natural examples from a different distribution (e.g., data entry errors).

My main remaining reservation is that this work is still at the stage of empirical observation -- I hope that future work (by these authors or others) can investigate the assumptions necessary for this method to work, and even characterize how well we should expect it to work. Without a framework for understanding generalization in this context, we may see a proliferation of heuristics that succeed on benchmarks without developing the underlying principles.","[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: ROC curve with Tiny Im-ageNet (D in ) and Textures (D test out ).
Table 2: Figure 2 :2Figure 2: OOD scores from PixelCNN++ on images from CIFAR-10 and SVHN.
Table 3: to refine a scraped D OE out auxiliary dataset to be appropriately close to D test in .SV HN CI FA R-10 CI FA R
Table 4: Figure 3 :3Figure 3: Root Mean Square Calibration Error values with temperature tuning and temperature tuning + OE across various datasets.
Table 5: .The Root Mean Square Calibration Error measures the square root of the expected squared difference between confidence and accuracy at a confidence level. It has the formula E C [(P(Y = Y |C = c) − c) 2 ] . A similar formulation which less severely penalizes large confidence-accuracy deviations is the Mean Absolute Value Calibration error, written E C [|P(Y = Y |C = c) − c|]. The MAD Calibration Error is a lower bound of the RMS Calibration Error. To empirically estimate these miscalibration measures, we partition the n samples of S into b bins {B 1 , B 2 , . . . , B b } with approximately 100 samples in each bin. UnlikeGuo et al. (2017), bins are not equally spaced since the distribution of confidence values is not uniform but dynamic. Concretely, the RMS Calibration Error is estimated with the numerically stable formula
Table 6: Figure 4 :4Figure 4: ROC curves with Tiny ImageNet as D in and Textures, Places365, LSUN, and ImageNet as D test out . Figures show the curves corresponding to the maximum softmax probability (MSP) baseline detector and the MSP detector with Outlier Exposure (OE).
Table 7: We use the ImageNet dataset with images from approximately 22 thousand classes as D OE WikiText-2 is a corpus of Wikipedia articles typically used for language modeling. We use WikiText-2 as D OE
Table 8: 8 in Appendix A. Notice that the SVHN classifier with OE can be used to detect new anomalies such as emojis and street view alphabet letters, even though D test OE is a dataset of natural images. Thus, Outlier Exposure helps models to generalize to unseen D test out distributions far better than the baseline. Comparisons between the MSP baseline and the MSP of the natural language classifier fine-tuned with OE. Results are percentages and averaged over 10 runs.Confidence Branch. A recently proposed OOD detection technique(DeVries & Taylor, 2018) involves appending an OOD scoring branch b : X → [0, 1] onto a deep network. Trained with samples from only D in , this branch estimates the network's confidence on any input. The creators of this technique made their code publicly available, so we use their code to train new 40-4 Wide Residual Network classifiers. We fine-tune the confidence branch with Outlier Exposure by adding 0.5E x∼D OE out[log b(x)] to the network's original optimization objective. In Table3, the baseline values are derived from the maximum softmax probabilities produced by the classifier trained with DeVries & Taylor (2018)'s publicly available training code. The confidence branch improves over this MSP detector, and after OE, the confidence branch detects anomalies more effectively. Comparison among the maximum softmax probability, Confidence Branch, and Confidence Branch + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages, and averaged across all D test out datasets.
Table 9: Comparison among the maximum softmax probability (MSP), MSP + GAN, and MSP + GAN + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages and averaged across all D test out datasets.
Table 10: Table 5: OOD detection results with a PixelCNN++ density estimator, and the same estimator after applying OE. The model's bits per pixel (BPP) scores each sample. All results are percentages. Test distributions D test out are described in Appendix A.
Table 11: ; expanded results and D test out descriptions are in Appendix F. In all cases, OE improves over the baseline, and the improvement is especially large for the word-level model.
Table 12: 
Table 13: NLP OOD example detection for the maximum softmax probability (MSP) baseline detector and the MSP detector after fine-tuning with Outlier Exposure (OE). All results are percentages and the result of 10 runs. Values are rounded so that 99.95% rounds to 100%.
Table 14: C TRAINING FROM SCRATCH WITH OUTLIER EXPOSURE USUALLY IMPROVES DETECTION PERFORMANCE Elsewhere we show results for pre-trained networks that are fine-tuned with OE. However, a network trained from scratch which simultaneously trains with OE tends to give superior results. For example, a CIFAR-10 Wide ResNet trained normally obtains a classification error rate of 5.16% and an FPR95 of 34.94%. Fine-tuned, this network has an error rate of 5.27% and an FPR95 of 9.50%. Yet if we instead train the network from scratch and expose it to outliers as it trains, then the error rate is 4.26% and the FPR95 is 6.15%. This architecture corresponds to a 9.50% RMS calibration error with OE fine-tuning, but by training with OE from scratch the RMS calibration error is 6.15%. Compared to fine-tuning, training a network in tandem with OE tends to produce a network with a better error rate, calibration, and OOD detection performance. The reason why we use OE for fine-tuning is because training from scratch requires more time and sometimes more GPU memory than fine-tuning.
Table 15: 
Table 16: OOD detection results on Penn Treebank examples and English Web Treebank outliers. All results are percentages.The D test out datasets come from the English Web Treebank(Bies et al., 2012), which contains text from five different domains: Yahoo! Answers, emails, newsgroups, product reviews, and weblogs. Other NLP D test out datasets we consider do not satisfy the language modeling assumption of continuity in the examples, so we do not evaluate on them.
Table 17: Table 12: Calibration results for the temperature tuned baseline and temperature tuning + OE.
Table 18: Calibration results for the softmax temperature tuning baseline, the same baseline after adding Posterior Rescaling, and temperature tuning + Posterior Rescaling + OE.

[INTRODUCTION]
Machine Learning systems in deployment often encounter data that is unlike the model's training data. This can occur in discovering novel astronomical phenomena, finding unknown diseases, or detecting sensor failure. In these situations, models that can detect anomalies (Liu et al., 2018;Emmott et al., 2013) are capable of correctly flagging unusual examples for human intervention, or carefully proceeding with a more conservative fallback policy.
Behind many machine learning systems are deep learning models (Krizhevsky et al., 2012) which can provide high performance in a variety of applications, so long as the data seen at test time is similar to the training data. However, when there is a distribution mismatch, deep neural network classifiers tend to give high confidence predictions on anomalous test examples . This can invalidate the use of prediction probabilities as calibrated confidence estimates (Guo et al., 2017), and makes detecting anomalous examples doubly important.
Several previous works seek to address these problems by giving deep neural network classifiers a means of assigning anomaly scores to inputs. These scores can then be used for detecting outof-distribution (OOD) examples (Hendrycks & Gimpel, 2017;Lee et al., 2018;Liu et al., 2018). These approaches have been demonstrated to work surprisingly well for complex input spaces, such as images, text, and speech. Moreover, they do not require modeling the full data distribution, but instead can use heuristics for detecting unmodeled phenomena. Several of these methods detect unmodeled phenomena by using representations from only in-distribution data.
In this paper, we investigate a complementary method where we train models to detect unmodeled data by learning cues for whether an input is unmodeled. While it is difficult to model the full data distribution, we can learn effective heuristics for detecting out-of-distribution inputs by exposing the model to OOD examples, thus learning a more conservative concept of the inliers and enabling the detection of novel forms of anomalies. We propose leveraging diverse, realistic datasets for this purpose, with a method we call Outlier Exposure (OE). OE provides a simple and effective way to consistently improve existing methods for OOD detection.
Through numerous experiments, we extensively evaluate the broad applicability of Outlier Exposure. For multiclass neural networks, we provide thorough results on Computer Vision and Natural Language Processing tasks which show that Outlier Exposure can help anomaly detectors generalize to and perform well on unseen distributions of outliers, even on large-scale images. We also demonstrate that Outlier Exposure provides gains over several existing approaches to out-of-distribution detection. Our results also show the flexibility of Outlier Exposure, as we can train various models with different sources of outlier distributions. Additionally, we establish that Outlier Exposure can make density estimates of OOD samples significantly more useful for OOD detection. Finally, we demonstrate that Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. Our code is made publicly available at https://github.com/hendrycks/outlier-exposure.

[RELATED WORK]
Out-of-Distribution Detection with Deep Networks. Hendrycks & Gimpel (2017) demonstrate that a deep, pre-trained classifier has a lower maximum softmax probability on anomalous examples than in-distribution examples, so a classifier can conveniently double as a consistently useful outof-distribution detector. Building on this work, DeVries & Taylor (2018) attach an auxiliary branch onto a pre-trained classifier and derive a new OOD score from this branch. Liang et al. (2018) present a method which can improve performance of OOD detectors that use a softmax distribution. In particular, they make the maximum softmax probability more discriminative between anomalies and in-distribution examples by pre-processing input data with adversarial perturbations (Goodfellow et al., 2015). Unlike in our work, their parameters are tailored to each source of anomalies. Lee et al. (2018) train a classifier concurrently with a GAN (Radford et al., 2016;Goodfellow et al., 2014), and the classifier is trained to have lower confidence on GAN samples. For each testing distribution of anomalies, they tune the classifier and GAN using samples from that out-distribution, as discussed in Appendix B of their work. Unlike Liang et al. (2018); Lee et al. (2018), in this work we train our method without tuning parameters to fit specific types of anomaly test distributions, so our results are not directly comparable with their results. Many other works (de Vries et al., 2016;Subramanya et al., 2017;Malinin & Gales, 2018;Bevandic et al., 2018) also encourage the model to have lower confidence on anomalous examples. Recently, Liu et al. (2018) provide theoretical guarantees for detecting out-of-distribution examples under the assumption that a suitably powerful anomaly detector is available.
Utilizing Auxiliary Datasets. Outlier Exposure uses an auxiliary dataset entirely disjoint from test-time data in order to teach the network better representations for anomaly detection. Goodfellow et al. (2015) train on adversarial examples to increased robustness. Salakhutdinov et al. (2011) pre-train unsupervised deep models on a database of web images for stronger features. Radford et al. (2017) train an unsupervised network on a corpus of Amazon reviews for a month in order to obtain quality sentiment representations. Zeiler & Fergus (2014) find that pre-training a network on the large ImageNet database (Russakovsky et al., 2015) endows the network with general representations that are useful in many fine-tuning applications. Chen & Gupta (2015); Mahajan et al. (2018) show that representations learned from images scraped from the nigh unlimited source of search engines and photo-sharing websites improve object detection performance.

[OUTLIER EXPOSURE]
We consider the task of deciding whether or not a sample is from a learned distribution called D in . Samples from D in are called ""in-distribution,"" and otherwise are said to be ""out-of-distribution"" (OOD) or samples from D out . In real applications, it may be difficult to know the distribution of outliers one will encounter in advance. Thus, we consider the realistic setting where D out is unknown. Given a parametrized OOD detector and an Outlier Exposure (OE) dataset D OE out , disjoint from D test out , we train the model to discover signals and learn heuristics to detect whether a query is sampled from D in or D OE out . We find that these heuristics generalize to unseen distributions D out . Deep parametrized anomaly detectors typically leverage learned representations from an auxiliary task, such as classification or density estimation. Given a model f and the original learning objective L, we can thus formalize Outlier Exposure as minimizing the objective
E (x,y)∼Din [L(f (x), y) + λE x ∼D OE out [L OE (f (x ), f (x), y)]
] over the parameters of f . In cases where labeled data is not available, then y can be ignored.
Outlier Exposure can be applied with many types of data and original tasks. Hence, the specific formulation of L OE is a design choice, and depends on the task at hand and the OOD detector used. For example, when using the maximum softmax probability baseline detector (Hendrycks & Gimpel, 2017), we set L OE to the cross-entropy from f (x ) to the uniform distribution (Lee et al., 2018). When the original objective L is density estimation and labels are not available, we set L OE to a margin ranking loss on the log probabilities f (x ) and f (x).

[EXPERIMENTS]
We evaluate OOD detectors with and without OE on a wide range of datasets. Each evaluation consists of an in-distribution dataset D in used to train an initial model, a dataset of anomalous examples D OE out , and a baseline detector to which we apply OE. We describe the datasets in Section 4.2. The OOD detectors and L OE losses are described on a case-by-case basis.
In the first experiment, we show that OE can help detectors generalize to new text and image anomalies. This is all accomplished without assuming access to the test distribution during training or tuning, unlike much previous work. In the confidence branch experiment, we show that OE is flexible and complements a binary anomaly detector. Then we demonstrate that using synthetic outliers does not work as well as using real and diverse data; previously it was assumed that we need synthetic data or carefully selected close-to-distribution data, but real and diverse data is enough. We conclude with experiments in density estimation. In these experiments we find that a cutting-edge density estimator unexpectedly assigns higher density to out-of-distribution samples than in-distribution samples, and we ameliorate this surprising behavior with Outlier Exposure.  We evaluate out-of-distribution detection methods on their ability to detect OOD points. For this purpose, we treat the OOD examples as the positive class, and we evaluate three metrics: area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPR), and the false positive rate at N % true positive rate (FPRN ). The AUROC and AUPR are holistic metrics that summarize the performance of a detection method across multiple thresholds. The AUROC can be thought of as the probability that an anomalous example is given a higher OOD score than a in-distribution example (Davis & Goadrich, 2006). Thus, a higher AUROC is better, and an uninformative detector has an AUROC of 50%. The AUPR is useful when anomalous examples are infrequent (Manning & Schütze, 1999), as it takes the base rate of anomalies into account. During evaluation with these metrics, the base rate of D test out to D test in test examples in all of our experiments is 1:5.
Whereas the previous two metrics represent the detection performance across various thresholds, the FPRN metric represents performance at one strict threshold. By observing performance at a strict threshold, we can make clear comparisons among strong detectors. The FPRN metric (Liu et al., 2018;Kumar et al., 2016;Balntas et al., 2016) is the probability that an in-distribution example (negative) raises a false alarm when N % of anomalous examples (positive) are detected, so a lower FPRN is better. Capturing nearly all anomalies with few false alarms can be of high practical value.

[IN-DISTRIBUTION DATASETS]
SVHN. The SVHN dataset (Netzer et al., 2011) contains 32 × 32 color images of house numbers. There are ten classes comprised of the digits 0-9. The training set has 604, 388 images, and the test set has 26, 032 images. For preprocessing, we rescale the pixels to be in the interval [0, 1]. CIFAR. The two CIFAR (Krizhevsky & Hinton, 2009) datasets contain 32 × 32 natural color images. CIFAR-10 has ten classes while CIFAR-100 has 100. CIFAR-10 and CIFAR-100 classes are disjoint but have similiarities. For example, CIFAR-10 has ""automobiles"" and ""trucks"" but not CIFAR-100's ""pickup truck"" class. Both have 50, 000 training images and 10, 000 test images. For this and the remaining image datasets, each image is standardized channel-wise. Tiny ImageNet. The Tiny ImageNet dataset (Johnson et al.) is a 200-class subset of the ImageNet (Russakovsky et al., 2015) dataset where images are resized and cropped to 64 × 64 resolution. The dataset's images were cropped using bounding box information so that cropped images contain the target, unlike Downsampled ImageNet (Chrabaszcz et al., 2017). The training set has 100, 000 images and the test set has 10, 000 images. Places365. The Places365 training dataset (Zhou et al., 2017)  For classification tasks on 20 Newsgroups, TREC, and SST, we treat each sentence of WikiText-2 as an individual example, and use simple filters to remove low-quality sentences.

[MULTICLASS CLASSIFICATION]
In what follows, we use Outlier Exposure to enhance the performance of existing OOD detection techniques with multiclass classification as the original task. Throughout the following experiments, we let x ∈ X be a classifier's input and y ∈ Y = {1, 2, . . . , k} be a class. We also represent the classifier with the function f : X → R k , such that for any x, 1 T f (x) = 1 and f (x) 0.

[MAXIMUM SOFTMAX PROBABILITY (MSP).]
Consider the maximum softmax probability baseline (Hendrycks & Gimpel, 2017) which gives an input x the OOD score − max c f c (x). Out-ofdistribution samples are drawn from various unseen distributions (Appendix A). For each task, we test with approximately twice the number of D test out distributions compared to most other papers, and we also test on NLP tasks. The quality of the OOD example scores are judged with the metrics described in Section 4.1. For this multiclass setting, we perform Outlier Exposure by fine-tuning a pre-trained classifier f so that its posterior is more uniform on D OE out samples. Specifically, the fine-
tuning objective is E (x,y)∼Din [− log f y (x)] + λE x∼D OE out [H(U; f (x))]
, where H is the cross entropy and U is the uniform distribution over k classes. When there is class imbalance, we could encourage f (x) to match (P (y = 1), . . . , P (y = k)); yet for the datasets we consider, matching U works well enough. Also, note that training from scratch with OE can result in even better performance than fine-tuning (Appendix C). This approach works on different architectures as well (Appendix D).
Unlike Liang et al. (2018); Lee et al. (2018) and like Hendrycks & Gimpel (2017); DeVries & Taylor (2018), we do not tune our hyperparameters for each D test out distribution, so that D test out is kept unknown like with real-world anomalies. Instead, the λ coefficients were determined early in experimentation with validation D val out distributions described in Appendix A. In particular, we use λ = 0.5 for vision experiments and λ = 1.0 for NLP experiments. Like previous OOD detection methods involving network fine-tuning, we chose λ so that impact on classification accuracy is negligible.
For nearly all of the vision experiments, we train Wide Residual Networks (Zagoruyko & Komodakis, 2016) and then fine-tune network copies with OE for 10 epochs. However we use a pretrained ResNet-18 for Places365. For NLP experiments, we train 2-layer GRUs (Cho et al., 2014) for 5 epochs, then fine-tune network copies with OE for 2 epochs. Networks trained on CIFAR-10 or CIFAR-100 are exposed to images from 80 Million Tiny Images, and the Tiny ImageNet and Places365 classifiers are exposed to ImageNet-22K. NLP classifiers are exposed to WikiText-2. Further architectural and training details are in Appendix B. For all tasks, OE improves average performance by a large margin. Averaged results are shown in Tables 1 and 2. Sample ROC curves are shown in Figures 1 and 4. Detailed results on individual D test out datasets are in Table 7 and Table (Hafner et al., 2018). A method with better success is from Lee et al. (2018). They carefully train a GAN to generate synthetic examples near the classifier's decision boundary. The classifier is encouraged to have a low maximum softmax probability on these synthetic examples. For CIFAR classifiers, they mention that a GAN can be a better source of anomalies than datasets such as SVHN. In contrast, we find that the simpler approach of drawing anomalies from a diverse dataset is sufficient for marked improvements in OOD detection.
We train a 40-4 Wide Residual Network using Lee et al. (2018)'s publicly available code, and use the network's maximum softmax probabilities as our baseline. Another classifier trains concurrently with a GAN so that the classifier assigns GAN-generated examples a high OOD score. We want each D test out to be novel. Consequently we use their code's default hyperparameters, and exactly one model encounters all tested D test out distributions. This is unlike their work since, for each D test out distribution, they train and tune a new network. We do not evaluate on Tiny ImageNet, Places365, nor text, since DCGANs cannot stably generate such images and text reliably. Lastly, we take the network trained in tandem with a GAN and fine-tune it with OE. Table 4 shows the large gains from using OE with a real and diverse dataset over using synthetic samples from a GAN.    (Nalisnick et al., 2019). Consequently, density estimates are another means by which to score anomalies (Zong et al., 2018). We show the ability of OE to improve density estimates on low-probability, outlying data.
PixelCNN++. Autoregressive neural density estimators provide a way to parametrize the probability density of image data. Although sampling from these architectures is slow, they allow for evaluating the probability density with a single forward pass through a CNN, making them promising candidates for OOD detection. We use Pix-elCNN++ (Salimans et al., 2017) as a baseline OOD detector, and we train it on CIFAR-10. The OOD score of example x is the bits per pixel (BPP), defined as nll(x)/num_pixels, where nll is the negative log-likelihood. With this loss we fine-tune for 2 epochs using OE, which we find is sufficient for the training loss to converge. Here OE is implemented with a margin loss over the log-likelihood difference between in-distribution and anomalous examples, so that the loss for a sample x in from D in and point x out from D OE out is max{0, num_pixels + nll(x in ) − nll(x out )}.
Results are shown in Table 5. Notice that PixelCNN++ without OE unexpectedly assigns lower BPP from SVHN images than CIFAR-10 images. For all D test out datasets, OE significantly improves results. Language Modeling. We next explore using OE on language models. We use QRNN (Merity et al., 2018a;b) language models as baseline OOD detectors. For the OOD score, we use bits per character (BPC) or bits per word (BPW), defined as nll(x)/sequence_length, where nll(x) is the negative log-likelihood of the sequence x. Outlier Exposure is implemented by adding the cross entropy to the uniform distribution on tokens from sequences in D OE out as an additional loss term. For D in , we convert the language-modeling version of Penn Treebank, split into sequences of length 70 for backpropagation for word-level models, and 150 for character-level models. We do not train or evaluate with preserved hidden states as in BPTT. This is because retaining hidden states would greatly simplify the task of OOD detection. Accordingly, the OOD detection task is to provide a score for 70-or 150-token sequences in the unseen D test out datasets. We train word-level models for 300 epochs, and character-level models for 50 epochs. We then fine-tune using OE on WikiText-2 for 5 epochs. For the character-level language model, we create a character-level version of WikiText-2 by converting words to lowercase and leaving out characters which do not appear in PTB. OOD detection results for the word-level and character-level language models are shown in 
FPR95 ↓ AUROC ↑ AUPR ↑ D in D

[DISCUSSION]
Extensions to Multilabel Classifiers and the Reject Option. Outlier Exposure can work in more classification regimes than just those considered above. For example, a multilabel classifier trained on CIFAR-10 obtains an 88.8% mean AUROC when using the maximum prediction probability as the OOD score. By training with OE to decrease the classifier's output probabilities on OOD samples, the mean AUROC increases to 97.1%. This is slightly less than the AUROC for a multiclass model tuned with OE. An alternative OOD detection formulation is to give classifiers a ""reject class"" (Bartlett & Wegkamp, 2008). Outlier Exposure is also flexible enough to improve performance in this setting, but we find that even with OE, classifiers with the reject option or multilabel outputs are not as competitive as OOD detectors with multiclass outputs.
Flexibility in Choosing D OE out . Early in experimentation, we found that the choice of D OE out is important for generalization to unseen D test out distributions. For example, adding Gaussian noise to samples from D in to create D OE out does not teach the network to generalize to unseen anomaly distributions for complex D in . Similarly, we found in Section 4.3 that synthetic anomalies do not work as well as real data for D OE out . In contrast, our experiments demonstrate that the large datasets of realistic anomalies described in Section 4.2.2 do generalize to unseen D test out distributions. In addition to size and realism, we found diversity of D OE out to be an important factor. Concretely, a CIFAR-100 classifier with CIFAR-10 as D OE out hardly improves over the baseline. A CIFAR-10 classifier exposed to ten CIFAR-100 outlier classes corresponds to an average AUPR of 78.5%. Exposed to 30 such classes, the classifier's average AUPR becomes 85.1%. Next, 50 classes corresponds to 85.3%, and from thereon additional CIFAR-100 classes barely improve performance. This suggests that dataset diversity is important, not just size. In fact, experiments in this paper often used around 1% of the images in the 80 Million Tiny Images dataset since we only briefly fine-tuned the models. We also found that using only 50,000 examples from this dataset led to a negligible degradation in detection performance. Additionally, D OE out datasets with significantly different statistics can perform similarly. For instance, using the Project Gutenberg dataset in lieu of WikiText-2 for D OE out in the SST experiments gives an average AUROC of 90.1% instead of 89.3%.

[CLOSENESS OF D TEST]
out , D OE out , and D test in . Our experiments show several interesting effects of the closeness of the datasets involved. Firstly, we find that D test out and D OE out need not be close for training with OE to improve performance on D test out . In Appendix A, we observe that an OOD detector for SVHN has its performance improve with Outlier Exposure even though (1) D OE out samples are images of natural scenes rather than digits, and (2) D test out includes unnatural examples such as emojis. We observed the same in our preliminary experiments with MNIST; using 80 Million Tiny Images as D OE out , OE increased the AUPR from 94.2% to 97.0%.
Secondly, we find that the closeness of D OE out to D test in can be an important factor in the success of OE. In the NLP experiments, preprocessing D OE out to be closer to D in improves OOD detection performance significantly. Without preprocessing, the network may discover easy-to-learn cues which reveal whether the input is in-or out-of-distribution, so the OE training objective can be optimized in unintended ways. That results in weaker detectors. In a separate experiment, we use Online Hard Example Mining so that difficult outliers have more weight in Outlier Exposure. Although this improves performance on the hardest anomalies, anomalies without plausible local statistics like noise are detected slightly less effectively than before. Thus hard or close-to-distribution examples do not necessarily teach the detector all valuable heuristics for detecting various forms of anomalies. Real-world applications of OE could use the method of Sun et al. (2018)   OE Improves Calibration. When using classifiers for prediction, it is important that confidence estimates given for the predictions do not misrepresent empirical performance. A calibrated classifier gives confidence probabilities that match the empirical frequency of correctness. That is, if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires.
Existing confidence calibration approaches consider the standard setting where data at test-time is always drawn from D in . We extend this setting to include examples from D test out at test-time since systems should provide calibrated probabilities on both in-and out-of-distribution samples. The classifier should have low-confidence predictions on these OOD examples, since they do not have a class. Building on the temperature tuning method of Guo et al. (2017), we demonstrate that OE can improve calibration performance in this realistic setting. Summary results are shown in Figure 3. Detailed results and a description of the metrics are in Appendix G.

[CONCLUSION]
In this paper, we proposed Outlier Exposure, a simple technique that enhances many current OOD detectors across various settings. It uses out-of-distribution samples to teach a network heuristics to detect new, unmodeled, out-of-distribution examples. We showed that this method is broadly applicable in vision and natural language settings, even for large-scale image tasks. OE can improve model calibration and several previous anomaly detection techniques. Further, OE can teach density estimation models to assign more plausible densities to out-of-distribution samples. Finally, Outlier Exposure is computationally inexpensive, and it can be applied with low overhead to existing systems. In summary, Outlier Exposure is an effective and complementary approach for enhancing out-of-distribution detection systems.

[A EXPANDED MULTICLASS RESULTS]
Expanded mutliclass out-of-distribution detection results are in Table 7 and Table 8   Anomalous Data. For each in-distribution dataset D in , we comprehensively evaluate OOD detectors on artificial and real anomalous distributions D test out following Hendrycks & Gimpel (2017). For each learned distribution D in , the number of test distributions that we compare against is approximately double that of most previous works.
Gaussian anomalies have each dimension i.i.d. sampled from an isotropic Gaussian distribution. Rademacher anomalies are images where each dimension is −1 or 1 with equal probability, so each dimension is sampled from a symmetric Rademacher distribution. Bernoulli images have each pixel sampled from a Bernoulli distribution if the input range is [0, 1]. Blobs data consist in algorithmically generated amorphous shapes with definite edges. Icons-50 is a dataset of icons and emojis (Hendrycks & Dietterich, 2019); icons from the ""Number"" class are removed. Textures is a dataset of describable textural images (Cimpoi et al., 2014). Places365 consists in images for scene recognition rather than object recognition (Zhou et al., 2017). LSUN is another scene understanding dataset with fewer classes than Places365 (Yu et al., 2015). ImageNet anomalous examples are taken from the 800 ImageNet-1K classes disjoint from Tiny ImageNet's 200 classes, and when possible each image is cropped with bounding box information as in Tiny ImageNet. For the Places365 experiment, ImageNet is ImageNet-1K with all 1000 classes. With CIFAR-10 as D in , we use also CIFAR-100 as D test out and vice versa; recall that the CIFAR-10 and CIFAR-100 classes do not overlap. Chars74K is a dataset of photographed characters in various styles; digits and letters such as ""O"" and ""l"" were removed since they can look like numbers. Places69 has images from 69 scene categories not found in the Places365 dataset.
SNLI is a dataset of predicates and hypotheses for natural language inference. We use the hypotheses for D OE out . IMDB is a sentiment classification dataset of movie reviews, with similar statistics to those of SST. Multi30K is a dataset of English-German image descriptions, of which we use the English descriptions. WMT16 is the English portion of the test set from WMT16. Yelp is a dataset of restaurant reviews. English Web Treebank (EWT) consists of five individual datasets: Answers (A), Email (E), Newsgroups (N), Reviews (R), and Weblog (W). Each contains examples from the indicated domain.
Validation Data. For each experiment, we create a set of validation distributions D val out . The first anomalies are uniform noise anomalies where each pixel is sampled from U[0, 1] or U[−1, 1] depending on the input space of the classifier. The remaining D val out validation sources are generated by corrupting in-distribution data, so that the data becomes out-of-distribution. One such source of anomalies is created by taking the pixelwise arithmetic mean of a random pair of in-distribution images. Other anomalies are created by taking the geometric mean of a random pair of in-distribution images. Jigsaw anomalies are created by taking an in-distribution example, partitioning the image into 16 equally sized patches, and permuting those patches. Speckle Noised anomalies are created by applying speckle noise to in-distribution images. RGB Ghosted anomalies involves shifting and reordering the color channels of in-distribution images. Inverted images are anomalies which have some or all of their color channels inverted.

[B ARCHITECTURES AND TRAINING DETAILS]
For CIFAR-10, CIFAR-100, and Tiny ImageNet classification experiments, we use a 40-2 Wide Residual Network (Zagoruyko & Komodakis, 2016). The network trains for 100 epochs with a dropout rate of 0.3. The initial learning rate of 0.1 decays following a cosine learning rate schedule . During fine-tuning of the entire network, we again use a cosine learning rate schedule but with an initial learning rate of 0.001. We use standard flipping and data cropping augmentation, Nesterov momentum, and 2 weight decay with a coefficient of 5 × 10 −4 . SVHN architectures are 16-4 Wide ResNets trained for 20 epochs with an initial learning rate of 0.01 and no data augmentation. For Places365, we use a ResNet-18 pre-trained on Places365. In this Places365 experiment, we tune with Outlier Exposure for 5 epochs, use 512 outlier samples per iteration, and start with a learning rate of 0.0001. Outlier Exposure fine-tuning occurs with each epoch being the length of in-distribution dataset epoch, so that Outlier Exposure completes quickly and does involve reading the entire D OE out dataset. 

[D OE WORKS ON OTHER VISION ARCHITECTURES]
Outlier Exposure also improves vision OOD detection performance for more than just Wide ResNets. Table 9 shows that Outlier Exposure also improves vision OOD detection performance for ""All Convolutional Networks"" (Salimans & Kingma, 2016 

[G CONFIDENCE CALIBRATION]
Models integrated into a decision making process should indicate when they are trustworthy, and such models should not have inordinate confidence in their predictions. In an effort to combat a false sense of certainty from overconfident models, we aim to calibrate model confidence. A model is calibrated if its predicted probabilities match empirical frequencies. Thus if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires. Prior research (Guo et al., 2017;Nguyen & O'Connor, 2015;Kuleshov & Liang, 2015) considers calibrating systems where test-time queries are samples from D in , but systems also encounter samples from D test out and should also ascribe low confidence to these samples. Hence, we use OE to control the confidence on these samples.

[G.1 METRICS]
In order to evaluate a multiclass classifier's calibration, we present three metrics. First we establish context. For input example X ∈ X , let Y ∈ Y = {1, 2, . . . , k} be the ground truth class. Let Y be the model's class prediction, and let C be the corresponding model confidence or prediction probability. Denote the set of prediction-label pairs made by the model with S = {( y 1 , c 1 ), ( y 2 , c 2 ), . . . , ( y n , c n )}. 

[RMS AND MAD CALIBRATION ERROR]
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k 2 .
Along similar lines, the MAD Calibration Error-which is an improper scoring rule due to its use of absolute differences rather than squared differences-is estimated with
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k .
Soft F1 Score. If a classifier makes only a few mistakes, then most examples should have high confidence. But if the classifier gives all predictions high confidence, including its mistakes, then the previous metrics will indicate that the model is calibrated on the vast majority of instances, despite having systematic miscalibration. The Soft F1 score (Pastor-Pellicer et al., 2013;Hendrycks & Gimpel, 2017) is suited for measuring the calibration of a system where there is an acute imbalance between mistaken and correct decisions. Since we treat mistakes a positive examples, we can write the model's confidence that the examples are anomalous with c a = (1 − c 1 , 1 − c 2 , . . . , 1 − c n ). To indicate that an example is positive (mistaken), we use the vector m ∈ {0, 1} n such that m i = 1(y i = y i ) for 1 ≤ i ≤ n. Then the Soft F1 score is c T a m 1 T (c a + m)/2 . 

[G.2 SETUP AND RESULTS]
There are many ways to estimate a classifier's confidence. One way is to bind a logistic regression branch onto the network, so that confidence values are in [0, 1]. Other confidence estimates use the model's logits l ∈ R k , such as the estimate σ(max i l i ) ∈ [0, 1], where σ is the logistic sigmoid.
Another common confidence estimate is max i exp (l i )/ k j=1 exp (l j ) . A modification of this estimate is our baseline. Softmax Temperature Tuning. Guo et al. (2017) show that good calibration can be obtained by including a tuned temperature parameter into the softmax: p(y = i | x) = exp(l i /T )/ k j=1 exp(l j /T ). We tune T to maximize log likelihood on a validation set after the network has been trained on the training set.
Results. In this calibration experiment, the baseline is confidence estimation with softmax temperature tuning. Therefore, we train SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet classifiers with 5000, 5000, 5000, and 10000 training examples held out, respectively. A copy of this classifier is fine-tuned with Outlier Exposure. Then we determine the optimal temperatures of the original and OE-fine-tuned classifiers on the held-out examples. To measure calibration, we take equally many examples from a given in-distribution dataset D test in and OOD dataset D test out . Out-of-distribution points are understood to be incorrectly classified since their label is not in the model's output space, so calibrated models should assign these out-of-distribution points low confidence. Results are in Table 12. Outlier Exposure noticeably improves model calibration.

[G.3 POSTERIOR RESCALING]
While temperature tuning improves calibration, the confidence estimate p(y = i | x) cannot be less than 1/k, k the number of classes. For an out-of-distribution example like Gaussian Noise, a good model should have no confidence in its prediction over k classes. One possibility is to add a reject option, or a (k + 1)st class, which we cover in Section 5. A simpler option we found is to perform an affine transformation of p(y = i | x) ∈ [1/k, 1] with the formula ( p(y = i | x) − 1/k)/(1 − 1/k) ∈ [0, 1]. This simple transformation makes it possible for a network to express no confidence on an out-of-distribution input and improves calibration performance. As Table 13 shows, this simple 0-1 posterior rescaling technique consistently improves calibration, and the model fine-tuned with OE using temperature tuning and posterior rescaling achieved large calibration improvements.   In Figure 4, we show additional PR and ROC Curves using the Tiny ImageNet dataset and various anomalous distributions.

[ACKNOWLEDGMENTS]
We thank NVIDIA for donating GPUs used in this research. This research was supported by a grant from the Future of Life Institute.","[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance."
Deep Anomaly Detection with Outlier Exposure,HyxCxhRcY7.json,"I have read authors' reply.  In response to authors' comprehensive reply and feedback. I upgrade my score to 6. As authors mentioned, the extension to density estimators is an original novelty of this paper, but I still have some concern that OE loss for classification is basically the same as [2]. I think it is better to clarify this in the draft. 

Summary===

This paper proposes a new fine-tuning method for improving the performance of existing anomaly detectors. The main idea is additionally optimizing the “Outlier Exposure (OE)” loss on outlier dataset. Specifically, for softmax classifier, the authors set the OE loss to the KL divergence loss between posterior distribution and uniform distribution. For density estimator, they set the OE loss to a margin ranking loss. The proposed method improves the detection performance of baseline methods on various vision and NLP datasets. While the research topic of this paper is interesting, I recommend rejections because I have concerns about novelty and the experimental results.

Detailed comments ===

1. OE loss for softmax classifier

For softmax classifier, the OE loss forces the posterior distribution to become uniform distribution on outlier dataset. I think this loss function is very similar to a confidence loss (equation 2) proposed in [2]: Lee et al., 2017 [2] also proposed the loss function minimizing the KL divergence between posterior distribution and uniform distribution on out-of-distribution, and evaluated the effects of it on ""unseen"" out-of-distribution (see Table 1 of [2]). Could the authors clarify the difference with the confidence loss in [2], and compare the performance with it? Without that, I feel that the novelty of this paper is not significant.

2. More comparison with baselines

The authors said that they didn’t compare the performance with simple inference methods like ODIN [3] since ODIN tunes the hyper-parameters using data from (tested) out-of-distribution. However, I think that the authors can compare the performance with ODIN by tuning the hyper-parameters of it on outlier dataset which is used for training OE loss. Could the authors provide more experimental results by comparing the performance with ODIN? 

3. Related work

I would appreciate if the authors can survey and compare more baselines such as [4] and [5]. 

[1] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. International Conference on Learning Representations, 2017. 
[2] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training confidence-calibrated classifiers for detecting out-of-distribution samples. International Conference on Learning Representations, 2018. 
[3] Shiyu Liang, Yixuan Li, and R. Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. International Conference on Learning Representations, 2018. 
[4] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. In NIPS, 2018.
[5] Apoorv Vyas, Nataraj Jammalamadaka, Xia Zhu, Dipankar Das, Bharat Kaul, and Theodore L. Willke. Out-of-Distribution Detection Using an Ensemble of Self Supervised Leave-out Classifiers, In ECCV, 2018.","[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance.

[CAPTIONS]
Table 1: Figure 1 :1Figure 1: ROC curve with Tiny Im-ageNet (D in ) and Textures (D test out ).
Table 2: Figure 2 :2Figure 2: OOD scores from PixelCNN++ on images from CIFAR-10 and SVHN.
Table 3: to refine a scraped D OE out auxiliary dataset to be appropriately close to D test in .SV HN CI FA R-10 CI FA R
Table 4: Figure 3 :3Figure 3: Root Mean Square Calibration Error values with temperature tuning and temperature tuning + OE across various datasets.
Table 5: .The Root Mean Square Calibration Error measures the square root of the expected squared difference between confidence and accuracy at a confidence level. It has the formula E C [(P(Y = Y |C = c) − c) 2 ] . A similar formulation which less severely penalizes large confidence-accuracy deviations is the Mean Absolute Value Calibration error, written E C [|P(Y = Y |C = c) − c|]. The MAD Calibration Error is a lower bound of the RMS Calibration Error. To empirically estimate these miscalibration measures, we partition the n samples of S into b bins {B 1 , B 2 , . . . , B b } with approximately 100 samples in each bin. UnlikeGuo et al. (2017), bins are not equally spaced since the distribution of confidence values is not uniform but dynamic. Concretely, the RMS Calibration Error is estimated with the numerically stable formula
Table 6: Figure 4 :4Figure 4: ROC curves with Tiny ImageNet as D in and Textures, Places365, LSUN, and ImageNet as D test out . Figures show the curves corresponding to the maximum softmax probability (MSP) baseline detector and the MSP detector with Outlier Exposure (OE).
Table 7: We use the ImageNet dataset with images from approximately 22 thousand classes as D OE WikiText-2 is a corpus of Wikipedia articles typically used for language modeling. We use WikiText-2 as D OE
Table 8: 8 in Appendix A. Notice that the SVHN classifier with OE can be used to detect new anomalies such as emojis and street view alphabet letters, even though D test OE is a dataset of natural images. Thus, Outlier Exposure helps models to generalize to unseen D test out distributions far better than the baseline. Comparisons between the MSP baseline and the MSP of the natural language classifier fine-tuned with OE. Results are percentages and averaged over 10 runs.Confidence Branch. A recently proposed OOD detection technique(DeVries & Taylor, 2018) involves appending an OOD scoring branch b : X → [0, 1] onto a deep network. Trained with samples from only D in , this branch estimates the network's confidence on any input. The creators of this technique made their code publicly available, so we use their code to train new 40-4 Wide Residual Network classifiers. We fine-tune the confidence branch with Outlier Exposure by adding 0.5E x∼D OE out[log b(x)] to the network's original optimization objective. In Table3, the baseline values are derived from the maximum softmax probabilities produced by the classifier trained with DeVries & Taylor (2018)'s publicly available training code. The confidence branch improves over this MSP detector, and after OE, the confidence branch detects anomalies more effectively. Comparison among the maximum softmax probability, Confidence Branch, and Confidence Branch + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages, and averaged across all D test out datasets.
Table 9: Comparison among the maximum softmax probability (MSP), MSP + GAN, and MSP + GAN + OE OOD detectors. The same network architecture is used for all three detectors. All results are percentages and averaged across all D test out datasets.
Table 10: Table 5: OOD detection results with a PixelCNN++ density estimator, and the same estimator after applying OE. The model's bits per pixel (BPP) scores each sample. All results are percentages. Test distributions D test out are described in Appendix A.
Table 11: ; expanded results and D test out descriptions are in Appendix F. In all cases, OE improves over the baseline, and the improvement is especially large for the word-level model.
Table 12: 
Table 13: NLP OOD example detection for the maximum softmax probability (MSP) baseline detector and the MSP detector after fine-tuning with Outlier Exposure (OE). All results are percentages and the result of 10 runs. Values are rounded so that 99.95% rounds to 100%.
Table 14: C TRAINING FROM SCRATCH WITH OUTLIER EXPOSURE USUALLY IMPROVES DETECTION PERFORMANCE Elsewhere we show results for pre-trained networks that are fine-tuned with OE. However, a network trained from scratch which simultaneously trains with OE tends to give superior results. For example, a CIFAR-10 Wide ResNet trained normally obtains a classification error rate of 5.16% and an FPR95 of 34.94%. Fine-tuned, this network has an error rate of 5.27% and an FPR95 of 9.50%. Yet if we instead train the network from scratch and expose it to outliers as it trains, then the error rate is 4.26% and the FPR95 is 6.15%. This architecture corresponds to a 9.50% RMS calibration error with OE fine-tuning, but by training with OE from scratch the RMS calibration error is 6.15%. Compared to fine-tuning, training a network in tandem with OE tends to produce a network with a better error rate, calibration, and OOD detection performance. The reason why we use OE for fine-tuning is because training from scratch requires more time and sometimes more GPU memory than fine-tuning.
Table 15: 
Table 16: OOD detection results on Penn Treebank examples and English Web Treebank outliers. All results are percentages.The D test out datasets come from the English Web Treebank(Bies et al., 2012), which contains text from five different domains: Yahoo! Answers, emails, newsgroups, product reviews, and weblogs. Other NLP D test out datasets we consider do not satisfy the language modeling assumption of continuity in the examples, so we do not evaluate on them.
Table 17: Table 12: Calibration results for the temperature tuned baseline and temperature tuning + OE.
Table 18: Calibration results for the softmax temperature tuning baseline, the same baseline after adding Posterior Rescaling, and temperature tuning + Posterior Rescaling + OE.

[INTRODUCTION]
Machine Learning systems in deployment often encounter data that is unlike the model's training data. This can occur in discovering novel astronomical phenomena, finding unknown diseases, or detecting sensor failure. In these situations, models that can detect anomalies (Liu et al., 2018;Emmott et al., 2013) are capable of correctly flagging unusual examples for human intervention, or carefully proceeding with a more conservative fallback policy.
Behind many machine learning systems are deep learning models (Krizhevsky et al., 2012) which can provide high performance in a variety of applications, so long as the data seen at test time is similar to the training data. However, when there is a distribution mismatch, deep neural network classifiers tend to give high confidence predictions on anomalous test examples . This can invalidate the use of prediction probabilities as calibrated confidence estimates (Guo et al., 2017), and makes detecting anomalous examples doubly important.
Several previous works seek to address these problems by giving deep neural network classifiers a means of assigning anomaly scores to inputs. These scores can then be used for detecting outof-distribution (OOD) examples (Hendrycks & Gimpel, 2017;Lee et al., 2018;Liu et al., 2018). These approaches have been demonstrated to work surprisingly well for complex input spaces, such as images, text, and speech. Moreover, they do not require modeling the full data distribution, but instead can use heuristics for detecting unmodeled phenomena. Several of these methods detect unmodeled phenomena by using representations from only in-distribution data.
In this paper, we investigate a complementary method where we train models to detect unmodeled data by learning cues for whether an input is unmodeled. While it is difficult to model the full data distribution, we can learn effective heuristics for detecting out-of-distribution inputs by exposing the model to OOD examples, thus learning a more conservative concept of the inliers and enabling the detection of novel forms of anomalies. We propose leveraging diverse, realistic datasets for this purpose, with a method we call Outlier Exposure (OE). OE provides a simple and effective way to consistently improve existing methods for OOD detection.
Through numerous experiments, we extensively evaluate the broad applicability of Outlier Exposure. For multiclass neural networks, we provide thorough results on Computer Vision and Natural Language Processing tasks which show that Outlier Exposure can help anomaly detectors generalize to and perform well on unseen distributions of outliers, even on large-scale images. We also demonstrate that Outlier Exposure provides gains over several existing approaches to out-of-distribution detection. Our results also show the flexibility of Outlier Exposure, as we can train various models with different sources of outlier distributions. Additionally, we establish that Outlier Exposure can make density estimates of OOD samples significantly more useful for OOD detection. Finally, we demonstrate that Outlier Exposure improves the calibration of neural network classifiers in the realistic setting where a fraction of the data is OOD. Our code is made publicly available at https://github.com/hendrycks/outlier-exposure.

[RELATED WORK]
Out-of-Distribution Detection with Deep Networks. Hendrycks & Gimpel (2017) demonstrate that a deep, pre-trained classifier has a lower maximum softmax probability on anomalous examples than in-distribution examples, so a classifier can conveniently double as a consistently useful outof-distribution detector. Building on this work, DeVries & Taylor (2018) attach an auxiliary branch onto a pre-trained classifier and derive a new OOD score from this branch. Liang et al. (2018) present a method which can improve performance of OOD detectors that use a softmax distribution. In particular, they make the maximum softmax probability more discriminative between anomalies and in-distribution examples by pre-processing input data with adversarial perturbations (Goodfellow et al., 2015). Unlike in our work, their parameters are tailored to each source of anomalies. Lee et al. (2018) train a classifier concurrently with a GAN (Radford et al., 2016;Goodfellow et al., 2014), and the classifier is trained to have lower confidence on GAN samples. For each testing distribution of anomalies, they tune the classifier and GAN using samples from that out-distribution, as discussed in Appendix B of their work. Unlike Liang et al. (2018); Lee et al. (2018), in this work we train our method without tuning parameters to fit specific types of anomaly test distributions, so our results are not directly comparable with their results. Many other works (de Vries et al., 2016;Subramanya et al., 2017;Malinin & Gales, 2018;Bevandic et al., 2018) also encourage the model to have lower confidence on anomalous examples. Recently, Liu et al. (2018) provide theoretical guarantees for detecting out-of-distribution examples under the assumption that a suitably powerful anomaly detector is available.
Utilizing Auxiliary Datasets. Outlier Exposure uses an auxiliary dataset entirely disjoint from test-time data in order to teach the network better representations for anomaly detection. Goodfellow et al. (2015) train on adversarial examples to increased robustness. Salakhutdinov et al. (2011) pre-train unsupervised deep models on a database of web images for stronger features. Radford et al. (2017) train an unsupervised network on a corpus of Amazon reviews for a month in order to obtain quality sentiment representations. Zeiler & Fergus (2014) find that pre-training a network on the large ImageNet database (Russakovsky et al., 2015) endows the network with general representations that are useful in many fine-tuning applications. Chen & Gupta (2015); Mahajan et al. (2018) show that representations learned from images scraped from the nigh unlimited source of search engines and photo-sharing websites improve object detection performance.

[OUTLIER EXPOSURE]
We consider the task of deciding whether or not a sample is from a learned distribution called D in . Samples from D in are called ""in-distribution,"" and otherwise are said to be ""out-of-distribution"" (OOD) or samples from D out . In real applications, it may be difficult to know the distribution of outliers one will encounter in advance. Thus, we consider the realistic setting where D out is unknown. Given a parametrized OOD detector and an Outlier Exposure (OE) dataset D OE out , disjoint from D test out , we train the model to discover signals and learn heuristics to detect whether a query is sampled from D in or D OE out . We find that these heuristics generalize to unseen distributions D out . Deep parametrized anomaly detectors typically leverage learned representations from an auxiliary task, such as classification or density estimation. Given a model f and the original learning objective L, we can thus formalize Outlier Exposure as minimizing the objective
E (x,y)∼Din [L(f (x), y) + λE x ∼D OE out [L OE (f (x ), f (x), y)]
] over the parameters of f . In cases where labeled data is not available, then y can be ignored.
Outlier Exposure can be applied with many types of data and original tasks. Hence, the specific formulation of L OE is a design choice, and depends on the task at hand and the OOD detector used. For example, when using the maximum softmax probability baseline detector (Hendrycks & Gimpel, 2017), we set L OE to the cross-entropy from f (x ) to the uniform distribution (Lee et al., 2018). When the original objective L is density estimation and labels are not available, we set L OE to a margin ranking loss on the log probabilities f (x ) and f (x).

[EXPERIMENTS]
We evaluate OOD detectors with and without OE on a wide range of datasets. Each evaluation consists of an in-distribution dataset D in used to train an initial model, a dataset of anomalous examples D OE out , and a baseline detector to which we apply OE. We describe the datasets in Section 4.2. The OOD detectors and L OE losses are described on a case-by-case basis.
In the first experiment, we show that OE can help detectors generalize to new text and image anomalies. This is all accomplished without assuming access to the test distribution during training or tuning, unlike much previous work. In the confidence branch experiment, we show that OE is flexible and complements a binary anomaly detector. Then we demonstrate that using synthetic outliers does not work as well as using real and diverse data; previously it was assumed that we need synthetic data or carefully selected close-to-distribution data, but real and diverse data is enough. We conclude with experiments in density estimation. In these experiments we find that a cutting-edge density estimator unexpectedly assigns higher density to out-of-distribution samples than in-distribution samples, and we ameliorate this surprising behavior with Outlier Exposure.  We evaluate out-of-distribution detection methods on their ability to detect OOD points. For this purpose, we treat the OOD examples as the positive class, and we evaluate three metrics: area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPR), and the false positive rate at N % true positive rate (FPRN ). The AUROC and AUPR are holistic metrics that summarize the performance of a detection method across multiple thresholds. The AUROC can be thought of as the probability that an anomalous example is given a higher OOD score than a in-distribution example (Davis & Goadrich, 2006). Thus, a higher AUROC is better, and an uninformative detector has an AUROC of 50%. The AUPR is useful when anomalous examples are infrequent (Manning & Schütze, 1999), as it takes the base rate of anomalies into account. During evaluation with these metrics, the base rate of D test out to D test in test examples in all of our experiments is 1:5.
Whereas the previous two metrics represent the detection performance across various thresholds, the FPRN metric represents performance at one strict threshold. By observing performance at a strict threshold, we can make clear comparisons among strong detectors. The FPRN metric (Liu et al., 2018;Kumar et al., 2016;Balntas et al., 2016) is the probability that an in-distribution example (negative) raises a false alarm when N % of anomalous examples (positive) are detected, so a lower FPRN is better. Capturing nearly all anomalies with few false alarms can be of high practical value.

[IN-DISTRIBUTION DATASETS]
SVHN. The SVHN dataset (Netzer et al., 2011) contains 32 × 32 color images of house numbers. There are ten classes comprised of the digits 0-9. The training set has 604, 388 images, and the test set has 26, 032 images. For preprocessing, we rescale the pixels to be in the interval [0, 1]. CIFAR. The two CIFAR (Krizhevsky & Hinton, 2009) datasets contain 32 × 32 natural color images. CIFAR-10 has ten classes while CIFAR-100 has 100. CIFAR-10 and CIFAR-100 classes are disjoint but have similiarities. For example, CIFAR-10 has ""automobiles"" and ""trucks"" but not CIFAR-100's ""pickup truck"" class. Both have 50, 000 training images and 10, 000 test images. For this and the remaining image datasets, each image is standardized channel-wise. Tiny ImageNet. The Tiny ImageNet dataset (Johnson et al.) is a 200-class subset of the ImageNet (Russakovsky et al., 2015) dataset where images are resized and cropped to 64 × 64 resolution. The dataset's images were cropped using bounding box information so that cropped images contain the target, unlike Downsampled ImageNet (Chrabaszcz et al., 2017). The training set has 100, 000 images and the test set has 10, 000 images. Places365. The Places365 training dataset (Zhou et al., 2017)  For classification tasks on 20 Newsgroups, TREC, and SST, we treat each sentence of WikiText-2 as an individual example, and use simple filters to remove low-quality sentences.

[MULTICLASS CLASSIFICATION]
In what follows, we use Outlier Exposure to enhance the performance of existing OOD detection techniques with multiclass classification as the original task. Throughout the following experiments, we let x ∈ X be a classifier's input and y ∈ Y = {1, 2, . . . , k} be a class. We also represent the classifier with the function f : X → R k , such that for any x, 1 T f (x) = 1 and f (x) 0.

[MAXIMUM SOFTMAX PROBABILITY (MSP).]
Consider the maximum softmax probability baseline (Hendrycks & Gimpel, 2017) which gives an input x the OOD score − max c f c (x). Out-ofdistribution samples are drawn from various unseen distributions (Appendix A). For each task, we test with approximately twice the number of D test out distributions compared to most other papers, and we also test on NLP tasks. The quality of the OOD example scores are judged with the metrics described in Section 4.1. For this multiclass setting, we perform Outlier Exposure by fine-tuning a pre-trained classifier f so that its posterior is more uniform on D OE out samples. Specifically, the fine-
tuning objective is E (x,y)∼Din [− log f y (x)] + λE x∼D OE out [H(U; f (x))]
, where H is the cross entropy and U is the uniform distribution over k classes. When there is class imbalance, we could encourage f (x) to match (P (y = 1), . . . , P (y = k)); yet for the datasets we consider, matching U works well enough. Also, note that training from scratch with OE can result in even better performance than fine-tuning (Appendix C). This approach works on different architectures as well (Appendix D).
Unlike Liang et al. (2018); Lee et al. (2018) and like Hendrycks & Gimpel (2017); DeVries & Taylor (2018), we do not tune our hyperparameters for each D test out distribution, so that D test out is kept unknown like with real-world anomalies. Instead, the λ coefficients were determined early in experimentation with validation D val out distributions described in Appendix A. In particular, we use λ = 0.5 for vision experiments and λ = 1.0 for NLP experiments. Like previous OOD detection methods involving network fine-tuning, we chose λ so that impact on classification accuracy is negligible.
For nearly all of the vision experiments, we train Wide Residual Networks (Zagoruyko & Komodakis, 2016) and then fine-tune network copies with OE for 10 epochs. However we use a pretrained ResNet-18 for Places365. For NLP experiments, we train 2-layer GRUs (Cho et al., 2014) for 5 epochs, then fine-tune network copies with OE for 2 epochs. Networks trained on CIFAR-10 or CIFAR-100 are exposed to images from 80 Million Tiny Images, and the Tiny ImageNet and Places365 classifiers are exposed to ImageNet-22K. NLP classifiers are exposed to WikiText-2. Further architectural and training details are in Appendix B. For all tasks, OE improves average performance by a large margin. Averaged results are shown in Tables 1 and 2. Sample ROC curves are shown in Figures 1 and 4. Detailed results on individual D test out datasets are in Table 7 and Table (Hafner et al., 2018). A method with better success is from Lee et al. (2018). They carefully train a GAN to generate synthetic examples near the classifier's decision boundary. The classifier is encouraged to have a low maximum softmax probability on these synthetic examples. For CIFAR classifiers, they mention that a GAN can be a better source of anomalies than datasets such as SVHN. In contrast, we find that the simpler approach of drawing anomalies from a diverse dataset is sufficient for marked improvements in OOD detection.
We train a 40-4 Wide Residual Network using Lee et al. (2018)'s publicly available code, and use the network's maximum softmax probabilities as our baseline. Another classifier trains concurrently with a GAN so that the classifier assigns GAN-generated examples a high OOD score. We want each D test out to be novel. Consequently we use their code's default hyperparameters, and exactly one model encounters all tested D test out distributions. This is unlike their work since, for each D test out distribution, they train and tune a new network. We do not evaluate on Tiny ImageNet, Places365, nor text, since DCGANs cannot stably generate such images and text reliably. Lastly, we take the network trained in tandem with a GAN and fine-tune it with OE. Table 4 shows the large gains from using OE with a real and diverse dataset over using synthetic samples from a GAN.    (Nalisnick et al., 2019). Consequently, density estimates are another means by which to score anomalies (Zong et al., 2018). We show the ability of OE to improve density estimates on low-probability, outlying data.
PixelCNN++. Autoregressive neural density estimators provide a way to parametrize the probability density of image data. Although sampling from these architectures is slow, they allow for evaluating the probability density with a single forward pass through a CNN, making them promising candidates for OOD detection. We use Pix-elCNN++ (Salimans et al., 2017) as a baseline OOD detector, and we train it on CIFAR-10. The OOD score of example x is the bits per pixel (BPP), defined as nll(x)/num_pixels, where nll is the negative log-likelihood. With this loss we fine-tune for 2 epochs using OE, which we find is sufficient for the training loss to converge. Here OE is implemented with a margin loss over the log-likelihood difference between in-distribution and anomalous examples, so that the loss for a sample x in from D in and point x out from D OE out is max{0, num_pixels + nll(x in ) − nll(x out )}.
Results are shown in Table 5. Notice that PixelCNN++ without OE unexpectedly assigns lower BPP from SVHN images than CIFAR-10 images. For all D test out datasets, OE significantly improves results. Language Modeling. We next explore using OE on language models. We use QRNN (Merity et al., 2018a;b) language models as baseline OOD detectors. For the OOD score, we use bits per character (BPC) or bits per word (BPW), defined as nll(x)/sequence_length, where nll(x) is the negative log-likelihood of the sequence x. Outlier Exposure is implemented by adding the cross entropy to the uniform distribution on tokens from sequences in D OE out as an additional loss term. For D in , we convert the language-modeling version of Penn Treebank, split into sequences of length 70 for backpropagation for word-level models, and 150 for character-level models. We do not train or evaluate with preserved hidden states as in BPTT. This is because retaining hidden states would greatly simplify the task of OOD detection. Accordingly, the OOD detection task is to provide a score for 70-or 150-token sequences in the unseen D test out datasets. We train word-level models for 300 epochs, and character-level models for 50 epochs. We then fine-tune using OE on WikiText-2 for 5 epochs. For the character-level language model, we create a character-level version of WikiText-2 by converting words to lowercase and leaving out characters which do not appear in PTB. OOD detection results for the word-level and character-level language models are shown in 
FPR95 ↓ AUROC ↑ AUPR ↑ D in D

[DISCUSSION]
Extensions to Multilabel Classifiers and the Reject Option. Outlier Exposure can work in more classification regimes than just those considered above. For example, a multilabel classifier trained on CIFAR-10 obtains an 88.8% mean AUROC when using the maximum prediction probability as the OOD score. By training with OE to decrease the classifier's output probabilities on OOD samples, the mean AUROC increases to 97.1%. This is slightly less than the AUROC for a multiclass model tuned with OE. An alternative OOD detection formulation is to give classifiers a ""reject class"" (Bartlett & Wegkamp, 2008). Outlier Exposure is also flexible enough to improve performance in this setting, but we find that even with OE, classifiers with the reject option or multilabel outputs are not as competitive as OOD detectors with multiclass outputs.
Flexibility in Choosing D OE out . Early in experimentation, we found that the choice of D OE out is important for generalization to unseen D test out distributions. For example, adding Gaussian noise to samples from D in to create D OE out does not teach the network to generalize to unseen anomaly distributions for complex D in . Similarly, we found in Section 4.3 that synthetic anomalies do not work as well as real data for D OE out . In contrast, our experiments demonstrate that the large datasets of realistic anomalies described in Section 4.2.2 do generalize to unseen D test out distributions. In addition to size and realism, we found diversity of D OE out to be an important factor. Concretely, a CIFAR-100 classifier with CIFAR-10 as D OE out hardly improves over the baseline. A CIFAR-10 classifier exposed to ten CIFAR-100 outlier classes corresponds to an average AUPR of 78.5%. Exposed to 30 such classes, the classifier's average AUPR becomes 85.1%. Next, 50 classes corresponds to 85.3%, and from thereon additional CIFAR-100 classes barely improve performance. This suggests that dataset diversity is important, not just size. In fact, experiments in this paper often used around 1% of the images in the 80 Million Tiny Images dataset since we only briefly fine-tuned the models. We also found that using only 50,000 examples from this dataset led to a negligible degradation in detection performance. Additionally, D OE out datasets with significantly different statistics can perform similarly. For instance, using the Project Gutenberg dataset in lieu of WikiText-2 for D OE out in the SST experiments gives an average AUROC of 90.1% instead of 89.3%.

[CLOSENESS OF D TEST]
out , D OE out , and D test in . Our experiments show several interesting effects of the closeness of the datasets involved. Firstly, we find that D test out and D OE out need not be close for training with OE to improve performance on D test out . In Appendix A, we observe that an OOD detector for SVHN has its performance improve with Outlier Exposure even though (1) D OE out samples are images of natural scenes rather than digits, and (2) D test out includes unnatural examples such as emojis. We observed the same in our preliminary experiments with MNIST; using 80 Million Tiny Images as D OE out , OE increased the AUPR from 94.2% to 97.0%.
Secondly, we find that the closeness of D OE out to D test in can be an important factor in the success of OE. In the NLP experiments, preprocessing D OE out to be closer to D in improves OOD detection performance significantly. Without preprocessing, the network may discover easy-to-learn cues which reveal whether the input is in-or out-of-distribution, so the OE training objective can be optimized in unintended ways. That results in weaker detectors. In a separate experiment, we use Online Hard Example Mining so that difficult outliers have more weight in Outlier Exposure. Although this improves performance on the hardest anomalies, anomalies without plausible local statistics like noise are detected slightly less effectively than before. Thus hard or close-to-distribution examples do not necessarily teach the detector all valuable heuristics for detecting various forms of anomalies. Real-world applications of OE could use the method of Sun et al. (2018)   OE Improves Calibration. When using classifiers for prediction, it is important that confidence estimates given for the predictions do not misrepresent empirical performance. A calibrated classifier gives confidence probabilities that match the empirical frequency of correctness. That is, if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires.
Existing confidence calibration approaches consider the standard setting where data at test-time is always drawn from D in . We extend this setting to include examples from D test out at test-time since systems should provide calibrated probabilities on both in-and out-of-distribution samples. The classifier should have low-confidence predictions on these OOD examples, since they do not have a class. Building on the temperature tuning method of Guo et al. (2017), we demonstrate that OE can improve calibration performance in this realistic setting. Summary results are shown in Figure 3. Detailed results and a description of the metrics are in Appendix G.

[CONCLUSION]
In this paper, we proposed Outlier Exposure, a simple technique that enhances many current OOD detectors across various settings. It uses out-of-distribution samples to teach a network heuristics to detect new, unmodeled, out-of-distribution examples. We showed that this method is broadly applicable in vision and natural language settings, even for large-scale image tasks. OE can improve model calibration and several previous anomaly detection techniques. Further, OE can teach density estimation models to assign more plausible densities to out-of-distribution samples. Finally, Outlier Exposure is computationally inexpensive, and it can be applied with low overhead to existing systems. In summary, Outlier Exposure is an effective and complementary approach for enhancing out-of-distribution detection systems.

[A EXPANDED MULTICLASS RESULTS]
Expanded mutliclass out-of-distribution detection results are in Table 7 and Table 8   Anomalous Data. For each in-distribution dataset D in , we comprehensively evaluate OOD detectors on artificial and real anomalous distributions D test out following Hendrycks & Gimpel (2017). For each learned distribution D in , the number of test distributions that we compare against is approximately double that of most previous works.
Gaussian anomalies have each dimension i.i.d. sampled from an isotropic Gaussian distribution. Rademacher anomalies are images where each dimension is −1 or 1 with equal probability, so each dimension is sampled from a symmetric Rademacher distribution. Bernoulli images have each pixel sampled from a Bernoulli distribution if the input range is [0, 1]. Blobs data consist in algorithmically generated amorphous shapes with definite edges. Icons-50 is a dataset of icons and emojis (Hendrycks & Dietterich, 2019); icons from the ""Number"" class are removed. Textures is a dataset of describable textural images (Cimpoi et al., 2014). Places365 consists in images for scene recognition rather than object recognition (Zhou et al., 2017). LSUN is another scene understanding dataset with fewer classes than Places365 (Yu et al., 2015). ImageNet anomalous examples are taken from the 800 ImageNet-1K classes disjoint from Tiny ImageNet's 200 classes, and when possible each image is cropped with bounding box information as in Tiny ImageNet. For the Places365 experiment, ImageNet is ImageNet-1K with all 1000 classes. With CIFAR-10 as D in , we use also CIFAR-100 as D test out and vice versa; recall that the CIFAR-10 and CIFAR-100 classes do not overlap. Chars74K is a dataset of photographed characters in various styles; digits and letters such as ""O"" and ""l"" were removed since they can look like numbers. Places69 has images from 69 scene categories not found in the Places365 dataset.
SNLI is a dataset of predicates and hypotheses for natural language inference. We use the hypotheses for D OE out . IMDB is a sentiment classification dataset of movie reviews, with similar statistics to those of SST. Multi30K is a dataset of English-German image descriptions, of which we use the English descriptions. WMT16 is the English portion of the test set from WMT16. Yelp is a dataset of restaurant reviews. English Web Treebank (EWT) consists of five individual datasets: Answers (A), Email (E), Newsgroups (N), Reviews (R), and Weblog (W). Each contains examples from the indicated domain.
Validation Data. For each experiment, we create a set of validation distributions D val out . The first anomalies are uniform noise anomalies where each pixel is sampled from U[0, 1] or U[−1, 1] depending on the input space of the classifier. The remaining D val out validation sources are generated by corrupting in-distribution data, so that the data becomes out-of-distribution. One such source of anomalies is created by taking the pixelwise arithmetic mean of a random pair of in-distribution images. Other anomalies are created by taking the geometric mean of a random pair of in-distribution images. Jigsaw anomalies are created by taking an in-distribution example, partitioning the image into 16 equally sized patches, and permuting those patches. Speckle Noised anomalies are created by applying speckle noise to in-distribution images. RGB Ghosted anomalies involves shifting and reordering the color channels of in-distribution images. Inverted images are anomalies which have some or all of their color channels inverted.

[B ARCHITECTURES AND TRAINING DETAILS]
For CIFAR-10, CIFAR-100, and Tiny ImageNet classification experiments, we use a 40-2 Wide Residual Network (Zagoruyko & Komodakis, 2016). The network trains for 100 epochs with a dropout rate of 0.3. The initial learning rate of 0.1 decays following a cosine learning rate schedule . During fine-tuning of the entire network, we again use a cosine learning rate schedule but with an initial learning rate of 0.001. We use standard flipping and data cropping augmentation, Nesterov momentum, and 2 weight decay with a coefficient of 5 × 10 −4 . SVHN architectures are 16-4 Wide ResNets trained for 20 epochs with an initial learning rate of 0.01 and no data augmentation. For Places365, we use a ResNet-18 pre-trained on Places365. In this Places365 experiment, we tune with Outlier Exposure for 5 epochs, use 512 outlier samples per iteration, and start with a learning rate of 0.0001. Outlier Exposure fine-tuning occurs with each epoch being the length of in-distribution dataset epoch, so that Outlier Exposure completes quickly and does involve reading the entire D OE out dataset. 

[D OE WORKS ON OTHER VISION ARCHITECTURES]
Outlier Exposure also improves vision OOD detection performance for more than just Wide ResNets. Table 9 shows that Outlier Exposure also improves vision OOD detection performance for ""All Convolutional Networks"" (Salimans & Kingma, 2016 

[G CONFIDENCE CALIBRATION]
Models integrated into a decision making process should indicate when they are trustworthy, and such models should not have inordinate confidence in their predictions. In an effort to combat a false sense of certainty from overconfident models, we aim to calibrate model confidence. A model is calibrated if its predicted probabilities match empirical frequencies. Thus if a calibrated model predicts an event with 30% probability, then 30% of the time the event transpires. Prior research (Guo et al., 2017;Nguyen & O'Connor, 2015;Kuleshov & Liang, 2015) considers calibrating systems where test-time queries are samples from D in , but systems also encounter samples from D test out and should also ascribe low confidence to these samples. Hence, we use OE to control the confidence on these samples.

[G.1 METRICS]
In order to evaluate a multiclass classifier's calibration, we present three metrics. First we establish context. For input example X ∈ X , let Y ∈ Y = {1, 2, . . . , k} be the ground truth class. Let Y be the model's class prediction, and let C be the corresponding model confidence or prediction probability. Denote the set of prediction-label pairs made by the model with S = {( y 1 , c 1 ), ( y 2 , c 2 ), . . . , ( y n , c n )}. 

[RMS AND MAD CALIBRATION ERROR]
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k 2 .
Along similar lines, the MAD Calibration Error-which is an improper scoring rule due to its use of absolute differences rather than squared differences-is estimated with
b i=1 |B i | n 1 |B i | k∈Bi 1(y k = y k ) − 1 |B i | k∈Bi c k .
Soft F1 Score. If a classifier makes only a few mistakes, then most examples should have high confidence. But if the classifier gives all predictions high confidence, including its mistakes, then the previous metrics will indicate that the model is calibrated on the vast majority of instances, despite having systematic miscalibration. The Soft F1 score (Pastor-Pellicer et al., 2013;Hendrycks & Gimpel, 2017) is suited for measuring the calibration of a system where there is an acute imbalance between mistaken and correct decisions. Since we treat mistakes a positive examples, we can write the model's confidence that the examples are anomalous with c a = (1 − c 1 , 1 − c 2 , . . . , 1 − c n ). To indicate that an example is positive (mistaken), we use the vector m ∈ {0, 1} n such that m i = 1(y i = y i ) for 1 ≤ i ≤ n. Then the Soft F1 score is c T a m 1 T (c a + m)/2 . 

[G.2 SETUP AND RESULTS]
There are many ways to estimate a classifier's confidence. One way is to bind a logistic regression branch onto the network, so that confidence values are in [0, 1]. Other confidence estimates use the model's logits l ∈ R k , such as the estimate σ(max i l i ) ∈ [0, 1], where σ is the logistic sigmoid.
Another common confidence estimate is max i exp (l i )/ k j=1 exp (l j ) . A modification of this estimate is our baseline. Softmax Temperature Tuning. Guo et al. (2017) show that good calibration can be obtained by including a tuned temperature parameter into the softmax: p(y = i | x) = exp(l i /T )/ k j=1 exp(l j /T ). We tune T to maximize log likelihood on a validation set after the network has been trained on the training set.
Results. In this calibration experiment, the baseline is confidence estimation with softmax temperature tuning. Therefore, we train SVHN, CIFAR-10, CIFAR-100, and Tiny ImageNet classifiers with 5000, 5000, 5000, and 10000 training examples held out, respectively. A copy of this classifier is fine-tuned with Outlier Exposure. Then we determine the optimal temperatures of the original and OE-fine-tuned classifiers on the held-out examples. To measure calibration, we take equally many examples from a given in-distribution dataset D test in and OOD dataset D test out . Out-of-distribution points are understood to be incorrectly classified since their label is not in the model's output space, so calibrated models should assign these out-of-distribution points low confidence. Results are in Table 12. Outlier Exposure noticeably improves model calibration.

[G.3 POSTERIOR RESCALING]
While temperature tuning improves calibration, the confidence estimate p(y = i | x) cannot be less than 1/k, k the number of classes. For an out-of-distribution example like Gaussian Noise, a good model should have no confidence in its prediction over k classes. One possibility is to add a reject option, or a (k + 1)st class, which we cover in Section 5. A simpler option we found is to perform an affine transformation of p(y = i | x) ∈ [1/k, 1] with the formula ( p(y = i | x) − 1/k)/(1 − 1/k) ∈ [0, 1]. This simple transformation makes it possible for a network to express no confidence on an out-of-distribution input and improves calibration performance. As Table 13 shows, this simple 0-1 posterior rescaling technique consistently improves calibration, and the model fine-tuned with OE using temperature tuning and posterior rescaling achieved large calibration improvements.   In Figure 4, we show additional PR and ROC Curves using the Tiny ImageNet dataset and various anomalous distributions.

[ACKNOWLEDGMENTS]
We thank NVIDIA for donating GPUs used in this research. This research was supported by a grant from the Future of Life Institute.","[TITLE]
DEEP ANOMALY DETECTION WITH OUTLIER EXPOSURE

[ABSTRACT]
It is important to detect anomalous inputs when deploying machine learning systems. The use of larger and more complex inputs in deep learning magnifies the difficulty of distinguishing between anomalous and in-distribution examples. At the same time, diverse image and text data are available in enormous quantities. We propose leveraging these data to improve deep anomaly detection by training anomaly detectors against an auxiliary dataset of outliers, an approach we call Outlier Exposure (OE). This enables anomaly detectors to generalize and detect unseen anomalies. In extensive experiments on natural language processing and small-and large-scale vision tasks, we find that Outlier Exposure significantly improves detection performance. We also observe that cutting-edge generative models trained on CIFAR-10 may assign higher likelihoods to SVHN images than to CIFAR-10 images; we use OE to mitigate this issue. We also analyze the flexibility and robustness of Outlier Exposure, and identify characteristics of the auxiliary dataset that improve performance."
Planning for Implicit Coordination using FOND,HJelxWI9w4.json,"This paper defines a decidable fragment of epistemic planning that can be compiled to fully-observable nondeterministic (FOND) planning, and keeps the same computational complexity. A small case-study is provided to demonstrate the fruitfulness of the compilation.

Generally, the paper provides the necessary context to the user, and it is well contextualised. The theoretical side is sound, and the compilation approach is described at an appropriate level of detail. The example helps to understand part of the details, and gives some useful descriptions of the resulting operators. 

As a suggestion for further improvements, and given the topic of the workshop, it would have been nice to provide some additional insights into the characteristics of the generated models. How can they be made easier to handle by planning engines? what are the most challenging aspects? Are there aspects of the language that are forcing the use of some caveats?

* Minor issues
- don't -> do not
- ""As explained in Section, "" -> the AAAI template does not enumerate sections.","[TITLE]
Planning for Implicit Coordination using FOND

[ABSTRACT]
Epistemic Planning can be used to achieve implicit coordination in cooperative multi-agent settings where knowledge and capabilities are distributed between the agents. In these scenarios, agents plan and act on their own without having to agree on a common plan or protocol beforehand. However, epistemic planning is undecidable in general. In this paper, we identify a decidable fragment of epistemic planning that allows for arbitrary initial state uncertainty and nondeterminism, but where actions can never increase the uncertainty of the agents. We show that in this fragment, planning with and without implicit coordination can be reduced to fully observable nondeterministic (FOND) planning and that it shares the same computational complexity. We also provide a small case study, modeling the problem of multi-agent path finding with destination uncertainty in FOND, to show that our compilation approach can be successfully applied in practice.

[CAPTIONS]
Table 1: Figure 1: A MAPF/DU instance.
Table 2: (:objects a1 a2 -agt l m r b -pos) (:init (adj l m) (adj m l) (adj m r) ; ... (ind a1 w1 w2) (ind a1 w2 w1) ; ... (ind a2 w1 w3) (ind a2 w3 w1) ; ... (goal w1 a1 r) (goal w1 a2 l) ; ... (goals for w2, w3, w4) (des w1) (next-choose)) (:goal (forall (?w -wld ?a -agt ?p -pos) (imply (and (des ?w) (goal ?w ?a ?p)) (at ?a ?p))))
Table 3: Case study.

[INTRODUCTION]
Epistemic planning has gained increasing interest in recent years (Baral et al. 2017). One of the main features of epistemic planning is the support of knowledge goals. For example, epistemic planning is well-suited to model problems in which information is to be confidentially passed between agents. The assumption is usually that there exists an explicit or implicit model about the distributed knowledge of the agents, as well as actions which can change the models.
However, recent work has shown that epistemic planning can also be used to achieve implicit coordination in a setting where multiple agents plan and act for themselves towards a cooperative goal (Engesser et al. 2017). The idea is that the explicit modeling of the agents' knowledge can be exploited as a means to enforce coordination via perspective taking. In particular, by putting themselves into the shoes of the others, agents can account for possible contributions of other agents in their own plans. Bolander et al. (2018) showed under which conditions such plans are guaranteed to be successful. This problem of planning for implicit coordination was originally formalized as a variant of contingent planning in the space of epistemic states (i.e., Kripke models), with actions represented by the action models from Dynamic Epis-temic Logic (van Ditmarsch et al. 2007). The formalization is very similar to the one of Bolander and Andersen (2011), which produces action sequences that can be interpreted as centralized plans. Bolander and Andersen have shown that this type of epistemic planning is undecidable in general. However, some decidable fragments have been identified that rely on restricting the structure of action models and the form of allowed preconditions (Aucher and Bolander 2013;Bolander et al. 2015;Charrier et al. 2016). On the practical side, Kominis and Geffner (2015) and Muise et al. (2015) have identified fragments of epistemic planning that can be solved by compilation to classical planning.
In this paper, we define a decidable fragment that captures contingent epistemic planning and that can be compiled to fully-observable nondeterministic (FOND) planning. Our fragment generalizes the fragment of Kominis and Geffner. We then show how our compilation can be extended to capture planning for implicit coordination. The key insight is that we can use nondeterminism to simulate perspective taking and thus account for the imperfect knowledge of the agents.

[THEORETICAL BACKGROUND]
We will first recapitulate the DEL planning framework using the conventions of Bolander and Andersen (2011), but including conditional effects in the style of van Benthem et al. (2006). We will then review strong fully-observable nondeterministic planning (Cimatti et al. 2003;Ghallab et al. 2004) as well as planning for implicit coordination (Engesser et al. 2017;Bolander et al. 2018).

[THE DEL PLANNING FRAMEWORK]
For a fixed set of agents A and a fixed set of atomic propositions P , the epistemic language L KC is given by the BNF
φ ::= p | ¬φ | φ ∧ φ | K i φ | Cφ,
where p ∈ P and i ∈ A.
We read K i φ as ""agent i knows φ"" and Cφ as ""it is common knowledge between all agents that φ"". The additional connectives ∨, ←, →, ↔ can be defined as abbreviations, analogously to their definition in propositional logic.
We evaluate such formulas over epistemic models. An epistemic model is a tuple M = W, (R i ) i∈A , V , where W is a non-empty, finite set of worlds (the domain of M), R i ⊆ W × W is an equivalence relation for each agent i ∈ A (the indistinguishability relation of i), and with V : P → 2 W (the valuation function). We write R * for the transitive closure of i∈A R i . The truth of a formula φ ∈ L KC in a world w of a model M is then given as follows, where the propositional cases are standard and hence left out:
M, w |= p iff w ∈ V (p) M, w |= K i φ iff M, w |= φ for all wR i w M, w |= Cφ iff M, w |= φ for all wR * w
We depict epistemic models as graphs where nodes correspond to the worlds in the model and are additionally labeled with the atomic propositions that are true in that particular world. The indistinguishability relations are given as labeled edges between the worlds. For readability, we will omit reflexive edges as well as edges that are implied by transitivity. Consider the following epistemic model:
M 0 = w 1 : p w 2 :
1, 2
In our example, both agents 1 and 2 do not know whether or not p is true (which is the case in w 1 ) or false (which is the case in w 2 ). Also, it is common knowledge between the two agents that they do not know. We will now define example actions for agent 1, first to sense the value of p and then to announce it to agent 2.
To define actions, we use event models. These can change the facts about the world as well as the knowledge of the agents. Analogous to epistemic models, an event model is a tuple E = E, (Q i ) i∈A , pre, eff , where E is a non-empty, finite set of events (the domain of E) and R i ⊆ W × W is an equivalence relation for each agent i ∈ A (the indistinguishability relation of i). Instead of a valuation function, we have two functions pre : E → L KC and eff : E → (P → L KC ), assigning a precondition and conditional effects to each event.
We depict event models analogously to epistemic models with the difference that nodes now correspond to events, which are additionally labeled with their respective preconditions and effects. Consider the following event model:
E sense = e 1 : p, {p → p} e 2 : ¬p, {p → p} 2
An event model updates an epistemic model by pairing up every world with every applicable event (i.e., of which the precondition is satisfied). Two updated worlds are indistinguishable for an agent if both the original worlds and the events are indistinguishable for that agent. Furthermore, a proposition is true in an updated world if and only if the event's conditional effect concerning that proposition evaluates to true in the original world.
For example, E sense consists of two events with preconditions p and ¬p. For both events, the effect is {p → p} meaning p will be true if p was true before (from now on, we will omit these trivial effects that preserve the value of an atomic proposition in our depiction of event models). Since the events are distinguishable for agent 1, the agent will, after the execution of the action, be able to distinguish worlds in which p is true from worlds in which p is false.
Formally, we define the product update M ⊗ E of model M = W, (R i ) i∈A , V with respect to an event model E = E, (Q i ) i∈A , pre, eff as model W , (R i ) i∈A , V where
• W = {(w, e) ∈ W × E | M, w |= pre(e)},
• R i = {((w, e), (w , e )) ∈ W × W | wR i w , eQ i e },
• V (p) = {(w, e) ∈ W | M, w |= eff(e, p)}.
In particular, if we apply E sense in M 0 , we obtain the following epistemic model:
M 0 ⊗ E sense = (w 1 , e 1 ) : p (w 2 , e 2 ) :
2
As intended, agent 1 knows now whether or not p is true. Note that additionally agent 2 is aware of this. The event model E sense represents semi-private sensing, meaning that even though the result of the sensing will only be known to agent 1, agent 2 will know that the sensing has taken place.
For planning, we usually consider pointed models (M, w), i.e., where one world w from the domain of M is designated as the actual world. In contrast, we model epistemic actions as multi-pointed event models (E, E d ) where E d is a subset of the domain of E. This is necessary, since sometimes we want the events to be deliberately chosen by the acting agents and sometimes by the environment. E.g., our semi-private sensing action should be defined as (E sense , {e 1 , e 2 }). Since both events are designated, it can be applied regardless of whether p is true or false. Applied in (M 0 , w 1 ), the action results in the pointed model (M 0 ⊗ E sense , (w 1 , e 1 )) and applied in (M 0 , w 2 ) it results in (M 0 ⊗ E sense , (w 2 , e 2 )). The similar action (E sense , {e 1 }) is only applicable in the case where p is true. It can, e.g., be used to model the action of a third agent semi-privately informing agent 1 that p is true.
Formally, an epistemic action (E, E d ) is applicable in (M, w) if there is an applicable event e ∈ E d , meaning that M, w |= pre(e). The application of (E, E d ) in (M, w) then nondeterministically leads to a pointed model (M ⊗ E(w, e)) such that M, w |= pre(e).
Note that any epistemic state represented by a pointed model (M, w), has infinitely many epistemically equivalent representations (i.e., other pointed models that satisfy the exact same set of formulas). It is a central theorem of modal logic that finite models are epistemically equivalent if and only if they are bisimilar. In the following, when using pointed models as states in a transition system, we think of them as representatives of their whole equivalence class. I.e., we consider two epistemic states (M, w) and (M , w ) as identical if they are epistemically equivalent. And we say two epistemic states (M, w) and (M , w ) are indistinguishable for an agent i if there is a world w in M that is indistinguishable to w for agent i such that (M, w ) and (M , w ) are identical. An initial epistemic state together with a set of epistemic actions thus induces a nondeterministic transition system where all states are epistemically different from each other.

[FOND PLANNING]
Our definition of FOND planning loosely follows the conventions of Ghallab et al. (2004). In particular, our actions consist of one common precondition and a set of possible effects, from which one will always be chosen nondeterministically. However, since we want to start out with a formalization that is as close as possible to our DEL formalism, we allow arbitrary propositional formulas as action preconditions and goals. We also use conditional effects which we restrict to effect normal form, which is a special case of Rintanens unary conditionality normal form (Rintanen 2003).
We define a FOND planning task as a tuple F, I, γ, Act where F is a set of fluents (atomic propositions), I ⊆ F is the initial state, γ is a propositional goal formula over F and Act is a set of actions. Each action a = pre a , effs a ∈ Act consists of a propositional formula pre a over F (the precondition) and a set effs a (the conditional effects). Each conditional effect e ∈ effs a is of the form f ∈F (χ e f £ f ) ∧ (χ e ¬f £ ¬f ), where χ e f and χ e ¬f are mutually inconsistent propositional formulas over F (i.e., their conjunction is unsatisfiable). They can be interpreted as ""effect e makes f true under the condition χ e f and false under the condition χ e ¬f "". Such a FOND task induces a finite transition system starting with the initial state I and connecting two states S and S via action a iff S |= pre a and there is an effect e ∈ effs a such that the conditional effects in e transform S to S . This gives us a trivial compilation from FOND to DEL. I.e., we compile the initial state into an epistemic state with exactly one world w 0 where V (p) = {w 0 } iff p ∈ I, or ∅ otherwise. And for each action a ∈ Act, we construct an epistemic action with one event for each nondeterministic effect e ∈ effs a , with precondition pre a and effect {f → χ e f ∨ (f ∧ ¬χ e ¬f ) | f ∈ F}. All events are designated and pairwise distinguishable for all agents. The transition system that we get from our compiled DEL state and actions is isomorphic to the FOND transition system and identified states share the same propositional valuation.
One solution to FOND planning tasks are strong plans. These are partial functions π from states to actions which satisfy the following properties (Cimatti et al. 2003):
• For every state s that is reachable via π from I, there is some state s that is reachable from s via π, s.t. s |= γ.
• There are no cycles, i.e. states s and s such that s is reachable via π from s and s is reachable via π from s.
Since the transition system is finite, following a strong policy always leads to a goal state in finitely many steps. It seems reasonable to assume that the concept of strong policies is also useful for contingent planning over epistemic states.

[IMPLICIT COORDINATION IN DEL]
We define an epistemic planning task as a tuple s 0 , A, ω, γ where s 0 is an epistemic state (the initial state), A is a finite set of epistemic actions (the action library), ω : A → A is a function mapping each action to its owner (the owner function), and γ ∈ L KC is the goal formula. E.g., consider the planning task with s 0 = (M 0 , w 1 ), A = {sense, ann p , ann ¬p } with sense = (E sense , {e 1 , e 2 }), ann p = (E annp , e 1 ) and ann ¬p = (E ann¬p , e 1 ). The actions ann p and ann ¬p are public announcement actions for agent 1, announcing that p is true, or respectively false. That is, the event models E ann¬p and E annp are given as follows:
E annp = e 1 : p, ∅ E ann¬p = e 1 : ¬p, ∅
We assume that all actions are owned by agent 1, i.e., ω = {sense → 1, ann p → 1, ann ¬p → 1}. The goal is for agent 2 to know whether or not p is true, i.e., γ = K 2 p ∨ K 2 ¬p.
A strong policy in the sense of Cimatti et al. (2003) would be to just apply the action ann p in s 0 . This is because the action is applicable in (M 0 , w 1 ) and its application would lead to a successor state consisting of only one world (w 1 , e 1 ) in which p (and K 2 p) is true. We argue that from the perspective of the agents (who initially do not know whether p is true or false), this is not a reasonable solution. If we want agent 1 to be able to come up with the plan for himself, we must consider his incomplete knowledge about the situation. Intuitively, a good plan for agent 1 is to first apply the sensing action and then, depending on the sensing result, apply the action ann p or ann q . This plan works for both states (M, w 1 ) and (M, w 2 ), which agent 1 considers possible.
To capture this, we have to require uniform policies. A uniform policy is a partial function π from epistemic states to sets of epistemic actions, satisfying the following constraints:
• Applicability: for each state s, and action a ∈ π(s), the action a has to be applicable in state s. • Uniformity: for each state s, and action a ∈ π(s), and states s that are indistinguishable to s for the owner ω(a) of the action, also a ∈ π(s ). This definition ensures that the agents can always infer from their own knowledge whether or not and how the policy wants them to act. This also implies that an action is only applicable by an agent, if the agent knows that the action is applicable. Note that because of the uniformity constraint, it is necessary to allow policies to assign multiple actions per state. E.g., sometimes we want a policy to assign an action a of agent 1 to some state s and an action b of agent 2 to some state s . Then by uniformity, if there is a state s that is indistinguishable to s for agent 1 and to s for agent 2, we have to assign both a and b to s .
We then say a uniform policy is subjectively strong, if it satisfies the exact properties of strong plans, but based on subjective reachability: A state s is a subjective successor of s given an action a if there is a successor state of s and a that is indistinguishable to s for agent ω(a). I.e., in our example, the subjective successors of (M 0 , w 1 ) and (E sense , {e 1 , e 2 }) are exactly the states (M 0 ⊗ E sense , (w 1 , e 1 )) and (M 0 ⊗ E sense , (w 2 , e 2 )). A state s is then subjectively reachable from s if either s is identical to s or s is subjectively reachable from a subjective successor of s.
In particular, a policy π that is subjectively strong for an epistemic planning task s 0 , A, ω, γ guarantees for each subjectively reachable state s and action a ∈ π(s), that π is also subjectively strong for s, A, ω, γ , as well as for all planning tasks s , A, ω, γ with an initial state s that is indistinguishable to s for ω(a).

[A DECIDABLE FRAGMENT OF DEL PLANNING]
A straight-forward way to obtain a decidable fragment of DEL planning is to ensure that the induced transition system is finite. This can be done by restricting the action set in such a way that the application of a single action can never lead to a state where the number of worlds is greater than in the state in which the action was applied. We achieve this by requiring our event models to be partitioned into disjoint connected components with mutually inconsistent preconditions. This allows us to think of each of the components as a single nondeterministic effect. Consider the following event model:
E pp = e 1 : p, ∅ e 2 : ¬p, ∅ e 3 : p, ∅2
For example, the action (E pp , {e 1 , e 3 }) could model an agent 3 trying to semi-privately announce p to agent 1. However, there is the possibility that the confidentiality of the announcement is compromised and p is thus effectively publicly announced. If we apply this action in
(M 0 , w 1 ), it re- sults either in (M 0 ⊗ E pp , (w 1 , e 1 )) or (M 0 ⊗ E pp , (w 1 , e 3 ))where
M 0 ⊗ E pp = (w 1 , e 1 ) : p (w 2 , e 2 ) : (w 1 , e 3 ) : p 2
Formally, for actions ( E, (Q i ), pre, eff , E d ) from our action set, we require that the domain E can be partitioned into disjoint subsets E 1 , . . . , E k such that (1) for each pair of events e, e ∈ E j from the same component j ∈ {1, . . . , k}, the preconditions pre(e) and pre(e ) are mutually inconsistent, and (2) two events e, e ∈ E are only allowed to be indistinguishable for an arbitrary agent i ∈ A, i.e. eQ i e , if they belong to the same component, i.e if there exists a j ∈ {1, . . . k} such that e, e ∈ E j .
We can see that if we apply such an action to an arbitrary epistemic state, due to condition (1), each world will be paired up by maximally one of the events of each component. Furthermore, due to condition (2), two worlds can only be distinguishable for any agent if the events they were generated with are from the same component. Thus the state resulting from an action application will consist of at most k connected components which consist each of less or equally many worlds than the original state. Since we can throw away all components that do not contain the updated designated world, we obtain a state that can be represented by less than or equally many worlds as the original state.
Our fragment is a generalization of the fragment introduced by Kominis and Geffner (2015), which allows exactly those actions that can be described with only mutually inconsistent preconditions (even between events from different components) and where all actions are thus deterministic.

[COMPILATION TO FOND]
In the following we will show how to generate a FOND planning task F, I, γ * , Act , given an epistemic planning task s 0 , A, ω, γ from our fragment.

[COMPILATION OF EPISTEMIC STATES]
We use the approach of Kominis and Geffner (2015) to represent epistemic states as classical states. The idea is that we generate fluents directly from the worlds and indistinguishability relation of the initial state s 0 , such that we can use them to encode the valuation functions and indistinguishability relations of arbitrary states reachable from s 0 .
Given the initial state s 0 = ( W, (R i ), V , w 0 ), we introduce a fluent p w ∈ F (read: ""p is true in world w"") for each proposition p ∈ P and world w ∈ W . Similarly, we introduce a fluent D {w1,w2} i (read: ""w 1 is distinguishable to w 2 for agent i"") for each agent i ∈ A and worlds w 1 , w 2 ∈ W with w 1 R i w 2 . Finally, for each world w ∈ W , we introduce the fluent w * ∈ F (read: ""w is the designated world"").
A propositional state S ⊆ F then represents an epistemic state ( W,
(R i ), V , w) where (1) w ∈ V (p) iff p w ∈ S, (2) w 1 R i w 2 iff D {w1,w2} i
∈ S, and (3) w = w iff w * ∈ S. For example, if s 0 = (M 0 , w 1 ), we will generate the set of fluents F = {p w1 , p w2 , D
{w1,w2} 1 , D {w1,w2} 2 , w * 1 , w * 2 }.
The initial state s 0 will then be I = {p w1 , w * 1 }.

[COMPILATION OF EPISTEMIC FORMULAS]
To check whether a propositional formula φ is true in world w of an epistemic state that is represented by a classical state S ⊆ F is simple. We replace the occurrences of each proposition p in φ by p w and check the resulting formula in S.
Checking formulas with knowledge operators is slightly more complicated. Kominis and Geffner (2015) use axioms to compile away all knowledge subformulas into derived variables, the values of which can be inferred in polynomial time.
We will simply assume that all of this is given and that we can thus compile each epistemic formula to a formula φ w that evaluates to true in a classical state representing an epistemic state (M, w 0 ) iff M, w |= φ.
For evaluating a formula directly in the designated world of a state (e.g., the goal formula), we use φ * , which we define as ( w∈W w * ) ∧ w∈W (w * → φ w ).

[COMPILATION OF EPISTEMIC ACTIONS]
We now show how an action a = ( E, (Q i ), pre, eff , E d ) that can be partitioned into distinct components E 1 , . . . , E k accordingly to our fragment, can be compiled into a FOND action pre a , effs a .
We know that an action is applicable in a state (M, w) if there is some event e ∈ E d such that M, w |= pre(e). We directly translate this to pre a = e∈E d pre(e) * . We can then translate each of the components of our event model into a different nondeterministic effect, i.e., we get effs a = {eff j | j = 1, ..., n}. These nondeterministic effects can make propositions true or false, as well as make worlds distinguishable or completely inaccessible. We construct each nondeterministic effect eff j as follows:
eff j = eff P + j ∧ eff P − j ∧ eff D+ j ∧ eff × j ∧ eff × × j
First, each fluent p w is made true or false accordingly to the effects of the event e ∈ E j that is applied in w.
eff P + j = w∈W
p∈P (∨ e∈Ej (pre(e) w ∧ eff(e, p) w ) £ p w ) eff P − j = w∈W p∈P (∨ e∈Ej (pre(e) w ∧ ¬ eff(e, p) w ) £ ¬p w )
Two worlds w and w become distinguishable if the events e and e they were updated with are distinguishable:
eff D+ j = w,w ∈W i∈A,w =w     
e,e ∈Ej ¬eQie (pre(e) w ∧ pre(e ) w ) £D
{w,w } i     
If in some world w, none of the events from E j are applicable, the world should not have a successor. We simulate this by making w distinguishable from all other worlds.
eff × j = w,w ∈W w =w ∧ e∈Ej ¬ pre(e) w £ D {w,w } i
If for the designated world w, there is no applicable event in E j , there should not even be a corresponding successor state. We model this by completely removing the designation w * . Thus, while the effect is still applicable, it leads to a state where all formulas φ * evaluate to false and therefore no actions are applicable and the goal is not satisfied.
eff × × j = ∧ e∈Ej ¬ pre(e) w £ ¬w *

[COMPILATION OF POLICIES]
Our compilation guarantees that the nondeterministic outcomes of an action that is applied in a propositional state corresponds exactly to the nondeterministic outcomes of the original epistemic action applied to the original epistemic state. Thus any strong policy for an epistemic planning task automatically corresponds to a strong policy in its FOND compilation. I.e., we can start in the initial state of the FOND compilation and extract a policy by successively applying the actions assigned by the original policy.
For the other direction, we can proceed similarly. However, we have to be careful about the fact that the policy can can contain multiple propositional states representing the same epistemic state. E.g., consider the states {p w1 , w * 1 } and {p w2 , w * 2 }. Having equivalent states in a policy is unproblematic, as long as one is never reachable from the other. To obtain a strong policy for the original problem, we can apply the same policy extraction procedure from above but ignore each state if we have seen an equivalent state before.
If the policy in our FOND compilation contains equivalent states such that one is reachable from the other, the extraction gets more difficult, as we have to take care of not introducing cycles into our policy. Fortunately, it is easy to argue that if there is no strong policy in the FOND compilation which doesn't include equivalent states that are reachable from each other, there will also be no strong policy that includes them. This is because the transition system looks exactly the same from these states and we do not gain anything from getting from one of the states to the other. This means that if there is a strong policy for the FOND compilation of an epistemic planning task, there also has to exist a strong policy that does not contain epistemically equivalent reachable states. Moreover, if the strong policy in the FOND compilation is optimal (i.e., its tree representation has minimal depth), it is clear that the policy cannot contain equivalent states that are reachable from each other. We thus obtain the following theorem.
Theorem 1. Let Π be an epistemic planning task from our fragment. Then there exists a strong policy for Π if and only if there exists a strong policy for the FOND compilation of Π. Any optimal strong policy for Π directly corresponds to an optimal strong policy for its compilation and vice versa.
The following theorem follows, given the EXPTIMEcompleteness of the plan existence problem for strong planning in FOND (Rintanen 2004).
Theorem 2. In our epistemic planning fragment, the problem of deciding whether there exists a strong policy for a given planning task is EXPTIME-complete.

[PLANNING FOR IMPLICIT COORDINATION]
As explained in our section about planning for implicit coordination, strong policies are not suitable if we want the agents to coordinate implicitly. In this section, we show how to use FOND planning to find subjectively strong plans for epistemic planning tasks from our fragment.

[THE COMPILATION]
We use the same compilation of states and formulas as before. However, we slightly modify the compilation of actions. The idea is to split each action from the action set into two: One auxiliary action for choosing the action that we want to apply in a state and one action that actually applies the effects of the previously selected action. In each choice action, we additionally simulate a perspective shift: We change the designated world nondeterministically to any of the worlds that are indistinguishable for the owner agent of the action. Thus, subjective successors of an action in the original problem are now objective successors.
This means that any strong policy in the compilation will correspond to a subjectively strong policy in the original problem. We can extract such a policy by taking all the apply-actions from our policy and and assigning the corresponding actions to the corresponding states in the original planning task.
Theorem 3. In our epistemic planning fragment, the problem of deciding whether there exists a subjectively strong policy is EXPTIME-complete.

[EXAMPLE: MAPF/DU]
We demonstrate our approach by modeling an instance of multi-agent path finding with destination uncertainty. The problem was first described by Bolander et al. (2018) and more thoroughly analyzed by Nebel et al. (2019). It is a generalization of the multi-agent path finding problem, relaxing the assumption that the agents' goals are commonly known. Instead, we assume that there are pairwise disjoint sets of plausible goal candidates for each agent, which are commonly known. Also, each agent can identify its own goal. As final action, each agent is allowed to announce that he has arrived at its true destination. The joint goal for the agents is that each agent is at his own true goal. Nebel et al. showed that the plan existence problem is PSPACE-complete. The naive algorithm they proposed has a runtime complexity of O(n a 2 +a ) where n is the number of graph vertices and a is the number of agents. Figure 1 shows an example of a MAPF/DU instance with two agents. The goal candidates of the square agent are r and b, and the goal candidates of the circle agent are l and m. One subjectively strong policy is for the square agent to first go to b and to let the circle agent move to l, independently of the actual destinations of the agents. Then, the square agent goes to its true destination (which, depending on the designated world, will be either r or b) and announces success there. Afterwards, the circle agent can go to his true destination (which will be either l or m). Note that after the initial movements of the square agent, the policy has to consider all 4 possible goal combinations. This is because the square agent does not know the actual goal of the circle agent and the circle agent will not know the actual goal of the square agent.
We now show how this problem can be modeled in PDDL (McDermott 1998). We will use the types agt for agents, pos for positions, and wld for worlds. We introduce fluents (at ?a ?p) to denote that agent ?a is at position ?p, (adj ?p ?q) to denote that an agent can step from position ?p to position ?q, and (announced ?a) to denote that the agent ?a has already announced success and will not move any longer. Furthermore, we use (goal ?w ?a ?p) to denote that the actual goal of agent ?a in world ?w is position ?p.
To denote indistinguishability of two worlds ?w1 and ?w2 for agent ?a, we use the fluent (ind ?a ?w1 ?w2). We mark the designated world ?w using the fluent (des ?w).
Finally, we use the predicates (next-choose), (next-move ?a ?p1 ?p2) and (next-announce ?a) to enforce the alternation of auxiliary perspectiveshifting actions and actual actions.
We now show how to split up the movement actions into the actions choose-move and move. The action (choose-move ?a ?w ?p ?q) simulates a perspec-tive shift to agent ?a by nondeterministically switching to an arbitrary world that is indistinguishable from the designated world for agent ?a. Furthermore, by setting the fluent (next-move ?a ?p ?q) to true, it enforces a movement action for agent ?a from ?p to ?q in the successor state. (and (not (des ?w)) (des w2))) ; ... Unfortunately, we have to enumerate all possible worlds to simulate the perspective shift. This forces us to include the worlds as constants into the domain definition. It would be more convenient if we had a dedicated construct in PDDL to automatically generate nondeterministic effects, e.g., by explicitly quantifying over objects (in our case, worlds).
The move action, which has to be applied afterwards, performs the actual change of the agent's position. This action also contains the actual precondition for movement actions: the field to move to has to be adjacent and empty. Also, the action prescribes the next action to be again a choose action by setting the fluent next-choose to true. The actions choose-announce and announce can be defined similarly. Announcing works by making all worlds where the agent has a different goal than its current position distinguishable to any other world for all agents. E.g., our example instance from Figure 1 can then be defined using the following initial state and goal descriptions: We tested our MAPF/DU planning domain using the myND planner of Mattmüller et al. (2010), which is to the Experiment time 2 agents, 4 cells, and 4 worlds 0.55s 3 agents, 6 cells, and 8 worlds 11.5s best of our knowledge the only publicly available FOND planner that supports both strong acyclic plans as well as conditional effects. It also supports axioms, although we did not need them for our example. Table 1 shows the performance of the planner on the example instance from Figure 1 as well as on a slightly bigger version with three agents.

[CONCLUSION]
In our paper, we have shown a decidable fragment of strong epistemic planning that has the same complexity than strong planning in FOND. We have also demonstrated how FOND planning can be used to generate subjectively strong plans. For future work, it is worth noticing that DEL can be used for modeling games. In particular, there is a translation from the game description language GDL-III to DEL (Engesser et al. 2018). There are some very interesting games which fall within our decidable fragment, one of which is Hanabi, which has gained some attention recently (Bard et al. 2019). While using a FOND planner does not seem to be feasible for problems of this size, it will be interesting to investigate how the idea of simulating perspective taking via nondeterminism can be incorporated into techniques such as Monte Carlo tree search or model-based reinforcement learning (e.g., value iteration in fully-observable MDPs).","[TITLE]
Planning for Implicit Coordination using FOND

[ABSTRACT]
Epistemic Planning can be used to achieve implicit coordination in cooperative multi-agent settings where knowledge and capabilities are distributed between the agents. In these scenarios, agents plan and act on their own without having to agree on a common plan or protocol beforehand. However, epistemic planning is undecidable in general. In this paper, we identify a decidable fragment of epistemic planning that allows for arbitrary initial state uncertainty and nondeterminism, but where actions can never increase the uncertainty of the agents. We show that in this fragment, planning with and without implicit coordination can be reduced to fully observable nondeterministic (FOND) planning and that it shares the same computational complexity. We also provide a small case study, modeling the problem of multi-agent path finding with destination uncertainty in FOND, to show that our compilation approach can be successfully applied in practice."
Planning for Implicit Coordination using FOND,HJelxWI9w4.json,"The paper introduces a compilation scheme from a fragment of epistemic planning into Fully Observable Non-Deterministic (FOND) planning. It is shown (although without a formal proof) that the considered fragment of epistemic planning is as hard as FOND planning (EXPTIME-complete).

The approach is evaluated in two small instances of MAPF/DU (Multi-Agent Path Finding with Destination Uncertainty) domain. Even looking at these small instances, it can be seen that the approach does not scale well (which is not surprising given the large computational complexity). On the other hand, more instances/domains could shed more light into what we can expect from the approach.

The paper is not a typical Knowledge Engineering paper, although it concerns remodeling from one formalism to another which can, in my opinion, attract some attention at the workshop. 


grammar: don't -> do not; doesn't -> does not","[TITLE]
Planning for Implicit Coordination using FOND

[ABSTRACT]
Epistemic Planning can be used to achieve implicit coordination in cooperative multi-agent settings where knowledge and capabilities are distributed between the agents. In these scenarios, agents plan and act on their own without having to agree on a common plan or protocol beforehand. However, epistemic planning is undecidable in general. In this paper, we identify a decidable fragment of epistemic planning that allows for arbitrary initial state uncertainty and nondeterminism, but where actions can never increase the uncertainty of the agents. We show that in this fragment, planning with and without implicit coordination can be reduced to fully observable nondeterministic (FOND) planning and that it shares the same computational complexity. We also provide a small case study, modeling the problem of multi-agent path finding with destination uncertainty in FOND, to show that our compilation approach can be successfully applied in practice.

[CAPTIONS]
Table 1: Figure 1: A MAPF/DU instance.
Table 2: (:objects a1 a2 -agt l m r b -pos) (:init (adj l m) (adj m l) (adj m r) ; ... (ind a1 w1 w2) (ind a1 w2 w1) ; ... (ind a2 w1 w3) (ind a2 w3 w1) ; ... (goal w1 a1 r) (goal w1 a2 l) ; ... (goals for w2, w3, w4) (des w1) (next-choose)) (:goal (forall (?w -wld ?a -agt ?p -pos) (imply (and (des ?w) (goal ?w ?a ?p)) (at ?a ?p))))
Table 3: Case study.

[INTRODUCTION]
Epistemic planning has gained increasing interest in recent years (Baral et al. 2017). One of the main features of epistemic planning is the support of knowledge goals. For example, epistemic planning is well-suited to model problems in which information is to be confidentially passed between agents. The assumption is usually that there exists an explicit or implicit model about the distributed knowledge of the agents, as well as actions which can change the models.
However, recent work has shown that epistemic planning can also be used to achieve implicit coordination in a setting where multiple agents plan and act for themselves towards a cooperative goal (Engesser et al. 2017). The idea is that the explicit modeling of the agents' knowledge can be exploited as a means to enforce coordination via perspective taking. In particular, by putting themselves into the shoes of the others, agents can account for possible contributions of other agents in their own plans. Bolander et al. (2018) showed under which conditions such plans are guaranteed to be successful. This problem of planning for implicit coordination was originally formalized as a variant of contingent planning in the space of epistemic states (i.e., Kripke models), with actions represented by the action models from Dynamic Epis-temic Logic (van Ditmarsch et al. 2007). The formalization is very similar to the one of Bolander and Andersen (2011), which produces action sequences that can be interpreted as centralized plans. Bolander and Andersen have shown that this type of epistemic planning is undecidable in general. However, some decidable fragments have been identified that rely on restricting the structure of action models and the form of allowed preconditions (Aucher and Bolander 2013;Bolander et al. 2015;Charrier et al. 2016). On the practical side, Kominis and Geffner (2015) and Muise et al. (2015) have identified fragments of epistemic planning that can be solved by compilation to classical planning.
In this paper, we define a decidable fragment that captures contingent epistemic planning and that can be compiled to fully-observable nondeterministic (FOND) planning. Our fragment generalizes the fragment of Kominis and Geffner. We then show how our compilation can be extended to capture planning for implicit coordination. The key insight is that we can use nondeterminism to simulate perspective taking and thus account for the imperfect knowledge of the agents.

[THEORETICAL BACKGROUND]
We will first recapitulate the DEL planning framework using the conventions of Bolander and Andersen (2011), but including conditional effects in the style of van Benthem et al. (2006). We will then review strong fully-observable nondeterministic planning (Cimatti et al. 2003;Ghallab et al. 2004) as well as planning for implicit coordination (Engesser et al. 2017;Bolander et al. 2018).

[THE DEL PLANNING FRAMEWORK]
For a fixed set of agents A and a fixed set of atomic propositions P , the epistemic language L KC is given by the BNF
φ ::= p | ¬φ | φ ∧ φ | K i φ | Cφ,
where p ∈ P and i ∈ A.
We read K i φ as ""agent i knows φ"" and Cφ as ""it is common knowledge between all agents that φ"". The additional connectives ∨, ←, →, ↔ can be defined as abbreviations, analogously to their definition in propositional logic.
We evaluate such formulas over epistemic models. An epistemic model is a tuple M = W, (R i ) i∈A , V , where W is a non-empty, finite set of worlds (the domain of M), R i ⊆ W × W is an equivalence relation for each agent i ∈ A (the indistinguishability relation of i), and with V : P → 2 W (the valuation function). We write R * for the transitive closure of i∈A R i . The truth of a formula φ ∈ L KC in a world w of a model M is then given as follows, where the propositional cases are standard and hence left out:
M, w |= p iff w ∈ V (p) M, w |= K i φ iff M, w |= φ for all wR i w M, w |= Cφ iff M, w |= φ for all wR * w
We depict epistemic models as graphs where nodes correspond to the worlds in the model and are additionally labeled with the atomic propositions that are true in that particular world. The indistinguishability relations are given as labeled edges between the worlds. For readability, we will omit reflexive edges as well as edges that are implied by transitivity. Consider the following epistemic model:
M 0 = w 1 : p w 2 :
1, 2
In our example, both agents 1 and 2 do not know whether or not p is true (which is the case in w 1 ) or false (which is the case in w 2 ). Also, it is common knowledge between the two agents that they do not know. We will now define example actions for agent 1, first to sense the value of p and then to announce it to agent 2.
To define actions, we use event models. These can change the facts about the world as well as the knowledge of the agents. Analogous to epistemic models, an event model is a tuple E = E, (Q i ) i∈A , pre, eff , where E is a non-empty, finite set of events (the domain of E) and R i ⊆ W × W is an equivalence relation for each agent i ∈ A (the indistinguishability relation of i). Instead of a valuation function, we have two functions pre : E → L KC and eff : E → (P → L KC ), assigning a precondition and conditional effects to each event.
We depict event models analogously to epistemic models with the difference that nodes now correspond to events, which are additionally labeled with their respective preconditions and effects. Consider the following event model:
E sense = e 1 : p, {p → p} e 2 : ¬p, {p → p} 2
An event model updates an epistemic model by pairing up every world with every applicable event (i.e., of which the precondition is satisfied). Two updated worlds are indistinguishable for an agent if both the original worlds and the events are indistinguishable for that agent. Furthermore, a proposition is true in an updated world if and only if the event's conditional effect concerning that proposition evaluates to true in the original world.
For example, E sense consists of two events with preconditions p and ¬p. For both events, the effect is {p → p} meaning p will be true if p was true before (from now on, we will omit these trivial effects that preserve the value of an atomic proposition in our depiction of event models). Since the events are distinguishable for agent 1, the agent will, after the execution of the action, be able to distinguish worlds in which p is true from worlds in which p is false.
Formally, we define the product update M ⊗ E of model M = W, (R i ) i∈A , V with respect to an event model E = E, (Q i ) i∈A , pre, eff as model W , (R i ) i∈A , V where
• W = {(w, e) ∈ W × E | M, w |= pre(e)},
• R i = {((w, e), (w , e )) ∈ W × W | wR i w , eQ i e },
• V (p) = {(w, e) ∈ W | M, w |= eff(e, p)}.
In particular, if we apply E sense in M 0 , we obtain the following epistemic model:
M 0 ⊗ E sense = (w 1 , e 1 ) : p (w 2 , e 2 ) :
2
As intended, agent 1 knows now whether or not p is true. Note that additionally agent 2 is aware of this. The event model E sense represents semi-private sensing, meaning that even though the result of the sensing will only be known to agent 1, agent 2 will know that the sensing has taken place.
For planning, we usually consider pointed models (M, w), i.e., where one world w from the domain of M is designated as the actual world. In contrast, we model epistemic actions as multi-pointed event models (E, E d ) where E d is a subset of the domain of E. This is necessary, since sometimes we want the events to be deliberately chosen by the acting agents and sometimes by the environment. E.g., our semi-private sensing action should be defined as (E sense , {e 1 , e 2 }). Since both events are designated, it can be applied regardless of whether p is true or false. Applied in (M 0 , w 1 ), the action results in the pointed model (M 0 ⊗ E sense , (w 1 , e 1 )) and applied in (M 0 , w 2 ) it results in (M 0 ⊗ E sense , (w 2 , e 2 )). The similar action (E sense , {e 1 }) is only applicable in the case where p is true. It can, e.g., be used to model the action of a third agent semi-privately informing agent 1 that p is true.
Formally, an epistemic action (E, E d ) is applicable in (M, w) if there is an applicable event e ∈ E d , meaning that M, w |= pre(e). The application of (E, E d ) in (M, w) then nondeterministically leads to a pointed model (M ⊗ E(w, e)) such that M, w |= pre(e).
Note that any epistemic state represented by a pointed model (M, w), has infinitely many epistemically equivalent representations (i.e., other pointed models that satisfy the exact same set of formulas). It is a central theorem of modal logic that finite models are epistemically equivalent if and only if they are bisimilar. In the following, when using pointed models as states in a transition system, we think of them as representatives of their whole equivalence class. I.e., we consider two epistemic states (M, w) and (M , w ) as identical if they are epistemically equivalent. And we say two epistemic states (M, w) and (M , w ) are indistinguishable for an agent i if there is a world w in M that is indistinguishable to w for agent i such that (M, w ) and (M , w ) are identical. An initial epistemic state together with a set of epistemic actions thus induces a nondeterministic transition system where all states are epistemically different from each other.

[FOND PLANNING]
Our definition of FOND planning loosely follows the conventions of Ghallab et al. (2004). In particular, our actions consist of one common precondition and a set of possible effects, from which one will always be chosen nondeterministically. However, since we want to start out with a formalization that is as close as possible to our DEL formalism, we allow arbitrary propositional formulas as action preconditions and goals. We also use conditional effects which we restrict to effect normal form, which is a special case of Rintanens unary conditionality normal form (Rintanen 2003).
We define a FOND planning task as a tuple F, I, γ, Act where F is a set of fluents (atomic propositions), I ⊆ F is the initial state, γ is a propositional goal formula over F and Act is a set of actions. Each action a = pre a , effs a ∈ Act consists of a propositional formula pre a over F (the precondition) and a set effs a (the conditional effects). Each conditional effect e ∈ effs a is of the form f ∈F (χ e f £ f ) ∧ (χ e ¬f £ ¬f ), where χ e f and χ e ¬f are mutually inconsistent propositional formulas over F (i.e., their conjunction is unsatisfiable). They can be interpreted as ""effect e makes f true under the condition χ e f and false under the condition χ e ¬f "". Such a FOND task induces a finite transition system starting with the initial state I and connecting two states S and S via action a iff S |= pre a and there is an effect e ∈ effs a such that the conditional effects in e transform S to S . This gives us a trivial compilation from FOND to DEL. I.e., we compile the initial state into an epistemic state with exactly one world w 0 where V (p) = {w 0 } iff p ∈ I, or ∅ otherwise. And for each action a ∈ Act, we construct an epistemic action with one event for each nondeterministic effect e ∈ effs a , with precondition pre a and effect {f → χ e f ∨ (f ∧ ¬χ e ¬f ) | f ∈ F}. All events are designated and pairwise distinguishable for all agents. The transition system that we get from our compiled DEL state and actions is isomorphic to the FOND transition system and identified states share the same propositional valuation.
One solution to FOND planning tasks are strong plans. These are partial functions π from states to actions which satisfy the following properties (Cimatti et al. 2003):
• For every state s that is reachable via π from I, there is some state s that is reachable from s via π, s.t. s |= γ.
• There are no cycles, i.e. states s and s such that s is reachable via π from s and s is reachable via π from s.
Since the transition system is finite, following a strong policy always leads to a goal state in finitely many steps. It seems reasonable to assume that the concept of strong policies is also useful for contingent planning over epistemic states.

[IMPLICIT COORDINATION IN DEL]
We define an epistemic planning task as a tuple s 0 , A, ω, γ where s 0 is an epistemic state (the initial state), A is a finite set of epistemic actions (the action library), ω : A → A is a function mapping each action to its owner (the owner function), and γ ∈ L KC is the goal formula. E.g., consider the planning task with s 0 = (M 0 , w 1 ), A = {sense, ann p , ann ¬p } with sense = (E sense , {e 1 , e 2 }), ann p = (E annp , e 1 ) and ann ¬p = (E ann¬p , e 1 ). The actions ann p and ann ¬p are public announcement actions for agent 1, announcing that p is true, or respectively false. That is, the event models E ann¬p and E annp are given as follows:
E annp = e 1 : p, ∅ E ann¬p = e 1 : ¬p, ∅
We assume that all actions are owned by agent 1, i.e., ω = {sense → 1, ann p → 1, ann ¬p → 1}. The goal is for agent 2 to know whether or not p is true, i.e., γ = K 2 p ∨ K 2 ¬p.
A strong policy in the sense of Cimatti et al. (2003) would be to just apply the action ann p in s 0 . This is because the action is applicable in (M 0 , w 1 ) and its application would lead to a successor state consisting of only one world (w 1 , e 1 ) in which p (and K 2 p) is true. We argue that from the perspective of the agents (who initially do not know whether p is true or false), this is not a reasonable solution. If we want agent 1 to be able to come up with the plan for himself, we must consider his incomplete knowledge about the situation. Intuitively, a good plan for agent 1 is to first apply the sensing action and then, depending on the sensing result, apply the action ann p or ann q . This plan works for both states (M, w 1 ) and (M, w 2 ), which agent 1 considers possible.
To capture this, we have to require uniform policies. A uniform policy is a partial function π from epistemic states to sets of epistemic actions, satisfying the following constraints:
• Applicability: for each state s, and action a ∈ π(s), the action a has to be applicable in state s. • Uniformity: for each state s, and action a ∈ π(s), and states s that are indistinguishable to s for the owner ω(a) of the action, also a ∈ π(s ). This definition ensures that the agents can always infer from their own knowledge whether or not and how the policy wants them to act. This also implies that an action is only applicable by an agent, if the agent knows that the action is applicable. Note that because of the uniformity constraint, it is necessary to allow policies to assign multiple actions per state. E.g., sometimes we want a policy to assign an action a of agent 1 to some state s and an action b of agent 2 to some state s . Then by uniformity, if there is a state s that is indistinguishable to s for agent 1 and to s for agent 2, we have to assign both a and b to s .
We then say a uniform policy is subjectively strong, if it satisfies the exact properties of strong plans, but based on subjective reachability: A state s is a subjective successor of s given an action a if there is a successor state of s and a that is indistinguishable to s for agent ω(a). I.e., in our example, the subjective successors of (M 0 , w 1 ) and (E sense , {e 1 , e 2 }) are exactly the states (M 0 ⊗ E sense , (w 1 , e 1 )) and (M 0 ⊗ E sense , (w 2 , e 2 )). A state s is then subjectively reachable from s if either s is identical to s or s is subjectively reachable from a subjective successor of s.
In particular, a policy π that is subjectively strong for an epistemic planning task s 0 , A, ω, γ guarantees for each subjectively reachable state s and action a ∈ π(s), that π is also subjectively strong for s, A, ω, γ , as well as for all planning tasks s , A, ω, γ with an initial state s that is indistinguishable to s for ω(a).

[A DECIDABLE FRAGMENT OF DEL PLANNING]
A straight-forward way to obtain a decidable fragment of DEL planning is to ensure that the induced transition system is finite. This can be done by restricting the action set in such a way that the application of a single action can never lead to a state where the number of worlds is greater than in the state in which the action was applied. We achieve this by requiring our event models to be partitioned into disjoint connected components with mutually inconsistent preconditions. This allows us to think of each of the components as a single nondeterministic effect. Consider the following event model:
E pp = e 1 : p, ∅ e 2 : ¬p, ∅ e 3 : p, ∅2
For example, the action (E pp , {e 1 , e 3 }) could model an agent 3 trying to semi-privately announce p to agent 1. However, there is the possibility that the confidentiality of the announcement is compromised and p is thus effectively publicly announced. If we apply this action in
(M 0 , w 1 ), it re- sults either in (M 0 ⊗ E pp , (w 1 , e 1 )) or (M 0 ⊗ E pp , (w 1 , e 3 ))where
M 0 ⊗ E pp = (w 1 , e 1 ) : p (w 2 , e 2 ) : (w 1 , e 3 ) : p 2
Formally, for actions ( E, (Q i ), pre, eff , E d ) from our action set, we require that the domain E can be partitioned into disjoint subsets E 1 , . . . , E k such that (1) for each pair of events e, e ∈ E j from the same component j ∈ {1, . . . , k}, the preconditions pre(e) and pre(e ) are mutually inconsistent, and (2) two events e, e ∈ E are only allowed to be indistinguishable for an arbitrary agent i ∈ A, i.e. eQ i e , if they belong to the same component, i.e if there exists a j ∈ {1, . . . k} such that e, e ∈ E j .
We can see that if we apply such an action to an arbitrary epistemic state, due to condition (1), each world will be paired up by maximally one of the events of each component. Furthermore, due to condition (2), two worlds can only be distinguishable for any agent if the events they were generated with are from the same component. Thus the state resulting from an action application will consist of at most k connected components which consist each of less or equally many worlds than the original state. Since we can throw away all components that do not contain the updated designated world, we obtain a state that can be represented by less than or equally many worlds as the original state.
Our fragment is a generalization of the fragment introduced by Kominis and Geffner (2015), which allows exactly those actions that can be described with only mutually inconsistent preconditions (even between events from different components) and where all actions are thus deterministic.

[COMPILATION TO FOND]
In the following we will show how to generate a FOND planning task F, I, γ * , Act , given an epistemic planning task s 0 , A, ω, γ from our fragment.

[COMPILATION OF EPISTEMIC STATES]
We use the approach of Kominis and Geffner (2015) to represent epistemic states as classical states. The idea is that we generate fluents directly from the worlds and indistinguishability relation of the initial state s 0 , such that we can use them to encode the valuation functions and indistinguishability relations of arbitrary states reachable from s 0 .
Given the initial state s 0 = ( W, (R i ), V , w 0 ), we introduce a fluent p w ∈ F (read: ""p is true in world w"") for each proposition p ∈ P and world w ∈ W . Similarly, we introduce a fluent D {w1,w2} i (read: ""w 1 is distinguishable to w 2 for agent i"") for each agent i ∈ A and worlds w 1 , w 2 ∈ W with w 1 R i w 2 . Finally, for each world w ∈ W , we introduce the fluent w * ∈ F (read: ""w is the designated world"").
A propositional state S ⊆ F then represents an epistemic state ( W,
(R i ), V , w) where (1) w ∈ V (p) iff p w ∈ S, (2) w 1 R i w 2 iff D {w1,w2} i
∈ S, and (3) w = w iff w * ∈ S. For example, if s 0 = (M 0 , w 1 ), we will generate the set of fluents F = {p w1 , p w2 , D
{w1,w2} 1 , D {w1,w2} 2 , w * 1 , w * 2 }.
The initial state s 0 will then be I = {p w1 , w * 1 }.

[COMPILATION OF EPISTEMIC FORMULAS]
To check whether a propositional formula φ is true in world w of an epistemic state that is represented by a classical state S ⊆ F is simple. We replace the occurrences of each proposition p in φ by p w and check the resulting formula in S.
Checking formulas with knowledge operators is slightly more complicated. Kominis and Geffner (2015) use axioms to compile away all knowledge subformulas into derived variables, the values of which can be inferred in polynomial time.
We will simply assume that all of this is given and that we can thus compile each epistemic formula to a formula φ w that evaluates to true in a classical state representing an epistemic state (M, w 0 ) iff M, w |= φ.
For evaluating a formula directly in the designated world of a state (e.g., the goal formula), we use φ * , which we define as ( w∈W w * ) ∧ w∈W (w * → φ w ).

[COMPILATION OF EPISTEMIC ACTIONS]
We now show how an action a = ( E, (Q i ), pre, eff , E d ) that can be partitioned into distinct components E 1 , . . . , E k accordingly to our fragment, can be compiled into a FOND action pre a , effs a .
We know that an action is applicable in a state (M, w) if there is some event e ∈ E d such that M, w |= pre(e). We directly translate this to pre a = e∈E d pre(e) * . We can then translate each of the components of our event model into a different nondeterministic effect, i.e., we get effs a = {eff j | j = 1, ..., n}. These nondeterministic effects can make propositions true or false, as well as make worlds distinguishable or completely inaccessible. We construct each nondeterministic effect eff j as follows:
eff j = eff P + j ∧ eff P − j ∧ eff D+ j ∧ eff × j ∧ eff × × j
First, each fluent p w is made true or false accordingly to the effects of the event e ∈ E j that is applied in w.
eff P + j = w∈W
p∈P (∨ e∈Ej (pre(e) w ∧ eff(e, p) w ) £ p w ) eff P − j = w∈W p∈P (∨ e∈Ej (pre(e) w ∧ ¬ eff(e, p) w ) £ ¬p w )
Two worlds w and w become distinguishable if the events e and e they were updated with are distinguishable:
eff D+ j = w,w ∈W i∈A,w =w     
e,e ∈Ej ¬eQie (pre(e) w ∧ pre(e ) w ) £D
{w,w } i     
If in some world w, none of the events from E j are applicable, the world should not have a successor. We simulate this by making w distinguishable from all other worlds.
eff × j = w,w ∈W w =w ∧ e∈Ej ¬ pre(e) w £ D {w,w } i
If for the designated world w, there is no applicable event in E j , there should not even be a corresponding successor state. We model this by completely removing the designation w * . Thus, while the effect is still applicable, it leads to a state where all formulas φ * evaluate to false and therefore no actions are applicable and the goal is not satisfied.
eff × × j = ∧ e∈Ej ¬ pre(e) w £ ¬w *

[COMPILATION OF POLICIES]
Our compilation guarantees that the nondeterministic outcomes of an action that is applied in a propositional state corresponds exactly to the nondeterministic outcomes of the original epistemic action applied to the original epistemic state. Thus any strong policy for an epistemic planning task automatically corresponds to a strong policy in its FOND compilation. I.e., we can start in the initial state of the FOND compilation and extract a policy by successively applying the actions assigned by the original policy.
For the other direction, we can proceed similarly. However, we have to be careful about the fact that the policy can can contain multiple propositional states representing the same epistemic state. E.g., consider the states {p w1 , w * 1 } and {p w2 , w * 2 }. Having equivalent states in a policy is unproblematic, as long as one is never reachable from the other. To obtain a strong policy for the original problem, we can apply the same policy extraction procedure from above but ignore each state if we have seen an equivalent state before.
If the policy in our FOND compilation contains equivalent states such that one is reachable from the other, the extraction gets more difficult, as we have to take care of not introducing cycles into our policy. Fortunately, it is easy to argue that if there is no strong policy in the FOND compilation which doesn't include equivalent states that are reachable from each other, there will also be no strong policy that includes them. This is because the transition system looks exactly the same from these states and we do not gain anything from getting from one of the states to the other. This means that if there is a strong policy for the FOND compilation of an epistemic planning task, there also has to exist a strong policy that does not contain epistemically equivalent reachable states. Moreover, if the strong policy in the FOND compilation is optimal (i.e., its tree representation has minimal depth), it is clear that the policy cannot contain equivalent states that are reachable from each other. We thus obtain the following theorem.
Theorem 1. Let Π be an epistemic planning task from our fragment. Then there exists a strong policy for Π if and only if there exists a strong policy for the FOND compilation of Π. Any optimal strong policy for Π directly corresponds to an optimal strong policy for its compilation and vice versa.
The following theorem follows, given the EXPTIMEcompleteness of the plan existence problem for strong planning in FOND (Rintanen 2004).
Theorem 2. In our epistemic planning fragment, the problem of deciding whether there exists a strong policy for a given planning task is EXPTIME-complete.

[PLANNING FOR IMPLICIT COORDINATION]
As explained in our section about planning for implicit coordination, strong policies are not suitable if we want the agents to coordinate implicitly. In this section, we show how to use FOND planning to find subjectively strong plans for epistemic planning tasks from our fragment.

[THE COMPILATION]
We use the same compilation of states and formulas as before. However, we slightly modify the compilation of actions. The idea is to split each action from the action set into two: One auxiliary action for choosing the action that we want to apply in a state and one action that actually applies the effects of the previously selected action. In each choice action, we additionally simulate a perspective shift: We change the designated world nondeterministically to any of the worlds that are indistinguishable for the owner agent of the action. Thus, subjective successors of an action in the original problem are now objective successors.
This means that any strong policy in the compilation will correspond to a subjectively strong policy in the original problem. We can extract such a policy by taking all the apply-actions from our policy and and assigning the corresponding actions to the corresponding states in the original planning task.
Theorem 3. In our epistemic planning fragment, the problem of deciding whether there exists a subjectively strong policy is EXPTIME-complete.

[EXAMPLE: MAPF/DU]
We demonstrate our approach by modeling an instance of multi-agent path finding with destination uncertainty. The problem was first described by Bolander et al. (2018) and more thoroughly analyzed by Nebel et al. (2019). It is a generalization of the multi-agent path finding problem, relaxing the assumption that the agents' goals are commonly known. Instead, we assume that there are pairwise disjoint sets of plausible goal candidates for each agent, which are commonly known. Also, each agent can identify its own goal. As final action, each agent is allowed to announce that he has arrived at its true destination. The joint goal for the agents is that each agent is at his own true goal. Nebel et al. showed that the plan existence problem is PSPACE-complete. The naive algorithm they proposed has a runtime complexity of O(n a 2 +a ) where n is the number of graph vertices and a is the number of agents. Figure 1 shows an example of a MAPF/DU instance with two agents. The goal candidates of the square agent are r and b, and the goal candidates of the circle agent are l and m. One subjectively strong policy is for the square agent to first go to b and to let the circle agent move to l, independently of the actual destinations of the agents. Then, the square agent goes to its true destination (which, depending on the designated world, will be either r or b) and announces success there. Afterwards, the circle agent can go to his true destination (which will be either l or m). Note that after the initial movements of the square agent, the policy has to consider all 4 possible goal combinations. This is because the square agent does not know the actual goal of the circle agent and the circle agent will not know the actual goal of the square agent.
We now show how this problem can be modeled in PDDL (McDermott 1998). We will use the types agt for agents, pos for positions, and wld for worlds. We introduce fluents (at ?a ?p) to denote that agent ?a is at position ?p, (adj ?p ?q) to denote that an agent can step from position ?p to position ?q, and (announced ?a) to denote that the agent ?a has already announced success and will not move any longer. Furthermore, we use (goal ?w ?a ?p) to denote that the actual goal of agent ?a in world ?w is position ?p.
To denote indistinguishability of two worlds ?w1 and ?w2 for agent ?a, we use the fluent (ind ?a ?w1 ?w2). We mark the designated world ?w using the fluent (des ?w).
Finally, we use the predicates (next-choose), (next-move ?a ?p1 ?p2) and (next-announce ?a) to enforce the alternation of auxiliary perspectiveshifting actions and actual actions.
We now show how to split up the movement actions into the actions choose-move and move. The action (choose-move ?a ?w ?p ?q) simulates a perspec-tive shift to agent ?a by nondeterministically switching to an arbitrary world that is indistinguishable from the designated world for agent ?a. Furthermore, by setting the fluent (next-move ?a ?p ?q) to true, it enforces a movement action for agent ?a from ?p to ?q in the successor state. (and (not (des ?w)) (des w2))) ; ... Unfortunately, we have to enumerate all possible worlds to simulate the perspective shift. This forces us to include the worlds as constants into the domain definition. It would be more convenient if we had a dedicated construct in PDDL to automatically generate nondeterministic effects, e.g., by explicitly quantifying over objects (in our case, worlds).
The move action, which has to be applied afterwards, performs the actual change of the agent's position. This action also contains the actual precondition for movement actions: the field to move to has to be adjacent and empty. Also, the action prescribes the next action to be again a choose action by setting the fluent next-choose to true. The actions choose-announce and announce can be defined similarly. Announcing works by making all worlds where the agent has a different goal than its current position distinguishable to any other world for all agents. E.g., our example instance from Figure 1 can then be defined using the following initial state and goal descriptions: We tested our MAPF/DU planning domain using the myND planner of Mattmüller et al. (2010), which is to the Experiment time 2 agents, 4 cells, and 4 worlds 0.55s 3 agents, 6 cells, and 8 worlds 11.5s best of our knowledge the only publicly available FOND planner that supports both strong acyclic plans as well as conditional effects. It also supports axioms, although we did not need them for our example. Table 1 shows the performance of the planner on the example instance from Figure 1 as well as on a slightly bigger version with three agents.

[CONCLUSION]
In our paper, we have shown a decidable fragment of strong epistemic planning that has the same complexity than strong planning in FOND. We have also demonstrated how FOND planning can be used to generate subjectively strong plans. For future work, it is worth noticing that DEL can be used for modeling games. In particular, there is a translation from the game description language GDL-III to DEL (Engesser et al. 2018). There are some very interesting games which fall within our decidable fragment, one of which is Hanabi, which has gained some attention recently (Bard et al. 2019). While using a FOND planner does not seem to be feasible for problems of this size, it will be interesting to investigate how the idea of simulating perspective taking via nondeterminism can be incorporated into techniques such as Monte Carlo tree search or model-based reinforcement learning (e.g., value iteration in fully-observable MDPs).","[TITLE]
Planning for Implicit Coordination using FOND

[ABSTRACT]
Epistemic Planning can be used to achieve implicit coordination in cooperative multi-agent settings where knowledge and capabilities are distributed between the agents. In these scenarios, agents plan and act on their own without having to agree on a common plan or protocol beforehand. However, epistemic planning is undecidable in general. In this paper, we identify a decidable fragment of epistemic planning that allows for arbitrary initial state uncertainty and nondeterminism, but where actions can never increase the uncertainty of the agents. We show that in this fragment, planning with and without implicit coordination can be reduced to fully observable nondeterministic (FOND) planning and that it shares the same computational complexity. We also provide a small case study, modeling the problem of multi-agent path finding with destination uncertainty in FOND, to show that our compilation approach can be successfully applied in practice."
Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Hyl_vjC5KQ.json,"Revision: The authors addressed most of my concerns and clearly put in effort to improve the paper. The paper explains the central idea better, is more precise in terminology in general, and the additional ablation gives more insight into the relative importance of the advantage weighting. I still think that the results are a bit limited in scope but the idea is interesting and seems to work for the tasks in the paper. I adjusted my score to reflect this.

Summary:
The paper proposes an HRL system in which the mutual information of the latent (option) variable and the state-action pairs is approximately maximized. To approximate the mutual information term, samples are reweighted based on their estimated advantage. TD3 is used to optimize the modules of the system. The system is evaluated on continuous control task from OpenAI gym and rllab.

For the most part, the paper is well-written and it provides a good overview of related work and relevant terminology. The experiments seem sound even though the results are not that impressive. The extra analysis of the option space and temporal distribution is interesting. 

Some parts of the theoretical justification for the method are not entirely clear to me and would benefit from some clarification. Most importantly, it is not clear to me why the policy in Equation 7 is considered to be optimal. Given some value or advantage function, the optimal policy would be the one that picks the action that maximizes it. The authors refer to earlier work in which similar equations are used, but in those papers this is typically in the context of some entropy maximizing penalty or KL constraint. A temperature parameter would also influence the exploration-exploitation trade-off in this ‘optimal’ policy. I understand that the rough intuition is to take actions with higher advantage more often while still being stochastic and exploring but the motivation could be more precise given that most of the subsequent arguments are built on top of it. However, this is not the policy that is used to generate behavior. In short, the paper is clear enough about how the method is constructed but it is not very clear to me *why* the mutual information should be optimized with respect to this 'optimal' policy instead of the actual policy one is generating trajectories from.

HRL is an interesting area of research with the potential to learn complicated behaviors. However, it is currently not clear how to evaluate the importance/usefulness of hierarchical RL systems directly and the tasks in the paper are still solvable by standard systems. That said, the occasional increase in sample efficiency over plain TD3 looks promising. It is somewhat disappointing that the number of beneficial option is generally so low. To get more insight in the methods it would have been nice to see a more systematic ablation of related methods with different mutual information pairings (action or state only) and without the advantage weighting. Could it be that the number of options has to remain limited because there is no parameter sharing between them? It would be interesting to see results on more challenging control problems where the hypothesized multi-modal advantage structure is more likely to be present.

All in all I think that this is an interesting paper but the foundations of the theoretical motivation need a bit more clarification. In addition, experiments on more challenging problems and a more systematic comparison with similar models would make this a much stronger paper.

Minor issues/typos:
- Contributions 2 and 3 have a lot of overlap.
- The ‘o’ in Equation 2 should not be bold font. 
- Appendix A. Shouldn’t there be summations over ‘o’ in the entropy definitions?


","[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Schematic sketch of our HRL approach. By using the advantage-weighted importance, the problem of finding the modes of the advantage-function can be reduced to that of finding the modes of the density of state action pairs.
Table 2: empirically show that this regularization improves the performance of learning latent discrete representations. When computing MI, we need to compute p(o) and H(o|s, a) given by p(o) = p πAd (s, a)p(o|s, a; η)dads = E (s,a)∼p π Ad (s,a) [p(o|s, a; η)] (8) H(o|s, a) = E (s,a)∼p π Ad (s,a) [p(o|s, a; η) log p(o|s, a; η)] .
Table 3: Figure 2 :2Figure 2: Activation of the four options over time steps on the Walker2d task.
Table 4: The activation of options over time and snapshots of the learned option policies on the Walker2d task are shown in Figure2, which visualizes the result from adInfoHRL with four options. One can see that the option policies are activated in different phases of locomotion. While the option indicated by yellow in Figure2corresponds to the phase for kicking the floor, the option indicated by blue corresponds to the phase when the agent was on the fly. Visualization of the options learned on the HalfCheetah and Ant tasks are shown in Appendix D.
Table 5: Output of the option network in the state-action space on Walker2d. Activation of options in the state space on Walker2d.
Table 6: Figure 3 :3Figure 3: Performance of adInfoHRL. (a)-(d) show comparison with baseline methods. (e) and (f) show the output of the option network and the activation of options on Walker2d, respectively.
Table 7: mutual information (MI) between the latent variable o and the state action pair (s, a) is defined as I (s, a), o = H(o) − H(o|s, a) (20) where H(o) = p(o) log p(o)do and H(o|s, a) = p(o|s, a) log p(o|s, a)do. We make the empirical estimate of MI employed by Gomes et al. (2010); Hu et al. (2017) and modify it to employ the importance weight. The empirical estimate of MI with respect to the density induced by a policy π is given byÎ (s, a; o) = o∈Op (o) logp(o) −Ĥ(o|s, a). (21) We consider the case where we have samples collected by a behavior policy β(s|a) and need to estimate MI with respect to the density induced by policy π. Given a model p(o|s, a; η) parameterized by vector η, p(o) can be rewritten as p(o) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η)dads = E [W (s, a)p(o|s, a; η)] ,
Table 8: whereW (s, a) =W (s,a) N j=1W (sj ,aj ) is the normalized importance weight. Likewise, the conditional entropy with respect to the density induced by a policy π is given by H(o|s, a) = p π (s, a)p(o|s, a; η) log p(o|s, a; η)dsda (24) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η) log p(o|s, a; η)dsda (25) = E [W (s, a)p(o|s, a; η) log p(o|s, a; η)] .
Table 9: o∈O π(o|s)π(a|s, o)Q π (s, a)da = o∈O π(o|s) π(a|s, o)Q π (s, a)da (28) Since option policies are deterministic given by µ o θ (s), the state-value function is given by V (s) = o∈O π(o|s)Q π (s, µ o θ (s))da. (29)
Table 10: Activation of options in the state space on HalfCheetah-v1.
Table 11: Figure 4 :4Figure 4: Distribution of options on the HalfCheetah-v1 task using adInfoHRL with two options. The dimensionality is reduced by t-SNE for visualization.
Table 12: Figure 5 :5Figure 5: Activation of options over time steps on the HalfCheetah-v1 task using adInfoHRL with two options.
Table 13: The outputs of the option network and the activation of options on Walker2d are shown in Fig-
Table 14: Hyperparameters of adInfoHRL used in the experiment.
Table 15: Hyperparameters of PPO used in the experiment. We tuned hyperparameters for our tasks, which are defferent from the default parameters in OpenAI baselines(Dhariwal et al., 2017).

[INTRODUCTION]
Reinforcement learning (RL) has been successfully applied to a variety of tasks, including board games , robotic manipulation tasks (Levine et al., 2016), and video games (Mnih et al., 2015). Hierarchical reinforcement learning (HRL) is a type of RL that leverages the hierarchical structure of a given task by learning a hierarchical policy (Sutton et al., 1999;Dietterich, 2000). Past studies in this field have shown that HRL can solve challenging tasks in the video game domain (Vezhnevets et al., 2017;Bacon et al., 2017) and robotic manipulation (Daniel et al., 2016;Osa et al., 2018b). In HRL, lower-level policies, which are often referred to as option policies, learn different behavior/control patterns, and the upper-level policy, which is often referred to as the gating policy, learns to select option policies. Recent studies have developed HRL methods using deep learning (Goodfellow et al., 2016) and have shown that HRL can yield impressive performance for complex tasks (Bacon et al., 2017;Frans et al., 2018;Vezhnevets et al., 2017;Haarnoja et al., 2018a). However, identifying the hierarchical policy structure that yields efficient learning is not a trivial task, since the problem involves learning a sufficient variety of types of behavior to solve a given task.
In this study, we present an HRL method via the mutual information (MI) maximization with advantage-weighted importance, which we refer to as adInfoHRL. We formulate the problem of learning a latent variable in a hierarchical policy as one of learning discrete and interpretable repre-sentations of states and actions. Ideally, each option policy should be located at separate modes of the advantage function. To estimate the latent variable that corresponds to modes of the advantage function, we introduce advantage-weighted importance weights. Our approach can be considered to divide the state-action space based on an information maximization criterion, and it learns option policies corresponding to each region of the state-action space. We derive adInfoHRL as an HRL method based on deterministic option policies that are trained based on an extension of the deterministic policy gradient (Silver et al., 2014;Fujimoto et al., 2018). The contributions of this paper are twofold:
1. We propose the learning of a latent variable of a hierarchical policy as a discrete and hidden representation of the state-action space. To learn option policies that correspond to the modes of the advantage function, we introduce advantage-weighted importance. 2. We propose an HRL method, where the option policies are optimized based on the deterministic policy gradient and the gating policy selects the option that maximizes the expected return. The experimental results show that our proposed method adInfoHRL can learn a diversity of options on continuous control tasks. Moreover, our approach can improve the performance of TD3 on such tasks as the Walker2d and Ant tasks in OpenAI Gym with MuJoco simulator.

[BACKGROUND]
In this section, we formulate the problem of HRL in this paper and describe methods related to our proposal.

[HIERARCHICAL REINFORCEMENT LEARNING]
We consider tasks that can be modeled as a Markov decision process (MDP), consisting of a state space S, an action space A, a reward function r : S × A → R, an initial state distribution ρ(s 0 ), and a transition probability p(s t+1 |s t , a t ) that defines the probability of transitioning from state s t and action a t at time t to next state s t+1 . The return is defined as R t =
T i=t γ i−t r(s i , a i ), where γ is a discount factor, and policy π(a|s) is defined as the density of action a given state s. Let d π (s) = T t=0 γ t p(s t = s) denote the discounted visitation frequency induced by the policy π. The goal of reinforcement learning is to learn a policy that maximizes the expected return J(π) = E s0,a0,... [R 0 ] where s 0 ∼ ρ(s 0 ), a ∼ π and s t+1 ∼ p(s t+1 |s t , a t ). By defining the Q-function as Q π (s, a) = E s0,a0,... [R t |s t = s, a t = a], the objective function of reinforcement learning can be rewritten as follows:
J(π) = d π (s)π(a|s)Q π (s, a)dads.(1)
Herein, we consider hierarchical policy π(a|s) = o∈O π(o|s)π(a|s, o), where o is the latent variable and O is the set of possible values of o. Many existing HRL methods employ a policy structure of this form (Frans et al., 2018;Vezhnevets et al., 2017;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016). In general, latent variable o can be discrete (Frans et al., 2018;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016;Osa & Sugiyama, 2018) or continuous (Vezhnevets et al., 2017). π(o|s) is often referred to as a gating policy (Daniel et al., 2016;Osa & Sugiyama, 2018), policy over options (Bacon et al., 2017), or manager (Vezhnevets et al., 2017). Likewise, π(a|s, o) is often referred to as an option policy (Osa & Sugiyama, 2018), sub-policy (Daniel et al., 2016), or worker (Vezhnevets et al., 2017). In HRL, the objective function is given by
J(π) = d π (s) o∈O π(o|s)π(a|s, o)Q π (s, a)dads.(2)
As discussed in the literature on inverse RL (Ziebart, 2010), multiple policies can yield equivalent expected returns. This indicates that there exist multiple solutions to latent variable o that maximizes the expected return. To obtain the preferable solution for o, we need to impose additional constraints in HRL. Although prior work has employed regularizers (Bacon et al., 2017) and constraints (Daniel et al., 2016) to obtain various option policies, the method of learning a good latent variable o that improves sample-efficiency of the learning process remains unclear. In this study we propose the learning of the latent variable by maximizing MI between latent variables and state-action pairs.

[DETERMINISTIC POLICY GRADIENT]
The deterministic policy gradient (DPG) algorithm was developed for learning a monolithic deterministic policy µ θ (s) : S → A by Silver et al. (2014). In off-policy RL, the objective is to maximize the expectation of the return, averaged over the state distribution induced by a behavior policy β(a|s):
J(π) = d β (s)π(a|s)Q π s, a)dads.(3)
When a policy is deterministic, the objective becomes J(π) = d β (s)Q π s, µ θ (s) ds. Silver et al. (2014) have shown that the gradient of a deterministic policy is given by
∇ θ E s∼d β (s) [Q π (s, a)] = E s∼d β (s) ∇ θ µ θ (s)∇ a Q π s, a | a=µ θ (s) .(4)
The DPG algorithm has been extended to the deep deterministic policy gradient (DDPG) for continuous control problems that require neural network policies . Twin Delayed Deep Deterministic policy gradient algorithm (TD3) proposed by Fujimoto et al. (2018) is a variant of DDPG that outperforms the state-of-the-art on-policy methods such as TRPO (Schulman et al., 2017a) and PPO (Schulman et al., 2017b) in certain domains. We extend this deterministic policy gradient to learn a hierarchical policy.

[REPRESENTATION LEARNING VIA INFORMATION MAXIMIZATION]
Recent studies such as those by ; Hu et al. (2017); Li et al. (2017) have shown that an interpretable representation can be learned by maximizing MI. Given a dataset X = (x 1 , ..., x n ), regularized information maximization (RIM) proposed by Gomes et al. (2010) involves learning a conditional modelp(y|x; η) with parameter vector η that predicts a label y. The objective of RIM is to minimize
(η) − λI η (x, y),(5)
where (η) is the regularization term, I η (x, y) is MI, and λ is a coefficient. MI can be decomposed as I η (x, y) = H(y) − H(y|x) where H(y) is entropy and H(y|x) the conditional entropy. Increasing H(y) conduces the label to be uniformly distributed, and decreasing H(y|x) conduces to clear cluster assignments. Although RIM was originally developed for unsupervised clustering problems, the concept is applicable to various problems that require learning a hidden discrete representation.
In this study, we formulate the problem of learning the latent variable o of a hierarchical policy as one of learning a latent representation of the state-action space.

[LEARNING OPTIONS VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
In this section, we propose a novel HRL method based on advantage-weighted information maximization. We first introduce the latent representation learning via advantage-weighted information maximization, and we then describe the HRL framework based on deterministic option policies.

[LATENT REPRESENTATION LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
Although prior work has often considered H(o|s) or I(s, o), which results in a division of the state space, we are interested in using I (s, a), o for dividing the state-action space instead. A schematic sketch of our approach is shown in Figure 1. As shown in the left side of Figure 1, the advantage function often has multiple modes. Ideally, each option policies should correspond to separate modes of the advantage function. However, it is non-trivial to find the modes of the advantage function in practice. For this purpose, we reduce the problem of finding modes of the advantage function to that of finding the modes of the probability density of state action pairs.
We consider a policy based on the advantage function of the form  where
π Ad (a|s) = f A π (s, a) Z ,(6)
A π (s, a) = Q π (s, a) − V π (s) is the advantage function, V π (s)
is the state value function, and Z is the partition function. f (•) is a functional, which is a function of a function. f (•) is a monotonically increasing function with respect to the input variable and always satisfies f (•) > 0.
In our implementation we used the exponential function f (•) = exp(•). When following such a policy, an action with the larger advantage is drawn with a higher probability. Under this assumption, finding the modes of the advantage function is equivalent to finding modes of the density induced by π Ad . Thus, finding the modes of the advantage function can be reduced to the problem of clustering samples induced by π Ad .
Following the formulation of RIM introduced in Section 2.3, we formulate the problem of clustering samples induced by π Ad as the learning of discrete representations via MI maximization. For this purpose, we consider a neural network that estimates p(o|s, a; η) parameterized with vector η, which we refer to as the option network. We formulate the learning of the latent variable o as minimizing L option (η) = (η) − λI o, (s, a); η ,
where I(o, (s, a)) =Ĥ(o|s, a; η) −Ĥ(o; η), and (η) is the regularization term. In practice, we need to approximate the advantage function, and we learn the discrete variable o that corresponds to the modes of the current estimate of the advantage function. For regularization, we used a simplified version of virtual adversarial training (VAT) proposed by Miyato et al. (2016). Namely, we set (η) = D KL p(o|s noise , a noise ; η)||p(o|s, a; η) where s noise = s + s , a noise = a + a , s and a denote white noise. This regularization term penalizes dissimilarity between an original state-action pair and a perturbed one, and Hu et al. (2017) 
Thus, the probability density of (s, a) induced by π Ad is necessary for computing MI for our purpose. To estimate the probability density of (s, a) induced by π Ad , we introduce the advantageweighted importance in the next section.

[IMPORTANCE WEIGHTS FOR MUTUAL INFORMATION ESTIMATION]
Although we show that the problem of finding the modes of the advantage function can be reduced to MI maximization with respect to the samples induced by π Ad , samples induced by π Ad are not available in practice. While those induced during the learning process are available, a discrete representation obtained from such samples does not correspond to the modes of the advantage function.
To estimate the density induced by π Ad , we employ an importance sampling approach.
We assume that the change of the state distribution induced by the policy update is sufficiently small, namely, d πAd (s) ≈ d β (s). Then, the importance weight can be approximated as
W (s, a) = p πAd (s, a) p β (s, a) = d πAd (s)π Ad (a|s) d β (s)β(a|s) ≈ π Ad (a|s) β(a|s) = f (A(s, a)) Zβ(a|s) . (10
)
and the normalized importance weight is given gỹ
W (s, a) = W (s, a) N j=1 W (s j , a j ) = f (A(s,a)) Zβ(a|s) N j=1 f (A(sj ,aj )) Zβ(aj |sj ) = f (A(s,a)) β(a|s) N j=1 f (A(sj ,aj )) β(aj |sj ) . (11
)
As the partition function Z is canceled, we do not need to compute Z when computing the importance weight in practice. We call this importance weight W the advantage-weighted importance and employ it to compute the objective function used to estimate the latent variable.
This advantage-weighted importance is used to compute the entropy terms for computing MI in Equation ( 7). The empirical estimate of the entropy H(o) is given bŷ
H(o; η) = − o∈Op (o; η) logp(o; η), wherep(o; η) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η). (12
)
where the samples (s i , a i ) are drawn from p β (s, a) induced by a behavior policy β(a|s). Likewise, the empirical estimate of the conditional entropy H(o|s, a) is given bŷ
H(o|s, a; η) = 1 N N i W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(13)
The derivations of Equations ( 12) and ( 13) are provided in Appendix A. To train the option network, we store the samples collected by the M most recent behavior policies, to which we refer as onpolicy buffer D on . Although the algorithm works with entire samples stored in the replay buffer, we observe that the use of the on-policy buffer for latent representation learning exhibits better performance. For this reason, we decided to use the on-policy buffer in our implementation. Therefore, while the algorithm is off-policy in the sense that the option is learned from samples collected by behavior policies, our implementation is ""semi""on-policy in the sense that we use samples collected by the most recent behavior policies.

[HRL OBJECTIVE WITH DETERMINISTIC OPTION POLICIES]
Instead of stochastic option policies, we consider deterministic option policies and model them using separate neural networks. We denote by π(a|s, o) = µ o θ (s) deterministic option policies parameterized by vector θ. The objective function of off-policy HRL with deterministic option policies can then be obtained by replacing π(a|s) with o∈O π(o|s)π(a|s, o) in Equation (3):
J(w, θ) = d β (s) o∈O π(o|s)Q π s, µ o θ (s); w ds, (14
)
where Q π (s, a; w) is an approximated Q-function parameterized using vector w. This form of the objective function is analogous to Equation (3). Thus, we can extend standard RL techniques to the learning of the gating policy π(o|s) in HRL with deterministic option policies.
In HRL, the goal of the gating policy is to generate a value of o that maximizes the conditional expectation of the return:
Q π Ω (s, o) = E [R|s t = s, o t = o] = π(a|s, o)Q π (s, a)da,(15)
which is often referred to as the option-value function (Sutton et al., 1999). When option policies are stochastic, it is often necessary to approximate the option-value function Q π Ω (s, o) in addition to the action-value function Q π (s, a). However, in our case, the option-value function for deterministic option policies is given by
Q π Ω (s, o) = Q π (s, µ o θ (s)),(16)
Algorithm 1 HRL via Advantage-Weighted Information Maximization (adInfoHRL)
Input: Number of options O, size of on-policy buffer Initialize: Replay buffer D R , on-policy buffer D on , network parameters η, θ, w, θ target , w target repeat for t = 0 to t = T do Draw an option for a given s by following Equation 17: o ∼ π(o|s) Draw an action a ∼ β(a|s, o) = µ o θ (s) + Record a data sample (s, a, r, s ) Aggregate the data in D R and D on if the on-policy buffer is full then Update the option network by minimizing Equation ( 7) for samples in D on Clear the on-policy buffer D on end if Sample a batch D batch ∈ D R Update the Q network parameter w if t mod d then Estimate p(o|s i , a i ) for (s i , a i ) ∈ D batch using the option network Assign samples (s i , a i ) ∈ D batch to the option o * = arg max p(o|s i , a i ) Update the option policy networks µ o θ (s) for o = 1, ..., O with Equation ( 19) Update the target networks:
w target ← τ w +(1−τ )w target , θ target ← τ θ +(1−τ )θ target end if end for until the convergence return θ
which we can estimate using the deterministic option policy µ o θ (s) and the approximated actionvalue function Q π (s, a; w). In this work we employ the softmax gating policy of the form
π(o|s) = exp Q π (s, µ o θ (s); w) o∈O exp Q π s, µ o θ (s); w ,(17)
which encodes the exploration in its form (Daniel et al., 2016). The state value function is given as
V π (s) = o∈O π(o|s)Q π (s, µ o θ (s); w),(18)
which can be computed using Equation (17). We use this state-value function when computing the advantage-weighted importance as A(s, a) = Q(s, a) − V (s). In this study, the Q-function is trained in a manner proposed by Fujimoto et al. (2018). Two neural networks (Q π w1 , Q π w2 ) are trained to estimate the Q-function, and the target value of the Q-function is computed as y i = r i + γ min 1,2 Q(s i , a i ) for sample (s i , a i , a i , r i ) in a batch sampled from a replay buffer, where r i = r(s i , a i ). In this study, the gating policy determines the option once every N time steps, i.e., t = 0, N, 2N, . . .

[NEURAL NETWORKS THAT MODEL Μ O]
θ (a|s) for o = 1, ..., O, which we refer to as option-policy networks, are trained separately for each option. In the learning phase, p(o|s, a) is estimated by the option network. Then, samples are assigned to option o * = arg max o p(o|s, a; η) and are used to update the option-policy network that corresponds to o * . When performing a rollout, o is drawn by following the gating policy in Equation ( 17), and an action is generated by the selected option-policy network.
Differentiating the objective function in Equation ( 14), we obtain the deterministic policy gradient of our option-policy µ o θ (s) given by
∇ θ J(w, θ) = E s∼d β (s)π(o|s) ∇ θ µ o θ (s)∇ a Q π s, a | a=µ o θ (s) . (19
)
The procedure of adInfoHRL is summarized by Algorithm 1. As in TD3 (Fujimoto et al., 2018), we employed the soft update using a target value network and a target policy network. 

[EXPERIMENTS]
We evaluated the proposed algorithm adInfoHRL on the OpenAI Gym platform (Brockman et al., 2016) with the MuJoCo Physics simulator (Todorov et al., 2012). We compared its performance with that of PPO implemented in OpenAI baselines (Dhariwal et al., 2017) and TD3. Henderson et al. (2018) have recently claimed that algorithm performance varies across environment, there is thus no clearly best method for all benchmark environments, and off-policy and on-policy methods have advantages in different problem domains. To analyze the performance of adInfoHRL, we compared it with state-of-the-art algorithms for both on-policy and off-policy methods, although we focused on the comparison with TD3, as our implementation of adInfoHRL is based on it. To determine the effect of learning the latent variable via information maximization, we used the same network architectures for the actor and critic in adInfoHRL and TD3. In addition, to evaluate the benefit of the advantage-weighted importance, we evaluated a variant of adInfoHRL, which does not use the advantage-weighted importance for computing mutual information.We refer to this variant of adInfoHRL as infoHRL. The gating policy updated variable o once every three time steps. We tested the performance of adInfoHRL with two and four options. The averaged return of five trials is reported in Figure 3(a)-(d). AdIfoHRL yields the best performance on Ant 1 and Walker2d, whereas the performance of TD3 and adInfoHRL was comparable on HalfCheetah and Hopper, and PPO outperformed the other methods on Hopper. Henderson et al. (2018) claimed that on-policy methods show their superiority on tasks with unstable dynamics, and our experimental results are in line with such previous studies. AdinfoHRL outperformed infoHRL, which isthe variant of adInfoHRL without the advantage-weighted importance on all the tasks. This result shows that the adavatage-weighted importance enhanced the performance of learning options.
AdInfoHRL exhibited the sample efficiency on Ant and Walker2d in the sense that it required fewer samples than TD3 to achieve comparable performance on those tasks. The concept underlying ad-InfoHRL is to divide the state-action space to deal with the multi-modal advantage function and learn option policies corresponding to separate modes of the advantage function. Therefore, adIn-foHRL shows its superiority on tasks with the multi-modal advantage function and not on tasks with a simple advantage function. Thus, it is natural that the benefit of adInfoHRL is dependent on the characteristics of the task.   

[RELATED WORK AND DISCUSSION]
Past studies have proposed several ways to deal with the latent variable in HRL. The recent work by Smith et al. (2018) proposed inferred option policy gradients (IOPG), which is derived as an extension of policy gradient to the option framework. Nachum et al. (2018) recently proposed off-policy target correction for HRL on goal-oriented tasks, where a higher-level policy instructs a lower-level policy by generating the goal signal instead of an inferred latent variable. A popular approach for learning the latent variable in HRL is the variational approach. The recent work by Haarnoja et al. (2018a) is based on soft actor critic (Haarnoja et al., 2018b), and the latent variable is inferred using the variational approach. The work by Hausman et al. (2018) is also closely related to the variational approach, and they proposed a method for learning a latent variable of a hierarchical policy via a variational bound. On the contrary, our method learns the latent variable by maximizing MI with advantage-weighted importance. Recent studies by Gregor et al. (2016); Florensa et al. (2017); Eysenbach et al. (2018) also considered the MI in their formulation. In these methods, MI between the state and the latent variable is considered so as to obtain diverse behaviors. Our approach is different from the previous studies in the sense that we employ MI between the latent variable and the state-action pairs, which leads to the division of the state-action space instead of considering only the state space. We think that dividing the state-action space is an efficient approach when the advantage function is multi-modal, as depicted in Figure 1. InfoGAIL proposed by Li et al. (2017) learns the interpretable representation of the state-action space via MI maximization. InfoGAIL can be interpreted as a method that divides the state-action space based on the density induced by an expert's policy by maximizing the regularized MI objective. In this sense, it is closely related to our method, although their problem setting is imitation learning (Osa et al., 2018a), which is different from our HRL problem setting.
The use of the importance weight based on the value function has appeared in previous studies (Dayan & Hinton, 1997;Kober & Peters, 2011;Neumann & Peters, 2009;Osa & Sugiyama, 2018). For example, the method proposed by Neumann & Peters (2009) employs the importance weight based on the advantage function for learning a monolithic policy, while our method uses a similar importance weight for learning a latent variable of a hierarchical policy. Although Osa & Sugiyama (2018) proposed to learn a latent variable in HRL with importance sampling, their method is limited to episodic settings where only a single option is used in an episode.
Our method can be interpreted as an approach that divides the state-action space based on the MI criterion. This concept is related to that of Divide and Conquer (DnC) proposed by Ghosh et al. (2018), although DnC clusters the initial states and does not consider switching between option policies during the execution of a single trajectory.
In this study we developed adInfoHRL based on deterministic option policies. However, the concept of dividing the state-action space via advantage-weighted importance can be applied to stochastic policy gradients as well. Further investigation in this direction is necessary in future work.

[CONCLUSIONS]
We proposed a novel HRL method, hierarchical reinforcement learning via advantage-weighted information maximization. In our framework, the latent variable of a hierarchical policy is learned as a discrete latent representation of the state-action space. Our HRL framework is derived by considering deterministic option policies and by leveraging the analogy between the gating policy for HRL and a monolithic policy for the standard RL. The results of the experiments indicate that adInfoHRL can learn diverse options on continuous control tasks. Our results also suggested that our approach can improve the performance of TD3 in certain problem domains.

[A MUTUAL INFORMATION WITH ADVANTAGE-WEIGHTED IMPORTANCE]
The 
where W (s, a) = p π (s,a) p β (s,a) is the importance weight. Therefore, the empirical estimate of p(o) with respect to the density induced by a policy π is given bŷ 
p(o) = 1 N N i=1W (s i , a i )p(o|s i , a i ; η),(23)
Therefore, the empirical estimate of the conditional entropy with respect to the density induced by a policy π is given bŷ
H(o|s, a) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(27)
Thus, the empirical estimates of MI can be computed by Equations ( 21), ( 23) and ( 27).

[B DERIVATION OF THE STATE-VALUE FUNCTION]
In HRL, the value function is given by V (s) =  

[C EXPERIMENTAL DETAILS]
We performed evaluations using benchmark tasks in the OpenAI Gym platform (Brockman et al., 2016) with Mujoco physics simulator (Todorov et al., 2012). Hyperparameters of reinforcement learning methods used in the experiment are shown in Tables 1-3. For exploration, both adInfoHRL and TD3 used the clipped noise drawn from the normal distribution as ∼ clip N (0, σ), −c, c , where σ = 0.2 and c = 0.5. For hyperparameters of PPO, we used the default values in OpenAI baselines (Dhariwal et al., 2017). For the Walker2d, HalfCheetah, and Hopper tasks, we used the Walker2d-v1, HalfCHeetah-v1, and Hopper-v1 in the OpenAI Gym, respectively. For the Ant task, we used the AntEnv implemented in the rllab . When training a policy with AdInfoHRL, infoHRL, and TD3, critics are trained once per time step, and actors are trained once every after two updates of the critics. The source code is available at https://github.com/ TakaOsa/adInfoHRL.
We performed the experiments five times with different seeds, and reported the averaged test return where the test return was computed once every 5000 time steps by executing 10 episodes without exploration. When performing the learned policy without exploration, the option was drawn as
o = max o Q π (s, µ o (s)),(30)
instead of following the stochastic gating policy in Equations (17).   The dimensionality is reduced by t-SNE for visualization.

[D ADDITIONAL INFORMATION ON EXPERIMENTAL RESULTS]
On the HalfCheetah task, adInfoHRL delivered the best performance with two options. The distribution of options on HalfCheetah0v1 after one million steps is shown in Figure 4. Although the state-action space is evenly divided, the options are not evenly activated. This behavior can occur because the state-action space is divided based on the density induced by the behavior policy while the activation of options is determined based on the quality of the option policies in a given state. Moreover, an even division in the action-state space is not necessarily the even division in the state space.
The activation of the options over time is shown in Figure 5. It is clear that one of the option corresponds to the stable running phase and the other corresponds to the phase for recovering from unstable states. Figure 6: Distribution of options on Ant-rllab task using adInfoHRL with four options. The dimensionality is reduced by t-SNE for visualization.
Figure 7: Activation of the options over time steps on Ant-rllab task. Four options are learned.
The distribution of four options on the Ant-rllab task after one million steps is shown in Figure 6. Four options are activated in the different domains of the state space. The activation of the options over time on the Ant-rllab task is shown in Figure 7. While four options are actively used in the beginning of the episode, two (blue and yellow) options are mainly activated during the stable locomotion.
Since the Ant task implemented in rllab is known to be harder than the Ant-v1 implemented in the OpenAI gym, we reported the result of the Ant task in rllab in the main manuscript. Here, we report the result of the Ant-v1 task implemented in the OpenAI gym. On the Ant-v1 task, adInfoHRL yielded the best performance with two options. The performance of adInfoHRL with two options is comparable to that of TD3 on Ant-v1. This result indicates that the Ant-v1 task does not require a hierarchical policy structure, while a hierarchical policy improves the performance of learning on Ant-rllab. The distribution of options on Ant-v1 task after one million steps is shown in Figure 8. The activation of the options over time is shown in Figure 9. It is evident that two option policies on the Ant-v1 task corresponded to different postures of the agent.
A recent study on HRL by Smith et al. (2018) reported the performance of IOPG on Walker2d-v1, Hopper-v1, and HalfCheetah-v1. The study by Haarnoja et al. (2018a) reported the performance of SAC-LSP on Walker2d-v1, Hopper-v1, HalfCheetah-v1, and Ant-rllab. A comparison of performance between our method, IOPG, and SAC-LSP is summarized in Table 4. We report the performance after 1 million steps. It is worth noting that adInfoHRL outperformed IOPG on these tasks in terms of the achieved return, although we are aware that the qualitative performance is also important in HRL. AdInfoHRL outperformed SAC-LSP on Walker2d-v1 and Ant-rllab, and SAC-LSP shows its superiority on HalfCheetah-v1 and Hopper-v1. However, the results of SAC-LSP were obtained by using reward scaling, which was not used in the evaluation of adInfoHRL. Therefore, further experiments are necessary for fair comparison under the same condition.

[ACKNOWLEDGMENTS]
MS was partially supported by KAKENHI 17H00757.","[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks."
Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Hyl_vjC5KQ.json,"The authors propose an HRL algorithm that attempts to learn options that maximize their mutual information with the state-action density under the optimal policy.

Several key terms are used in ways that differ from the rest of the literature. The authors claim options are learned in an ""unsupervised"" manner, but it is unclear what this means. Previous work (none of which is cited) has dealt with unsupervised option discovery in the context of mutual information maximization (Variational intrinsic control, diversity is all you need, etc), but they do so in the absence of reward, unlike this paper. ""Optimal policy"" is similarly abused, with it appearing to mean optimal from the perspective of the current model parameters, rather than optimal in any global sense. Or at least I think that is what the authors intend. If they do mean the globally optimal policy, then its unclear how to interpret Equation 8, with its reference to a behavior policy and an advantage function, neither of which would be available if meant to represent the global optimum.

Equation 10 comes out of nowhere. One must assume they meant ""maximize mutual information"" and not ""minimize"", but who knows. Why is white-noise being added to the states and actions? Is this some sort of noise-contrastive estimation approach to mutual information estimation? It doesn't appear to be, but it is unclear what else could motivate it. Even the appendices fail to shine light on this equation.

The algorithm block isn't terribly helpful. The ""t"" variable is used outside of its for loop, which draws into question the exact nesting structure of the underlying algorithm (which isn't obvious for HRL methods). There aren't any equations referenced, with the option policy network's update not even referencing the loss nor data over which the loss would be evaluated.

Some of the experimental results show promise, but the PPO Ant result raises some questions. Clearly the OpenAI implementation of PPO used would have tuned for the OpenAI gym Ant implementation, and the appendix shows it getting decent results. But it never takes off in the harder RlLab version -- were the hyper-parameters adjusted for this new environment?

It is also odd that no other HRL approaches are evaluated against, given the number cited. Running these methods might be too costly, but surely a table comparing results reported in those papers should be included.

A minor point: another good baseline would be TD3 with the action repeat adjusted to be inline with the gating policy.

I apologise if this review came off as too harsh -- I believe a good paper can be made of this with extensive rewrites and additional experiments. But the complete lack of clarity makes it feel like it was rushed out prematurely.

EDIT: Now this is a paper that makes sense! With the terminology cleared up and the algorithm fully unpacked, this approach seems quite interesting. The experimental results could always be stronger, but no longer have any holes in them. Score 3-->6","[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Schematic sketch of our HRL approach. By using the advantage-weighted importance, the problem of finding the modes of the advantage-function can be reduced to that of finding the modes of the density of state action pairs.
Table 2: empirically show that this regularization improves the performance of learning latent discrete representations. When computing MI, we need to compute p(o) and H(o|s, a) given by p(o) = p πAd (s, a)p(o|s, a; η)dads = E (s,a)∼p π Ad (s,a) [p(o|s, a; η)] (8) H(o|s, a) = E (s,a)∼p π Ad (s,a) [p(o|s, a; η) log p(o|s, a; η)] .
Table 3: Figure 2 :2Figure 2: Activation of the four options over time steps on the Walker2d task.
Table 4: The activation of options over time and snapshots of the learned option policies on the Walker2d task are shown in Figure2, which visualizes the result from adInfoHRL with four options. One can see that the option policies are activated in different phases of locomotion. While the option indicated by yellow in Figure2corresponds to the phase for kicking the floor, the option indicated by blue corresponds to the phase when the agent was on the fly. Visualization of the options learned on the HalfCheetah and Ant tasks are shown in Appendix D.
Table 5: Output of the option network in the state-action space on Walker2d. Activation of options in the state space on Walker2d.
Table 6: Figure 3 :3Figure 3: Performance of adInfoHRL. (a)-(d) show comparison with baseline methods. (e) and (f) show the output of the option network and the activation of options on Walker2d, respectively.
Table 7: mutual information (MI) between the latent variable o and the state action pair (s, a) is defined as I (s, a), o = H(o) − H(o|s, a) (20) where H(o) = p(o) log p(o)do and H(o|s, a) = p(o|s, a) log p(o|s, a)do. We make the empirical estimate of MI employed by Gomes et al. (2010); Hu et al. (2017) and modify it to employ the importance weight. The empirical estimate of MI with respect to the density induced by a policy π is given byÎ (s, a; o) = o∈Op (o) logp(o) −Ĥ(o|s, a). (21) We consider the case where we have samples collected by a behavior policy β(s|a) and need to estimate MI with respect to the density induced by policy π. Given a model p(o|s, a; η) parameterized by vector η, p(o) can be rewritten as p(o) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η)dads = E [W (s, a)p(o|s, a; η)] ,
Table 8: whereW (s, a) =W (s,a) N j=1W (sj ,aj ) is the normalized importance weight. Likewise, the conditional entropy with respect to the density induced by a policy π is given by H(o|s, a) = p π (s, a)p(o|s, a; η) log p(o|s, a; η)dsda (24) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η) log p(o|s, a; η)dsda (25) = E [W (s, a)p(o|s, a; η) log p(o|s, a; η)] .
Table 9: o∈O π(o|s)π(a|s, o)Q π (s, a)da = o∈O π(o|s) π(a|s, o)Q π (s, a)da (28) Since option policies are deterministic given by µ o θ (s), the state-value function is given by V (s) = o∈O π(o|s)Q π (s, µ o θ (s))da. (29)
Table 10: Activation of options in the state space on HalfCheetah-v1.
Table 11: Figure 4 :4Figure 4: Distribution of options on the HalfCheetah-v1 task using adInfoHRL with two options. The dimensionality is reduced by t-SNE for visualization.
Table 12: Figure 5 :5Figure 5: Activation of options over time steps on the HalfCheetah-v1 task using adInfoHRL with two options.
Table 13: The outputs of the option network and the activation of options on Walker2d are shown in Fig-
Table 14: Hyperparameters of adInfoHRL used in the experiment.
Table 15: Hyperparameters of PPO used in the experiment. We tuned hyperparameters for our tasks, which are defferent from the default parameters in OpenAI baselines(Dhariwal et al., 2017).

[INTRODUCTION]
Reinforcement learning (RL) has been successfully applied to a variety of tasks, including board games , robotic manipulation tasks (Levine et al., 2016), and video games (Mnih et al., 2015). Hierarchical reinforcement learning (HRL) is a type of RL that leverages the hierarchical structure of a given task by learning a hierarchical policy (Sutton et al., 1999;Dietterich, 2000). Past studies in this field have shown that HRL can solve challenging tasks in the video game domain (Vezhnevets et al., 2017;Bacon et al., 2017) and robotic manipulation (Daniel et al., 2016;Osa et al., 2018b). In HRL, lower-level policies, which are often referred to as option policies, learn different behavior/control patterns, and the upper-level policy, which is often referred to as the gating policy, learns to select option policies. Recent studies have developed HRL methods using deep learning (Goodfellow et al., 2016) and have shown that HRL can yield impressive performance for complex tasks (Bacon et al., 2017;Frans et al., 2018;Vezhnevets et al., 2017;Haarnoja et al., 2018a). However, identifying the hierarchical policy structure that yields efficient learning is not a trivial task, since the problem involves learning a sufficient variety of types of behavior to solve a given task.
In this study, we present an HRL method via the mutual information (MI) maximization with advantage-weighted importance, which we refer to as adInfoHRL. We formulate the problem of learning a latent variable in a hierarchical policy as one of learning discrete and interpretable repre-sentations of states and actions. Ideally, each option policy should be located at separate modes of the advantage function. To estimate the latent variable that corresponds to modes of the advantage function, we introduce advantage-weighted importance weights. Our approach can be considered to divide the state-action space based on an information maximization criterion, and it learns option policies corresponding to each region of the state-action space. We derive adInfoHRL as an HRL method based on deterministic option policies that are trained based on an extension of the deterministic policy gradient (Silver et al., 2014;Fujimoto et al., 2018). The contributions of this paper are twofold:
1. We propose the learning of a latent variable of a hierarchical policy as a discrete and hidden representation of the state-action space. To learn option policies that correspond to the modes of the advantage function, we introduce advantage-weighted importance. 2. We propose an HRL method, where the option policies are optimized based on the deterministic policy gradient and the gating policy selects the option that maximizes the expected return. The experimental results show that our proposed method adInfoHRL can learn a diversity of options on continuous control tasks. Moreover, our approach can improve the performance of TD3 on such tasks as the Walker2d and Ant tasks in OpenAI Gym with MuJoco simulator.

[BACKGROUND]
In this section, we formulate the problem of HRL in this paper and describe methods related to our proposal.

[HIERARCHICAL REINFORCEMENT LEARNING]
We consider tasks that can be modeled as a Markov decision process (MDP), consisting of a state space S, an action space A, a reward function r : S × A → R, an initial state distribution ρ(s 0 ), and a transition probability p(s t+1 |s t , a t ) that defines the probability of transitioning from state s t and action a t at time t to next state s t+1 . The return is defined as R t =
T i=t γ i−t r(s i , a i ), where γ is a discount factor, and policy π(a|s) is defined as the density of action a given state s. Let d π (s) = T t=0 γ t p(s t = s) denote the discounted visitation frequency induced by the policy π. The goal of reinforcement learning is to learn a policy that maximizes the expected return J(π) = E s0,a0,... [R 0 ] where s 0 ∼ ρ(s 0 ), a ∼ π and s t+1 ∼ p(s t+1 |s t , a t ). By defining the Q-function as Q π (s, a) = E s0,a0,... [R t |s t = s, a t = a], the objective function of reinforcement learning can be rewritten as follows:
J(π) = d π (s)π(a|s)Q π (s, a)dads.(1)
Herein, we consider hierarchical policy π(a|s) = o∈O π(o|s)π(a|s, o), where o is the latent variable and O is the set of possible values of o. Many existing HRL methods employ a policy structure of this form (Frans et al., 2018;Vezhnevets et al., 2017;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016). In general, latent variable o can be discrete (Frans et al., 2018;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016;Osa & Sugiyama, 2018) or continuous (Vezhnevets et al., 2017). π(o|s) is often referred to as a gating policy (Daniel et al., 2016;Osa & Sugiyama, 2018), policy over options (Bacon et al., 2017), or manager (Vezhnevets et al., 2017). Likewise, π(a|s, o) is often referred to as an option policy (Osa & Sugiyama, 2018), sub-policy (Daniel et al., 2016), or worker (Vezhnevets et al., 2017). In HRL, the objective function is given by
J(π) = d π (s) o∈O π(o|s)π(a|s, o)Q π (s, a)dads.(2)
As discussed in the literature on inverse RL (Ziebart, 2010), multiple policies can yield equivalent expected returns. This indicates that there exist multiple solutions to latent variable o that maximizes the expected return. To obtain the preferable solution for o, we need to impose additional constraints in HRL. Although prior work has employed regularizers (Bacon et al., 2017) and constraints (Daniel et al., 2016) to obtain various option policies, the method of learning a good latent variable o that improves sample-efficiency of the learning process remains unclear. In this study we propose the learning of the latent variable by maximizing MI between latent variables and state-action pairs.

[DETERMINISTIC POLICY GRADIENT]
The deterministic policy gradient (DPG) algorithm was developed for learning a monolithic deterministic policy µ θ (s) : S → A by Silver et al. (2014). In off-policy RL, the objective is to maximize the expectation of the return, averaged over the state distribution induced by a behavior policy β(a|s):
J(π) = d β (s)π(a|s)Q π s, a)dads.(3)
When a policy is deterministic, the objective becomes J(π) = d β (s)Q π s, µ θ (s) ds. Silver et al. (2014) have shown that the gradient of a deterministic policy is given by
∇ θ E s∼d β (s) [Q π (s, a)] = E s∼d β (s) ∇ θ µ θ (s)∇ a Q π s, a | a=µ θ (s) .(4)
The DPG algorithm has been extended to the deep deterministic policy gradient (DDPG) for continuous control problems that require neural network policies . Twin Delayed Deep Deterministic policy gradient algorithm (TD3) proposed by Fujimoto et al. (2018) is a variant of DDPG that outperforms the state-of-the-art on-policy methods such as TRPO (Schulman et al., 2017a) and PPO (Schulman et al., 2017b) in certain domains. We extend this deterministic policy gradient to learn a hierarchical policy.

[REPRESENTATION LEARNING VIA INFORMATION MAXIMIZATION]
Recent studies such as those by ; Hu et al. (2017); Li et al. (2017) have shown that an interpretable representation can be learned by maximizing MI. Given a dataset X = (x 1 , ..., x n ), regularized information maximization (RIM) proposed by Gomes et al. (2010) involves learning a conditional modelp(y|x; η) with parameter vector η that predicts a label y. The objective of RIM is to minimize
(η) − λI η (x, y),(5)
where (η) is the regularization term, I η (x, y) is MI, and λ is a coefficient. MI can be decomposed as I η (x, y) = H(y) − H(y|x) where H(y) is entropy and H(y|x) the conditional entropy. Increasing H(y) conduces the label to be uniformly distributed, and decreasing H(y|x) conduces to clear cluster assignments. Although RIM was originally developed for unsupervised clustering problems, the concept is applicable to various problems that require learning a hidden discrete representation.
In this study, we formulate the problem of learning the latent variable o of a hierarchical policy as one of learning a latent representation of the state-action space.

[LEARNING OPTIONS VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
In this section, we propose a novel HRL method based on advantage-weighted information maximization. We first introduce the latent representation learning via advantage-weighted information maximization, and we then describe the HRL framework based on deterministic option policies.

[LATENT REPRESENTATION LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
Although prior work has often considered H(o|s) or I(s, o), which results in a division of the state space, we are interested in using I (s, a), o for dividing the state-action space instead. A schematic sketch of our approach is shown in Figure 1. As shown in the left side of Figure 1, the advantage function often has multiple modes. Ideally, each option policies should correspond to separate modes of the advantage function. However, it is non-trivial to find the modes of the advantage function in practice. For this purpose, we reduce the problem of finding modes of the advantage function to that of finding the modes of the probability density of state action pairs.
We consider a policy based on the advantage function of the form  where
π Ad (a|s) = f A π (s, a) Z ,(6)
A π (s, a) = Q π (s, a) − V π (s) is the advantage function, V π (s)
is the state value function, and Z is the partition function. f (•) is a functional, which is a function of a function. f (•) is a monotonically increasing function with respect to the input variable and always satisfies f (•) > 0.
In our implementation we used the exponential function f (•) = exp(•). When following such a policy, an action with the larger advantage is drawn with a higher probability. Under this assumption, finding the modes of the advantage function is equivalent to finding modes of the density induced by π Ad . Thus, finding the modes of the advantage function can be reduced to the problem of clustering samples induced by π Ad .
Following the formulation of RIM introduced in Section 2.3, we formulate the problem of clustering samples induced by π Ad as the learning of discrete representations via MI maximization. For this purpose, we consider a neural network that estimates p(o|s, a; η) parameterized with vector η, which we refer to as the option network. We formulate the learning of the latent variable o as minimizing L option (η) = (η) − λI o, (s, a); η ,
where I(o, (s, a)) =Ĥ(o|s, a; η) −Ĥ(o; η), and (η) is the regularization term. In practice, we need to approximate the advantage function, and we learn the discrete variable o that corresponds to the modes of the current estimate of the advantage function. For regularization, we used a simplified version of virtual adversarial training (VAT) proposed by Miyato et al. (2016). Namely, we set (η) = D KL p(o|s noise , a noise ; η)||p(o|s, a; η) where s noise = s + s , a noise = a + a , s and a denote white noise. This regularization term penalizes dissimilarity between an original state-action pair and a perturbed one, and Hu et al. (2017) 
Thus, the probability density of (s, a) induced by π Ad is necessary for computing MI for our purpose. To estimate the probability density of (s, a) induced by π Ad , we introduce the advantageweighted importance in the next section.

[IMPORTANCE WEIGHTS FOR MUTUAL INFORMATION ESTIMATION]
Although we show that the problem of finding the modes of the advantage function can be reduced to MI maximization with respect to the samples induced by π Ad , samples induced by π Ad are not available in practice. While those induced during the learning process are available, a discrete representation obtained from such samples does not correspond to the modes of the advantage function.
To estimate the density induced by π Ad , we employ an importance sampling approach.
We assume that the change of the state distribution induced by the policy update is sufficiently small, namely, d πAd (s) ≈ d β (s). Then, the importance weight can be approximated as
W (s, a) = p πAd (s, a) p β (s, a) = d πAd (s)π Ad (a|s) d β (s)β(a|s) ≈ π Ad (a|s) β(a|s) = f (A(s, a)) Zβ(a|s) . (10
)
and the normalized importance weight is given gỹ
W (s, a) = W (s, a) N j=1 W (s j , a j ) = f (A(s,a)) Zβ(a|s) N j=1 f (A(sj ,aj )) Zβ(aj |sj ) = f (A(s,a)) β(a|s) N j=1 f (A(sj ,aj )) β(aj |sj ) . (11
)
As the partition function Z is canceled, we do not need to compute Z when computing the importance weight in practice. We call this importance weight W the advantage-weighted importance and employ it to compute the objective function used to estimate the latent variable.
This advantage-weighted importance is used to compute the entropy terms for computing MI in Equation ( 7). The empirical estimate of the entropy H(o) is given bŷ
H(o; η) = − o∈Op (o; η) logp(o; η), wherep(o; η) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η). (12
)
where the samples (s i , a i ) are drawn from p β (s, a) induced by a behavior policy β(a|s). Likewise, the empirical estimate of the conditional entropy H(o|s, a) is given bŷ
H(o|s, a; η) = 1 N N i W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(13)
The derivations of Equations ( 12) and ( 13) are provided in Appendix A. To train the option network, we store the samples collected by the M most recent behavior policies, to which we refer as onpolicy buffer D on . Although the algorithm works with entire samples stored in the replay buffer, we observe that the use of the on-policy buffer for latent representation learning exhibits better performance. For this reason, we decided to use the on-policy buffer in our implementation. Therefore, while the algorithm is off-policy in the sense that the option is learned from samples collected by behavior policies, our implementation is ""semi""on-policy in the sense that we use samples collected by the most recent behavior policies.

[HRL OBJECTIVE WITH DETERMINISTIC OPTION POLICIES]
Instead of stochastic option policies, we consider deterministic option policies and model them using separate neural networks. We denote by π(a|s, o) = µ o θ (s) deterministic option policies parameterized by vector θ. The objective function of off-policy HRL with deterministic option policies can then be obtained by replacing π(a|s) with o∈O π(o|s)π(a|s, o) in Equation (3):
J(w, θ) = d β (s) o∈O π(o|s)Q π s, µ o θ (s); w ds, (14
)
where Q π (s, a; w) is an approximated Q-function parameterized using vector w. This form of the objective function is analogous to Equation (3). Thus, we can extend standard RL techniques to the learning of the gating policy π(o|s) in HRL with deterministic option policies.
In HRL, the goal of the gating policy is to generate a value of o that maximizes the conditional expectation of the return:
Q π Ω (s, o) = E [R|s t = s, o t = o] = π(a|s, o)Q π (s, a)da,(15)
which is often referred to as the option-value function (Sutton et al., 1999). When option policies are stochastic, it is often necessary to approximate the option-value function Q π Ω (s, o) in addition to the action-value function Q π (s, a). However, in our case, the option-value function for deterministic option policies is given by
Q π Ω (s, o) = Q π (s, µ o θ (s)),(16)
Algorithm 1 HRL via Advantage-Weighted Information Maximization (adInfoHRL)
Input: Number of options O, size of on-policy buffer Initialize: Replay buffer D R , on-policy buffer D on , network parameters η, θ, w, θ target , w target repeat for t = 0 to t = T do Draw an option for a given s by following Equation 17: o ∼ π(o|s) Draw an action a ∼ β(a|s, o) = µ o θ (s) + Record a data sample (s, a, r, s ) Aggregate the data in D R and D on if the on-policy buffer is full then Update the option network by minimizing Equation ( 7) for samples in D on Clear the on-policy buffer D on end if Sample a batch D batch ∈ D R Update the Q network parameter w if t mod d then Estimate p(o|s i , a i ) for (s i , a i ) ∈ D batch using the option network Assign samples (s i , a i ) ∈ D batch to the option o * = arg max p(o|s i , a i ) Update the option policy networks µ o θ (s) for o = 1, ..., O with Equation ( 19) Update the target networks:
w target ← τ w +(1−τ )w target , θ target ← τ θ +(1−τ )θ target end if end for until the convergence return θ
which we can estimate using the deterministic option policy µ o θ (s) and the approximated actionvalue function Q π (s, a; w). In this work we employ the softmax gating policy of the form
π(o|s) = exp Q π (s, µ o θ (s); w) o∈O exp Q π s, µ o θ (s); w ,(17)
which encodes the exploration in its form (Daniel et al., 2016). The state value function is given as
V π (s) = o∈O π(o|s)Q π (s, µ o θ (s); w),(18)
which can be computed using Equation (17). We use this state-value function when computing the advantage-weighted importance as A(s, a) = Q(s, a) − V (s). In this study, the Q-function is trained in a manner proposed by Fujimoto et al. (2018). Two neural networks (Q π w1 , Q π w2 ) are trained to estimate the Q-function, and the target value of the Q-function is computed as y i = r i + γ min 1,2 Q(s i , a i ) for sample (s i , a i , a i , r i ) in a batch sampled from a replay buffer, where r i = r(s i , a i ). In this study, the gating policy determines the option once every N time steps, i.e., t = 0, N, 2N, . . .

[NEURAL NETWORKS THAT MODEL Μ O]
θ (a|s) for o = 1, ..., O, which we refer to as option-policy networks, are trained separately for each option. In the learning phase, p(o|s, a) is estimated by the option network. Then, samples are assigned to option o * = arg max o p(o|s, a; η) and are used to update the option-policy network that corresponds to o * . When performing a rollout, o is drawn by following the gating policy in Equation ( 17), and an action is generated by the selected option-policy network.
Differentiating the objective function in Equation ( 14), we obtain the deterministic policy gradient of our option-policy µ o θ (s) given by
∇ θ J(w, θ) = E s∼d β (s)π(o|s) ∇ θ µ o θ (s)∇ a Q π s, a | a=µ o θ (s) . (19
)
The procedure of adInfoHRL is summarized by Algorithm 1. As in TD3 (Fujimoto et al., 2018), we employed the soft update using a target value network and a target policy network. 

[EXPERIMENTS]
We evaluated the proposed algorithm adInfoHRL on the OpenAI Gym platform (Brockman et al., 2016) with the MuJoCo Physics simulator (Todorov et al., 2012). We compared its performance with that of PPO implemented in OpenAI baselines (Dhariwal et al., 2017) and TD3. Henderson et al. (2018) have recently claimed that algorithm performance varies across environment, there is thus no clearly best method for all benchmark environments, and off-policy and on-policy methods have advantages in different problem domains. To analyze the performance of adInfoHRL, we compared it with state-of-the-art algorithms for both on-policy and off-policy methods, although we focused on the comparison with TD3, as our implementation of adInfoHRL is based on it. To determine the effect of learning the latent variable via information maximization, we used the same network architectures for the actor and critic in adInfoHRL and TD3. In addition, to evaluate the benefit of the advantage-weighted importance, we evaluated a variant of adInfoHRL, which does not use the advantage-weighted importance for computing mutual information.We refer to this variant of adInfoHRL as infoHRL. The gating policy updated variable o once every three time steps. We tested the performance of adInfoHRL with two and four options. The averaged return of five trials is reported in Figure 3(a)-(d). AdIfoHRL yields the best performance on Ant 1 and Walker2d, whereas the performance of TD3 and adInfoHRL was comparable on HalfCheetah and Hopper, and PPO outperformed the other methods on Hopper. Henderson et al. (2018) claimed that on-policy methods show their superiority on tasks with unstable dynamics, and our experimental results are in line with such previous studies. AdinfoHRL outperformed infoHRL, which isthe variant of adInfoHRL without the advantage-weighted importance on all the tasks. This result shows that the adavatage-weighted importance enhanced the performance of learning options.
AdInfoHRL exhibited the sample efficiency on Ant and Walker2d in the sense that it required fewer samples than TD3 to achieve comparable performance on those tasks. The concept underlying ad-InfoHRL is to divide the state-action space to deal with the multi-modal advantage function and learn option policies corresponding to separate modes of the advantage function. Therefore, adIn-foHRL shows its superiority on tasks with the multi-modal advantage function and not on tasks with a simple advantage function. Thus, it is natural that the benefit of adInfoHRL is dependent on the characteristics of the task.   

[RELATED WORK AND DISCUSSION]
Past studies have proposed several ways to deal with the latent variable in HRL. The recent work by Smith et al. (2018) proposed inferred option policy gradients (IOPG), which is derived as an extension of policy gradient to the option framework. Nachum et al. (2018) recently proposed off-policy target correction for HRL on goal-oriented tasks, where a higher-level policy instructs a lower-level policy by generating the goal signal instead of an inferred latent variable. A popular approach for learning the latent variable in HRL is the variational approach. The recent work by Haarnoja et al. (2018a) is based on soft actor critic (Haarnoja et al., 2018b), and the latent variable is inferred using the variational approach. The work by Hausman et al. (2018) is also closely related to the variational approach, and they proposed a method for learning a latent variable of a hierarchical policy via a variational bound. On the contrary, our method learns the latent variable by maximizing MI with advantage-weighted importance. Recent studies by Gregor et al. (2016); Florensa et al. (2017); Eysenbach et al. (2018) also considered the MI in their formulation. In these methods, MI between the state and the latent variable is considered so as to obtain diverse behaviors. Our approach is different from the previous studies in the sense that we employ MI between the latent variable and the state-action pairs, which leads to the division of the state-action space instead of considering only the state space. We think that dividing the state-action space is an efficient approach when the advantage function is multi-modal, as depicted in Figure 1. InfoGAIL proposed by Li et al. (2017) learns the interpretable representation of the state-action space via MI maximization. InfoGAIL can be interpreted as a method that divides the state-action space based on the density induced by an expert's policy by maximizing the regularized MI objective. In this sense, it is closely related to our method, although their problem setting is imitation learning (Osa et al., 2018a), which is different from our HRL problem setting.
The use of the importance weight based on the value function has appeared in previous studies (Dayan & Hinton, 1997;Kober & Peters, 2011;Neumann & Peters, 2009;Osa & Sugiyama, 2018). For example, the method proposed by Neumann & Peters (2009) employs the importance weight based on the advantage function for learning a monolithic policy, while our method uses a similar importance weight for learning a latent variable of a hierarchical policy. Although Osa & Sugiyama (2018) proposed to learn a latent variable in HRL with importance sampling, their method is limited to episodic settings where only a single option is used in an episode.
Our method can be interpreted as an approach that divides the state-action space based on the MI criterion. This concept is related to that of Divide and Conquer (DnC) proposed by Ghosh et al. (2018), although DnC clusters the initial states and does not consider switching between option policies during the execution of a single trajectory.
In this study we developed adInfoHRL based on deterministic option policies. However, the concept of dividing the state-action space via advantage-weighted importance can be applied to stochastic policy gradients as well. Further investigation in this direction is necessary in future work.

[CONCLUSIONS]
We proposed a novel HRL method, hierarchical reinforcement learning via advantage-weighted information maximization. In our framework, the latent variable of a hierarchical policy is learned as a discrete latent representation of the state-action space. Our HRL framework is derived by considering deterministic option policies and by leveraging the analogy between the gating policy for HRL and a monolithic policy for the standard RL. The results of the experiments indicate that adInfoHRL can learn diverse options on continuous control tasks. Our results also suggested that our approach can improve the performance of TD3 in certain problem domains.

[A MUTUAL INFORMATION WITH ADVANTAGE-WEIGHTED IMPORTANCE]
The 
where W (s, a) = p π (s,a) p β (s,a) is the importance weight. Therefore, the empirical estimate of p(o) with respect to the density induced by a policy π is given bŷ 
p(o) = 1 N N i=1W (s i , a i )p(o|s i , a i ; η),(23)
Therefore, the empirical estimate of the conditional entropy with respect to the density induced by a policy π is given bŷ
H(o|s, a) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(27)
Thus, the empirical estimates of MI can be computed by Equations ( 21), ( 23) and ( 27).

[B DERIVATION OF THE STATE-VALUE FUNCTION]
In HRL, the value function is given by V (s) =  

[C EXPERIMENTAL DETAILS]
We performed evaluations using benchmark tasks in the OpenAI Gym platform (Brockman et al., 2016) with Mujoco physics simulator (Todorov et al., 2012). Hyperparameters of reinforcement learning methods used in the experiment are shown in Tables 1-3. For exploration, both adInfoHRL and TD3 used the clipped noise drawn from the normal distribution as ∼ clip N (0, σ), −c, c , where σ = 0.2 and c = 0.5. For hyperparameters of PPO, we used the default values in OpenAI baselines (Dhariwal et al., 2017). For the Walker2d, HalfCheetah, and Hopper tasks, we used the Walker2d-v1, HalfCHeetah-v1, and Hopper-v1 in the OpenAI Gym, respectively. For the Ant task, we used the AntEnv implemented in the rllab . When training a policy with AdInfoHRL, infoHRL, and TD3, critics are trained once per time step, and actors are trained once every after two updates of the critics. The source code is available at https://github.com/ TakaOsa/adInfoHRL.
We performed the experiments five times with different seeds, and reported the averaged test return where the test return was computed once every 5000 time steps by executing 10 episodes without exploration. When performing the learned policy without exploration, the option was drawn as
o = max o Q π (s, µ o (s)),(30)
instead of following the stochastic gating policy in Equations (17).   The dimensionality is reduced by t-SNE for visualization.

[D ADDITIONAL INFORMATION ON EXPERIMENTAL RESULTS]
On the HalfCheetah task, adInfoHRL delivered the best performance with two options. The distribution of options on HalfCheetah0v1 after one million steps is shown in Figure 4. Although the state-action space is evenly divided, the options are not evenly activated. This behavior can occur because the state-action space is divided based on the density induced by the behavior policy while the activation of options is determined based on the quality of the option policies in a given state. Moreover, an even division in the action-state space is not necessarily the even division in the state space.
The activation of the options over time is shown in Figure 5. It is clear that one of the option corresponds to the stable running phase and the other corresponds to the phase for recovering from unstable states. Figure 6: Distribution of options on Ant-rllab task using adInfoHRL with four options. The dimensionality is reduced by t-SNE for visualization.
Figure 7: Activation of the options over time steps on Ant-rllab task. Four options are learned.
The distribution of four options on the Ant-rllab task after one million steps is shown in Figure 6. Four options are activated in the different domains of the state space. The activation of the options over time on the Ant-rllab task is shown in Figure 7. While four options are actively used in the beginning of the episode, two (blue and yellow) options are mainly activated during the stable locomotion.
Since the Ant task implemented in rllab is known to be harder than the Ant-v1 implemented in the OpenAI gym, we reported the result of the Ant task in rllab in the main manuscript. Here, we report the result of the Ant-v1 task implemented in the OpenAI gym. On the Ant-v1 task, adInfoHRL yielded the best performance with two options. The performance of adInfoHRL with two options is comparable to that of TD3 on Ant-v1. This result indicates that the Ant-v1 task does not require a hierarchical policy structure, while a hierarchical policy improves the performance of learning on Ant-rllab. The distribution of options on Ant-v1 task after one million steps is shown in Figure 8. The activation of the options over time is shown in Figure 9. It is evident that two option policies on the Ant-v1 task corresponded to different postures of the agent.
A recent study on HRL by Smith et al. (2018) reported the performance of IOPG on Walker2d-v1, Hopper-v1, and HalfCheetah-v1. The study by Haarnoja et al. (2018a) reported the performance of SAC-LSP on Walker2d-v1, Hopper-v1, HalfCheetah-v1, and Ant-rllab. A comparison of performance between our method, IOPG, and SAC-LSP is summarized in Table 4. We report the performance after 1 million steps. It is worth noting that adInfoHRL outperformed IOPG on these tasks in terms of the achieved return, although we are aware that the qualitative performance is also important in HRL. AdInfoHRL outperformed SAC-LSP on Walker2d-v1 and Ant-rllab, and SAC-LSP shows its superiority on HalfCheetah-v1 and Hopper-v1. However, the results of SAC-LSP were obtained by using reward scaling, which was not used in the evaluation of adInfoHRL. Therefore, further experiments are necessary for fair comparison under the same condition.

[ACKNOWLEDGMENTS]
MS was partially supported by KAKENHI 17H00757.","[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks."
Hierarchical Reinforcement Learning via Advantage-Weighted Information Maximization,Hyl_vjC5KQ.json,"The paper considers the problem of hierarchical reinforcement learning, and proposes a criterion that aims to maximize the mutual information between options and state-action pairs.

The idea of having options partition the state-action space is appealing, because this allows options visit the same states, so long as they act differently, which is natural. The authors show empirically that the learned options do indeed decompose the state-action space, but not the state space.

There is a lot in the paper already, but the exposition could be much improved. Many of the design choices appear very ad hoc, and some are outright confusing. Some detailed comments:

* I got really confused in Section 3 re: advantage-weighted importance sampling. Why do this? If the option policies are trying to optimize reward, won’t they become optimal eventually (or so we usually hope in RL)? This section seems to assume that the advantage function is somehow given. It also doesn’t look like this gets used in the actual algorithm, and in fact on page 5 it is stated that “we decided to use the on-policy buffer in our implementation”. Then why introduce the off-policy bit at all, and list it as a contribution?
* Please motivate the choices. The paper mentions that one of its contributions are options with deterministic policies. This isn’t a contribution unless it addresses some problem that stochastic policies fail at. For example, DPG allows one to address continuous control problems.
Same with using information maximization. The paper literally states that “an interpretable representation can be learned by maximizing mutual information”. Representation of what? MI between what?
* Although the qualitative results are nice (separation of the state-action space), empirical results are modest at best. This may be ok, because based on the partition of the state-action space it seems that the option policies learn diverse behaviors in the same states. Maybe videos visualizing different options from the same states would be informative.
* Please add more discussion on why the options are switched at every step","[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks.

[CAPTIONS]
Table 1: Figure 1 :1Figure1: Schematic sketch of our HRL approach. By using the advantage-weighted importance, the problem of finding the modes of the advantage-function can be reduced to that of finding the modes of the density of state action pairs.
Table 2: empirically show that this regularization improves the performance of learning latent discrete representations. When computing MI, we need to compute p(o) and H(o|s, a) given by p(o) = p πAd (s, a)p(o|s, a; η)dads = E (s,a)∼p π Ad (s,a) [p(o|s, a; η)] (8) H(o|s, a) = E (s,a)∼p π Ad (s,a) [p(o|s, a; η) log p(o|s, a; η)] .
Table 3: Figure 2 :2Figure 2: Activation of the four options over time steps on the Walker2d task.
Table 4: The activation of options over time and snapshots of the learned option policies on the Walker2d task are shown in Figure2, which visualizes the result from adInfoHRL with four options. One can see that the option policies are activated in different phases of locomotion. While the option indicated by yellow in Figure2corresponds to the phase for kicking the floor, the option indicated by blue corresponds to the phase when the agent was on the fly. Visualization of the options learned on the HalfCheetah and Ant tasks are shown in Appendix D.
Table 5: Output of the option network in the state-action space on Walker2d. Activation of options in the state space on Walker2d.
Table 6: Figure 3 :3Figure 3: Performance of adInfoHRL. (a)-(d) show comparison with baseline methods. (e) and (f) show the output of the option network and the activation of options on Walker2d, respectively.
Table 7: mutual information (MI) between the latent variable o and the state action pair (s, a) is defined as I (s, a), o = H(o) − H(o|s, a) (20) where H(o) = p(o) log p(o)do and H(o|s, a) = p(o|s, a) log p(o|s, a)do. We make the empirical estimate of MI employed by Gomes et al. (2010); Hu et al. (2017) and modify it to employ the importance weight. The empirical estimate of MI with respect to the density induced by a policy π is given byÎ (s, a; o) = o∈Op (o) logp(o) −Ĥ(o|s, a). (21) We consider the case where we have samples collected by a behavior policy β(s|a) and need to estimate MI with respect to the density induced by policy π. Given a model p(o|s, a; η) parameterized by vector η, p(o) can be rewritten as p(o) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η)dads = E [W (s, a)p(o|s, a; η)] ,
Table 8: whereW (s, a) =W (s,a) N j=1W (sj ,aj ) is the normalized importance weight. Likewise, the conditional entropy with respect to the density induced by a policy π is given by H(o|s, a) = p π (s, a)p(o|s, a; η) log p(o|s, a; η)dsda (24) = p β (s, a) p π (s, a) p β (s, a) p(o|s, a; η) log p(o|s, a; η)dsda (25) = E [W (s, a)p(o|s, a; η) log p(o|s, a; η)] .
Table 9: o∈O π(o|s)π(a|s, o)Q π (s, a)da = o∈O π(o|s) π(a|s, o)Q π (s, a)da (28) Since option policies are deterministic given by µ o θ (s), the state-value function is given by V (s) = o∈O π(o|s)Q π (s, µ o θ (s))da. (29)
Table 10: Activation of options in the state space on HalfCheetah-v1.
Table 11: Figure 4 :4Figure 4: Distribution of options on the HalfCheetah-v1 task using adInfoHRL with two options. The dimensionality is reduced by t-SNE for visualization.
Table 12: Figure 5 :5Figure 5: Activation of options over time steps on the HalfCheetah-v1 task using adInfoHRL with two options.
Table 13: The outputs of the option network and the activation of options on Walker2d are shown in Fig-
Table 14: Hyperparameters of adInfoHRL used in the experiment.
Table 15: Hyperparameters of PPO used in the experiment. We tuned hyperparameters for our tasks, which are defferent from the default parameters in OpenAI baselines(Dhariwal et al., 2017).

[INTRODUCTION]
Reinforcement learning (RL) has been successfully applied to a variety of tasks, including board games , robotic manipulation tasks (Levine et al., 2016), and video games (Mnih et al., 2015). Hierarchical reinforcement learning (HRL) is a type of RL that leverages the hierarchical structure of a given task by learning a hierarchical policy (Sutton et al., 1999;Dietterich, 2000). Past studies in this field have shown that HRL can solve challenging tasks in the video game domain (Vezhnevets et al., 2017;Bacon et al., 2017) and robotic manipulation (Daniel et al., 2016;Osa et al., 2018b). In HRL, lower-level policies, which are often referred to as option policies, learn different behavior/control patterns, and the upper-level policy, which is often referred to as the gating policy, learns to select option policies. Recent studies have developed HRL methods using deep learning (Goodfellow et al., 2016) and have shown that HRL can yield impressive performance for complex tasks (Bacon et al., 2017;Frans et al., 2018;Vezhnevets et al., 2017;Haarnoja et al., 2018a). However, identifying the hierarchical policy structure that yields efficient learning is not a trivial task, since the problem involves learning a sufficient variety of types of behavior to solve a given task.
In this study, we present an HRL method via the mutual information (MI) maximization with advantage-weighted importance, which we refer to as adInfoHRL. We formulate the problem of learning a latent variable in a hierarchical policy as one of learning discrete and interpretable repre-sentations of states and actions. Ideally, each option policy should be located at separate modes of the advantage function. To estimate the latent variable that corresponds to modes of the advantage function, we introduce advantage-weighted importance weights. Our approach can be considered to divide the state-action space based on an information maximization criterion, and it learns option policies corresponding to each region of the state-action space. We derive adInfoHRL as an HRL method based on deterministic option policies that are trained based on an extension of the deterministic policy gradient (Silver et al., 2014;Fujimoto et al., 2018). The contributions of this paper are twofold:
1. We propose the learning of a latent variable of a hierarchical policy as a discrete and hidden representation of the state-action space. To learn option policies that correspond to the modes of the advantage function, we introduce advantage-weighted importance. 2. We propose an HRL method, where the option policies are optimized based on the deterministic policy gradient and the gating policy selects the option that maximizes the expected return. The experimental results show that our proposed method adInfoHRL can learn a diversity of options on continuous control tasks. Moreover, our approach can improve the performance of TD3 on such tasks as the Walker2d and Ant tasks in OpenAI Gym with MuJoco simulator.

[BACKGROUND]
In this section, we formulate the problem of HRL in this paper and describe methods related to our proposal.

[HIERARCHICAL REINFORCEMENT LEARNING]
We consider tasks that can be modeled as a Markov decision process (MDP), consisting of a state space S, an action space A, a reward function r : S × A → R, an initial state distribution ρ(s 0 ), and a transition probability p(s t+1 |s t , a t ) that defines the probability of transitioning from state s t and action a t at time t to next state s t+1 . The return is defined as R t =
T i=t γ i−t r(s i , a i ), where γ is a discount factor, and policy π(a|s) is defined as the density of action a given state s. Let d π (s) = T t=0 γ t p(s t = s) denote the discounted visitation frequency induced by the policy π. The goal of reinforcement learning is to learn a policy that maximizes the expected return J(π) = E s0,a0,... [R 0 ] where s 0 ∼ ρ(s 0 ), a ∼ π and s t+1 ∼ p(s t+1 |s t , a t ). By defining the Q-function as Q π (s, a) = E s0,a0,... [R t |s t = s, a t = a], the objective function of reinforcement learning can be rewritten as follows:
J(π) = d π (s)π(a|s)Q π (s, a)dads.(1)
Herein, we consider hierarchical policy π(a|s) = o∈O π(o|s)π(a|s, o), where o is the latent variable and O is the set of possible values of o. Many existing HRL methods employ a policy structure of this form (Frans et al., 2018;Vezhnevets et al., 2017;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016). In general, latent variable o can be discrete (Frans et al., 2018;Bacon et al., 2017;Florensa et al., 2017;Daniel et al., 2016;Osa & Sugiyama, 2018) or continuous (Vezhnevets et al., 2017). π(o|s) is often referred to as a gating policy (Daniel et al., 2016;Osa & Sugiyama, 2018), policy over options (Bacon et al., 2017), or manager (Vezhnevets et al., 2017). Likewise, π(a|s, o) is often referred to as an option policy (Osa & Sugiyama, 2018), sub-policy (Daniel et al., 2016), or worker (Vezhnevets et al., 2017). In HRL, the objective function is given by
J(π) = d π (s) o∈O π(o|s)π(a|s, o)Q π (s, a)dads.(2)
As discussed in the literature on inverse RL (Ziebart, 2010), multiple policies can yield equivalent expected returns. This indicates that there exist multiple solutions to latent variable o that maximizes the expected return. To obtain the preferable solution for o, we need to impose additional constraints in HRL. Although prior work has employed regularizers (Bacon et al., 2017) and constraints (Daniel et al., 2016) to obtain various option policies, the method of learning a good latent variable o that improves sample-efficiency of the learning process remains unclear. In this study we propose the learning of the latent variable by maximizing MI between latent variables and state-action pairs.

[DETERMINISTIC POLICY GRADIENT]
The deterministic policy gradient (DPG) algorithm was developed for learning a monolithic deterministic policy µ θ (s) : S → A by Silver et al. (2014). In off-policy RL, the objective is to maximize the expectation of the return, averaged over the state distribution induced by a behavior policy β(a|s):
J(π) = d β (s)π(a|s)Q π s, a)dads.(3)
When a policy is deterministic, the objective becomes J(π) = d β (s)Q π s, µ θ (s) ds. Silver et al. (2014) have shown that the gradient of a deterministic policy is given by
∇ θ E s∼d β (s) [Q π (s, a)] = E s∼d β (s) ∇ θ µ θ (s)∇ a Q π s, a | a=µ θ (s) .(4)
The DPG algorithm has been extended to the deep deterministic policy gradient (DDPG) for continuous control problems that require neural network policies . Twin Delayed Deep Deterministic policy gradient algorithm (TD3) proposed by Fujimoto et al. (2018) is a variant of DDPG that outperforms the state-of-the-art on-policy methods such as TRPO (Schulman et al., 2017a) and PPO (Schulman et al., 2017b) in certain domains. We extend this deterministic policy gradient to learn a hierarchical policy.

[REPRESENTATION LEARNING VIA INFORMATION MAXIMIZATION]
Recent studies such as those by ; Hu et al. (2017); Li et al. (2017) have shown that an interpretable representation can be learned by maximizing MI. Given a dataset X = (x 1 , ..., x n ), regularized information maximization (RIM) proposed by Gomes et al. (2010) involves learning a conditional modelp(y|x; η) with parameter vector η that predicts a label y. The objective of RIM is to minimize
(η) − λI η (x, y),(5)
where (η) is the regularization term, I η (x, y) is MI, and λ is a coefficient. MI can be decomposed as I η (x, y) = H(y) − H(y|x) where H(y) is entropy and H(y|x) the conditional entropy. Increasing H(y) conduces the label to be uniformly distributed, and decreasing H(y|x) conduces to clear cluster assignments. Although RIM was originally developed for unsupervised clustering problems, the concept is applicable to various problems that require learning a hidden discrete representation.
In this study, we formulate the problem of learning the latent variable o of a hierarchical policy as one of learning a latent representation of the state-action space.

[LEARNING OPTIONS VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
In this section, we propose a novel HRL method based on advantage-weighted information maximization. We first introduce the latent representation learning via advantage-weighted information maximization, and we then describe the HRL framework based on deterministic option policies.

[LATENT REPRESENTATION LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION]
Although prior work has often considered H(o|s) or I(s, o), which results in a division of the state space, we are interested in using I (s, a), o for dividing the state-action space instead. A schematic sketch of our approach is shown in Figure 1. As shown in the left side of Figure 1, the advantage function often has multiple modes. Ideally, each option policies should correspond to separate modes of the advantage function. However, it is non-trivial to find the modes of the advantage function in practice. For this purpose, we reduce the problem of finding modes of the advantage function to that of finding the modes of the probability density of state action pairs.
We consider a policy based on the advantage function of the form  where
π Ad (a|s) = f A π (s, a) Z ,(6)
A π (s, a) = Q π (s, a) − V π (s) is the advantage function, V π (s)
is the state value function, and Z is the partition function. f (•) is a functional, which is a function of a function. f (•) is a monotonically increasing function with respect to the input variable and always satisfies f (•) > 0.
In our implementation we used the exponential function f (•) = exp(•). When following such a policy, an action with the larger advantage is drawn with a higher probability. Under this assumption, finding the modes of the advantage function is equivalent to finding modes of the density induced by π Ad . Thus, finding the modes of the advantage function can be reduced to the problem of clustering samples induced by π Ad .
Following the formulation of RIM introduced in Section 2.3, we formulate the problem of clustering samples induced by π Ad as the learning of discrete representations via MI maximization. For this purpose, we consider a neural network that estimates p(o|s, a; η) parameterized with vector η, which we refer to as the option network. We formulate the learning of the latent variable o as minimizing L option (η) = (η) − λI o, (s, a); η ,
where I(o, (s, a)) =Ĥ(o|s, a; η) −Ĥ(o; η), and (η) is the regularization term. In practice, we need to approximate the advantage function, and we learn the discrete variable o that corresponds to the modes of the current estimate of the advantage function. For regularization, we used a simplified version of virtual adversarial training (VAT) proposed by Miyato et al. (2016). Namely, we set (η) = D KL p(o|s noise , a noise ; η)||p(o|s, a; η) where s noise = s + s , a noise = a + a , s and a denote white noise. This regularization term penalizes dissimilarity between an original state-action pair and a perturbed one, and Hu et al. (2017) 
Thus, the probability density of (s, a) induced by π Ad is necessary for computing MI for our purpose. To estimate the probability density of (s, a) induced by π Ad , we introduce the advantageweighted importance in the next section.

[IMPORTANCE WEIGHTS FOR MUTUAL INFORMATION ESTIMATION]
Although we show that the problem of finding the modes of the advantage function can be reduced to MI maximization with respect to the samples induced by π Ad , samples induced by π Ad are not available in practice. While those induced during the learning process are available, a discrete representation obtained from such samples does not correspond to the modes of the advantage function.
To estimate the density induced by π Ad , we employ an importance sampling approach.
We assume that the change of the state distribution induced by the policy update is sufficiently small, namely, d πAd (s) ≈ d β (s). Then, the importance weight can be approximated as
W (s, a) = p πAd (s, a) p β (s, a) = d πAd (s)π Ad (a|s) d β (s)β(a|s) ≈ π Ad (a|s) β(a|s) = f (A(s, a)) Zβ(a|s) . (10
)
and the normalized importance weight is given gỹ
W (s, a) = W (s, a) N j=1 W (s j , a j ) = f (A(s,a)) Zβ(a|s) N j=1 f (A(sj ,aj )) Zβ(aj |sj ) = f (A(s,a)) β(a|s) N j=1 f (A(sj ,aj )) β(aj |sj ) . (11
)
As the partition function Z is canceled, we do not need to compute Z when computing the importance weight in practice. We call this importance weight W the advantage-weighted importance and employ it to compute the objective function used to estimate the latent variable.
This advantage-weighted importance is used to compute the entropy terms for computing MI in Equation ( 7). The empirical estimate of the entropy H(o) is given bŷ
H(o; η) = − o∈Op (o; η) logp(o; η), wherep(o; η) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η). (12
)
where the samples (s i , a i ) are drawn from p β (s, a) induced by a behavior policy β(a|s). Likewise, the empirical estimate of the conditional entropy H(o|s, a) is given bŷ
H(o|s, a; η) = 1 N N i W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(13)
The derivations of Equations ( 12) and ( 13) are provided in Appendix A. To train the option network, we store the samples collected by the M most recent behavior policies, to which we refer as onpolicy buffer D on . Although the algorithm works with entire samples stored in the replay buffer, we observe that the use of the on-policy buffer for latent representation learning exhibits better performance. For this reason, we decided to use the on-policy buffer in our implementation. Therefore, while the algorithm is off-policy in the sense that the option is learned from samples collected by behavior policies, our implementation is ""semi""on-policy in the sense that we use samples collected by the most recent behavior policies.

[HRL OBJECTIVE WITH DETERMINISTIC OPTION POLICIES]
Instead of stochastic option policies, we consider deterministic option policies and model them using separate neural networks. We denote by π(a|s, o) = µ o θ (s) deterministic option policies parameterized by vector θ. The objective function of off-policy HRL with deterministic option policies can then be obtained by replacing π(a|s) with o∈O π(o|s)π(a|s, o) in Equation (3):
J(w, θ) = d β (s) o∈O π(o|s)Q π s, µ o θ (s); w ds, (14
)
where Q π (s, a; w) is an approximated Q-function parameterized using vector w. This form of the objective function is analogous to Equation (3). Thus, we can extend standard RL techniques to the learning of the gating policy π(o|s) in HRL with deterministic option policies.
In HRL, the goal of the gating policy is to generate a value of o that maximizes the conditional expectation of the return:
Q π Ω (s, o) = E [R|s t = s, o t = o] = π(a|s, o)Q π (s, a)da,(15)
which is often referred to as the option-value function (Sutton et al., 1999). When option policies are stochastic, it is often necessary to approximate the option-value function Q π Ω (s, o) in addition to the action-value function Q π (s, a). However, in our case, the option-value function for deterministic option policies is given by
Q π Ω (s, o) = Q π (s, µ o θ (s)),(16)
Algorithm 1 HRL via Advantage-Weighted Information Maximization (adInfoHRL)
Input: Number of options O, size of on-policy buffer Initialize: Replay buffer D R , on-policy buffer D on , network parameters η, θ, w, θ target , w target repeat for t = 0 to t = T do Draw an option for a given s by following Equation 17: o ∼ π(o|s) Draw an action a ∼ β(a|s, o) = µ o θ (s) + Record a data sample (s, a, r, s ) Aggregate the data in D R and D on if the on-policy buffer is full then Update the option network by minimizing Equation ( 7) for samples in D on Clear the on-policy buffer D on end if Sample a batch D batch ∈ D R Update the Q network parameter w if t mod d then Estimate p(o|s i , a i ) for (s i , a i ) ∈ D batch using the option network Assign samples (s i , a i ) ∈ D batch to the option o * = arg max p(o|s i , a i ) Update the option policy networks µ o θ (s) for o = 1, ..., O with Equation ( 19) Update the target networks:
w target ← τ w +(1−τ )w target , θ target ← τ θ +(1−τ )θ target end if end for until the convergence return θ
which we can estimate using the deterministic option policy µ o θ (s) and the approximated actionvalue function Q π (s, a; w). In this work we employ the softmax gating policy of the form
π(o|s) = exp Q π (s, µ o θ (s); w) o∈O exp Q π s, µ o θ (s); w ,(17)
which encodes the exploration in its form (Daniel et al., 2016). The state value function is given as
V π (s) = o∈O π(o|s)Q π (s, µ o θ (s); w),(18)
which can be computed using Equation (17). We use this state-value function when computing the advantage-weighted importance as A(s, a) = Q(s, a) − V (s). In this study, the Q-function is trained in a manner proposed by Fujimoto et al. (2018). Two neural networks (Q π w1 , Q π w2 ) are trained to estimate the Q-function, and the target value of the Q-function is computed as y i = r i + γ min 1,2 Q(s i , a i ) for sample (s i , a i , a i , r i ) in a batch sampled from a replay buffer, where r i = r(s i , a i ). In this study, the gating policy determines the option once every N time steps, i.e., t = 0, N, 2N, . . .

[NEURAL NETWORKS THAT MODEL Μ O]
θ (a|s) for o = 1, ..., O, which we refer to as option-policy networks, are trained separately for each option. In the learning phase, p(o|s, a) is estimated by the option network. Then, samples are assigned to option o * = arg max o p(o|s, a; η) and are used to update the option-policy network that corresponds to o * . When performing a rollout, o is drawn by following the gating policy in Equation ( 17), and an action is generated by the selected option-policy network.
Differentiating the objective function in Equation ( 14), we obtain the deterministic policy gradient of our option-policy µ o θ (s) given by
∇ θ J(w, θ) = E s∼d β (s)π(o|s) ∇ θ µ o θ (s)∇ a Q π s, a | a=µ o θ (s) . (19
)
The procedure of adInfoHRL is summarized by Algorithm 1. As in TD3 (Fujimoto et al., 2018), we employed the soft update using a target value network and a target policy network. 

[EXPERIMENTS]
We evaluated the proposed algorithm adInfoHRL on the OpenAI Gym platform (Brockman et al., 2016) with the MuJoCo Physics simulator (Todorov et al., 2012). We compared its performance with that of PPO implemented in OpenAI baselines (Dhariwal et al., 2017) and TD3. Henderson et al. (2018) have recently claimed that algorithm performance varies across environment, there is thus no clearly best method for all benchmark environments, and off-policy and on-policy methods have advantages in different problem domains. To analyze the performance of adInfoHRL, we compared it with state-of-the-art algorithms for both on-policy and off-policy methods, although we focused on the comparison with TD3, as our implementation of adInfoHRL is based on it. To determine the effect of learning the latent variable via information maximization, we used the same network architectures for the actor and critic in adInfoHRL and TD3. In addition, to evaluate the benefit of the advantage-weighted importance, we evaluated a variant of adInfoHRL, which does not use the advantage-weighted importance for computing mutual information.We refer to this variant of adInfoHRL as infoHRL. The gating policy updated variable o once every three time steps. We tested the performance of adInfoHRL with two and four options. The averaged return of five trials is reported in Figure 3(a)-(d). AdIfoHRL yields the best performance on Ant 1 and Walker2d, whereas the performance of TD3 and adInfoHRL was comparable on HalfCheetah and Hopper, and PPO outperformed the other methods on Hopper. Henderson et al. (2018) claimed that on-policy methods show their superiority on tasks with unstable dynamics, and our experimental results are in line with such previous studies. AdinfoHRL outperformed infoHRL, which isthe variant of adInfoHRL without the advantage-weighted importance on all the tasks. This result shows that the adavatage-weighted importance enhanced the performance of learning options.
AdInfoHRL exhibited the sample efficiency on Ant and Walker2d in the sense that it required fewer samples than TD3 to achieve comparable performance on those tasks. The concept underlying ad-InfoHRL is to divide the state-action space to deal with the multi-modal advantage function and learn option policies corresponding to separate modes of the advantage function. Therefore, adIn-foHRL shows its superiority on tasks with the multi-modal advantage function and not on tasks with a simple advantage function. Thus, it is natural that the benefit of adInfoHRL is dependent on the characteristics of the task.   

[RELATED WORK AND DISCUSSION]
Past studies have proposed several ways to deal with the latent variable in HRL. The recent work by Smith et al. (2018) proposed inferred option policy gradients (IOPG), which is derived as an extension of policy gradient to the option framework. Nachum et al. (2018) recently proposed off-policy target correction for HRL on goal-oriented tasks, where a higher-level policy instructs a lower-level policy by generating the goal signal instead of an inferred latent variable. A popular approach for learning the latent variable in HRL is the variational approach. The recent work by Haarnoja et al. (2018a) is based on soft actor critic (Haarnoja et al., 2018b), and the latent variable is inferred using the variational approach. The work by Hausman et al. (2018) is also closely related to the variational approach, and they proposed a method for learning a latent variable of a hierarchical policy via a variational bound. On the contrary, our method learns the latent variable by maximizing MI with advantage-weighted importance. Recent studies by Gregor et al. (2016); Florensa et al. (2017); Eysenbach et al. (2018) also considered the MI in their formulation. In these methods, MI between the state and the latent variable is considered so as to obtain diverse behaviors. Our approach is different from the previous studies in the sense that we employ MI between the latent variable and the state-action pairs, which leads to the division of the state-action space instead of considering only the state space. We think that dividing the state-action space is an efficient approach when the advantage function is multi-modal, as depicted in Figure 1. InfoGAIL proposed by Li et al. (2017) learns the interpretable representation of the state-action space via MI maximization. InfoGAIL can be interpreted as a method that divides the state-action space based on the density induced by an expert's policy by maximizing the regularized MI objective. In this sense, it is closely related to our method, although their problem setting is imitation learning (Osa et al., 2018a), which is different from our HRL problem setting.
The use of the importance weight based on the value function has appeared in previous studies (Dayan & Hinton, 1997;Kober & Peters, 2011;Neumann & Peters, 2009;Osa & Sugiyama, 2018). For example, the method proposed by Neumann & Peters (2009) employs the importance weight based on the advantage function for learning a monolithic policy, while our method uses a similar importance weight for learning a latent variable of a hierarchical policy. Although Osa & Sugiyama (2018) proposed to learn a latent variable in HRL with importance sampling, their method is limited to episodic settings where only a single option is used in an episode.
Our method can be interpreted as an approach that divides the state-action space based on the MI criterion. This concept is related to that of Divide and Conquer (DnC) proposed by Ghosh et al. (2018), although DnC clusters the initial states and does not consider switching between option policies during the execution of a single trajectory.
In this study we developed adInfoHRL based on deterministic option policies. However, the concept of dividing the state-action space via advantage-weighted importance can be applied to stochastic policy gradients as well. Further investigation in this direction is necessary in future work.

[CONCLUSIONS]
We proposed a novel HRL method, hierarchical reinforcement learning via advantage-weighted information maximization. In our framework, the latent variable of a hierarchical policy is learned as a discrete latent representation of the state-action space. Our HRL framework is derived by considering deterministic option policies and by leveraging the analogy between the gating policy for HRL and a monolithic policy for the standard RL. The results of the experiments indicate that adInfoHRL can learn diverse options on continuous control tasks. Our results also suggested that our approach can improve the performance of TD3 in certain problem domains.

[A MUTUAL INFORMATION WITH ADVANTAGE-WEIGHTED IMPORTANCE]
The 
where W (s, a) = p π (s,a) p β (s,a) is the importance weight. Therefore, the empirical estimate of p(o) with respect to the density induced by a policy π is given bŷ 
p(o) = 1 N N i=1W (s i , a i )p(o|s i , a i ; η),(23)
Therefore, the empirical estimate of the conditional entropy with respect to the density induced by a policy π is given bŷ
H(o|s, a) = 1 N N i=1 W (s i , a i )p(o|s i , a i ; η) log p(o|s i , a i ; η).(27)
Thus, the empirical estimates of MI can be computed by Equations ( 21), ( 23) and ( 27).

[B DERIVATION OF THE STATE-VALUE FUNCTION]
In HRL, the value function is given by V (s) =  

[C EXPERIMENTAL DETAILS]
We performed evaluations using benchmark tasks in the OpenAI Gym platform (Brockman et al., 2016) with Mujoco physics simulator (Todorov et al., 2012). Hyperparameters of reinforcement learning methods used in the experiment are shown in Tables 1-3. For exploration, both adInfoHRL and TD3 used the clipped noise drawn from the normal distribution as ∼ clip N (0, σ), −c, c , where σ = 0.2 and c = 0.5. For hyperparameters of PPO, we used the default values in OpenAI baselines (Dhariwal et al., 2017). For the Walker2d, HalfCheetah, and Hopper tasks, we used the Walker2d-v1, HalfCHeetah-v1, and Hopper-v1 in the OpenAI Gym, respectively. For the Ant task, we used the AntEnv implemented in the rllab . When training a policy with AdInfoHRL, infoHRL, and TD3, critics are trained once per time step, and actors are trained once every after two updates of the critics. The source code is available at https://github.com/ TakaOsa/adInfoHRL.
We performed the experiments five times with different seeds, and reported the averaged test return where the test return was computed once every 5000 time steps by executing 10 episodes without exploration. When performing the learned policy without exploration, the option was drawn as
o = max o Q π (s, µ o (s)),(30)
instead of following the stochastic gating policy in Equations (17).   The dimensionality is reduced by t-SNE for visualization.

[D ADDITIONAL INFORMATION ON EXPERIMENTAL RESULTS]
On the HalfCheetah task, adInfoHRL delivered the best performance with two options. The distribution of options on HalfCheetah0v1 after one million steps is shown in Figure 4. Although the state-action space is evenly divided, the options are not evenly activated. This behavior can occur because the state-action space is divided based on the density induced by the behavior policy while the activation of options is determined based on the quality of the option policies in a given state. Moreover, an even division in the action-state space is not necessarily the even division in the state space.
The activation of the options over time is shown in Figure 5. It is clear that one of the option corresponds to the stable running phase and the other corresponds to the phase for recovering from unstable states. Figure 6: Distribution of options on Ant-rllab task using adInfoHRL with four options. The dimensionality is reduced by t-SNE for visualization.
Figure 7: Activation of the options over time steps on Ant-rllab task. Four options are learned.
The distribution of four options on the Ant-rllab task after one million steps is shown in Figure 6. Four options are activated in the different domains of the state space. The activation of the options over time on the Ant-rllab task is shown in Figure 7. While four options are actively used in the beginning of the episode, two (blue and yellow) options are mainly activated during the stable locomotion.
Since the Ant task implemented in rllab is known to be harder than the Ant-v1 implemented in the OpenAI gym, we reported the result of the Ant task in rllab in the main manuscript. Here, we report the result of the Ant-v1 task implemented in the OpenAI gym. On the Ant-v1 task, adInfoHRL yielded the best performance with two options. The performance of adInfoHRL with two options is comparable to that of TD3 on Ant-v1. This result indicates that the Ant-v1 task does not require a hierarchical policy structure, while a hierarchical policy improves the performance of learning on Ant-rllab. The distribution of options on Ant-v1 task after one million steps is shown in Figure 8. The activation of the options over time is shown in Figure 9. It is evident that two option policies on the Ant-v1 task corresponded to different postures of the agent.
A recent study on HRL by Smith et al. (2018) reported the performance of IOPG on Walker2d-v1, Hopper-v1, and HalfCheetah-v1. The study by Haarnoja et al. (2018a) reported the performance of SAC-LSP on Walker2d-v1, Hopper-v1, HalfCheetah-v1, and Ant-rllab. A comparison of performance between our method, IOPG, and SAC-LSP is summarized in Table 4. We report the performance after 1 million steps. It is worth noting that adInfoHRL outperformed IOPG on these tasks in terms of the achieved return, although we are aware that the qualitative performance is also important in HRL. AdInfoHRL outperformed SAC-LSP on Walker2d-v1 and Ant-rllab, and SAC-LSP shows its superiority on HalfCheetah-v1 and Hopper-v1. However, the results of SAC-LSP were obtained by using reward scaling, which was not used in the evaluation of adInfoHRL. Therefore, further experiments are necessary for fair comparison under the same condition.

[ACKNOWLEDGMENTS]
MS was partially supported by KAKENHI 17H00757.","[TITLE]
HIERARCHICAL REINFORCEMENT LEARNING VIA ADVANTAGE-WEIGHTED INFORMATION MAXIMIZATION

[ABSTRACT]
Real-world tasks are often highly structured. Hierarchical reinforcement learning (HRL) has attracted research interest as an approach for leveraging the hierarchical structure of a given task in reinforcement learning (RL). However, identifying the hierarchical policy structure that enhances the performance of RL is not a trivial task. In this paper, we propose an HRL method that learns a latent variable of a hierarchical policy using mutual information maximization. Our approach can be interpreted as a way to learn a discrete and latent representation of the state-action space. To learn option policies that correspond to modes of the advantage function, we introduce advantage-weighted importance sampling. In our HRL method, the gating policy learns to select option policies based on an option-value function, and these option policies are optimized based on the deterministic policy gradient method. This framework is derived by leveraging the analogy between a monolithic policy in standard RL and a hierarchical policy in HRL by using a deterministic option policy. Experimental results indicate that our HRL approach can learn a diversity of options and that it can enhance the performance of RL in continuous control tasks."
An Adversarial Learning Framework for a Persona-based Multi-turn Dialogue Model,rkeYUsRqKQ.json,"===============================
I have read the authors' response and other reviewers' comments carefully. Thank you for taking great efforts to improve the paper, including providing additional results on human evaluation. (Btw, Table 1 and Table 2 are also much nicer now.)

However, from the reviews it seems that all the reviewers agree that the novelty of this paper is limited, and the contribution is incremental.  I understand that this paper is the first and only work using adversarial framework for persona multi-turn conversation models. However, from the modeling perspective, I still think the novelty is limited.

As a summary, I have updated the score from 4 to 5 to reflect the efforts that the authors have been taken to improve the paper. However, due to reasons above, I still prefer a rejection recommendation. 

===============================

Contributions:

The main contribution of this paper is the proposed phredGAN, which is a persona-based GAN framework for multi-turn dialogue modeling. Specifically, a persona-based HRED generator is developed, with two different kinds of discriminator design. Experiments are conducted on both the UDC and the TV series transcript datasets.  

Weaknesses:

(1) Novelty: I would say the novelty of this paper is rather limited. This paper heavily rely on the previous hredGAN work (Olabiyi et al., 2018), and extends it by injecting attributes into the system, borrowing ideas from the persona-based Seq2seq model (Li et al, 2016b). 

phredGAN_a is a straightforward extension of hredGAN, while phredGAN_d further introduces a collaborative discriminator that tries to predict the attribute that generated the input utterance. However, in summary, I think this paper is not novel enough. 

(2) Presentation: The paper is generally easy to follow and understand. However, I would say the paper is poorly written, and needs further polishing. For example, Table 1 & 2 are pretty ugly. 

(3) Evaluation: Generally, I think the experiments are not convincing and also not well-executed, with detailed comments listed below. 

Questions:

(1) In phredGAN_a, as shown in Eqn. (4), the attribute is used as input of the discriminator, while in phredGAN_d, as shown in Eqn. (5) & (6), the attribute is used as the target of the discriminator. My question is: why not use the attribute as both input & output? That is, why not combine (4) & (6), instead of using (5) & (6)? Please clarify this. 

(2) In experiments, Section 3.1, the authors mention that the generator and the discriminator use a shared encoder. However, the generator and discriminator has a different role. Since the encoder is shared, then: in one step, we update the encoder to minimize the GAN objective, in the alternative step, we update the encoder again to maximize the GAN objective. So, how to deal with this conflicting role of encoder during the training? Please clarify this. 

(3) From Table 2, it seems that it is difficult to see that phredGAN is better than hredGAN. Can you provide some explanations here?

(4) In Table 4, if the responses generated by hredGAN can be provided, that would be better to demonstrate the advantage of phredGAN. How does phredGAN compare with hredGAN qualitatively?

(5) From Table 1 & 2, it seems to me there is no metric that is specifically designed to evaluate whether the model captures the attribute information. Is there a way to quantitatively evaluate this? For example, pretrain an attribute classifier, or use the collaborative discriminator in the phredGAN_d model to measure how the generated response reflect the attribute. If we can observe the performance of phredGAN is better than that of hredGAN, that would be helpful for the paper.  

(6) Since the task is challenging, and the automatic metrics designed for this task is not perfect, like other papers, I think human evaluation is essential and desired for this task. However, such human evaluation is lacked in this paper.
  ","[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).

[CAPTIONS]
Table 1: phredGAN vs.Li et al. (2016b) on BBT Friends TV Transcripts.
Table 2: phredGAN vs. Li et al. (2016b) on UDC.
Table 3: hredGAN vs phredGAN vs Li et al. (2016b) on TV Transcripts
Table 4: hredGAN vs phredGAN vsLi et al. (2016b) on UDC Of course I love you."" to the dialogue context, ""Do you love me?"" despite the fact that some of the responders sometimes have unfriendly relationship with the addressee. Many of the novel situations explored by phredGAN are unachievable with the Speaker-Addressee model due to lack of informative responses. For example, by conditioning as Sheldon from The Big Bang Theory and asking ""Do you like me?"", our model responds with annoyance if conditioned as Penny (""No, you don't understand. You're an idiot""), brevity with Leonard (""Yes?"") and sarcasm with Raj (""Well , you know , we could be a little more than my friend's friends."") The wide range of responses indicate our model's ability to construct distinct attribute embeddings for each character even from a limited dataset. The other interesting responses in table 3 indicate phredGAN 's ability to infer not only the context of the conversation but important character information about the addressee.
Table 5: Sample of PHRED outputs on UDC and TV Series Howard Okay, you have to understand something, we're in a hospital right now. Response 0 Penny Oh , I ' m sorry . I was just trying to be a girl .Context 0 Sheldon Did I? Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah Response 0 Leonard I ' m sorry . I just don ' t want to be a man who ' s going to be my girlfriend .Context 0 Leonard She's gotten really hooked on Age of Conan, she's playing non-stop.
Table 6: Ranked phredGAN a outputs according to adversarial word-level discrimination score Source Speaker D adv (G(.)) Utterance Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah cd , it should be in the repos , it's a good place to get the source of the kernel Response 1 helper 0.1984 use the UNK package , it should work . . .

[INTRODUCTION]
Recent advances in machine learning especially with deep neural networks has lead to tremendous progress in natural language processing and dialogue modeling research (Sutskever et al., 2014;Vinyals & Le, 2015;Serban et al., 2016). Nevertheless, developing a good conversation model capable of fluent interaction between a human and a machine is still in its infancy stage. Most existing work relies on limited dialogue history to produce response with the assumption that the model parameters will capture all the modalities within a dataset. However, this is not true as dialogue corpora tend to be strongly multi-modal and practical neural network models find it difficult to disambiguate characteristics such as speaker personality, location and sub-topic in the data.
Most work in this domain has primarily focused on optimizing dialogue consistency. For example, Serban et al. (Serban et al., 2016;2017b;a) and Xing et al. (2017) introduced a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not have any guarantee on the generator output since the output conditional probability is not calibrated. Olabiyi et al. (2018) tackles these problems by training a modified HRED generator alongside an adversarial discriminator in order to increase diversity and provide a strong and calibrated guarantee to the generator's output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modality within a dataset and fails to generate persona specific responses in datasets with multiple modalities.
On the other hand, there has been some recent work on introducing persona into dialogue models. For example, Li et al. (2016b) integrates attribute embeddings into a single turn (Seq2Seq) generative dialogue model. In this work, Li et al. consider persona models one with Speaker-only representation and the other with Speaker and Addressee representations (Speaker-Addressee model), both of which capture certain speaker identity and interactions. Nguyen et al. (2018) continue along the Figure 1: The PHRED generator with local attention -The attributes C, allows the generator to condition its response on the utterance attributes such as speaker identity, subtopics and so on. same line of thought by considering a Seq2Seq dialogue model with Responder-only representation. In both of these cases, the attribute representation is learned during the system training. Zhang et al. (2018) proposed a slightly different approach. Here, the attributes are a set of sentences describing the profile of the speaker. In this case, the attributes representation is not learned. The system however learns how to attend to different parts of the attributes during training. Still, the above persona-based models have limited dialogue history (single turn); suffer from exposure bias worsening the trade-off between personalization and conversation quality and cannot generate multiple responses given a dialogue context. This is evident in the relatively short and generic responses produced by these systems, even though they generally capture the persona of the speaker.
In order to overcome these limitations, we propose two variants of an adversarially trained persona conversational generative system, phredGAN , namely phredGAN a and phredGAN d . Both systems aim to maintain the response quality of hredGAN and still capture speaker and other attribute modalities within the conversation. In fact, both systems use the same generator architecture (PHRED generator), i.e., an hredGAN generator (Olabiyi et al., 2018) with additional utterance attribute representation at its encoder and decoder inputs as depicted in Figure 1. Conditioning on external attributes can be seen as another input modality as is the utterance into the underlying system. The attribute representation is an embedding that is learned together with the rest of model parameters similar to Li et al. (2016b). Injecting attributes into a multi-turn dialogue system allows the model to generate responses conditioned on particular attribute(s) across conversation turns. Since the attributes are discrete, it also allows for exploring different what-if scenarios of model responses. The difference between the two systems is in the discriminator architecture based on how the attribute is treated.
We train and sample both variants of phredGAN similar to the procedure for hredGAN (Olabiyi et al., 2018). To demonstrate model capability, we train on a customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poser and answerer, and transcripts from a multi-modal TV series The Big Bang Theory and Friends with quantitative and qualitative analysis. We examine the trade-offs between using either system in bi-modal or multi-modal datasets, and demonstrate system superiority over state-of-the-art persona conversational models in terms of dialogue response quality and quantitatively with perplexity, BLEU, ROUGE and distinct n-gram scores.

[MODEL ARCHITECTURE]
In this section, we briefly introduce the state-of-the-art hredGAN model and subsequently show how we derive the two persona versions by combining it with the distributed representation of the dialogue speaker and utterance attributes, or with an attribute discrimination layer at the end of the model pipeline.
Figure 2: The phredGAN d dual discriminator -Left: D adv is a word-level discriminator used by both phredGAN a and phredGAN d to judge normal dialogue coherency as in hredGAN . Right: D att , an utterance-level attribute discriminator is used only in phredGAN d to predict the likelihood a given utterance was generated from a particular attribute.

[HREDGAN : ADVERSARIAL LEARNING FRAMEWORK]
Problem Formulation: The hredGAN (Olabiyi et al., 2018) formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances,
X i = X 1 , X 2 , • • • , X i , where each utterance X i = X 1 i , X 2 i , • • • , X Mi i
contains a variable-length sequence of M i word tokens such that X i j ∈ V for vocabulary V , the dialogue model produces an output
Y i = Y 1 i , Y 2 i , • • • , Y Ti i
, where T i is the number of generated tokens. The framework uses conditional GAN structure to learn a mapping from an observed dialogue history to a sequence of output tokens. The generator, G, is trained to produce sequences that cannot be distinguished from the ground truth by an adversarially trained discriminator, D akin to a two-player min-max optimization problem. The generator is also trained to minimize the cross-entropy loss L M LE (G) between the ground truth X i+1 , and the generator output Y i . The following objective summarizes both goals:
G * , D * = arg min G max D λ G L cGAN (G, D) + λ M L M LE (G) .
(
)1
where λ G and λ M are training hyperparamters and L cGAN (G, D) and L M LE (G) are defined in Eqs.
(5) and ( 7) of Olabiyi et al. (2018) respectively. Please note that the generator G and discriminator D share the same encoder and embedding representation of the word tokens.

[PHREDGAN : PERSONA ADVERSARIAL LEARNING FRAMEWORK]
The proposed architecture of phredGAN is very similar to that of hredGAN (Olabiyi et al., 2018). The only difference is that the dialogue history is now
X i = (X 1 , C 1 ), (X 2 , C 2 ), • • • , (X i , C i )
where C i is additional input that represents the speaker and/or utterance attributes. Please note that C i can either be a sequence of tokens or single token such that C i j ∈ V c for vocabulary V c. Also, at the ith turn, C i and C i+1 are the source/input attribute and target/output attribute to the generator respectively. The embedding for attribute tokens is also learned similar to that of word tokens.
Both versions of phredGAN shares the same generator architecture (PHRED) but different discriminators. Below is the highlight of how they are derived from the hredGAN architecture.
Encoder: The context RNN, cRN N takes the source attribute C i as an additional input by concatenating its representation with the output of eRN N as in Figure 1. If the attribute C i is a sequence of tokens, then an attention (using the output of eRN N ) over the source attribute representations is concatenated with the output of eRN N . This output is used by the generator to create a context state for a turn i.
Generator: The generator decoder RNN, dRN N takes the target attribute C i+1 as an additional input as in Fig. 1. If the attribute C i+1 is a sequence of tokens, then an attention (using the output of dRN N ) over the attribute representations is concatenated with the rest of the decoder inputs. This forces the generator to draw a connection between the generated responses and the utterance attributes such as speaker identity.
Noise Injection: As in Olabiyi et al. (2018), we also explore different noise injection methods.
Objective: For phredGAN , the optimization objective in eq. ( 1) can be updated as:
G * , D * adv , D * att = arg min G max D adv λ G adv L adv cGAN (G, D adv ) (2) + min Datt λ Gatt L att c (G, D att ) + λ M L M LE (G) .
where L adv cGAN (G, D adv ) and L att c (G, D att ) are the traditional adversarial and attribute prediction loss respectively and dependent on the architectural variation. It is worth to point out that while the former is adversarial, the later is collaborative in nature. The MLE loss is common and can be expressed as:
L M LE (G) = E Xi+1 [−log P G X i+1 |X i , C i+1 , Z i ].
(3) where Z i the noise sample and depends on the choice of either utterance-level or word-level noise input into the generator (Olabiyi et al., 2018).

[PHREDGAN A : ATTRIBUTES AS A DISCRIMINATOR INPUT]
phredGAN a shares the same discriminator architecture as the hredGAN but with additional input, C i+1 . Since it does not use attribute prediction, λ Gatt = 0.
The adversarial loss, L adv cGAN (G, D) can then be expressed as:
L adv cGAN (G, D adv ) = E X i ,Ci+1,Xi+1 [log D adv (X i , C i+1 , X i+1 )] (4) + E X i ,Ci+1,Zi [1 − log D adv (X i , C i+1 , G(X i , C i+1 , Z i ))]
The addition of speaker or utterance attributes allows the dialogue model to exhibit personality traits given consistent responses across style, gender, location, and so on.

[PHREDGAN D : ATTRIBUTES AS A DISCRIMINATOR TARGET]
phredGAN d does not take the attribute representation at its input but rather use the attributes as the target of an additional discriminator D att . The adversarial and the attribute prediction losses can be respectively expressed as:
L adv cGAN (G, D adv ) = E X i ,Xi+1 [log D adv (X i , X i+1 )] (5) + E X i ,Zi [1 − log D adv (X i , G(X i , C i+1 , Z i ))] L att c (G, D att ) = E Ci+1 [− log D att (C i+1 |X i , X i+1 )] (6) + E Ci+1 [− log D att (C i+1 |X i , G(X i , C i+1 , Z i ))]
Attribute Discriminator: In addition to the existing word-level adversarial discriminator D adv from hredGAN , we add an attribute discriminator, D att , that discriminates on an utterance level to capture attribute modalities since attributes are assigned at utterance level. The discriminator uses a unidirectional RNN (D attRN N ) that maps the input utterance to the particular attribute(s) that generated it. The attributes can be seen as hidden states that inform or shape the generator outputs. The attribute discriminator can be expressed as:
D att (C i+1 |X i , χ) = D attRN N (h i , E(χ))(7)
where E(.) is the word embedding lookup (Olabiyi et al., 2018), χ = X i+1 for groundtruth and χ = Y i for the generator output.

[MODEL TRAINING]
We train both the generator and the discriminator (with shared encoder) of both variants of phredGAN using the training procedure in Algorithm 1 (Olabiyi et al., 2018). For both variants, λ G adv = λ M = 1, and for phredGAN a and phredGAN d , λ Gatt = 0 and λ Gatt = 1 respectively. Since the encoder, word embedding and attribute embedding are shared, we are able to train the system end-to-end with back-propagation.
Encoder: The encoder RNN, eRN N , is bidirectional while cRRN is unidirectional. All RNN units are 3-layer GRU cell with hidden state size of 512. We use word vocabulary size, V = 50, 000 with word embedding size of 512. The number of attributes, V c is dataset dependent but we use an attribute embedding size of 512. In this study, we only use one attribute per utterance so that is no need to use attention to combine the attribute embeddings.
Generator: The generator decoder RNN, dRN N is also a 3-layer GRU cell with hidden state size of 512. The aRN N outputs are connected to the dRN N input using an additive attention mechanism (Bahdanau et al., 2015).
Adversarial Discriminator: The word-level discriminator RNN, D RN N is a bidirectional RNN, each 3-layer GRU cell with hidden state size of 512. The output of both the forward and the backward cells for each word are concatenated and passed to a fully-connected layer with binary output.
The output is the probability that the word is from the ground truth given the past and future words of the sequence, and in the case of phredGAN a , the responding speaker's embedding.
Attribute Discriminator: The attribute discriminator RNN, D attRN N is a unidirectional RNN with a 3-layer GRU cell, each of hidden state size 512. A softmax layer is then applied to project the final hidden state to a prespecified number of attributes, V c . The output is the probability distribution over the attributes.
Others: All parameters are initialized with Xavier uniform random initialization (Glorot & Bengio, 2010). Due to the large word vocabulary size, we use sampled softmax loss (Jean et al., 2015) for MLE loss to expedite the training process. However, we use full softmax for model evaluation. For both systems, parameters updates are conditioned on the word-level discriminator accuracy performance as in Olabiyi et al. (2018) with acc D th adv = 0.99 and acc G th = 0.75. The model is trained end-to-end using the stochastic gradient descent algorithm. Finally, the model is implemented, trained, and evaluated using the TensorFlow deep learning framework.

[MODEL INFERENCE]
We use an inference strategy similar to the approach in Olabiyi et al. (2018).
For the modified noise sample, we perform a linear search for α with sample size L = 1 based on the average word-level discriminator loss, −logD adv (G(.)) (Olabiyi et al., 2018) using trained models run in autoregressive mode to reflect performance in actual deployment. The optimum α value is then used for all inferences and evaluations. During inference, we condition the dialogue response generation on the encoder outputs, noise samples, word embedding and the attribute embedding of the intended responder. With multiple noise samples, L = 64, we rank the generator outputs by the discriminator which is also conditioned on encoder outputs, and the intended responder's attribute embedding. The final response is the response ranked highest by the discriminator. For phredGAN d , we average the confidences produced by D adv and D att .

[EXPERIMENTS AND RESULTS]
In this section, we explore the performance of PHRED, phredGAN a and phredGAN d on two conversational datasets and compare its performance to non-adversarial persona Seq2seq models Li et al. (2016b) as well as to the adversarial hredGAN (Olabiyi et al., 2018) with no explicit persona.

[DATASETS]
TV Series Transcripts dataset (Serban et al., 2016). We train all models on transcripts from the two popular TV drama series, Big Bang Theory and Friends. Following a similar preprocessing setup in Li et al. (2016b), we collect utterances from the top 12 speakers from both series to construct a corpus of 5,008 lines of multi-turn dialogue. We split the corpus into training, development, and test set with a 94%, 3%, and 3% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset.
Due to the small size of the combined transcripts dataset, we first train our model on the larger Movie Triplets Corpus (MTC) by Banchs (2012) which consists of 240,000 dialogue triples. We pre-train  our model on this dataset to initialize our model parameters to avoid overfitting on a relatively small persona TV series dataset. After pre-training on MTC, we reinitialize the attribute embeddings in the generator from a uniform distribution following a Xavier initialization (Glorot & Bengio, 2010) for training on the combined person TV series dataset.
Ubuntu Dialogue Corpus (UDC) dataset (Serban et al., 2017b). We train our model on 1.85 million conversations of multi-turn dialogue from the Ubuntu community hub, with an average of 5 utterances per conversation. We assign two types of speaker IDs to utterances in this dataset: questioner and helper. We follow a similar training, development, and test split as the UDC dataset in Olabiyi et al. (2018), with 90%, 5%, and 5% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset While the overwhelming majority of utterances in UDC follow two speaker types, the dataset does include utterances that do not classify under either a questioner or helper speaker type. In order to remain consistent, we assume that there are only two speaker types within this dataset and that the first utterance of every dialogue is from a questioner. This simplifying assumption does introduce a degree of noise into each persona model's ability to construct attribute embeddings. However, our experiment results demonstrate that both phredGAN a and phredGAN d is still able to differentiate between the larger two speaker types in the dataset.

[EVALUATION METRICS]
We use similar evaluation metrics as in Olabiyi et al. (2018) including perplexity, BLEU (Papineni et al., 2002), ROUGE (Lin, 2014), distinct n-gram (Li et al., 2016a) and normalized average sequence length (NASL) scores. For human evaluation, we follow a similar setup as Li et al. (2016a), employing crowd-sourced judges to evaluate a random selection of 200 samples. We present both the multi-turn context and the generated responses from the models to 3 judges and asked them to rank the general response quality in terms of relevance, informativeness, and persona. For N models, the model with the lowest quality is assigned a score 0 and the highest is assigned a score N-1. Ties are not allowed. The scores are normalized between 0 and 1 and averaged over the total number of samples and judges. 

[BASELINE]
We compare the non-adversarial persona HRED model, PHRED with the adversarially trained ones, i.e. hredGAN , phredGAN a and phredGAN d , to demonstrate the impact of adversarial training. Please note that no noise was added to the PHRED model.
We also compare the persona models to Li et al.'s work (Li et al., 2016b) which uses a Seq2Seq framework in conjunction with learnable persona embeddings. Their work explores two persona models in order to incorporate vector representations of speaker interaction and speaker attributes into the decoder of their Seq2Seq model i.e., Speaker model (SM) and Speaker-Addressee model (SAM). All reported results are based on our implementation of their models in Li et al. (2016b).

[HYPERPARAMETER SEARCH]
For both phredGAN a and phredGAN d , we determine the noise injection method and the optimum noise variance α that allows for the best performance on both datasets. We find that phredGAN d performs optimally with word-level noise injection on both Ubuntu and TV transcripts, while phredGAN a performs the best with utterance-level noise injection on TV transcripts and word-level injection on UDC. For all phredGAN models, we perform a linear search for optimal noise variance values between 1 and 30 at an increment of 1, with a sample size of L = 1. For phredGAN d , we obtain an optimal α of 4 and 6 for the UDC and TV Transcripts respectively. For phredGAN a , we obtain an optimal value of 2 and 5 for the combined TV series dataset and the much larger UDC respectively.

[RESULTS]
We will now present our assessment of performance comparisons of phredGAN against the baselines, PHRED, hredGAN and Li et al.'s persona Seq2Seq models.

[QUANTITATIVE ANALYSIS]
We first report the performance on TV series transcripts in table 1. The performance of both SM and SAM models in Li et al. (2016b) compared to the hredGAN shows a strong baseline and indicates that the effect of persona is more important than that of multi-turn and adversarial training for datasets with weak multiple persona. However, once the persona information is added to the hredGAN , the resulting phredGAN shows a significant improvement over the SM and SAM baselines with phredGAN a performing best. We also observe that PHRED performs worse than the baseline S(A)M models on a number of metrics but we attribute this to the effect of persona on a limited dataset that results into less informative responses. This behavior was also reported in Li et al. (2016b) where the persona models produce less informative responses than the non-personal Seq2seq models but it seems to be even worse in multi-turn context. However, unlike the Speaker-Addressee and PHRED models that suffer from lower response quality due to persona conditioning, we note that conditioning the generator and discriminator of phredGAN on speaker embeddings does not compromise the systems ability to produce diverse responses. This problem might have been alleviated by the adversarial training that encourages the generator model to produce longer, more informative, and diverse responses that have high persona relevance even with a limited dataset.
We also compare the models performances on the UDC. The evaluation result is summarized in table 2. While the deleterious effect of persona conditioning on response diversity is still worse with PHRED than with S(A)M models, we note that hredGAN performs much better than the S(A)M models. This is because, the external persona only provides just a little more information than is already available from the UDC utterances. We also note an improvement of phredGAN variants over the hredGAN in a variety of evaluation metrics including perplexity, ROUGE with the exception of distinct n-grams. This is expected as phredGAN should be generally less diverse than hredGAN since the number of distinct data distribution modes is more for phredGAN dataset due to the persona attributes. However, this leads to better response quality with persona, something not achievable with hredGAN . Also, the much better ROUGE(F1) score indicates that phredGAN is able to strike a better balance between diversity and precision while still capturing the characteristics of the speaker attribute modality in the UDC dataset. Within the phredGAN variants, phredGAN d seems to perform better. This is not surprising as speaker classification is much easier on UDC than on TV series. The attribute discriminator, D att is able to provide more informative feedback on UDC than on TV series where it is more difficult to accurately predict the speaker. Therefore, we recommend phredGAN a for datasets with weak attribute distinction and phredGAN d for strong attribute distinction.

[QUALITATIVE ANALYSIS]
In addition to the quantitative analysis above, we report the results of the human evaluation in the last column of tables 1 and 2 for the TV Series and UDC datasets respectively. The human evaluation scores largely agrees with the automatic evaluations on the TV Series with phredGAN a clearly giving the best performance. However, on the UDC, both hredGAN and phredGAN d performs similarly which indicates that there is a trade off between diversity and persona by each model. We believe this is due to the strong persona information that already exists in the UDC utterances.
An additional qualitative assessment of these results are in Table 3 with responses from several characters in the TV series dataset and the two characters in UDC.
We see that for TV drama series, phredGAN responses are comparatively more informative than that of the Speaker-Addressee model of Li et al. (2016b). For example, all the characters in the TV series respond the same to the dialogue context. Similar behavior is reported in Li et al. (2016b) where for the Speaker-Addressee model, nearly all the characters in the TV series respond with We also see similar results with our model's output on UDC in table 4. We demonstrate that by conditioning as either a helper or questioner from the UDC dataset, phredGAN models are able to respond differently to input utterances as well as stay close to the context of the conversation.

[CONCLUSION AND FUTURE WORK]
In this paper, we improve upon state-of-the-art persona-based response generation models by exploring two persona conversational models: phredGAN a which passes the attribute representation as an additional input into a traditional adversarial discriminator, and phredGAN d a dual discriminator system which in addition to the adversarial discriminator from hredGAN , collaboratively predicts the attribute(s) that are intrinsic to the input utterance. Both systems demonstrate quantitative improvements upon state-of-the-art persona conversational systems such as the work from Li et al. (2016b) with respect to both quantitative automatic and qualitative human measures.
Our analysis also demonstrates how both variants of phredGAN perform differently on datasets with weak and strong modality. One of our future direction is to take advantage of phredGAN d 's ability to predict utterance attribute such as speaker identity from just the utterance. We believe its performance can be improved even with weak modality by further conditioning adversarial updates on both the attribute and adversarial discriminator accuracies. Overall, this paper demonstrates clear benefits from adversarial training of persona generative dialogue system and leaves the door open for more interesting work to be accomplished in this domain.  

[APPENDIX ALGORITHM 1 ADVERSARIAL LEARNING OF PHREDGAN]
Require: A generator G with parameters θ G . Require: An adversarial discriminator D adv with parameters θ D adv . Require: An attribute discriminator Datt with parameters θ D att . Require: Training hyperparameters, isT arget, λ G att , λ G adv , and λ M .
for number of training iterations do Initialize cRN N to zero state, h0 Sample a mini-batch of conversations, X = {Xi, Ci} N i=1 , X i = (X1, C1), (X2, C2), • • • , (Xi, Ci) with N utterances. Each utterance mini batch i contains Mi word tokens.
Compute the generator output similar to Eq. (11) in Olabiyi et al. (2018).
Sample a corresponding mini batch of utterance Yi.
else Update phredGANa's θ D adv with gradient of the discriminator loss.
Update θ G with attribute, adversarial and MLE losses.

[RESULTS -DISCRIMINATOR]
After training both phredGAN models on the TV series and UDC datasets, we ran inference on some example dialogue contexts. The responses and their discriminator scores from phredGAN s are listed in Tables 6, and 7. The tables shows that phredGAN (i) can handle multi-turn dialogue context with utterances and corresponding persona attributes; (ii) generates responses conditioned on a persona attribute; (iii) generates multiple responses per dialogue context and score their human likelihood by the discriminator; and (iv) in case of phredGAN d , can predict the attribute such as speaker identity that might have produced the utterance. We observe that the discriminator score(s) is/are generally reasonable with longer, more informative and more persona-related responses receiving higher scores. It worth to note that this behavior, although similar to the behavior of a human judge is learned without supervision. More so, we observe that phredGAN responses retain contextual consistency sometimes referencing background information that is inherent in the conversation between two speakers. For example, in the second sample of the TV series in Table 6, phredGAN a generator, conditioned on Leonard refers to Sheldon by name who is the second interlocutor. Also, in the third sample, phredGAN a , conditioned on Raj refers to Penny when responding to Leonard who happens to be Penny's boy friend. We see similar persona-based response generation for the UDC dataset with distinct communication style between the asker and the helper. For example, in Table 7, when the asker could not hear some music, phredGAN d , conditioned on helper suggested the asker might not be using the right driver. For the purpose of completion, we also show some samples from PHRED generator on both UDC and TV series dataset in Table 5.","[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset)."
An Adversarial Learning Framework for a Persona-based Multi-turn Dialogue Model,rkeYUsRqKQ.json,"This paper proposes an extension to hredGAN, which is an adversarial framework for multi-turn dialogue model, to simultaneously learns a set of attribute embeddings that represents the persona of each speaker and generate persona-based responses. The generator of the proposed system phredGAN is conditioned on both the history utterances and the speakers’ persona by concatenating the utterance encoding with attribute embeddings. For discriminator, the authors explore two versions: 1) phredGAN_a takes attributes as inputs; 2) phredGAN_d adds a dual discriminator that predicts the attribute(s) for each utterance. 


Strength: 1) to the best of my knowledge, adding persona information to an adversarial multi-turn dialogue model is novel; 2) the authors explore two different approaches to build the discriminator(s) and the idea of adding a second discriminator that predicts the attributes seems interesting.    


Weakness:

1) Novelty: The idea of learning speaker-specific attribute embeddings is very similar to the Speaker Model proposed by Li et al.(2016) http://www.aclweb.org/anthology/P16-1094 and the proposed system only makes minor changes to hredGAN https://arxiv.org/abs/1805.11752.  


2) Presentation: 
The writing of this paper is a little hard to follow, for example, it presents the two discriminators after the objective function (Equation 2) and does not explain the intuition behind each model. In Equation 2, the objective function, why training the discriminator to minimize the attributes prediction probability？ Simply saying the attribute prediction loss is collaborative is not clear enough. Or is the min for the second term a typo? 


3) Model:
The idea of adding a discriminator that predicts the attributes seems interesting. However the loss is not adversarial for the second discriminator (Equation 6), you should not indicate L_att is GAN in your notation. I’m also not convinced that this should be collaborative. Despite that the “discriminator” is trying to predict the correct attribute id, the input of the two terms in Equation 6 is different, one comes from the true data, the other comes from the generator. Shouldn’t the discriminator try to differentiate these two cases? Otherwise, it’s not a discriminator (also raise the question for Equation 2, why argmin min(L_att)).


4) Evaluation：
The evaluation is not strong enough to demonstrate the benefit of the proposed model. 

a. It only compares against one previous work that takes speaker identity into account on one dataset. Despite that the authors apply several different metrics to evaluate the proposed model, they only compare with previous models by Li et al. (2016) on perplexity and BLEU. 

b. The perplexity scores of the proposed models are worse than SAM by Li et al. (2016). The authors explain this by stating that the entropy for a multi-turn model is supposed to be higher than the single-turn model. It’s better to provide a more rigorous analysis. For a fair comparison, they could also train the proposed model using only one-turn history, which should be identical to Li et al.’s setting (How many turns history are you using?). The improvements of the BLEU score might also be the consequence of substituting the past generated sequence in the generator with ground truth (since the model uses the same training algorithm as hredGAN https://arxiv.org/abs/1805.11752). It’s unclear if this is the cause unless the authors provide the comparison among SAM, hredGAN, and phredGAN. 

c. Table 2 compares the non-persona hredGAN with phredGAN on UDC, but the authors do not provide a comparison between these two on the TV dataset in Table 1. 

d. The comparison between phredGAN_a and phredGAN_d is inconsistent for the two datasets (Table 1 and Table 2).","[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).

[CAPTIONS]
Table 1: phredGAN vs.Li et al. (2016b) on BBT Friends TV Transcripts.
Table 2: phredGAN vs. Li et al. (2016b) on UDC.
Table 3: hredGAN vs phredGAN vs Li et al. (2016b) on TV Transcripts
Table 4: hredGAN vs phredGAN vsLi et al. (2016b) on UDC Of course I love you."" to the dialogue context, ""Do you love me?"" despite the fact that some of the responders sometimes have unfriendly relationship with the addressee. Many of the novel situations explored by phredGAN are unachievable with the Speaker-Addressee model due to lack of informative responses. For example, by conditioning as Sheldon from The Big Bang Theory and asking ""Do you like me?"", our model responds with annoyance if conditioned as Penny (""No, you don't understand. You're an idiot""), brevity with Leonard (""Yes?"") and sarcasm with Raj (""Well , you know , we could be a little more than my friend's friends."") The wide range of responses indicate our model's ability to construct distinct attribute embeddings for each character even from a limited dataset. The other interesting responses in table 3 indicate phredGAN 's ability to infer not only the context of the conversation but important character information about the addressee.
Table 5: Sample of PHRED outputs on UDC and TV Series Howard Okay, you have to understand something, we're in a hospital right now. Response 0 Penny Oh , I ' m sorry . I was just trying to be a girl .Context 0 Sheldon Did I? Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah Response 0 Leonard I ' m sorry . I just don ' t want to be a man who ' s going to be my girlfriend .Context 0 Leonard She's gotten really hooked on Age of Conan, she's playing non-stop.
Table 6: Ranked phredGAN a outputs according to adversarial word-level discrimination score Source Speaker D adv (G(.)) Utterance Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah cd , it should be in the repos , it's a good place to get the source of the kernel Response 1 helper 0.1984 use the UNK package , it should work . . .

[INTRODUCTION]
Recent advances in machine learning especially with deep neural networks has lead to tremendous progress in natural language processing and dialogue modeling research (Sutskever et al., 2014;Vinyals & Le, 2015;Serban et al., 2016). Nevertheless, developing a good conversation model capable of fluent interaction between a human and a machine is still in its infancy stage. Most existing work relies on limited dialogue history to produce response with the assumption that the model parameters will capture all the modalities within a dataset. However, this is not true as dialogue corpora tend to be strongly multi-modal and practical neural network models find it difficult to disambiguate characteristics such as speaker personality, location and sub-topic in the data.
Most work in this domain has primarily focused on optimizing dialogue consistency. For example, Serban et al. (Serban et al., 2016;2017b;a) and Xing et al. (2017) introduced a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not have any guarantee on the generator output since the output conditional probability is not calibrated. Olabiyi et al. (2018) tackles these problems by training a modified HRED generator alongside an adversarial discriminator in order to increase diversity and provide a strong and calibrated guarantee to the generator's output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modality within a dataset and fails to generate persona specific responses in datasets with multiple modalities.
On the other hand, there has been some recent work on introducing persona into dialogue models. For example, Li et al. (2016b) integrates attribute embeddings into a single turn (Seq2Seq) generative dialogue model. In this work, Li et al. consider persona models one with Speaker-only representation and the other with Speaker and Addressee representations (Speaker-Addressee model), both of which capture certain speaker identity and interactions. Nguyen et al. (2018) continue along the Figure 1: The PHRED generator with local attention -The attributes C, allows the generator to condition its response on the utterance attributes such as speaker identity, subtopics and so on. same line of thought by considering a Seq2Seq dialogue model with Responder-only representation. In both of these cases, the attribute representation is learned during the system training. Zhang et al. (2018) proposed a slightly different approach. Here, the attributes are a set of sentences describing the profile of the speaker. In this case, the attributes representation is not learned. The system however learns how to attend to different parts of the attributes during training. Still, the above persona-based models have limited dialogue history (single turn); suffer from exposure bias worsening the trade-off between personalization and conversation quality and cannot generate multiple responses given a dialogue context. This is evident in the relatively short and generic responses produced by these systems, even though they generally capture the persona of the speaker.
In order to overcome these limitations, we propose two variants of an adversarially trained persona conversational generative system, phredGAN , namely phredGAN a and phredGAN d . Both systems aim to maintain the response quality of hredGAN and still capture speaker and other attribute modalities within the conversation. In fact, both systems use the same generator architecture (PHRED generator), i.e., an hredGAN generator (Olabiyi et al., 2018) with additional utterance attribute representation at its encoder and decoder inputs as depicted in Figure 1. Conditioning on external attributes can be seen as another input modality as is the utterance into the underlying system. The attribute representation is an embedding that is learned together with the rest of model parameters similar to Li et al. (2016b). Injecting attributes into a multi-turn dialogue system allows the model to generate responses conditioned on particular attribute(s) across conversation turns. Since the attributes are discrete, it also allows for exploring different what-if scenarios of model responses. The difference between the two systems is in the discriminator architecture based on how the attribute is treated.
We train and sample both variants of phredGAN similar to the procedure for hredGAN (Olabiyi et al., 2018). To demonstrate model capability, we train on a customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poser and answerer, and transcripts from a multi-modal TV series The Big Bang Theory and Friends with quantitative and qualitative analysis. We examine the trade-offs between using either system in bi-modal or multi-modal datasets, and demonstrate system superiority over state-of-the-art persona conversational models in terms of dialogue response quality and quantitatively with perplexity, BLEU, ROUGE and distinct n-gram scores.

[MODEL ARCHITECTURE]
In this section, we briefly introduce the state-of-the-art hredGAN model and subsequently show how we derive the two persona versions by combining it with the distributed representation of the dialogue speaker and utterance attributes, or with an attribute discrimination layer at the end of the model pipeline.
Figure 2: The phredGAN d dual discriminator -Left: D adv is a word-level discriminator used by both phredGAN a and phredGAN d to judge normal dialogue coherency as in hredGAN . Right: D att , an utterance-level attribute discriminator is used only in phredGAN d to predict the likelihood a given utterance was generated from a particular attribute.

[HREDGAN : ADVERSARIAL LEARNING FRAMEWORK]
Problem Formulation: The hredGAN (Olabiyi et al., 2018) formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances,
X i = X 1 , X 2 , • • • , X i , where each utterance X i = X 1 i , X 2 i , • • • , X Mi i
contains a variable-length sequence of M i word tokens such that X i j ∈ V for vocabulary V , the dialogue model produces an output
Y i = Y 1 i , Y 2 i , • • • , Y Ti i
, where T i is the number of generated tokens. The framework uses conditional GAN structure to learn a mapping from an observed dialogue history to a sequence of output tokens. The generator, G, is trained to produce sequences that cannot be distinguished from the ground truth by an adversarially trained discriminator, D akin to a two-player min-max optimization problem. The generator is also trained to minimize the cross-entropy loss L M LE (G) between the ground truth X i+1 , and the generator output Y i . The following objective summarizes both goals:
G * , D * = arg min G max D λ G L cGAN (G, D) + λ M L M LE (G) .
(
)1
where λ G and λ M are training hyperparamters and L cGAN (G, D) and L M LE (G) are defined in Eqs.
(5) and ( 7) of Olabiyi et al. (2018) respectively. Please note that the generator G and discriminator D share the same encoder and embedding representation of the word tokens.

[PHREDGAN : PERSONA ADVERSARIAL LEARNING FRAMEWORK]
The proposed architecture of phredGAN is very similar to that of hredGAN (Olabiyi et al., 2018). The only difference is that the dialogue history is now
X i = (X 1 , C 1 ), (X 2 , C 2 ), • • • , (X i , C i )
where C i is additional input that represents the speaker and/or utterance attributes. Please note that C i can either be a sequence of tokens or single token such that C i j ∈ V c for vocabulary V c. Also, at the ith turn, C i and C i+1 are the source/input attribute and target/output attribute to the generator respectively. The embedding for attribute tokens is also learned similar to that of word tokens.
Both versions of phredGAN shares the same generator architecture (PHRED) but different discriminators. Below is the highlight of how they are derived from the hredGAN architecture.
Encoder: The context RNN, cRN N takes the source attribute C i as an additional input by concatenating its representation with the output of eRN N as in Figure 1. If the attribute C i is a sequence of tokens, then an attention (using the output of eRN N ) over the source attribute representations is concatenated with the output of eRN N . This output is used by the generator to create a context state for a turn i.
Generator: The generator decoder RNN, dRN N takes the target attribute C i+1 as an additional input as in Fig. 1. If the attribute C i+1 is a sequence of tokens, then an attention (using the output of dRN N ) over the attribute representations is concatenated with the rest of the decoder inputs. This forces the generator to draw a connection between the generated responses and the utterance attributes such as speaker identity.
Noise Injection: As in Olabiyi et al. (2018), we also explore different noise injection methods.
Objective: For phredGAN , the optimization objective in eq. ( 1) can be updated as:
G * , D * adv , D * att = arg min G max D adv λ G adv L adv cGAN (G, D adv ) (2) + min Datt λ Gatt L att c (G, D att ) + λ M L M LE (G) .
where L adv cGAN (G, D adv ) and L att c (G, D att ) are the traditional adversarial and attribute prediction loss respectively and dependent on the architectural variation. It is worth to point out that while the former is adversarial, the later is collaborative in nature. The MLE loss is common and can be expressed as:
L M LE (G) = E Xi+1 [−log P G X i+1 |X i , C i+1 , Z i ].
(3) where Z i the noise sample and depends on the choice of either utterance-level or word-level noise input into the generator (Olabiyi et al., 2018).

[PHREDGAN A : ATTRIBUTES AS A DISCRIMINATOR INPUT]
phredGAN a shares the same discriminator architecture as the hredGAN but with additional input, C i+1 . Since it does not use attribute prediction, λ Gatt = 0.
The adversarial loss, L adv cGAN (G, D) can then be expressed as:
L adv cGAN (G, D adv ) = E X i ,Ci+1,Xi+1 [log D adv (X i , C i+1 , X i+1 )] (4) + E X i ,Ci+1,Zi [1 − log D adv (X i , C i+1 , G(X i , C i+1 , Z i ))]
The addition of speaker or utterance attributes allows the dialogue model to exhibit personality traits given consistent responses across style, gender, location, and so on.

[PHREDGAN D : ATTRIBUTES AS A DISCRIMINATOR TARGET]
phredGAN d does not take the attribute representation at its input but rather use the attributes as the target of an additional discriminator D att . The adversarial and the attribute prediction losses can be respectively expressed as:
L adv cGAN (G, D adv ) = E X i ,Xi+1 [log D adv (X i , X i+1 )] (5) + E X i ,Zi [1 − log D adv (X i , G(X i , C i+1 , Z i ))] L att c (G, D att ) = E Ci+1 [− log D att (C i+1 |X i , X i+1 )] (6) + E Ci+1 [− log D att (C i+1 |X i , G(X i , C i+1 , Z i ))]
Attribute Discriminator: In addition to the existing word-level adversarial discriminator D adv from hredGAN , we add an attribute discriminator, D att , that discriminates on an utterance level to capture attribute modalities since attributes are assigned at utterance level. The discriminator uses a unidirectional RNN (D attRN N ) that maps the input utterance to the particular attribute(s) that generated it. The attributes can be seen as hidden states that inform or shape the generator outputs. The attribute discriminator can be expressed as:
D att (C i+1 |X i , χ) = D attRN N (h i , E(χ))(7)
where E(.) is the word embedding lookup (Olabiyi et al., 2018), χ = X i+1 for groundtruth and χ = Y i for the generator output.

[MODEL TRAINING]
We train both the generator and the discriminator (with shared encoder) of both variants of phredGAN using the training procedure in Algorithm 1 (Olabiyi et al., 2018). For both variants, λ G adv = λ M = 1, and for phredGAN a and phredGAN d , λ Gatt = 0 and λ Gatt = 1 respectively. Since the encoder, word embedding and attribute embedding are shared, we are able to train the system end-to-end with back-propagation.
Encoder: The encoder RNN, eRN N , is bidirectional while cRRN is unidirectional. All RNN units are 3-layer GRU cell with hidden state size of 512. We use word vocabulary size, V = 50, 000 with word embedding size of 512. The number of attributes, V c is dataset dependent but we use an attribute embedding size of 512. In this study, we only use one attribute per utterance so that is no need to use attention to combine the attribute embeddings.
Generator: The generator decoder RNN, dRN N is also a 3-layer GRU cell with hidden state size of 512. The aRN N outputs are connected to the dRN N input using an additive attention mechanism (Bahdanau et al., 2015).
Adversarial Discriminator: The word-level discriminator RNN, D RN N is a bidirectional RNN, each 3-layer GRU cell with hidden state size of 512. The output of both the forward and the backward cells for each word are concatenated and passed to a fully-connected layer with binary output.
The output is the probability that the word is from the ground truth given the past and future words of the sequence, and in the case of phredGAN a , the responding speaker's embedding.
Attribute Discriminator: The attribute discriminator RNN, D attRN N is a unidirectional RNN with a 3-layer GRU cell, each of hidden state size 512. A softmax layer is then applied to project the final hidden state to a prespecified number of attributes, V c . The output is the probability distribution over the attributes.
Others: All parameters are initialized with Xavier uniform random initialization (Glorot & Bengio, 2010). Due to the large word vocabulary size, we use sampled softmax loss (Jean et al., 2015) for MLE loss to expedite the training process. However, we use full softmax for model evaluation. For both systems, parameters updates are conditioned on the word-level discriminator accuracy performance as in Olabiyi et al. (2018) with acc D th adv = 0.99 and acc G th = 0.75. The model is trained end-to-end using the stochastic gradient descent algorithm. Finally, the model is implemented, trained, and evaluated using the TensorFlow deep learning framework.

[MODEL INFERENCE]
We use an inference strategy similar to the approach in Olabiyi et al. (2018).
For the modified noise sample, we perform a linear search for α with sample size L = 1 based on the average word-level discriminator loss, −logD adv (G(.)) (Olabiyi et al., 2018) using trained models run in autoregressive mode to reflect performance in actual deployment. The optimum α value is then used for all inferences and evaluations. During inference, we condition the dialogue response generation on the encoder outputs, noise samples, word embedding and the attribute embedding of the intended responder. With multiple noise samples, L = 64, we rank the generator outputs by the discriminator which is also conditioned on encoder outputs, and the intended responder's attribute embedding. The final response is the response ranked highest by the discriminator. For phredGAN d , we average the confidences produced by D adv and D att .

[EXPERIMENTS AND RESULTS]
In this section, we explore the performance of PHRED, phredGAN a and phredGAN d on two conversational datasets and compare its performance to non-adversarial persona Seq2seq models Li et al. (2016b) as well as to the adversarial hredGAN (Olabiyi et al., 2018) with no explicit persona.

[DATASETS]
TV Series Transcripts dataset (Serban et al., 2016). We train all models on transcripts from the two popular TV drama series, Big Bang Theory and Friends. Following a similar preprocessing setup in Li et al. (2016b), we collect utterances from the top 12 speakers from both series to construct a corpus of 5,008 lines of multi-turn dialogue. We split the corpus into training, development, and test set with a 94%, 3%, and 3% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset.
Due to the small size of the combined transcripts dataset, we first train our model on the larger Movie Triplets Corpus (MTC) by Banchs (2012) which consists of 240,000 dialogue triples. We pre-train  our model on this dataset to initialize our model parameters to avoid overfitting on a relatively small persona TV series dataset. After pre-training on MTC, we reinitialize the attribute embeddings in the generator from a uniform distribution following a Xavier initialization (Glorot & Bengio, 2010) for training on the combined person TV series dataset.
Ubuntu Dialogue Corpus (UDC) dataset (Serban et al., 2017b). We train our model on 1.85 million conversations of multi-turn dialogue from the Ubuntu community hub, with an average of 5 utterances per conversation. We assign two types of speaker IDs to utterances in this dataset: questioner and helper. We follow a similar training, development, and test split as the UDC dataset in Olabiyi et al. (2018), with 90%, 5%, and 5% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset While the overwhelming majority of utterances in UDC follow two speaker types, the dataset does include utterances that do not classify under either a questioner or helper speaker type. In order to remain consistent, we assume that there are only two speaker types within this dataset and that the first utterance of every dialogue is from a questioner. This simplifying assumption does introduce a degree of noise into each persona model's ability to construct attribute embeddings. However, our experiment results demonstrate that both phredGAN a and phredGAN d is still able to differentiate between the larger two speaker types in the dataset.

[EVALUATION METRICS]
We use similar evaluation metrics as in Olabiyi et al. (2018) including perplexity, BLEU (Papineni et al., 2002), ROUGE (Lin, 2014), distinct n-gram (Li et al., 2016a) and normalized average sequence length (NASL) scores. For human evaluation, we follow a similar setup as Li et al. (2016a), employing crowd-sourced judges to evaluate a random selection of 200 samples. We present both the multi-turn context and the generated responses from the models to 3 judges and asked them to rank the general response quality in terms of relevance, informativeness, and persona. For N models, the model with the lowest quality is assigned a score 0 and the highest is assigned a score N-1. Ties are not allowed. The scores are normalized between 0 and 1 and averaged over the total number of samples and judges. 

[BASELINE]
We compare the non-adversarial persona HRED model, PHRED with the adversarially trained ones, i.e. hredGAN , phredGAN a and phredGAN d , to demonstrate the impact of adversarial training. Please note that no noise was added to the PHRED model.
We also compare the persona models to Li et al.'s work (Li et al., 2016b) which uses a Seq2Seq framework in conjunction with learnable persona embeddings. Their work explores two persona models in order to incorporate vector representations of speaker interaction and speaker attributes into the decoder of their Seq2Seq model i.e., Speaker model (SM) and Speaker-Addressee model (SAM). All reported results are based on our implementation of their models in Li et al. (2016b).

[HYPERPARAMETER SEARCH]
For both phredGAN a and phredGAN d , we determine the noise injection method and the optimum noise variance α that allows for the best performance on both datasets. We find that phredGAN d performs optimally with word-level noise injection on both Ubuntu and TV transcripts, while phredGAN a performs the best with utterance-level noise injection on TV transcripts and word-level injection on UDC. For all phredGAN models, we perform a linear search for optimal noise variance values between 1 and 30 at an increment of 1, with a sample size of L = 1. For phredGAN d , we obtain an optimal α of 4 and 6 for the UDC and TV Transcripts respectively. For phredGAN a , we obtain an optimal value of 2 and 5 for the combined TV series dataset and the much larger UDC respectively.

[RESULTS]
We will now present our assessment of performance comparisons of phredGAN against the baselines, PHRED, hredGAN and Li et al.'s persona Seq2Seq models.

[QUANTITATIVE ANALYSIS]
We first report the performance on TV series transcripts in table 1. The performance of both SM and SAM models in Li et al. (2016b) compared to the hredGAN shows a strong baseline and indicates that the effect of persona is more important than that of multi-turn and adversarial training for datasets with weak multiple persona. However, once the persona information is added to the hredGAN , the resulting phredGAN shows a significant improvement over the SM and SAM baselines with phredGAN a performing best. We also observe that PHRED performs worse than the baseline S(A)M models on a number of metrics but we attribute this to the effect of persona on a limited dataset that results into less informative responses. This behavior was also reported in Li et al. (2016b) where the persona models produce less informative responses than the non-personal Seq2seq models but it seems to be even worse in multi-turn context. However, unlike the Speaker-Addressee and PHRED models that suffer from lower response quality due to persona conditioning, we note that conditioning the generator and discriminator of phredGAN on speaker embeddings does not compromise the systems ability to produce diverse responses. This problem might have been alleviated by the adversarial training that encourages the generator model to produce longer, more informative, and diverse responses that have high persona relevance even with a limited dataset.
We also compare the models performances on the UDC. The evaluation result is summarized in table 2. While the deleterious effect of persona conditioning on response diversity is still worse with PHRED than with S(A)M models, we note that hredGAN performs much better than the S(A)M models. This is because, the external persona only provides just a little more information than is already available from the UDC utterances. We also note an improvement of phredGAN variants over the hredGAN in a variety of evaluation metrics including perplexity, ROUGE with the exception of distinct n-grams. This is expected as phredGAN should be generally less diverse than hredGAN since the number of distinct data distribution modes is more for phredGAN dataset due to the persona attributes. However, this leads to better response quality with persona, something not achievable with hredGAN . Also, the much better ROUGE(F1) score indicates that phredGAN is able to strike a better balance between diversity and precision while still capturing the characteristics of the speaker attribute modality in the UDC dataset. Within the phredGAN variants, phredGAN d seems to perform better. This is not surprising as speaker classification is much easier on UDC than on TV series. The attribute discriminator, D att is able to provide more informative feedback on UDC than on TV series where it is more difficult to accurately predict the speaker. Therefore, we recommend phredGAN a for datasets with weak attribute distinction and phredGAN d for strong attribute distinction.

[QUALITATIVE ANALYSIS]
In addition to the quantitative analysis above, we report the results of the human evaluation in the last column of tables 1 and 2 for the TV Series and UDC datasets respectively. The human evaluation scores largely agrees with the automatic evaluations on the TV Series with phredGAN a clearly giving the best performance. However, on the UDC, both hredGAN and phredGAN d performs similarly which indicates that there is a trade off between diversity and persona by each model. We believe this is due to the strong persona information that already exists in the UDC utterances.
An additional qualitative assessment of these results are in Table 3 with responses from several characters in the TV series dataset and the two characters in UDC.
We see that for TV drama series, phredGAN responses are comparatively more informative than that of the Speaker-Addressee model of Li et al. (2016b). For example, all the characters in the TV series respond the same to the dialogue context. Similar behavior is reported in Li et al. (2016b) where for the Speaker-Addressee model, nearly all the characters in the TV series respond with We also see similar results with our model's output on UDC in table 4. We demonstrate that by conditioning as either a helper or questioner from the UDC dataset, phredGAN models are able to respond differently to input utterances as well as stay close to the context of the conversation.

[CONCLUSION AND FUTURE WORK]
In this paper, we improve upon state-of-the-art persona-based response generation models by exploring two persona conversational models: phredGAN a which passes the attribute representation as an additional input into a traditional adversarial discriminator, and phredGAN d a dual discriminator system which in addition to the adversarial discriminator from hredGAN , collaboratively predicts the attribute(s) that are intrinsic to the input utterance. Both systems demonstrate quantitative improvements upon state-of-the-art persona conversational systems such as the work from Li et al. (2016b) with respect to both quantitative automatic and qualitative human measures.
Our analysis also demonstrates how both variants of phredGAN perform differently on datasets with weak and strong modality. One of our future direction is to take advantage of phredGAN d 's ability to predict utterance attribute such as speaker identity from just the utterance. We believe its performance can be improved even with weak modality by further conditioning adversarial updates on both the attribute and adversarial discriminator accuracies. Overall, this paper demonstrates clear benefits from adversarial training of persona generative dialogue system and leaves the door open for more interesting work to be accomplished in this domain.  

[APPENDIX ALGORITHM 1 ADVERSARIAL LEARNING OF PHREDGAN]
Require: A generator G with parameters θ G . Require: An adversarial discriminator D adv with parameters θ D adv . Require: An attribute discriminator Datt with parameters θ D att . Require: Training hyperparameters, isT arget, λ G att , λ G adv , and λ M .
for number of training iterations do Initialize cRN N to zero state, h0 Sample a mini-batch of conversations, X = {Xi, Ci} N i=1 , X i = (X1, C1), (X2, C2), • • • , (Xi, Ci) with N utterances. Each utterance mini batch i contains Mi word tokens.
Compute the generator output similar to Eq. (11) in Olabiyi et al. (2018).
Sample a corresponding mini batch of utterance Yi.
else Update phredGANa's θ D adv with gradient of the discriminator loss.
Update θ G with attribute, adversarial and MLE losses.

[RESULTS -DISCRIMINATOR]
After training both phredGAN models on the TV series and UDC datasets, we ran inference on some example dialogue contexts. The responses and their discriminator scores from phredGAN s are listed in Tables 6, and 7. The tables shows that phredGAN (i) can handle multi-turn dialogue context with utterances and corresponding persona attributes; (ii) generates responses conditioned on a persona attribute; (iii) generates multiple responses per dialogue context and score their human likelihood by the discriminator; and (iv) in case of phredGAN d , can predict the attribute such as speaker identity that might have produced the utterance. We observe that the discriminator score(s) is/are generally reasonable with longer, more informative and more persona-related responses receiving higher scores. It worth to note that this behavior, although similar to the behavior of a human judge is learned without supervision. More so, we observe that phredGAN responses retain contextual consistency sometimes referencing background information that is inherent in the conversation between two speakers. For example, in the second sample of the TV series in Table 6, phredGAN a generator, conditioned on Leonard refers to Sheldon by name who is the second interlocutor. Also, in the third sample, phredGAN a , conditioned on Raj refers to Penny when responding to Leonard who happens to be Penny's boy friend. We see similar persona-based response generation for the UDC dataset with distinct communication style between the asker and the helper. For example, in Table 7, when the asker could not hear some music, phredGAN d , conditioned on helper suggested the asker might not be using the right driver. For the purpose of completion, we also show some samples from PHRED generator on both UDC and TV series dataset in Table 5.","[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset)."
An Adversarial Learning Framework for a Persona-based Multi-turn Dialogue Model,rkeYUsRqKQ.json,"This paper uses the idea from 'A Persona-Based Neural Conversation Model' by Li et al and incrementally applies it to the 'Multi-turn Dialogue Response Generation in an Adversarial Learning Framework' work-in-progress by Olabiyi et al. The paper by Olabiyi uses the idea of adversarial training to the HRED work by Xing et al (Hierarchical Recurrent Attention Network for Response Generation). The paper shows very promising results for controlling the response generation based on input attributes with adversarial training. Compared to the persona based model, this work seems to outperform that model significantly as reported in Table 1 (in terms of Perplexity/Bleu). It would have been great to see the quantitative comparison in terms of other metrics (if the authors could try to reproduce their results). There are other interesting ways to incorporate attribute information into the dialogue model such as reported in the work of Lee et al (SCALABLE SENTIMENT FOR SEQUENCE-TO-SEQUENCE CHATBOT RESPONSE WITH PERFORMANCE ANALYSIS) - since this paper is primarily about personalization of responses - a comparison to some of the methods used in Lee's work would have been very relevant and made the paper much more convincing in terms of core contributions. The model and architecture is pretty convincing but the paper lacks more in-depth analysis, comparison and evaluation of the model.","[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset).

[CAPTIONS]
Table 1: phredGAN vs.Li et al. (2016b) on BBT Friends TV Transcripts.
Table 2: phredGAN vs. Li et al. (2016b) on UDC.
Table 3: hredGAN vs phredGAN vs Li et al. (2016b) on TV Transcripts
Table 4: hredGAN vs phredGAN vsLi et al. (2016b) on UDC Of course I love you."" to the dialogue context, ""Do you love me?"" despite the fact that some of the responders sometimes have unfriendly relationship with the addressee. Many of the novel situations explored by phredGAN are unachievable with the Speaker-Addressee model due to lack of informative responses. For example, by conditioning as Sheldon from The Big Bang Theory and asking ""Do you like me?"", our model responds with annoyance if conditioned as Penny (""No, you don't understand. You're an idiot""), brevity with Leonard (""Yes?"") and sarcasm with Raj (""Well , you know , we could be a little more than my friend's friends."") The wide range of responses indicate our model's ability to construct distinct attribute embeddings for each character even from a limited dataset. The other interesting responses in table 3 indicate phredGAN 's ability to infer not only the context of the conversation but important character information about the addressee.
Table 5: Sample of PHRED outputs on UDC and TV Series Howard Okay, you have to understand something, we're in a hospital right now. Response 0 Penny Oh , I ' m sorry . I was just trying to be a girl .Context 0 Sheldon Did I? Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah Response 0 Leonard I ' m sorry . I just don ' t want to be a man who ' s going to be my girlfriend .Context 0 Leonard She's gotten really hooked on Age of Conan, she's playing non-stop.
Table 6: Ranked phredGAN a outputs according to adversarial word-level discrimination score Source Speaker D adv (G(.)) Utterance Well, that can't be right. No one's ever done that before. Except me, because I just did it. Sheldon and his brain, yeah cd , it should be in the repos , it's a good place to get the source of the kernel Response 1 helper 0.1984 use the UNK package , it should work . . .

[INTRODUCTION]
Recent advances in machine learning especially with deep neural networks has lead to tremendous progress in natural language processing and dialogue modeling research (Sutskever et al., 2014;Vinyals & Le, 2015;Serban et al., 2016). Nevertheless, developing a good conversation model capable of fluent interaction between a human and a machine is still in its infancy stage. Most existing work relies on limited dialogue history to produce response with the assumption that the model parameters will capture all the modalities within a dataset. However, this is not true as dialogue corpora tend to be strongly multi-modal and practical neural network models find it difficult to disambiguate characteristics such as speaker personality, location and sub-topic in the data.
Most work in this domain has primarily focused on optimizing dialogue consistency. For example, Serban et al. (Serban et al., 2016;2017b;a) and Xing et al. (2017) introduced a Hierarchical Recurrent Encoder-Decoder (HRED) network architecture that combines a series of recurrent neural networks to capture long-term context state within a dialogue. However, the HRED system suffers from lack of diversity and does not have any guarantee on the generator output since the output conditional probability is not calibrated. Olabiyi et al. (2018) tackles these problems by training a modified HRED generator alongside an adversarial discriminator in order to increase diversity and provide a strong and calibrated guarantee to the generator's output. While the hredGAN system improves upon response quality, it does not capture speaker and other attributes modality within a dataset and fails to generate persona specific responses in datasets with multiple modalities.
On the other hand, there has been some recent work on introducing persona into dialogue models. For example, Li et al. (2016b) integrates attribute embeddings into a single turn (Seq2Seq) generative dialogue model. In this work, Li et al. consider persona models one with Speaker-only representation and the other with Speaker and Addressee representations (Speaker-Addressee model), both of which capture certain speaker identity and interactions. Nguyen et al. (2018) continue along the Figure 1: The PHRED generator with local attention -The attributes C, allows the generator to condition its response on the utterance attributes such as speaker identity, subtopics and so on. same line of thought by considering a Seq2Seq dialogue model with Responder-only representation. In both of these cases, the attribute representation is learned during the system training. Zhang et al. (2018) proposed a slightly different approach. Here, the attributes are a set of sentences describing the profile of the speaker. In this case, the attributes representation is not learned. The system however learns how to attend to different parts of the attributes during training. Still, the above persona-based models have limited dialogue history (single turn); suffer from exposure bias worsening the trade-off between personalization and conversation quality and cannot generate multiple responses given a dialogue context. This is evident in the relatively short and generic responses produced by these systems, even though they generally capture the persona of the speaker.
In order to overcome these limitations, we propose two variants of an adversarially trained persona conversational generative system, phredGAN , namely phredGAN a and phredGAN d . Both systems aim to maintain the response quality of hredGAN and still capture speaker and other attribute modalities within the conversation. In fact, both systems use the same generator architecture (PHRED generator), i.e., an hredGAN generator (Olabiyi et al., 2018) with additional utterance attribute representation at its encoder and decoder inputs as depicted in Figure 1. Conditioning on external attributes can be seen as another input modality as is the utterance into the underlying system. The attribute representation is an embedding that is learned together with the rest of model parameters similar to Li et al. (2016b). Injecting attributes into a multi-turn dialogue system allows the model to generate responses conditioned on particular attribute(s) across conversation turns. Since the attributes are discrete, it also allows for exploring different what-if scenarios of model responses. The difference between the two systems is in the discriminator architecture based on how the attribute is treated.
We train and sample both variants of phredGAN similar to the procedure for hredGAN (Olabiyi et al., 2018). To demonstrate model capability, we train on a customer service related data such as the Ubuntu Dialogue Corpus (UDC) that is strongly bimodal between question poser and answerer, and transcripts from a multi-modal TV series The Big Bang Theory and Friends with quantitative and qualitative analysis. We examine the trade-offs between using either system in bi-modal or multi-modal datasets, and demonstrate system superiority over state-of-the-art persona conversational models in terms of dialogue response quality and quantitatively with perplexity, BLEU, ROUGE and distinct n-gram scores.

[MODEL ARCHITECTURE]
In this section, we briefly introduce the state-of-the-art hredGAN model and subsequently show how we derive the two persona versions by combining it with the distributed representation of the dialogue speaker and utterance attributes, or with an attribute discrimination layer at the end of the model pipeline.
Figure 2: The phredGAN d dual discriminator -Left: D adv is a word-level discriminator used by both phredGAN a and phredGAN d to judge normal dialogue coherency as in hredGAN . Right: D att , an utterance-level attribute discriminator is used only in phredGAN d to predict the likelihood a given utterance was generated from a particular attribute.

[HREDGAN : ADVERSARIAL LEARNING FRAMEWORK]
Problem Formulation: The hredGAN (Olabiyi et al., 2018) formulates multi-turn dialogue response generation as: given a dialogue history of sequence of utterances,
X i = X 1 , X 2 , • • • , X i , where each utterance X i = X 1 i , X 2 i , • • • , X Mi i
contains a variable-length sequence of M i word tokens such that X i j ∈ V for vocabulary V , the dialogue model produces an output
Y i = Y 1 i , Y 2 i , • • • , Y Ti i
, where T i is the number of generated tokens. The framework uses conditional GAN structure to learn a mapping from an observed dialogue history to a sequence of output tokens. The generator, G, is trained to produce sequences that cannot be distinguished from the ground truth by an adversarially trained discriminator, D akin to a two-player min-max optimization problem. The generator is also trained to minimize the cross-entropy loss L M LE (G) between the ground truth X i+1 , and the generator output Y i . The following objective summarizes both goals:
G * , D * = arg min G max D λ G L cGAN (G, D) + λ M L M LE (G) .
(
)1
where λ G and λ M are training hyperparamters and L cGAN (G, D) and L M LE (G) are defined in Eqs.
(5) and ( 7) of Olabiyi et al. (2018) respectively. Please note that the generator G and discriminator D share the same encoder and embedding representation of the word tokens.

[PHREDGAN : PERSONA ADVERSARIAL LEARNING FRAMEWORK]
The proposed architecture of phredGAN is very similar to that of hredGAN (Olabiyi et al., 2018). The only difference is that the dialogue history is now
X i = (X 1 , C 1 ), (X 2 , C 2 ), • • • , (X i , C i )
where C i is additional input that represents the speaker and/or utterance attributes. Please note that C i can either be a sequence of tokens or single token such that C i j ∈ V c for vocabulary V c. Also, at the ith turn, C i and C i+1 are the source/input attribute and target/output attribute to the generator respectively. The embedding for attribute tokens is also learned similar to that of word tokens.
Both versions of phredGAN shares the same generator architecture (PHRED) but different discriminators. Below is the highlight of how they are derived from the hredGAN architecture.
Encoder: The context RNN, cRN N takes the source attribute C i as an additional input by concatenating its representation with the output of eRN N as in Figure 1. If the attribute C i is a sequence of tokens, then an attention (using the output of eRN N ) over the source attribute representations is concatenated with the output of eRN N . This output is used by the generator to create a context state for a turn i.
Generator: The generator decoder RNN, dRN N takes the target attribute C i+1 as an additional input as in Fig. 1. If the attribute C i+1 is a sequence of tokens, then an attention (using the output of dRN N ) over the attribute representations is concatenated with the rest of the decoder inputs. This forces the generator to draw a connection between the generated responses and the utterance attributes such as speaker identity.
Noise Injection: As in Olabiyi et al. (2018), we also explore different noise injection methods.
Objective: For phredGAN , the optimization objective in eq. ( 1) can be updated as:
G * , D * adv , D * att = arg min G max D adv λ G adv L adv cGAN (G, D adv ) (2) + min Datt λ Gatt L att c (G, D att ) + λ M L M LE (G) .
where L adv cGAN (G, D adv ) and L att c (G, D att ) are the traditional adversarial and attribute prediction loss respectively and dependent on the architectural variation. It is worth to point out that while the former is adversarial, the later is collaborative in nature. The MLE loss is common and can be expressed as:
L M LE (G) = E Xi+1 [−log P G X i+1 |X i , C i+1 , Z i ].
(3) where Z i the noise sample and depends on the choice of either utterance-level or word-level noise input into the generator (Olabiyi et al., 2018).

[PHREDGAN A : ATTRIBUTES AS A DISCRIMINATOR INPUT]
phredGAN a shares the same discriminator architecture as the hredGAN but with additional input, C i+1 . Since it does not use attribute prediction, λ Gatt = 0.
The adversarial loss, L adv cGAN (G, D) can then be expressed as:
L adv cGAN (G, D adv ) = E X i ,Ci+1,Xi+1 [log D adv (X i , C i+1 , X i+1 )] (4) + E X i ,Ci+1,Zi [1 − log D adv (X i , C i+1 , G(X i , C i+1 , Z i ))]
The addition of speaker or utterance attributes allows the dialogue model to exhibit personality traits given consistent responses across style, gender, location, and so on.

[PHREDGAN D : ATTRIBUTES AS A DISCRIMINATOR TARGET]
phredGAN d does not take the attribute representation at its input but rather use the attributes as the target of an additional discriminator D att . The adversarial and the attribute prediction losses can be respectively expressed as:
L adv cGAN (G, D adv ) = E X i ,Xi+1 [log D adv (X i , X i+1 )] (5) + E X i ,Zi [1 − log D adv (X i , G(X i , C i+1 , Z i ))] L att c (G, D att ) = E Ci+1 [− log D att (C i+1 |X i , X i+1 )] (6) + E Ci+1 [− log D att (C i+1 |X i , G(X i , C i+1 , Z i ))]
Attribute Discriminator: In addition to the existing word-level adversarial discriminator D adv from hredGAN , we add an attribute discriminator, D att , that discriminates on an utterance level to capture attribute modalities since attributes are assigned at utterance level. The discriminator uses a unidirectional RNN (D attRN N ) that maps the input utterance to the particular attribute(s) that generated it. The attributes can be seen as hidden states that inform or shape the generator outputs. The attribute discriminator can be expressed as:
D att (C i+1 |X i , χ) = D attRN N (h i , E(χ))(7)
where E(.) is the word embedding lookup (Olabiyi et al., 2018), χ = X i+1 for groundtruth and χ = Y i for the generator output.

[MODEL TRAINING]
We train both the generator and the discriminator (with shared encoder) of both variants of phredGAN using the training procedure in Algorithm 1 (Olabiyi et al., 2018). For both variants, λ G adv = λ M = 1, and for phredGAN a and phredGAN d , λ Gatt = 0 and λ Gatt = 1 respectively. Since the encoder, word embedding and attribute embedding are shared, we are able to train the system end-to-end with back-propagation.
Encoder: The encoder RNN, eRN N , is bidirectional while cRRN is unidirectional. All RNN units are 3-layer GRU cell with hidden state size of 512. We use word vocabulary size, V = 50, 000 with word embedding size of 512. The number of attributes, V c is dataset dependent but we use an attribute embedding size of 512. In this study, we only use one attribute per utterance so that is no need to use attention to combine the attribute embeddings.
Generator: The generator decoder RNN, dRN N is also a 3-layer GRU cell with hidden state size of 512. The aRN N outputs are connected to the dRN N input using an additive attention mechanism (Bahdanau et al., 2015).
Adversarial Discriminator: The word-level discriminator RNN, D RN N is a bidirectional RNN, each 3-layer GRU cell with hidden state size of 512. The output of both the forward and the backward cells for each word are concatenated and passed to a fully-connected layer with binary output.
The output is the probability that the word is from the ground truth given the past and future words of the sequence, and in the case of phredGAN a , the responding speaker's embedding.
Attribute Discriminator: The attribute discriminator RNN, D attRN N is a unidirectional RNN with a 3-layer GRU cell, each of hidden state size 512. A softmax layer is then applied to project the final hidden state to a prespecified number of attributes, V c . The output is the probability distribution over the attributes.
Others: All parameters are initialized with Xavier uniform random initialization (Glorot & Bengio, 2010). Due to the large word vocabulary size, we use sampled softmax loss (Jean et al., 2015) for MLE loss to expedite the training process. However, we use full softmax for model evaluation. For both systems, parameters updates are conditioned on the word-level discriminator accuracy performance as in Olabiyi et al. (2018) with acc D th adv = 0.99 and acc G th = 0.75. The model is trained end-to-end using the stochastic gradient descent algorithm. Finally, the model is implemented, trained, and evaluated using the TensorFlow deep learning framework.

[MODEL INFERENCE]
We use an inference strategy similar to the approach in Olabiyi et al. (2018).
For the modified noise sample, we perform a linear search for α with sample size L = 1 based on the average word-level discriminator loss, −logD adv (G(.)) (Olabiyi et al., 2018) using trained models run in autoregressive mode to reflect performance in actual deployment. The optimum α value is then used for all inferences and evaluations. During inference, we condition the dialogue response generation on the encoder outputs, noise samples, word embedding and the attribute embedding of the intended responder. With multiple noise samples, L = 64, we rank the generator outputs by the discriminator which is also conditioned on encoder outputs, and the intended responder's attribute embedding. The final response is the response ranked highest by the discriminator. For phredGAN d , we average the confidences produced by D adv and D att .

[EXPERIMENTS AND RESULTS]
In this section, we explore the performance of PHRED, phredGAN a and phredGAN d on two conversational datasets and compare its performance to non-adversarial persona Seq2seq models Li et al. (2016b) as well as to the adversarial hredGAN (Olabiyi et al., 2018) with no explicit persona.

[DATASETS]
TV Series Transcripts dataset (Serban et al., 2016). We train all models on transcripts from the two popular TV drama series, Big Bang Theory and Friends. Following a similar preprocessing setup in Li et al. (2016b), we collect utterances from the top 12 speakers from both series to construct a corpus of 5,008 lines of multi-turn dialogue. We split the corpus into training, development, and test set with a 94%, 3%, and 3% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset.
Due to the small size of the combined transcripts dataset, we first train our model on the larger Movie Triplets Corpus (MTC) by Banchs (2012) which consists of 240,000 dialogue triples. We pre-train  our model on this dataset to initialize our model parameters to avoid overfitting on a relatively small persona TV series dataset. After pre-training on MTC, we reinitialize the attribute embeddings in the generator from a uniform distribution following a Xavier initialization (Glorot & Bengio, 2010) for training on the combined person TV series dataset.
Ubuntu Dialogue Corpus (UDC) dataset (Serban et al., 2017b). We train our model on 1.85 million conversations of multi-turn dialogue from the Ubuntu community hub, with an average of 5 utterances per conversation. We assign two types of speaker IDs to utterances in this dataset: questioner and helper. We follow a similar training, development, and test split as the UDC dataset in Olabiyi et al. (2018), with 90%, 5%, and 5% proportions, respectively, and pair each set with a corresponding attribute file that maps speaker IDs to utterances in the combined dataset While the overwhelming majority of utterances in UDC follow two speaker types, the dataset does include utterances that do not classify under either a questioner or helper speaker type. In order to remain consistent, we assume that there are only two speaker types within this dataset and that the first utterance of every dialogue is from a questioner. This simplifying assumption does introduce a degree of noise into each persona model's ability to construct attribute embeddings. However, our experiment results demonstrate that both phredGAN a and phredGAN d is still able to differentiate between the larger two speaker types in the dataset.

[EVALUATION METRICS]
We use similar evaluation metrics as in Olabiyi et al. (2018) including perplexity, BLEU (Papineni et al., 2002), ROUGE (Lin, 2014), distinct n-gram (Li et al., 2016a) and normalized average sequence length (NASL) scores. For human evaluation, we follow a similar setup as Li et al. (2016a), employing crowd-sourced judges to evaluate a random selection of 200 samples. We present both the multi-turn context and the generated responses from the models to 3 judges and asked them to rank the general response quality in terms of relevance, informativeness, and persona. For N models, the model with the lowest quality is assigned a score 0 and the highest is assigned a score N-1. Ties are not allowed. The scores are normalized between 0 and 1 and averaged over the total number of samples and judges. 

[BASELINE]
We compare the non-adversarial persona HRED model, PHRED with the adversarially trained ones, i.e. hredGAN , phredGAN a and phredGAN d , to demonstrate the impact of adversarial training. Please note that no noise was added to the PHRED model.
We also compare the persona models to Li et al.'s work (Li et al., 2016b) which uses a Seq2Seq framework in conjunction with learnable persona embeddings. Their work explores two persona models in order to incorporate vector representations of speaker interaction and speaker attributes into the decoder of their Seq2Seq model i.e., Speaker model (SM) and Speaker-Addressee model (SAM). All reported results are based on our implementation of their models in Li et al. (2016b).

[HYPERPARAMETER SEARCH]
For both phredGAN a and phredGAN d , we determine the noise injection method and the optimum noise variance α that allows for the best performance on both datasets. We find that phredGAN d performs optimally with word-level noise injection on both Ubuntu and TV transcripts, while phredGAN a performs the best with utterance-level noise injection on TV transcripts and word-level injection on UDC. For all phredGAN models, we perform a linear search for optimal noise variance values between 1 and 30 at an increment of 1, with a sample size of L = 1. For phredGAN d , we obtain an optimal α of 4 and 6 for the UDC and TV Transcripts respectively. For phredGAN a , we obtain an optimal value of 2 and 5 for the combined TV series dataset and the much larger UDC respectively.

[RESULTS]
We will now present our assessment of performance comparisons of phredGAN against the baselines, PHRED, hredGAN and Li et al.'s persona Seq2Seq models.

[QUANTITATIVE ANALYSIS]
We first report the performance on TV series transcripts in table 1. The performance of both SM and SAM models in Li et al. (2016b) compared to the hredGAN shows a strong baseline and indicates that the effect of persona is more important than that of multi-turn and adversarial training for datasets with weak multiple persona. However, once the persona information is added to the hredGAN , the resulting phredGAN shows a significant improvement over the SM and SAM baselines with phredGAN a performing best. We also observe that PHRED performs worse than the baseline S(A)M models on a number of metrics but we attribute this to the effect of persona on a limited dataset that results into less informative responses. This behavior was also reported in Li et al. (2016b) where the persona models produce less informative responses than the non-personal Seq2seq models but it seems to be even worse in multi-turn context. However, unlike the Speaker-Addressee and PHRED models that suffer from lower response quality due to persona conditioning, we note that conditioning the generator and discriminator of phredGAN on speaker embeddings does not compromise the systems ability to produce diverse responses. This problem might have been alleviated by the adversarial training that encourages the generator model to produce longer, more informative, and diverse responses that have high persona relevance even with a limited dataset.
We also compare the models performances on the UDC. The evaluation result is summarized in table 2. While the deleterious effect of persona conditioning on response diversity is still worse with PHRED than with S(A)M models, we note that hredGAN performs much better than the S(A)M models. This is because, the external persona only provides just a little more information than is already available from the UDC utterances. We also note an improvement of phredGAN variants over the hredGAN in a variety of evaluation metrics including perplexity, ROUGE with the exception of distinct n-grams. This is expected as phredGAN should be generally less diverse than hredGAN since the number of distinct data distribution modes is more for phredGAN dataset due to the persona attributes. However, this leads to better response quality with persona, something not achievable with hredGAN . Also, the much better ROUGE(F1) score indicates that phredGAN is able to strike a better balance between diversity and precision while still capturing the characteristics of the speaker attribute modality in the UDC dataset. Within the phredGAN variants, phredGAN d seems to perform better. This is not surprising as speaker classification is much easier on UDC than on TV series. The attribute discriminator, D att is able to provide more informative feedback on UDC than on TV series where it is more difficult to accurately predict the speaker. Therefore, we recommend phredGAN a for datasets with weak attribute distinction and phredGAN d for strong attribute distinction.

[QUALITATIVE ANALYSIS]
In addition to the quantitative analysis above, we report the results of the human evaluation in the last column of tables 1 and 2 for the TV Series and UDC datasets respectively. The human evaluation scores largely agrees with the automatic evaluations on the TV Series with phredGAN a clearly giving the best performance. However, on the UDC, both hredGAN and phredGAN d performs similarly which indicates that there is a trade off between diversity and persona by each model. We believe this is due to the strong persona information that already exists in the UDC utterances.
An additional qualitative assessment of these results are in Table 3 with responses from several characters in the TV series dataset and the two characters in UDC.
We see that for TV drama series, phredGAN responses are comparatively more informative than that of the Speaker-Addressee model of Li et al. (2016b). For example, all the characters in the TV series respond the same to the dialogue context. Similar behavior is reported in Li et al. (2016b) where for the Speaker-Addressee model, nearly all the characters in the TV series respond with We also see similar results with our model's output on UDC in table 4. We demonstrate that by conditioning as either a helper or questioner from the UDC dataset, phredGAN models are able to respond differently to input utterances as well as stay close to the context of the conversation.

[CONCLUSION AND FUTURE WORK]
In this paper, we improve upon state-of-the-art persona-based response generation models by exploring two persona conversational models: phredGAN a which passes the attribute representation as an additional input into a traditional adversarial discriminator, and phredGAN d a dual discriminator system which in addition to the adversarial discriminator from hredGAN , collaboratively predicts the attribute(s) that are intrinsic to the input utterance. Both systems demonstrate quantitative improvements upon state-of-the-art persona conversational systems such as the work from Li et al. (2016b) with respect to both quantitative automatic and qualitative human measures.
Our analysis also demonstrates how both variants of phredGAN perform differently on datasets with weak and strong modality. One of our future direction is to take advantage of phredGAN d 's ability to predict utterance attribute such as speaker identity from just the utterance. We believe its performance can be improved even with weak modality by further conditioning adversarial updates on both the attribute and adversarial discriminator accuracies. Overall, this paper demonstrates clear benefits from adversarial training of persona generative dialogue system and leaves the door open for more interesting work to be accomplished in this domain.  

[APPENDIX ALGORITHM 1 ADVERSARIAL LEARNING OF PHREDGAN]
Require: A generator G with parameters θ G . Require: An adversarial discriminator D adv with parameters θ D adv . Require: An attribute discriminator Datt with parameters θ D att . Require: Training hyperparameters, isT arget, λ G att , λ G adv , and λ M .
for number of training iterations do Initialize cRN N to zero state, h0 Sample a mini-batch of conversations, X = {Xi, Ci} N i=1 , X i = (X1, C1), (X2, C2), • • • , (Xi, Ci) with N utterances. Each utterance mini batch i contains Mi word tokens.
Compute the generator output similar to Eq. (11) in Olabiyi et al. (2018).
Sample a corresponding mini batch of utterance Yi.
else Update phredGANa's θ D adv with gradient of the discriminator loss.
Update θ G with attribute, adversarial and MLE losses.

[RESULTS -DISCRIMINATOR]
After training both phredGAN models on the TV series and UDC datasets, we ran inference on some example dialogue contexts. The responses and their discriminator scores from phredGAN s are listed in Tables 6, and 7. The tables shows that phredGAN (i) can handle multi-turn dialogue context with utterances and corresponding persona attributes; (ii) generates responses conditioned on a persona attribute; (iii) generates multiple responses per dialogue context and score their human likelihood by the discriminator; and (iv) in case of phredGAN d , can predict the attribute such as speaker identity that might have produced the utterance. We observe that the discriminator score(s) is/are generally reasonable with longer, more informative and more persona-related responses receiving higher scores. It worth to note that this behavior, although similar to the behavior of a human judge is learned without supervision. More so, we observe that phredGAN responses retain contextual consistency sometimes referencing background information that is inherent in the conversation between two speakers. For example, in the second sample of the TV series in Table 6, phredGAN a generator, conditioned on Leonard refers to Sheldon by name who is the second interlocutor. Also, in the third sample, phredGAN a , conditioned on Raj refers to Penny when responding to Leonard who happens to be Penny's boy friend. We see similar persona-based response generation for the UDC dataset with distinct communication style between the asker and the helper. For example, in Table 7, when the asker could not hear some music, phredGAN d , conditioned on helper suggested the asker might not be using the right driver. For the purpose of completion, we also show some samples from PHRED generator on both UDC and TV series dataset in Table 5.","[TITLE]
AN ADVERSARIAL LEARNING FRAMEWORK FOR A PERSONA-BASED MULTI-TURN DIALOGUE MODEL

[ABSTRACT]
In this paper, we extend the persona-based sequence-to-sequence (Seq2Seq) neural network conversation model to a multi-turn dialogue scenario by modifying the state-of-the-art hredGAN architecture to simultaneously capture utterance attributes such as speaker identity, dialogue topic, speaker sentiments and so on. The proposed system, phredGAN has a persona-based HRED generator (PHRED) and a conditional discriminator. We also explore two approaches to accomplish the conditional discriminator: (1) phredGAN a , a system that passes the attribute representation as an additional input into a traditional adversarial discriminator, and (2) phredGAN d , a dual discriminator system which in addition to the adversarial discriminator, collaboratively predicts the attribute(s) that generated the input utterance. To demonstrate the superior performance of phredGAN over the persona SeqSeq model, we experiment with two conversational datasets, the Ubuntu Dialogue Corpus (UDC) and TV series transcripts from the Big Bang Theory and Friends. Performance comparison is made with respect to a variety of quantitative measures as well as crowd-sourced human evaluation. We also explore the trade-offs from using either variant of phredGAN on datasets with many but weak attribute modalities (such as with Big Bang Theory and Friends) and ones with few but strong attribute modalities (customer-agent interactions in Ubuntu dataset)."
