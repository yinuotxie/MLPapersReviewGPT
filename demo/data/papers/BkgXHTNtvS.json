{"title": "BOUNDS ON OVER-PARAMETERIZATION FOR GUARANTEED EXISTENCE OF DESCENT PATHS IN SHALLOW RELU NETWORKS", "authors": "Arsalan Sharifnassab; Saber Salehkaleybar; S Jamaloddin Golestani", "pub_date": "", "abstract": "We study the landscape of squared loss in neural networks with one-hidden layer and ReLU activation functions. Let m and d be the widths of hidden and input layers, respectively. We show that there exist poor local minima with positive curvature for some training sets of size n \u2265 m + 2d \u2212 2. By positive curvature of a local minimum, we mean that within a small neighborhood the loss function is strictly increasing in all directions. Consequently, for such training sets, there are initialization of weights from which there is no descent path to global optima. It is known that for n \u2264 m, there always exist descent paths to global optima from all initial weights. In this perspective, our results provide a somewhat sharp characterization of the over-parameterization required for \"existence of descent paths\" in the loss landscape.", "sections": [{"heading": "INTRODUCTION", "text": "We consider shallow neural networks of the form shown in Fig. 1. The network comprises a hidden layer and an input layer of widths m and d, respectively; and is to be trained over a training set of size n. Our results concern the slightly over-parameterized regime where n \u2248 m. We study the existence of poor local minima that have positive curvature in the empirical squared loss landscape.\nIt is well-known that poor local minima exist in the loss landscape of shallow networks of arbitrary width. In fact, in a shallow network with ReLU activation functions, it is easy to construct training sets whose empirical loss landscape has high plateaus. 1 It is however not fully understood that under what conditions poor local minima may have positive curvature. This paper presents results that improve this understanding.\nNon-existence of spurious local minima is closely connected to the so called descent path property: a loss landscape is said to have the descent path property if starting from any initial point there is a path of descent loss to a global minimum. From optimization perspective, the descent path property favors descent optimization algorithms like the pure gradient descent (GD) method. For SGD as well, non-existence of poor local minima is known to be a favorable property for guaranteed convergence (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016). The descent path property is shown to be satisfied in over-parameterized shallow networks with sufficiently large widths (Venturi et al., 2018). The results we present in this work, tighten the existing bounds on the over-parameterization required to guarantee this property.", "n_publication_ref": 5, "n_figure_ref": 1}, {"heading": "BACKGROUND", "text": "Over the past few years, deep neural networks have achieved tremendous performance in various artificial intelligence applications such as computer vision, reinforcement learning, and natural language processing, etc. Despite their remarkable success in practice, theoretical aspects of this success remain a mystery. It has long been an open problem why simple local search algorithms for training deep neural networks, like stochastic gradient descent (SGD), typically converge to local minima with low training error despite the highly non-convex behavior of empirical loss. It has been observed, e.g., in (Choromanska et al., 2015), that these methods may get stuck in poor local minima (i.e., local minima with empirical loss much larger than the global optimum) for small networks, while the problem fades away as the number of parameters grows larger. Such observations are often explained by studying the loss landscape in over-parameterized regime where the number of parameters in the network exceeds the training sample size.\nRecently, several attempts have been made to characterize properties of squared loss landscape by conditioning on the layers' dimensions and sample size. Soudry and Hoffer (2017) showed that weights of a neural network can be adjusted such that the empirical loss is zero almost surely if m > 4 n/(2d \u2212 2) \u2248 (2n)/d. This result is consistent with experimental observations that neural networks can fit training data if the number of parameters (here approximately 2n) is greater than the sample size. They also proved for normally distributed input that as n goes to infinity, the ratio between the volume of poor flat local minima regions to the volume of flat global minima fades exponentially if d =\u03a9( \u221a n) and m =\u03a9(n/d). Safran and Shamir (2016) showed that if the number of neurons in the hidden layer is \u2126(n rank(X) ) (where X is the matrix containing all input), then with high probability, random initialization of weights will put them in a region of parameter space at which the loss surface has a basin-like structure, i.e., every local minimum in that region is global. In another work (Safran and Shamir, 2017), the same authors provide a computer-assisted proof to show that spurious local minima are common in the expected loss landscape of shallow under-parameterized (small-width) networks. Xie et al. (2016) showed that if the input data is drawn uniformly at random from the unit sphere, and if m =\u03a9(n \u03b2 ) and d =\u03a9(n \u03b2 ) with \u03b2 \u2208 (0, 1) being the decay exponent of the smallest eigenvalue of a kernel matrix, then every critical point is a global minimum.  proved that for any continuous activation function and under the assumption that data samples are distinct, there exist no poor local minima with positive curvature if m \u2265 n . In the same spirit, Venturi et al. (2018) showed that for any continuous activation function, there is always a descent path to an optimal solution with zero loss in the empirical loss landscape if m \u2265 n.\nSeveral works have proposed similar results in other settings and under different assumptions. Soudry and Carmon (2016) showed that in a network of leaky ReLU activation functions with randomized perturbation of slopes, all differentiable local minima are global minima if m \u2265 n/d. Kawaguchi (2016) proved that in shallow networks with linear activation functions, every local minimum is a global minimum and all the saddle points are strict in the sense that they have a direction of strictly negative curvature. Soltanolkotabi et al. (2019) showed that the same result carries over to quadratic activation functions under the assumption that the last layer comprises at east d positive and d negative weights. Du and Lee (2018) established similar results for quadratic activation functions, assuming m \u2265 \u221a 2n. For deep neural networks with linear activation functions, Freeman and Bruna (2016) showed that all local minima are global minima if there is a hidden layer whose number of neurons exceeds the minimum of the widths of input and output layers. For deep neural networks with analytical activation functions, Nguyen and Hein (2017) proved a similar property under the assumptions that the number of neurons in some hidden layer is greater than sample size and the network has a pyramidal structure.\nSuch studies on the properties of loss landscape do not only provide insights into the complication of training, but are also beneficial for proving performance guarantees for some local search algorithms. For the class of loss functions whose landscape satisfy the properties of: a) all local minima are global, b) all saddle points are strict, it has been shown in several works (Ge et al., 2015;Jin et al., 2017;Lee et al., 2016) that perturbed gradient descent converges to global optima in polynomial time. Another direction of research concerns the convergence of specific optimization algorithm such as Figure 1: Architecture of the shallow network considered in this paper. The network has a single hidden layer of m neurons with ReLU activation functions, and a neuron with linear activation function in its output layer. pure gradient descent and SGD without assuming such properties for the loss landscape Allen-Zhu et al., 2018;.", "n_publication_ref": 15, "n_figure_ref": 1}, {"heading": "OUR CONTRIBUTIONS", "text": "We study the amount of over-parameterization required for guaranteed existence of descent paths to zero loss in the empirical loss landscape. Previous works suggest that zero loss is always possible for m \u2265 2n/d (Soudry and Hoffer, 2017). On the other hand, the best existing bound for guaranteed existence of descent paths to this zero loss requires m \u2265 n neurons in the hidden layer (Venturi et al., 2018). Prior to the present work, it was not known whether the \"descent path property\" holds for m < n. Even for m \u2208 (2n/d, n), where zero empirical risk is known to be achievable (Soudry and Hoffer, 2017), the existence of descent paths was in question. In this work, we tighten this gap and prove that there are training sets, under which in any network of width m \u2264 n \u2212 2d + 2, there exist initial weights that have no descent path to global minima. We do this by showing that the loss landscape, in this regime, admits poor local minima with positive curvature. We also provide evidences and make conjectures that these results carry over to networks of width m = n \u2212 4, which, if true, provides a sharp characterization of the over-parameterization required for guaranteed existence of descent paths. We also wish to point that unlike most previous works, we do not restrict to differentiable local minima; for a simple argument shows that local minima with positive curvature cannot be differentiable if m > n/d (cf. Appendix A).", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "OUTLINE", "text": "We continue by discussing details of the system model and introducing our key definitions in Section 2. We then present, in Section 3, the main results of the paper. Proof of the main results are then given in Section 4. We finally discuss implications and possible extensions of our results in Section 5 along with a number of open problems and directions for future research.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "PRELIMINARIES", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "MODEL", "text": "We consider shallow networks of the form shown in Fig. 1. The network takes d-dimensional inputs denoted by X. There is a single hidden layer comprising m neurons with ReLU activation function. For simplicity of our proofs, we only consider even values of m. We denote the input weights of r-th neuron by a d-dimensional vector w r , for r = 1, . . . , m. We then let w \u2208 R md be the vector representation of all weights in the first layer.\nThe output layer has a single neuron, whose activation function is linear with an m-dimensional weight vector denoted by v. The network outputs a scalar\u0177(w, v) = m r=1 v r w T r X1(w T r X \u2265 0). We fix a training set (X 1 , y 1 ), . . . , (X n , y n ) of size n, and consider the landscape of empirical squared loss function:\nF (w, v) n i=1 \u0177 i (w, v) \u2212 y i 2 .\n(1)", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "PROPERTIES OF THE LANDSCAPE", "text": "We first provide a formal definition for the descent path property, which is a necessary condition for guaranteed performance of descent optimization algorithms.\nDefinition 1 (Descent path property). Consider a continuous function f : R d \u2192 R and let f * = inf x\u2208R d f (x) be its infimum. We say that f has the descent path property if for any x \u2208 R d , there exists a continuous curve with \u03b3 : [0, 1] \u2192 R d such that \u03b3(0) = x, f \u03b3(1) = f * , and f \u03b3(t) is a non-increasing function of t.\nThe descent path property is a necessary condition for any descent optimization algorithm to provably find a global minimum from all initial conditions. It was shown in Venturi et al. (2018) that the empirical loss landscape of a shallow neural network with ReLU activation and squared loss has the descent path property if the size of training data is no larger than the width of the hidden layer, i.e., n \u2264 m. We now characterize a class of local minima of specific form in the following definition.\nDefinition 2 (Cupped minima). Given a function f : R d \u2192 R, we call x \u2208 R d a cupped minimum of f if there are , \u03b4 > 0 such that for any y in the \u03b4-neighborhood of x, we have f (y) \u2265 f (x)+ y\u2212x 2 . By a sub-optimal cupped minimum we mean a cupped minimum that is not a global minimum.\nNote that every cupped minimum is a local minimum, but not every local minimum is cupped (e.g., flat local minima are not curved downwards, and hence are not cupped). Also note that a function is not necessarily differentiable at its cupped minima. We study cupped minima of the loss function in equation 1. Note however that for any \u03b1 > 0, F (\u03b1w, v/\u03b1) = F (w, v). Therefore, F (\u2022, \u2022) has no cupped minima if both arguments are taken as variables. For that matter, when talking about cupped minima of F , we fix a v and consider F (\u2022, v) as a function of its first argument. Interestingly, existence of cupped minima for F (\u2022, v) leads to violation of descent path property for F (\u2022, \u2022) over both arguments, as shown in the following lemma. The proof is given in Appendix B.\nLemma 1. Consider a shallow network with loss function F in equation 1, and a pair of weights (w, v). Suppose that w is a sub-optimal cupped minimum of F (\u2022, v), and that w r = 0, for r = 1, . . . , m. Then, F (\u2022, \u2022) has no descent path w(t), v(t) , initiated at (w, v), to its global minima.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "MAIN RESULTS", "text": "The following theorem and corollary state the main results of the paper.\nTheorem 1. For any d \u2265 4, m \u2265 8 + 4 3/(d \u2212 3) , and n \u2265 m + 2d \u2212 2, there exists a training set of size n such that the empirical loss function F of a shallow neural network of width m has the following property. For any m-dimensional vector v, with m/2 number of positive and m/2 number of negative entries, F (\u2022, v) has exponentially many sub-optimal cupped minima.\nThe proof is constructive and is given in Section 4. In particular, we devise a training sequence (X 1 , y 1 ), . . . , (X n , y n ) such that for weights (w, v) at the cupped minima, we have\nv r w r = 1/ \u221a m, for r = 1, . . . , m. Moreover, X i \u2264 1, |y i | \u2264 2, and |e i | = 1 for i = 1, . . . , n (cf. Remark 2).\nAccording to Theorem 1, there are training sequences tailored to give rise to sub-optimal cupped minima. However, we wish to point that the existence of such cupped minima does not stem from measure-zero incidents like placement of several data points on a low dimensional plane. In fact, in view of Lemma 1, any path that starts from a cupped minimum and end up in a global minimum would have an uphill climb of at least , for some > 0. Since the loss surface is a continuous function of (X i , y i ), a small perturbation of (X i , y i )'s leads continuously to a small change in F . Therefore, for small enough perturbations of (X i , y i ), any path to the set of global minima would still witness a positive uphill-climb. Hence, the descent path property remains out of order, even when the training data is slightly perturbed. Based on the above intuition, we can establish the following corollary 2 , Corollary 1. For any d \u2265 4, m \u2265 8 + 4 3/(d \u2212 3) , and n \u2265 m + 2d \u2212 2; and when the inputs X and labels y are randomly drawn from independent normal distributions, there is a non-zero probability that F (\u2022, \u2022) does not have the descent path property. Venturi et al. (2018) that n \u2264 m is sufficient for the descent path property to hold. In contrast, Corollary 1 show that if n \u2265 m + 2d \u2212 2, then the descent path property is not necessarily in effect. This leave a gap of size 2d \u2212 2 for the edge of over-parameterization required to guarantee the descent path property. We believe that this edge lies sharp at n = m. We conjecture a stronger version of Theorem 1, that cupped minima can emerge for training data sizes as small as m = n \u2212 4. Conjecture 1. Statement of Theorem 1 holds for all d \u2265 4, m \u2265 2d + 4, and n \u2265 m + 4.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "It was shown in", "text": "See Remark 1 for insights into the possibility of this conjecture.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "PROOF OF THE MAIN RESULT", "text": "In this section, we present the proof of Theorem 1 organized in a sequence of four subsections. We first present some preliminaries in Subsection 4.1. In Subsection 4.2, we introduce a geometric structure called \"(d, t, k)-configuration\", based on which we construct, in Subsection 4.3, the training set that gives rise to cupped minima in the loss landscape. Finally, in Subsection 4.4, we prove the existence of cupped mimima in the devised setting. In order to provide intuitions on the loss landscape at cupped minima and motivate our construction of the training set in Subsection 4.3, we make a short note on different types of cupped minima in Appendix A. We also defer the proofs of some lemmas from this section to appendices for improved readability.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "PRELIMINARIES", "text": "Consider weights (w, v) and let (w , v ) be another set of weights such that for r = 1, . . . , m, v r and v r have the same sign and w r = (v r /v r )w r . Then, F (w, v) = F (w , v ). Moreover, it is no difficult to see that w is a cupped minimum of F (\u2022, v) if and only if w is a cupped minimum of F (\u2022, v ). For this reason, it suffices to prove existence of cupped minima for a fixed vector v. Note also that where w r 's are distinct, any permutation of the order of neurons would give rise to a new cupped minimum. Hence, existence of a cupped minimum for F (\u2022, v) implies existence of exponentially many cupped minima for F (\u2022, v).\nWe denote by e i =\u0177 i (w, v) \u2212 y i the estimation error for input X i . We let u d = [0, . . . , 0, 1] T be a d-dimensional vector with the last entry equal to one and all other entries equal to zero. For a region P \u2286 R d , we denote its interior and and its convex-hull by int(P) and Conv(P), respectively. Assuming differentiability of F at w, the partial derivatives of the loss function with respect to w r , r = 1, . . . , m, is as follows\n\u2207 wr F (w, v) = v r n i=1 e i X i 1 w T r X i \u2265 0 . (2)", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "A GEOMETRIC CONFIGURATION", "text": "We introduce a geometric structure for sets of points in R d . This configuration will be used in Subsection 4.3 to construct a landscape with cupped minima.\nDefinition 3 ((d, t, k)-Configuration). Given integers d, t, k \u2265 0 and disjoint sets A,\u00c3, B, andB of points in R d , we say that A,\u00c3, B,B forms a (d, t, k)-configuration if the following properties are satisfied:\n(p1) Each of A and B consists of t points, and each of\u00c3 andB consists of k points.\n(p2) (A) The convex hull of A \u00c3 forms a polytope P A that has exactly t + k vertices. Equivalently, no point in A \u00c3 is a convex combination of other points in A \u00c3 . (B) Similarly, the convex hull of B B forms a polytope P B that has exactly t + k vertices.\n(p3) We have 0 \u2208 int(P A ) and 0 \u2208 int(P B ).\n(p4) There exists a constant \u03b2 \u2208 (0, 1) such that (A) For any a \u2208 A, \u03b2a lies on a facet of P B . We denote this facet by S B (a). (p8) Consider a 2t \u00d7 2t matrix M whose rows and columns are associated to points p \u2208 A B and q \u2208 A B, and whose entries are as follows\nM pq = d p, H q , if p = q, OR p \u2208 A and q \u2208 B, OR p \u2208 B and q \u2208 A, 0, otherwise,(3)\nwhere H q is the hyperplane define in Property (p7), and d(\u2022, \u2022) is the euclidean distance.\nThe property requires M to be full-rank.\nAmong the above properties, the most difficult of all is Property (p4), and the requirement that it involves the same \u03b2 for all points in A B. In fact, elimination of Property (p4) gives rise to trivial configurations. 3 In the two dimensional space, for any t \u2265 4 there exists a (2, t, 0)-configuration of the form illustrated in Fig. 2. In the following proposition, we generalize this observation to higher dimensions. Proposition 1. For any d \u2265 2 and t \u2265 4, there exists a (d, t, d \u2212 2)-configuration.\nThe proof is constructive, and is given in Appendix C. We conjecture that there also exist (d, t, 0)configurations.\nConjecture 2. For any d \u2265 2 and t \u2265 2d, there exists a (d, t, 0)-configuration. Remark 1. Using a configuration given by Conjecture 2 instead of the configuration from Proposition 1 in the construction and the proof that follow, we obtain a proof for Conjecture 1. In this view, establishing Conjecture 2 would also resolve Conjecture 1.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "CONSTRUCTING A LANDSCAPE WITH CUPPED MINIMA", "text": "Here we present a set of training data (X 1 , y 1 ), . . . , (X n , y n ) and a set of wights (w 1 , v 1 ), . . . , (w m , v m ) such that the empirical loss surface corresponding to (X 1 , y 1 ), . . . , (X n , y n ) has a sub-optimal cupped minimum at (w 1 , v 1 ), . . . , (w m , v m ). Without loss of generality, we assume n = m + 2d \u2212 2. Extension to larger values of n is straightforward via replication.\nLet A,\u00c3, B,B be a (d \u2212 1, m/2, d \u2212 3)-configuration in the (d \u2212 1)\n-dimensional space, as in Proposition 1. In view of Property (p3), let 0 > 0 be such that P A and P B contain the 0neighborhood of 0. Let\n\u03be 1 0 a\u2208A \u00c3 a + b\u2208B B b + n + 1 \u03b2 . (4\n)\nWe proceed by introducing the data points X 1 , . . . , X n . An illustration of these data points in the three dimensional space is shown in Fig. 3.  Data points X: We consider a total number of n = m + 2d \u2212 2 data points as follows.\n\u2022 For each a \u2208 A \u00c3 , we consider a new data point X a as follows.\nLet [z 1 , . . . , z d\u22121 ] \u2208 R d\u22121\nbe the representation of a in the Cartesian coordinates. We let\nX a = z 1 , . . . , z d\u22121 , 1 . \u2022 For each b \u2208 B B , we consider a new data point X b as follows. Let [z 1 , . . . , z d\u22121 ] \u2208 R d\u22121 be the representation of b in the Cartesian coordinates. We let X b = \u2212 z 1 , . . . , z d\u22121 , 1 .\n\u2022 We consider two extra points X A + , X A \u2212 , X B + , and X B \u2212 as follows. We let\nX A \u2212 \u03beu d , X B \u2212\n\u2212\u03beu d , and\nX A + X A \u2212 \u2212 a\u2208A \u00c3 X a + 1/\u03b2 \u2212 1 u d (5) X B + X B \u2212 + b\u2208B B X b + 1/\u03b2 \u2212 1 u d (6\n)\nwhere u d = [0, . . . , 0, 1] T , and \u03be and \u03b2 are defined in equation 4 and Property (p4), respectively.\nWeights at cupped minimum: We associate each of m neurons to a point p in A B, in a one-one manner; and denote the vector of input weights and the output weight of that neuron by w p and v p , respectively. These weights are chosen as follows:  p6)). We let w a be the unique vector such that w a = 1 and We let w b be the unique vector such that w b = 1 and\n\u2022 For each a \u2208 A, we let v a = \u22121/ \u221a m. \u2022 For each b \u2208 B, we let v b = 1/ \u221a m. \u2022 For each a \u2208 A,\nw T a X si(a) = 0, i = 1, . . . , d \u2212 1,(7)\nw T a u d < 0. (8\nw T b X si(b) = 0, i = 1, . . . , d \u2212 1,(9)\nw T b u d > 0. (10\n)\nLabels y: Having determined the data points X and the weights (w, v), the output\u0177(w, v) of the network is determined for all input X. In the following, we choose the true labels y to obtain a desired error e =\u0177 \u2212 y for each input data. In particular:\n\u2022 For each a \u2208 A \u00c3 , we associate to X a a label y a so that e a \u0177 a (w, v) \u2212 y a = 1.\n\u2022 For each b \u2208 B B , we associate to X b a label y b so that e b \u0177 b (w, v) \u2212 y b = \u22121.\n\u2022 We choose the labels associated to X A + , X A \u2212 , X B + , and X B \u2212 such that e A + = e B + = 1 and e A \u2212 = e B \u2212 = \u22121.\nThis completes the description of the training set. As shown in in Soudry and Carmon (2016), there exist weights that achieve zero loss if m > 4 n/(2d \u2212 2) . In our case, n = m + 2d \u2212 2, and its easy to check that m > 4 (m + 2d \u2212 2)/(2d \u2212 2) for all d \u2265 4 and m \u2265 8 + 4 3/(d \u2212 3) . It follows that in our setting the global optimum has zero loss, showing that the above (w, v) is sub-optimal. Remark 2. In the above construction, the norms of inputs vectors may be very large. If we scale down the inputs such that X i \u2264 1/ \u221a m for i = 1, . . . , n, and modify the corresponding labels y i such that e i remains unchanged, then a same set of weights will still be a cupped minimum for the landscape defined in terms of new (X i , y i )'s. Moreover, for this setting, it is easy to check that w r = 1, e i = 1, and |y i | \u2264 2.", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "PROVING THE CUPPED", "text": "MINIMA PROPERTY Let \u03b4 min |w T r X i | X i w T r X i = 0, r = 1, . . . , m, i = 1, . . . , n .(11)\nThen, \u03b4 > 0. For any \u03b8 \u2208 R md with \u03b8 = 1 and for any t \u2208 [0, \u03b4], let\nF \u03b8 (t) = F (w + \u03b8t, v).(12)\nWe show that there is an > 0 such that for any unit-norm \u03b8 and any t \u2208 [0, \u03b4],\nF \u03b8 (t) \u2265 F \u03b8 (0) + t 2 . (13\n)\nLemma 2. For any \u03b8 \u2208 R md with \u03b8 = 1, F \u03b8 (\u2022) is a quadratic and convex function over\n[0, \u03b4].\nThe proof is give in Appendix D and relies on the fact that neuron activations do not alter at w + \u03b8t for t \u2208 [0, \u03b4]. Consider now the following m-dimensional subspace of R md\nH w \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 \uf8ee \uf8ef \uf8f0 \u03b1 1 w 1 . . . \u03b1 m w m \uf8f9 \uf8fa \uf8fb \u03b1 1 , . . . , \u03b1 m \u2208 R \uf8fc \uf8f4 \uf8fd \uf8f4 \uf8fe . (14\n)\nFor any \u03b8 \u2208 R md , let \u03b8 and \u03b8 \u22a5 be the orthogonal projections of \u03b8 on H w and H \u22a5 w , respectively. Then, \u03b8 = \u03b8 + \u03b8 \u22a5 . In order to establish equation 13, we need lower bounds on F \u03b8 (0) and F \u03b8 , which we derive in the next two lemmas. Lemma 3. There exists \u00b5 > 0 such that for any \u03b8 \u2208 R md with \u03b8 = 1, we have\ndF \u03b8 (t) d + t t=0 \u2265 \u00b5 \u03b8 \u22a5 . (15\n)\nThe proof is given in Appendix E, and relies in a subtle way on the choice of data points in subsection 4.3. We now bound the curvature of F \u03b8 . Lemma 4. There exist constants \u03b7 1 , \u03b7 2 > 0 such that for any \u03b8 \u2208 R md with \u03b8 = 1, and for any t \u2208 (0, \u03b4),\nd 2 F \u03b8 (t) dt 2 \u2265 max 2\u03b7 1 \u03b8 2 \u2212 2\u03b7 2 \u03b8 \u22a5 , 0 . (16\n)\nThe proof is given in Appendix F, and relies on Property (p8) of the underlying configuration.\nConsider now the second order polynomial p(x) = \u03b7 1 \u03b4 1 \u2212 x 2 \u2212 \u03b7 2 \u03b4x \u2212 \u00b5x, where \u00b5, \u03b7 1 , and \u03b7 2 are the constants in Lemmas 3 and 4. Since p(0) > 0 and p(1) < 0, the polynomial p has exactly one root in the interval (0, 1), which we denote by x 0 . Let\n\u00b5x 0 /\u03b4. (17\n)\nLemma 5. For any \u03b8 \u2208 R md with \u03b8 = 1, and any t \u2208 [0, \u03b4], we have\nF \u03b8 (t) \u2265 F \u03b8 (0) + t 2 .\nThis lemma is a simple consequence of Lemmas 3 and 4, and its proof is given in Appendix G. It follows from Lemma 5 that for any w in the \u03b4-neighborhood of w, we have F (w , v) \u2265 F (w, v) + w \u2212 w 2 . This shows that w is a cupped minimum for F (\u2022, v), and completes the proof of Theorem 1.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "DISCUSSION", "text": "The guaranteed existence of descent paths in shallow networks of ReLU neurons was previously established (Venturi et al., 2018), given an over-parameterization of m \u2265 n (where m and n are the number of neurons and the size of training data, respectively). This left an uncertainty gap of 2n/d < m < n, where zero empirical risk is known to be achievable (for m \u2265 2n/d) (Soudry and Hoffer, 2017), but the existence of descent paths was in question. In this work, we have tightened this uncertainty gap to n \u2212 2d + 2 < m < n, by proving that for any m \u2264 n \u2212 2d + 2, there are input data and initial weights for which a descent path does not exist. This conclusion we reach by establishing the existence of cupped minima for m \u2264 n \u2212 2d + 2, and for the right choice of input data.\nCompared to similar existing results for other activation functions, our results suggest that the edge m \u2248 n of over-parameterization required for elimination of sub-optimal cupped minima in ReLU networks is much higher than that of networks with quadratic activation functions, m \u2248 \u221a 2n (Du and Lee, 2018), and linear activation functions, m \u2248 n/d (Kawaguchi, 2016), and is almost as high as general continuous activation functions of any form, m \u2264 n (Venturi et al., 2018).\nNon-existence of spurious local minima and the decent path property favor the convergence of decent optimization methods like GD. However, for different variants of noisy GD, like SGD and Langevin dynamics, it is quite common for the empirical loss to fluctuate during the training. Nevertheless, for theoretical analysis purposes it usually helps to take the noise away, for example by tending the step size to zero. The resulting GD, which always follows a descent path, is usually easier to analyze and can also help to study the SGD dynamics. On the other hand, from a practical view, convergence of SGD in local-min-free landscapes is well-studied.\nAside from addressing Conjecture 2, there remain several open problems, which we review next. As an important direction for future research, it would be interesting if one could obtain bounds on the probability of existence of cupped minima over random data sets, underneath the edge of over-parameterization. In particular, we showed in Corollary 1 that this probability is non-zero, however we gave no clue on either the size or scaling of this probability. Another class of problems concerns basins of local minima, and how they affect dynamics of first order optimization algorithms. As a step toward this goal, one might characterize the true over-parameterization regime in which the basins of poor local minima have considerable volume. Among other directions are extensions of our results to deep ReLU networks, shallow non-ReLU networks, and shallow ReLU networks under loss functions more general than the squared loss.", "n_publication_ref": 4, "n_figure_ref": 0}, {"heading": "APPENDICES A DIFFERENT TYPES OF CUPPED MINIMA IN TERMS OF DIFFERENTIABILITY", "text": "Here we discuss different types of cupped minima and provide elementary intuitions on the loss landscape at cupped minima.\nWe first characterize curvature of the loss function at differentiable points. For r = 1, . . . , m, let J r be an n \u00d7 n diagonal matrix whose (i, i) entry equals 1 w T r X i \u2265 0 . Let G be an n \u00d7 md matrix of the form:\nG = v 1 J 1 X T \u2022 \u2022 \u2022 v m J m X T , (18\n)\nwhere X is a d \u00d7 n matrix that has X i in its i-th column. If F is differentiable at (w, v), its gradient is \u2207 w F (w, v) = G T [e 1 , . . . , e m ] T , where e i =\u0177 i \u2212 y i is the output error for input X i . Moreover, if F is differentiable at (w, v), its Hessian with respect to w equals\n\u2207 2 w F (w, v) = G T G. (19\n)\nWe classify cupped minima into three categories in terms of differentiability. Specifically, for a cupped minimum w of F (\u2022, v), we consider three cases:\nType 1) F is differentiable at w.\nType 2) F (w + \u03b8t, v) as a function of t is non-differentiable at t = 0, for all \u03b8 \u2208 R md .\nType 3) There are \u03b8 1 , \u03b8 2 \u2208 R md such that F (w+\u03b8 1 t, v) is differentiable at t = 0, while F (w+\u03b8 2 t, v) is non-differentiable at t = 0.\nFig. 4 illustrates examples of loss surface at the above three types of cupped minima. We now argue that the first two types are not possible in the loss landscape of shallow networks.\nIf F (\u2022, v) is differentiable at w, its Hessian given in equation 19 equals G T G. Since G is an n \u00d7 md matrix, assuming md > n, G T G would have zero eigenvalues. Therefore, w cannot be a cupped minimum of F (\u2022, v). It follows that there exists no differentiable cupped minimum (nor saddle point) if md > n.\nFor non-differentiable points, note that F (w + wt, v) as a function of t is a differentiable quadratic function. This is because the output scales proportionally with w. Therefore, cupped minima of the second type are not possible, as well. In the same spirit, it can be shown that F (w + \u03b8t, v) is differentiable as a function of t, if \u03b8 \u2208 R md belongs to the m dimensional subspace H w defined in equation 14.\nFor the above reasons, all cupped minima, if any, are of the third type. Therefore in the construction of Subsection 4.3, we introduce a training set and a pair of weights (w, v) such that w is a cupped minimum of F (\u2022, v), and F (w + \u03b8t, v) is differentiable in t only for \u03b8 \u2208 H w .", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "B PROOF OF LEMMA 1", "text": "If v has a zero entry, v r = 0 for some r, then F (\u2022, v) is a constant function with respect to w r , and thereby has no cupped minima. Therefore, we assume that v has no zero entries. Let\nt 0 = inf t | \u2203r, v r (t) = 0 .\nFor t > 0, we definew\n(t) = \uf8ee \uf8ef \uf8f0 v 1 (t)/v 1 \u00d7 w 1 (t) . . . v m (t)/v m \u00d7 w m (t) \uf8f9 \uf8fa \uf8fb . (20\n)\nLet t 1 = inf t |w(t) = w . We show that t 1 < t 0 . If t 0 < \u221e, then continuity of v(t) implies that v r (t 0 ) = 0, for some r \u2264 m. Therefore,w r (t 0 ) = 0 = w r . It then follows from the continuity ofw r (\u2022) that there is an > 0 such thatw r (t 0 \u2212 ) = w r . Consequently, t 1 < t 0 .\nThe inequality t 1 < t 0 implies that there is an > 0 such thatw r (t 1 + ) = w, and for any t \u2208 [0, t 1 + ], we have sgn v r (t) = sgn(v r ), r = 1, . . . , m. Therefore, for any t \u2208 [0, w, v). This shows that w(t), v(t) cannot be a descent path for F (\u2022, \u2022), and completes the proof of Lemma 1.\nt 1 + ], we have F w(t), v = F w(t), v(t) . Since w is a cupped minimum of F (\u2022, v), there is an s \u2208 [t 1 , t 1 + ] such that F w(t), v > F (", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "C PROOF OF PROPOSITION 1", "text": "For i = 3, . . . , d, let\nc i 1 i \u2212 1 + 2 cos(\u03c0/t) . (21\n)\nFix a constantc\nd i=3 1 + c i = d i=3 i + 2 cos(\u03c0/t) i \u2212 1 + 2 cos(\u03c0/t) = d + 2 cos(\u03c0/t) 2 + 2 cos(\u03c0/t) . (22\n)\nLet \u03b3 be a uniform random variable\n\u03b3 \u223c unif cos(\u03c0/t) \u2212 cos(2\u03c0/t) 4c , cos(\u03c0/t) \u2212 cos(2\u03c0/t) 2c . (23\n)\nWe take a sample from the above distribution and fix a \u03b3 for the rest of the proof.\nWe now introduce the points in the configuration. For i = 0, . . . , t \u2212 1, we consider points a i \u2208 A and b i \u2208 B as follows\na i = cos 2\u03c0(i \u2212 1/2) t , sin 2\u03c0(i \u2212 1/2) t , \u03b3c 3 , \u03b3c 4 , . . . , \u03b3 d c d ,(24)\nb i = cos 2\u03c0i t , sin 2\u03c0i t , \u2212\u03b3c 3 , \u2212\u03b3c 4 , . . . , \u2212\u03b3 d c d . (25\n)\nFor i = 3, . . . , d, we consider points\u00e3 i \u2208\u00c3 andb i \u2208B as follows\na i = 0, . . . , 0 i\u22121 , \u22121, c i+1 , . . . , c d , (26\n) b i = 0, . . . , 0 i\u22121 , 1, \u2212c i+1 , . . . , \u2212c d . (27\n)\nFig. 5 shows an illustration of these points for d = 3.\nWe proceed by verifying Properties (p1)-(p8).\nProperty (p1): Property (p1) is straightforward from the above construction.\nProperty (p2): Considering only the first two coordinates, it is easy to see that no point of A is a convex combination of other points in A \u00c3 . Also note that for i = 3, . . . , d,\u00e3 i is the only point Property (p3): Let\n\u03b1 i = 1 \u03b3t , i = 1, . . . , t,(28)\n\u03b1 i = (c 3 + 1) \u2022 \u2022 \u2022 (c i\u22121 + 1)c i , i = 3, . . . , d.(29)\nA simple induction on j shows that\nj\u22121 i=3\u03b1 i =\u03b1 j c j \u2212 1. (30\n)\nfor j = 4, . . . , d. We show that\n1 t\u22121 i=0 \u03b1 i + d i=3\u03b1 i t\u22121 i=0 \u03b1 i a i + d i=3\u03b1 i\u00e3 i = 0. (31\n)\nFor the first two coordinates, equation 31 is easy. Let a i j and\u00e3 i j be the j-th entries of a i and\u00e3 i , respectively. For the j-th coordinate, j = 3, . . . , d, we then have\nt\u22121 i=0 \u03b1 i a i j + d i=3\u03b1 i\u00e3 i j = t \u00d7 1 t\u03b3 \u00d7 \u03b3c j + c j j\u22121 i=3\u03b1 i \u2212\u03b1 j = c j + c j \u03b1 j c j \u2212 1 \u2212\u03b1 j = 0,\nwhere the second equality is due to equation 30. This establishes equation 31. It then follows from equation 31 that 0 is a convex combination of points in A \u00c3 . Consequently, 0 \u2208 int(P A ). A similar argument shows that 0 \u2208 int(P B ). Properties (p4) and (p5): For the j-th coordinate, j = 3, . . . , d, we have\na 0 j + a 1 j + \u03b3 d i=3\u00e3 i j = \u03b3c j + \u03b3c j + \u03b3 j\u22121 i=3 c j \u2212 \u03b3 = \u03b3 (j \u2212 1)c j \u2212 1 = \u03b3 (j \u2212 1) 1 j \u2212 1 + 2 cos(\u03c0/t) \u2212 1 = \u22122\u03b3 cos(\u03c0/t) j \u2212 1 + 2 cos(\u03c0/t) = \u2212 2 cos(\u03c0/t) \u03b3c j = 2 cos(\u03c0/t) b 0 j ,(32)\nwhere b 0 j is the j-th entry of b 0 defined in equation 25. Similarly, for the first two coordinates, we have:\na 0 1 + a 1 1 + \u03b3 d i=3\u00e3 i 1 = cos(\u03c0/t) + cos(\u03c0/t) = 2 cos(\u03c0/t) b 0 1 , a 0 2 + a 1 2 + \u03b3 d i=3\u00e3 i 2 = sin(\u03c0/t) \u2212 sin(\u03c0/t) = 0 = 2 cos(\u03c0/t) b 0 2 .(33)\nIt then follows from equation 32 and equation 33 that\nb 0 = 1 2 cos(\u03c0/t) a 0 + a 1 + \u03b3 d i=3\u00e3 i .(34)\nLet\n\u03b2 2 cos(\u03c0/t) (d \u2212 2)\u03b3 + 2 . (35\n)\nThen, from equation 34,\n\u03b2b 0 = 1 (d \u2212 2)\u03b3 + 2 a 0 + a 1 + \u03b3 d i=3\u00e3 i .(36)\nTherefore, \u03b2b 0 is a convex combination of a 0 , a 1 ,\u00e3 3 , . . . ,\u00e3 d , and therefore lies on the simplex S that has a 0 , a 1 ,\u00e3 3 , . . . ,\u00e3 d as its vertices. Next, we show that S is a facet of P A .\nConsider a vector z \u2208 R d with entries\nz 1 = (c \u2212 1)\u03b3 + 1 cos(\u03c0/t) , z 2 = 0, z i = \u2212 d j=i+1 (c j + 1), i = 3, . . . , d \u2212 1, z d = \u22121.(37)\nThen, a simple backward induction, with base case i = d, shows that for i = 3, . . . , d, we have\nd j=i+1 c j z j = z i + 1. In the same vein, d j=3 c j z j = \u2212c + 1.(38)\nIt follows that for i = 3, . . . , d,\nz T\u00e3i = d j=i+1 z j c j \u2212 z i = 1.(39)\nMoreover, for i = 0, 1,\nz T a i = d j=3 z j \u03b3c j + z 1 cos(\u03c0/t) = \u03b3(\u2212c + 1) + z 1 cos(\u03c0/t) = \u2212\u03b3(c \u2212 1) + (c \u2212 1)\u03b3 + 1 cos(\u03c0/t) cos(\u03c0/t) = 1,(40)\nwhere the second equality is due to equation 38. Let H be the hyperplane that passes through a 0 , a 1 ,\u00e3 3 , . . . ,\u00e3 d . It follows from equation 39 and equation 40 that for any p \u2208 a 0 , a 1 ,\u00e3 3 , . . . ,\u00e3 d , we have z T p = 1. Therefore, z is orthogonal to H. Consequently,\nH = x \u2208 R d | z T x = 1 .(41)\nFor i = 2, . . . , t \u2212 1, we have\nz T a i = z T a i \u2212 a 0 + z T a 0 = z T a i \u2212 a 0 + 1 = z 1 cos(2\u03c0(i \u2212 1/2)/t) \u2212 cos(\u03c0/t) + 1 < 1,(42)\nwhere the second equality is due to equation 40, and the inequality is because z 1 > 0 and cos(2\u03c0(i \u2212 1/2)/t) \u2212 cos(\u03c0/t) < 0. It follows from equation 41 and equation 42 that all points of A\\{a 0 , a 1 } lie on one side of H, while all points of\u00c3 {a 0 , a 1 } lie on H. Then, H is a tangent hyperplane to P A . Thus, the simplex S is a facet of P A . This completes the proofs of Properties (p4) and (p5). Moreover, from the definition H b in Property (p7), we have\nH b 0 = H.(43)\nProperty (p6): Since t \u2265 4, we have 2 cos(\u03c0/2) > 1. Moreover, recall from equation 23 that \u03b3 < 1. Property (p6) then follows from equation 34 and the fact that S is a facet of P A .\nProperty (p7): For i = 0, . . . , t \u2212 1,\nz T b i = z T (b i \u2212 a 0 ) + z T a 0 = z 1 cos(2\u03c0i/t) \u2212 cos(\u03c0/t) \u2212 2 d j=3 z j \u03b3c j + 1 = z 1 cos(2\u03c0i/t) \u2212 cos(\u03c0/t) + 2\u03b3(c \u2212 1) + 1.(44)\nwhere the second equality is due to equation 40 and definitions of a 0 and b i , and the third equality is from equation 38. It follows that\nz T b 0 = z 1 1 \u2212 cos(\u03c0/t) + 2\u03b3(c \u2212 1) + 1 > 1,(45)\nwhere the inequality is because the first two terms on the left hand side of the inequality are positive.\nIn the same vein, for i = 1, . . . , t \u2212 1\nz T b i = z 1 cos(2\u03c0i/t) \u2212 cos(\u03c0/t) + 2\u03b3(c \u2212 1) + 1 < z 1 cos(2\u03c0/t) \u2212 cos(\u03c0/t) + 2\u03b3c + 1 < cos(2\u03c0/t) \u2212 cos(\u03c0/t) + 2\u03b3c + 1 \u2264 cos(2\u03c0/t) \u2212 cos(\u03c0/t) + cos(\u03c0/t) \u2212 cos(2\u03c0/t) + 1 = 1,(46)\nwhere the second inequality follows from the definition of z 1 in equation 37 and the fact that z 1 > 1, and the third inequality is from the definition of \u03b3 in equation 23 and the fact that 2\u03b3c \u2264 D PROOF OF LEMMA 2\nConsider a block representation of \u03b8 as follows\n\u03b8 = \uf8ee \uf8ef \uf8f0 \u03b8 1 . . . \u03b8 m \uf8f9 \uf8fa \uf8fb ,(54)\nwhere each \u03b8 r is a d-dimensional vector.\nIt follows from the definition of \u03b4 that for any t \u2208 (0, \u03b4), if w T r X i > 0, then (w r + \u03b8 r t) T X i > 0; and if w T r X i < 0, then (w r + \u03b8 r t) T X i < 0. Therefore, for r = 1, . . . , m and i = 1, . . . , n, and for any t \u2208 [0, \u03b4) ,\n1 (w r + \u03b8 r t) T X i \u2265 0 = 1 w T r X i > 0 + 1 w T r X i = 0, \u03b8 T r X i \u2265 0(55)\nConsequently, for any t \u2208 [0, \u03b4] and i = 1, . . . , n, we hav\u00ea\ny i w + \u03b8t, v = m r=1 v r w T r X i + t\u03b8 T r X i 1 (w r + \u03b8 r t) T x i \u2265 0 = m r=1 v r w T r X i + t\u03b8 T r X i 1 w T r X i > 0 + 1 w T r X i = 0, \u03b8 T r X i \u2265 0 .(56)\nIt follows that\u0177 i w + \u03b8t, v is a linear function of t over the interval t \u2208 [0, \u03b4]. Therefore, F \u03b8 (t) = n i=1 \u0177 i (w + \u03b8t, v) \u2212 y i 2 is a quadratic and convex function of t, for t \u2208 [0, \u03b4].", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "E PROOF OF LEMMA 3", "text": "We first characterize active neurons for different inputs. For two subsets S 1 , S 2 \u2282 R d we let S 1 \\S 2 = S 1 S c 2 . Recall the definition of s i (a) and s i (b) from Property (p6). Claim 1. For any a \u2208 A, we have\nw T a X si(a) = 0, i = 1, . . . , d \u2212 1,(57)\nw T a X a > 0,(58)\nw T a X b > 0, b \u2208 B B \\ s 1 (a), . . . , s d\u22121 (a) ,(59)\nw T a X B + > 0,(60)\nw T a X B \u2212 > 0,(61)\nw T a X A + < 0,(62)\nw T a X A \u2212 < 0,(63)\nw T a X a < 0, a \u2208 A \u00c3 \\ {a}.(64)\nSimilarly, for any b \u2208 B, we have\nw T b X si(b) = 0, i = 1, . . . , d \u2212 1,(65)\nw T b X b > 0,(66)\nw T b X a > 0, a \u2208 A \u00c3 \\ s 1 (b), . . . , s d\u22121 (b) ,(67)\nw T b X A + > 0,(68)\nw T b X A \u2212 > 0,(69)\nw T b X B + < 0,(70)\nw T b X B \u2212 < 0,(71)\nw T b X b < 0, b \u2208 B B \\{b}. (72\n)\nProof of Claim 1. Fix a b \u2208 B. We begin by introducing some notations. Let H be the (d \u2212 2)dimensional hyperplane in the (d \u2212 1)-dimensional space that passes through s 1 (b), . . . , s d\u22121 (b). In the same spirit, letH be the (d \u2212 1)-dimensional subspace in the d-dimensional space that passes through X s1(b) , . . . , X s d\u22121 (b) , equivalentlyH is the subspace orthogonal to w b . We denote the convex hull of X s1(b) , . . . , X s d\u22121 (b) by C B Conv X s1(b) , . . . , X s d\u22121 (b) . Similarly, we let\nQ A conv X a | a \u2208 A \u00c3 and Q B Conv X b | b \u2208 B B .\nBefore presenting the proofs of properties equation 57-equation 72, we review make some easy observations. Recall the definition of 0 from the paragraph proceeding equation 4. Let B d\u22121 0 be the intersection of the 0 -ball centered at 0 with the orthogonal space of u d . Then, from the definition of , we have\nB d\u22121 0 + u d \u2282 Q A , B d\u22121 0 \u2212 u d \u2282 Q B . (73\n)\nFor x \u2208 R d , let \u03c0(x) be the projection of x on the span of first d \u2212 1 coordinates, i.e., the orthogonal space of u d . Then,\n\u03c0 ( X A + ) \u03be + 1/\u03b2 \u2212 n \u2212 1 = a\u2208A \u00c3 (X a \u2212 1) \u03be + 1/\u03b2 \u2212 n \u2212 1 = a\u2208A \u00c3 a \u03be + 1/\u03b2 \u2212 n \u2212 1 < a\u2208A \u00c3 a \u03be \u2212 n < a\u2208A \u00c3 a a\u2208A \u00c3 a / 0 = 0 ,(74)\nwhere the first equality is from the definition of X A + and the second inequality follows from the definition of \u03be in equation 4. Therefore, \u03c0(X\nA + )/ \u03be + 1/\u03b2 \u2212 n \u2212 1 \u2208 B d\u22121 0 . Consequently, 1 \u03be + 1/\u03b2 \u2212 n \u2212 1 X A + = \u03c0(X A + ) \u03be + 1/\u03b2 \u2212 n \u2212 1 + u T d X A + u d \u03be + 1/\u03b2 \u2212 n \u2212 1 = \u03c0(X A + ) \u03be + 1/\u03b2 \u2212 n \u2212 1 + \u03be + 1/\u03b2 \u2212 n \u2212 1 u d \u03be + 1/\u03b2 \u2212 n \u2212 1 = \u03c0(X A + ) \u03be + 1/\u03b2 \u2212 n \u2212 1 + u d \u2208 B d\u22121 0 + u d \u2208 Q A ,(75)\nwhere the first equality is orthogonal decomposition of X A + , and last inclusion follows from equation 73. In the same vein, we can show that , then all points in Q A lie on a same side ofH. In other words, either we have w T b x \u2265 0, for all x \u2208 Q A ; or we have w T b x \u2264 0, for all x \u2208 Q A . In view of Property (p3), u d \u2208 int(Q A ). It then follows from equation 10 that for any x \u2208 Q A , we have w T b x \u2265 0. Consequently, for any x \u2208 Q A \\H, we have w T b x > 0. In particular,\n\u2212 1 \u03be + n + 1 \u2212 1/\u03b2 X B + \u2208 Q A . (76\nw T b X a > 0, a \u2208 A \u00c3 \\ s 1 (b), . . . , s d\u22121 (b) , w T b X A + > 0, \u2212w T b X B + > 0,\nwhere the first inequality is because Property (p5) implies that X a \u2208H for a \u2208 A \u00c3 \\ s 1 (b), . . . , s d\u22121 (b) , and the last two inequalities are due to equation 75 and equation 76, respectively. This establishes equation 67, equation 68, and equation 70. In light of Claim 1, it is easy to see for r = 1, . . . , m and i = 1, . . . , n that if w T r X i = 0, then\nv r e i = 1 \u221a m .(77)\nIn our next claim, we examine a linear combination of data points for which a particular neuron is active.\nClaim 2. For r = 1, . . . , m, there exist constants \u03b3 r 1 , . . . , \u03b3 r m such that\nn i=1 \u03b3 r i e i X i 1 w T r X i = 0 + n i=1 e i X i 1 w T r X i > 0 = 0. (78\n)\nProof of Claim 2. Fix a b \u2208 B. We prove the claim for the neuron associated to b. It follows from Claim 1 that\nn i=1 e i X i 1 w T b X i > 0 = e b X b + a\u2208(A \u00c3 )\\ s1(b),...,s d\u22121 (b) e a X a + e A + X A + + e A \u2212 X A \u2212 = \u2212X b + a\u2208(A \u00c3 )\\ s1(b),...,s d\u22121 (b) X a + X A + \u2212 X A \u2212 = \u2212X b + a\u2208(A \u00c3 )\\ s1(b),...,s d\u22121 (b) X a \u2212 \uf8eb \uf8ed a\u2208A \u00c3 X a \u2212 (1/\u03b2 \u2212 1) u d \uf8f6 \uf8f8 = \u2212X b \u2212 d\u22121 i=1 X si(b) + (1/\u03b2 \u2212 1) u d ,(79)\nwhere the second equality is due to the definitions of e a , e b , e A + , and e A \u2212 , and the third equality is from the definitions of X A + in equation 5.\nOn the other hand, it follows from Property (p6) that there exist scalars \u03b1 1 , . . . , \u03b1 \nd \u2208 (0, 1) such that b = d\u22121 i=1 \u03b1 i s i (b). Therefore, from the definition of X b , \u2212 X b + u d = d\u22121 i=1 \u03b1 i X si(b) \u2212 u d (80) Moreover, Property (p4) implies that d\u22121 i=1 \u03b1 i = 1/\u03b2. Then, from equation 80, \u2212X b = d\u22121 i=1 \u03b1 i X si(b) \u2212 d\u22121 i=1 \u03b1 i \u2212 1 u d = d\u22121 i=1 \u03b1 i X si(b) \u2212 (1/\u03b2 \u2212 1) u d . (81\n) For i = 1, . . . , d \u2212 1, let \u03b3 i = 1 \u2212 \u03b1 i . Then, Claim 1 implies that n i=1 \u03b3 i e i X i 1 w T b X i = 0 = d\u22121 i=1 \u03b3 i e i X si(b) = d\u22121 i=1 (1 \u2212 \u03b1 i )X si(b) = d\u22121 i=1 X si(b) \u2212 d\u22121 i=1 \u03b1 i X si(b) = d\u22121 i=1 X si(b) + X b \u2212 1 \u03b2 \u2212 1 u d ,(82)\nr i , (1 \u2212 \u03b3 r i ) ,(83)\nfor the constant \u03b3 r i defined in Claim 2. It follows that r > 0, for r = 1, . . . , m. For any r \u2264 m, X s1(r) , . . . , X s d\u22121 (r) are linearly independent and, by definition, are all orthogonal to w r . Therefore, there exists a constant r > 0 such that for any\n\u03b6 r \u2208 R d with \u03b6 r = 1, we have max i=1,...,d\u22121 |\u03b6 T r X si(r) | \u2265 r \u03b6 \u22a5 r , where \u03b6 \u22a5 r is the projection of \u03b6 r on the null-space of w r . Consequently, for any \u03b6 r \u2208 R d with \u03b6 r = 1, max i=1,...,n |\u03b6 T r X i | \u00d7 1 w T r X i = 0 \u2265 r \u03b6 \u22a5 r .\nIn particular, considering the block-vector representation of \u03b8 in equation 54, we obtain for r = 1, . . . , m, max i=1,...,n\n|\u03b8 T r X i | \u00d7 1 w T r X i = 0 \u2265 max i=1,...,n r \u03b8 \u22a5 r .(84)\nLet \u00b5 min r r r / \u221a m. Then, \u00b5 > 0. It then follows from Claim 2 that, for r = 1, . . . , m,\nv r n i=1 e i \u03b8 T X i 1 w T r X i = 0, \u03b8 T r X i \u2265 0 + v r n i=1 e i \u03b8 T r X i 1 w T r X i > 0 = v r n i=1 e i \u03b8 T r X i 1 w T r X i = 0, \u03b8 T r X i \u2265 0 \u2212 v r n i=1 \u03b3 r i e i \u03b8 T r X i 1 w T r X i = 0 = v r n i=1 e i \u03b8 T r X i 1(\u03b8 T r X i \u2265 0) \u2212 \u03b3 r i 1 w T r X i = 0 = 1 \u221a m n i=1 \u03b8 T r X i 1(\u03b8 T r X i \u2265 0) \u2212 \u03b3 r i 1 w T r X i = 0 = 1 \u221a m n i=1 \u03b8 T r X i \u00d7 1(\u03b8 T r X i > 0) \u2212 \u03b3 r i \u00d7 1 w T r X i = 0 \u2265 1 \u221a m r n i=1 \u03b8 T r X i 1 w T r X i = 0 \u2265 r \u221a m max i=1,...,n \u03b8 T r X i 1 w T r X i = 0 \u2265 r \u221a m r \u03b8 \u22a5 r \u2265 \u00b5 \u03b8 \u22a5 r ,(85)\nwhere the third equality is due to equation 77, the fourth equality is because \u03b8 T r X i and 1(\u03b8 T r X i > 0) \u2212 \u03b3 r i have always the same sign, the first inequality is by definition of r in equation 83, the third inequality follows from equation 84, and the last inequality is from the definition of \u00b5.\nOn the other hand, equation 2 implies that dF \u03b8 (t)\nd + t t=0 = lim t\u21930 m r=1 v r \u03b8 T r n i=1 e i X i 1 (w r + \u03b8 r t) T X i \u2265 0 = m r=1 v r n i=1 \u03b8 T r e i X i 1 w T r X i = 0, \u03b8 T r X i \u2265 0 + 1 w T r X i > 0 = m r=1 v r n i=1 e i \u03b8 T X i 1 w T r X i = 0, \u03b8 T r X i \u2265 0 + v r n i=1 e i \u03b8 T r X i 1 w T r X i > 0 \u2265 m r=1 \u00b5 \u03b8 \u22a5 r \u2265 \u00b5 \u03b8 \u22a5 (86)\nwhere the second equality is due to equation 55 and the first inequality follows from equation 85. This completes the proof of Lemma 3.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "F PROOF OF LEMMA 4", "text": "We begin by a claim. Given a q \u2208 A B, recall the definition of hyperplane H q from Property (p7).\nClaim 3. For any pair of points p, q \u2208 A B, we have\nw T q X p = 1 1 + d(0, H q ) 2 d p, H q . (87\n)\nwhere the third equality is due to equation 19. On the other hand, since \u03b8 = \u03b8 + \u03b8 \u22a5 , we have\nG\u03b8 = G\u03b8 + G\u03b8 \u22a5 .(96)\nIn the following claim, we elaborate on G\u03b8 . Claim 4. There exists a constant \u03b7 1 > 0 such that G\u03b8 2 \u2265 2\u03b7 1 \u03b8 2 , for all \u03b8 \u2208 R d .\nProof of Claim 4. Recall that \u03b8 is the projection of \u03b8 on subspace H w defined in equation 14. Then, there exist constants \u03b1 1 . . . , \u03b1 m such that\n\u03b8 = \uf8ee \uf8ef \uf8f0 \u03b1 1 w 1 . . . \u03b1 m w m \uf8f9 \uf8fa \uf8fb . (97\n)\nLet \u03b1 be the vector representation of \u03b1 1 , . . . , \u03b1 m . Then,\n\u03b8 2 = m i=1 \u03b1 2 i w i 2 = m i=1 \u03b1 2 i = \u03b1 2 . (98\n)\nConsider the n \u00d7 m matrixG = v 1 J 1 X T w 1 \u2022 \u2022 \u2022 v m J m X T w m .(99)\nThen, from the definition of matrix G in equation 18,\nG\u03b8 = v 1 J 1 X T \u2022 \u2022 \u2022 v m J m X T \uf8ee \uf8ef \uf8f0 \u03b1 1 w 1 . . . \u03b1 m w m \uf8f9 \uf8fa \uf8fb = v 1 J 1 X T w 1 \u2022 \u2022 \u2022 v m J m X T w m \u03b1 =G\u03b1.(100)\nEach column ofG corresponds to a neuron, and thereby is associated to a point in A B. In the same vein, every row ofG is associated to an input vector. By removing some rows ofG, we devise a matrixM so that each row ofM is associated to an input X p for p \u2208 A B. Therefore,M is an m \u00d7 m matrix, whose rows and columns are associated to the points in A B. It follows thatM \u03b1 is a vector obtained by removing some entries from vectorG\u03b1. As a result, M \u03b1 \u2264 G \u03b1 .\nIn the following, we capitalize on Property (p8) to show thatM is full-rank.\nFor q \u2208 A B, let\n\u03b3 q v q 1 + d(0, H q )\n.\nFor p, q \u2208 A B, the entry in row p and column q ofM equals\nM pq =G pq = v q X T p w q 1 w T q X p > 0 + 1 w T q X p = 0, \u03b8 T q X p \u2265 0 = v q X T p w q 1 w T q X p > 0 = v q 1 + d(0, H q ) d p, H q 1 w T q X p > 0 = \u03b3 q d p, H q 1 w T q X p > 0 . (102\n)\nwhere the first equality is from the definition ofM , the second equality follows from the definitions ofG and J q in equation 99 and equation 93, the third equality is because X T p w q 1 w T q X p = 0 = 0, and the fourth equality is due to Claim 3. Then, Claim 1 implies that for any p, q \u2208 A B, M pq = \u03b3 q d p, H q , if p = q, OR p \u2208 A and q \u2208 B, OR p \u2208 B and q \u2208 A, 0, otherwise.\nCompared to matrix M defined in equation 3, each column q ofM equals the column q of M multiplied by a non-zero constant \u03b3 q . In view of Property (p8), M is full-rank. It follows thatM is full-rank, as well.\nLet \u03c3 be the smallest singular value ofM . SinceM is full-rank, we have \u03c3 > 0. Moreover, M \u03b1 \u2265 \u03c3 \u03b1 .\nThen, G\u03b8 2 = G \u03b1 2 \u2265 M \u03b1 2 \u2265 \u03c3 2 \u03b1 2 = \u03c3 2 \u03b8 2 , (105) where the equations are due to equation 100, equation 101, equation 104, and equation 98, respectively. Claim 4 then follows for \u03b7 1 = \u03c3 2 /2.\nBack to the proof of Lemma 4, we denote by \u03c3 max the largest singular value of G. Let \u03b7 2 \u03c3 2 max . Then,\nG\u03b8 2 = G\u03b8 + G\u03b8 \u22a5 2 = G\u03b8 2 + G\u03b8 \u22a5 2 + 2 G\u03b8 T G\u03b8 \u22a5 \u2265 G\u03b8 2 \u2212 2 G\u03b8 \u00d7 G\u03b8 \u22a5 \u2265 G\u03b8 2 \u2212 2\u03c3 max \u03b8 \u00d7 \u03c3 max \u03b8 \u22a5 \u2265 G\u03b8 2 \u2212 2\u03c3 2 max \u03b8 \u22a5 = G\u03b8 2 \u2212 2\u03b7 2 \u03b8 \u22a5 \u2265 2\u03b7 1 \u03b8 2 \u2212 2\u03b7 2 \u03b8 \u22a5 ,(106)\nwhere the second inequality is from the definition of \u03c3 max , the third inequality is because \u03b8 \u2264 \u03b8 = 1, the last equality is by the definition of \u03b7 2 , and the last inequality follows from Claim 4. This completes the proof of Lemma 4.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "G PROOF OF LEMMA 5", "text": "Fix a \u03b8 \u2208 R md with \u03b8 = 1. Recall the definition of x 0 from the paragraph proceeding equation 17. If \u03b8 \u22a5 \u2265 x 0 , then for any t \u2208 [0, \u03b4],\nF \u03b8 (t) \u2212 F \u03b8 (0) \u2265 F \u03b8 (0)t \u2265 \u00b5 \u03b8 \u22a5 t \u2265 \u00b5x 0 t = \u03b4t \u2265 t 2 ,\nwhere the first inequality is from convexity of F \u03b8 in Lemma 2, the second inequality is due to Lemma 3, the equality is by the definition of in equation 17, and the last inequality is because t \u2264 \u03b4.\nOn the other hand, if \u03b8 \u22a5 < x 0 , then for any t \u2208 [0, \u03b4], F \u03b8 (t) \u2212 F \u03b8 (0) = F \u03b8 (0)t + 1 2\nF \u03b8 t 2 \u2265 1 2 F \u03b8 t 2 \u2265 \u03b7 1 \u03b8 2 \u2212 \u03b7 2 \u03b8 \u22a5 t 2 = \u03b7 1 1 \u2212 \u03b8 \u22a5 2 \u2212 \u03b7 2 \u03b8 \u22a5 t 2 < \u03b7 1 (1 \u2212 x 2 0 ) \u2212 \u03b7 2 x 0 t 2 = \u00b5x 0 \u03b4 t 2 = t 2 ,\nwhere the first equality is because F \u03b8 is quadratic (c.f. Lemma 2), the first inequality follows from Lemma 3, the second inequality is due to Lemma 4, the third inequality is because \u03b8 \u22a5 < x 0 , and the last two equalities are due to p(x 0 ) = 0 and the definition of in equation 17, respectively. Combining the above two cases, we obtain Lemma 5.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "", "text": "cos(\u03c0/t) \u2212 cos(2\u03c0/t). It then follows from equation 41, equation 45, and equation 46 that b 0 and b i lie on opposite sides of H, for i = 1, . . . , t \u2212 1.\nOn the other hand, sinceb i = \u2212\u00e3 i , for i = 3, . . . , d, it follows from equation 39 that z Tbi = \u2212z T\u00e3i = \u22121 < 1. Therefore, for i = 1, . . . , 3,b i and b 0 lie on opposite sides of H. This completes the proof of Property (p7).\nProperty (p8): We have\nwhere the first equality is due to equation 43 and equation 41, and the second equality follows from the equality in equation 45. Similarly, from equation 42, we have for i = 0, . . . , t \u2212 1\nFor i, j \u2208 {0, . . . , t\u22121}, letm a i ,b j cos 2\u03c0(i\u2212j \u22121/2)/t \u2212 cos(\u03c0/t) andm b i ,a j cos 2\u03c0(i\u2212 j + 1/2)/t \u2212 cos(\u03c0/t). Then, it follows from equation 48 and rotational symmetry of A and B in the first two coordinates that for i, j \u2208 {0, . . . , t \u2212 1},\nNote that for any p, q \u2208 A B,m p,q is a constant independent of the value of \u03b3. LetM be a 2t \u00d7 2t matrix, with entrie\u015d\nfor p, q \u2208 A B. Then, all entries ofM are constants independent of \u03b3. Let\u03bb 1 , . . . ,\u03bb 2t be the eigenvalues ofm. It follows that\u03bb 1 , . . . ,\u03bb 2t are also constants independent of \u03b3.\nConsider the matrix M defined in equation 3. It follows from equation 47 and equation 49 that for any p, q \u2208 A B,\nConsider the order a 0 , . . . , a t\u22121 , b 0 , . . . , b t\u22121 on the elements of A B. Then,\nwhere I is the 2t\u00d72t identity matrix and the second equality is from the definition of z 1 in equation 37.\nDenote the eigenvalues of M by \u03bb 1 , . . . , \u03bb 2t . Then, from elementary linear algebra,\nfor i = 1, . . . , 2t. Therefore, there is at most one value of \u03b3 for which \u03bb i = 0. Then, in view of equation 23, we have Pr(\u03bb i = 0) = 0, over the random choice of \u03b3. Thus, with probability one, M has no zero eigenvalues and is thereby full-rank. As an immediate consequence, M is full-rank for suitable choice of \u03b3. This establishes Property (p8) and completes the proof of Proposition 1.\nProof of Claim 3. In the (d \u2212 1)-dimensional space, let \u03c9 be the unit normal vector of H q . Recall from Property (p6) that s 1 (q), . . . , s d\u22121 (q) are located on H q . Let,\nWithout loss of generality suppose that q \u2208 A. Let\u03c9 be the lifting of \u03c9 from the (d \u2212 1)-dimensional space to the d-dimensional space by appending \u03c9 by a new coordinate with zero coefficient, i.e.,\u03c9 is a d-dimensional vector with\u03c9\nThen, we have z = 1 and z T u\nwhere the second equality is because s i (q) \u2208 B B for q \u2208 A. It follows from equation 91 and the definition of w q in equation 7 and equation 8 that w q = z. Therefore,\nwhere the second equality is from the definition of z in equation 90, and the last equality is due to equation 88 and equation 89. This completes the proof of Claim 3.\nWe now proceed to the proof of Lemma 4. Fix an arbitrary \u03b8 \u2208 R d with \u03b8 = 1. Without loss of generality 4 assume that F (\u2022, \u2022) is differentiable at w + \u03b4\u03b8/2, v . For r = 1, . . . , m, let J r be a diagonal matrix whose (i, i) entry, for i = 1, . . . , n, equals 1 (w r + \u03b8 r t)\nwhere the equality is due to equation 55. Recall the definition of matrix G in equation 18:\nThen, for any t \u2208 (0, \u03b4),", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "A convergence theory for deep learning via overparameterization", "journal": "", "year": "2018", "authors": "Zeyuan Allen-Zhu; Yuanzhi Li; Zhao Song"}, {"title": "Open problem: The landscape of the loss surfaces of multilayer networks", "journal": "", "year": "2015", "authors": "Anna Choromanska; Yann Lecun; G\u00e9rard Ben Arous"}, {"title": "On the power of over-parametrization in neural networks with quadratic activation", "journal": "", "year": "2018", "authors": "S Simon; Jason D Du;  Lee"}, {"title": "Gradient descent provably optimizes over-parameterized neural networks", "journal": "", "year": "2018", "authors": "Xiyu Simon S Du; Barnabas Zhai; Aarti Poczos;  Singh"}, {"title": "Topology and geometry of half-rectified network optimization", "journal": "", "year": "2016", "authors": "Daniel Freeman; Joan Bruna"}, {"title": "Escaping from saddle points-online stochastic gradient for tensor decomposition", "journal": "", "year": "2015", "authors": "Rong Ge; Furong Huang; Chi Jin; Yang Yuan"}, {"title": "How to escape saddle points efficiently", "journal": "", "year": "2017", "authors": "Chi Jin; Rong Ge; Praneeth Netrapalli; M Sham; Michael I Jordan Kakade"}, {"title": "Deep learning without poor local minima", "journal": "", "year": "2016", "authors": "Kenji Kawaguchi"}, {"title": "Gradient descent only converges to minimizers", "journal": "", "year": "2016", "authors": "Max Jason D Lee;  Simchowitz; Benjamin Michael I Jordan;  Recht"}, {"title": "Over-parameterized deep neural networks have no strict local minima for any continuous activations", "journal": "", "year": "2018", "authors": "Dawei Li; Tian Ding; Ruoyu Sun"}, {"title": "The loss surface of deep and wide neural networks", "journal": "", "year": "2017", "authors": "Quynh Nguyen; Matthias Hein"}, {"title": "On the quality of the initial basin in overspecified neural networks", "journal": "", "year": "2016", "authors": "Itay Safran; Ohad Shamir"}, {"title": "Spurious local minima are common in two-layer relu neural networks", "journal": "", "year": "2017", "authors": "Itay Safran; Ohad Shamir"}, {"title": "Theoretical insights into the optimization landscape of over-parameterized shallow neural networks", "journal": "IEEE Transactions on Information Theory", "year": "2019", "authors": "Mahdi Soltanolkotabi; Adel Javanmard; Jason D Lee"}, {"title": "No bad local minima: Data independent training error guarantees for multilayer neural networks", "journal": "", "year": "2016", "authors": "Daniel Soudry; Yair Carmon"}, {"title": "Exponentially vanishing sub-optimal local minima in multilayer neural networks", "journal": "", "year": "2017", "authors": "Daniel Soudry; Elad Hoffer"}, {"title": "Spurious valleys in two-layer neural network optimization landscapes", "journal": "", "year": "2018", "authors": "Luca Venturi; S Afonso; Joan Bandeira;  Bruna"}, {"title": "Diverse neural network learns true target functions", "journal": "", "year": "2016", "authors": "Bo Xie; Yingyu Liang; Le Song"}], "figures": [{"figure_label": "", "figure_type": "", "figure_id": "fig_0", "figure_caption": "(B) For any b \u2208 B, \u03b2b lies on a facet of P A . We denote this facet by S A (b). (p5) (A) For any a \u2208 A, S B (a) is a (d \u2212 1)-dimensional simplex. (B) For any b \u2208 B, S A (b) is a (d \u2212 1)-dimensional simplex. (p6) (A) For any a \u2208 A, there exist scalars \u03b1 1 , . . . , \u03b1 d \u2208 (0, 1) such that a = d i=1 \u03b1 i s i (a), where s 1 (a), . . . , s d (a) are the vertices of simplex S B (a). (B) For any b \u2208 B, there exist scalars \u03b1 1 , . . . , \u03b1 d \u2208 (0, 1) such that b = d i=1 \u03b1 i s i (b), where s 1 (b), . . . , s d (b) are the vertices of simplex S A (b). (p7) (A) For any pair a and a of distinct points in A, we have S B (a) = S B (a ). Moreover, letting H a be the hyperplane that contains S B (a), a and a lie on opposite sides of H a , for all a \u2208 A \u00c3 with a = a. (B) For any pair b and b of distinct points in B, we have S A (b) = S A (b ). Moreover, letting H b be the hyperplane that contains S A (b), b and b lie on opposite sides of H b , for all b \u2208 B B with b = b.", "figure_data": ""}, {"figure_label": "2", "figure_type": "", "figure_id": "fig_1", "figure_caption": "Figure 2 :2Figure 2: A (2, t, 0)-configuration with t = 8. The blue dots show the points in A and red crosses are the points in B.", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: Illustration of data points X 1 , . . . , X n for d = 3 and m = 12.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_3", "figure_caption": "consider the facet S B (a) defined in Property (p4), and let s 1 (a), . . . , s d\u22121 (a) be the vertices of S B (a) (as in Property (", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": ")\u2022For each b \u2208 B, consider the facet S A (b) and let s 1 (b), . . . , s d\u22121 (b) be the vertices of S A (b).", "figure_data": ""}, {"figure_label": "4", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure 4 :4Figure 4: Different types of cupped minima in terms of differentiability, discussed in Appendix A.", "figure_data": ""}, {"figure_label": "5", "figure_type": "", "figure_id": "fig_6", "figure_caption": "Figure 5 :5Figure 5: An illustration of the points in equation 24-equation 27 for d = 3 and t = 4. The red crosses indicate the points in A \u00c3 and the blue dots correspond to the points in B B .", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_7", "figure_caption": "Properties (p4)-(p7): We only prove Part (B) for each of these properties; as similar proofs work also for Part (A)'s. Moreover, because of the rotational symmetry of A and B in the first two coordinates, it suffices to prove of the Properties (p4)-(p7) only for b 0 .", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_8", "figure_caption": ")We proceed to verify equation 65-equation 72. Eq. equation 65 follows from equation 9. Recall the definitions X A \u2212 = \u03beu d and X B \u2212 = \u2212\u03beu d . Then, equation 10 implies equation 69 and equation 71. Since X s1(b) , . . . , X s d\u22121 (b) define a boundary of the (d \u2212 1)-dimensional convex set Q A , andH passes through X s1(b) , . . . , X s d\u22121 (b)", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_9", "figure_caption": "Forequation 66, it follows from Property (p4) that b and the origin, 0, lie on opposite sides of hyperplane H. Consequently, \u2212X b and u d also lie on opposite sides of hyperplaneH. Therefore, w T b X b and w T b u d have a same sing. It then follows from equation 10 that w T b X b > 0, and equation 66 follows.For equation 72, it follows from Property (p7) that for any b \u2208 B B with b = b, X b and X b lie on opposite sides ofH. Eq. equation 66 then implies that w T b X b < 0. This establishes equation 72, and completes the proof of Claim 1.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_10", "figure_caption": "where last equality in due to equation 81. Combing equation 79 and equation 82, we obtain equation 78 for w r = w b . A similar argument implies equation 78 for w r = w a , a \u2208 A. This completes the proof of Claim 2. Back to the proof of Lemma 3, for r = 1, . . . , m, let r min i=1,...,d\u22121 min \u03b3", "figure_data": ""}], "formulas": [], "doi": ""}