{"title": "Model-Free Model Reconciliation", "authors": "Sarath Sreedharan; Alberto Olmo Hernandez; Aditya Prasad Mishra; Subbarao Kambhampati", "pub_date": "", "abstract": "Designing agents capable of explaining complex sequential decisions remains a significant open problem in human-AI interaction. Recently, there has been a lot of interest in developing approaches for generating such explanations for various decision-making paradigms. One such approach has been the idea of explanation as modelreconciliation. The framework hypothesizes that one of the common reasons for a user's confusion could be the mismatch between the user's model of the agent's task model and the model used by the agent to generate the decisions. While this is a general framework, most works that have been explicitly built on this explanatory philosophy have focused on classical planning settings where the model of user's knowledge is available in a declarative form. Our goal in this paper is to adapt the model reconciliation approach to a more general planning paradigm and discuss how such methods could be used when user models are no longer explicitly available. Specifically, we present a simple and easy to learn labeling model that can help an explainer decide what information could help achieve model reconciliation between the user and the agent with in the context of planning with MDPs.", "sections": [{"heading": "Introduction", "text": "A significant barrier to integrating AI systems into our daily lives has been their inability to interact and work with us humans in an intuitive and explicable manner. Orchestrating such interactions would require the agents to have the ability to help users in the loop better understand the rationale behind their various actions. Thankfully there has been a lot of effort within the AI research community to develop systems capable of holding explanatory dialogues with users and thus help them understand the decisions under question [David W. Aha and Magazzeni, 2018;Daniele Magazzeni, 2018]. Such explanatory systems could help users resolve confusions regarding agent decisions that may stem from either a (1) lack of understanding (or even misunderstanding) of the task or (2) from their inferential limitations. While many earlier works Figure 1: A general overview of the explanation as model reconciliation.\nin explanation have generally focused on the latter (cf. [Khan et al., 2009;Hayes and Shah, 2017;Seegebarth et al., 2012;Topin and Veloso, 2019]), there is a growing consensus on the importance of explanatory mechanisms that can help bridge the knowledge asymmetry between the system and the user.\nIn particular, in explanation as model-reconciliation [Chakraborti et al., 2017] we had studied the problem of reconciling knowledge asymmetries between the user and the agent within the context of planning. Works in this direction have generally looked at cases where the user's model of the task (i.e their belief about the initial state, the transition dynamics, and the goal) is known beforehand (in a representation scheme comparable to the one used by the agent) and do not match the agent's model. This mismatch means that the user would not be able to correctly evaluate the validity or the optimality of a given plan. Thus in this paradigm the explanations consist of information about the agent's model that the user could incorporate into his/her own model to correctly evaluate the plan in question.\nUnfortunately, it is not always possible to have access to such models. In the most general case, we are dealing with the user's model of the agent and hence the user may not be capable of presenting traces or decisions that could be generated from this model. Even if the system tries to learn such a representation based on interactions with the user, there is no guarantee that the specific representational assumptions of the learned model and the vocabulary used would be satisfied by the user's mental model.\nThe definition of explanation as model reconciliation may leave one with the idea that there is no way around it. How could one ever truly perform effective reconciliation when there exists no user model guiding us to choose the parts of the model, which when revealed to the user will help them correctly evaluate the current decision? Are we left with revealing the entire agent model to the user as the only option? In this paper, we propose a simple and intuitive way we could still generate minimal explanations in the absence of declarative models. We argue that we could drive such explanations by using learned models that can predict how human expectations could be affected by possible explanations (derived completely from information about the agent model) and in fact show how this method could be viewed as a variation of previous approaches that have been put forth to identify explicable behavior.\nWe will start by extending model reconciliation to the more general setting of planning with Markov Decision Processes (Section 4). The rest of the paper will investigate how these ideas could be used when the human mental model of the task is unavailable, and will formulate a learning problem that allows us to learn simple models that could be used to identify minimal explanations (Section 5). Finally, we will evaluate our method on a set of standard MDP benchmarks and perform user studies to validate its viability (Section 6).", "n_publication_ref": 7, "n_figure_ref": 1}, {"heading": "Background", "text": "Figure 1 presents a general schematic representation for explanation as model reconciliation. The automated agent (henceforth referred to as robot) starts with a model M R that can be used to generate a decision \u03c0 (where depending on the context, \u03c0 may be a single action, plan, policy or a program). In this setting, M R h corresponds to the human's preconceived notions about the robot model. The explainer's job then becomes providing information about the model M R , such that the updated human model can correctly evaluate the validity of the robot decisions.\nIn this case, the robot could have chosen to provide the entire model, but for most realistic tasks, such models could be quite large, so dumping the entire model could be both unnecessary and impractical. It's also well known that people generally prefer explanations that are selective [Miller, 2018;Lombrozo, 2006]. Thus the users would be happier with explanations that asks them to update a subset of beliefs as opposed to a complete update. Note that M R need not be the original agent model, but rather some abstraction/approximation of the underlying robot model (that conserves some desired property of the decision like optimality or validity). In [Chakraborti et al., 2017] where model reconciliation was first introduced, M R was a classical planning model hence inherently interpretable and thus the reconciliation could easily be achieved, but the idea could be applied beyond just planning models. For example, one could understand the explanation methodology used by LIME [Ribeiro et al., 2016] as being a special case of model reconciliation. In their case, they assume the human model is empty and M R is an approximation of the underlying decision model is automatically generated for each decision using a set of prede-fined features.\nIn this work, we will be looking at the agents that use discounted infinite horizon Markov Decision Processes (or MDPs) [Russell and Norvig, 2003] as the decision making framework. Each MDP M is defined by a tuple S, A, T, R, \u03b3, \u00b5 , where the S provides the set of possible atomic states, A defines the set of actions, T is the transition function, R the reward, \u03b3 the discounting factor (where 0 \u2264 \u03b3 < 1) and \u00b5 corresponds to the distribution of possible initial states. T : S \u00d7 A \u00d7 S \u2192 [0, 1] provides the probability that for given state s \u2208 S, the execution of an action a would induce a transition to a new state s , and R : S \u00d7 A \u00d7 S \u2192 R defines the reward corresponding to this transition. The solution concept in MDP takes the form of a policy \u03c0 that maps each state to a potential action. A policy is said to be optimal for M (denoted as \u03c0 * M ) if there exists no other policy that dominates the given policy in terms of the expected value of states (where the value of a state s under a policy \u03c0 for a model M is denoted as V \u03c0 M (s)). Executing the policy in a state results in a sequence of state action state tuples called execution trajectory or simply a trajectory, denoted as \u03c4 = (s 1 , a 1 , s 2 ), ..., (s n\u22121 , a n\u22121 , s n ) and we will use P M (\u03c4 |\u03c0) to denote the probability of sampling the given trajectory \u03c4 for a policy \u03c0 in model M.\nIn the explanatory setting we are interested in, the robot uses a model M R = S, A, T R , R R , \u03b3 R of the task to come up with the policy to act on. For now we will assume this MDP already defines an interpretable model and the human uses a model M R h = S, A, T R h , R R h , \u03b3 R h to evaluate it (we will relax this assumption in later sections). Now the task ahead of us will be to formulate how we could still identify minimal information that could resolve user confusion when M R h , but before we can do that we need to reinterpret the ideas of inexplicability and the idea of model reconciliation that was defined in [Chakraborti et al., 2017] in the context of MDPs and we will start by considering a simple scenario.", "n_publication_ref": 6, "n_figure_ref": 1}, {"heading": "Illustrative Example", "text": "Consider a warehouse scenario, where a robot is tasked with moving packages from racks and dropping them off at the dispatch chute. The robot is powered by a battery pack that can be recharged by visiting its docking station. The docking station also doubles as a quality assurance station that the robot needs to visit whenever it picks up a box labeled #013 (which means the box is fragile). The robot's operations are mostly deterministic, apart from a small probability of slipping (0.25) in some cells, that could leave the robot in the same position. Now suppose the warehouse has just hired a new part-time employee to oversee the operations. The employee is just getting used to this new setting and is puzzled by the robot's decision to once in a while take a detour from the drop-off activity and visit a specific position of the factory floor (which is, in fact, the docking location). If we wished the robot to be explainable, then it would need to be capable of helping the employee better understand the underlying model used by the robot (i.e achieve some form of model reconciliation). Given the fact that the robot may not have an exact model of the user, one way to achieve this could be by providing robot's entire model to the user. Unfortunately, this could easily overwhelm the user.\nAnother possibility could be to allow the user to specify which robot actions appear inexplicable, and focus on providing facts relevant to those actions. This explanation may still prove to be quite verbose and may in fact not help resolve their confusion. For example, imagine a case where the robot is visiting the station to recharge its batteries and the human says that the visit action is inexplicable. Now even if the robot mentions that visiting the station recharges it, the employee may still be confused if they are under the incorrect assumption that the robot is operating on full battery. Similarly, if the human had expected the robot to go to the docking station due to some confusion regarding the box codes, the human may mark the robot decision to not go to the dropoff as being inexplicable and the explanations that could resolve the confusion may have little to do with that specific action marked as inexplicable.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Explanation as Model Reconciliation For MDPs", "text": "In this setting the human and robot models are captured as MDPs defined over the same set of states and thus we can characterize both models by the tuple \u03b8 = \u03b8 T , \u03b8 R , \u03b8 \u03b3 , \u03b8 \u00b5 , where the \u03b8 T provides the set of parameters that defines the transition probabilities P (.|s, a), while \u03b8 R the parameters corresponding to the reward function, \u03b8 \u03b3 the parameters corresponding to the discount factor and \u03b8 \u00b5 the parameters for the initial state distribution. For simple MDP models with atomic states, \u03b8 T contains parameters of the categorical distribution for each transition (\u03b8 \u00b5 will contain similar parameters for the initial state distribution), \u03b8 R contains the reward associated with each transition (an s, a, s tuple) and \u03b8 \u03b3 just contains the value of the discount factor. The specific instantiations of the parameters for each model M is captured as \u03b8(M). For simplicity, we will denote each of the unique parameters in the tuple \u03b8 using indexes. For example, \u03b8 s,a T (M R ), will correspond to the parameters for the distribution P (.|s, a) for the model M R .\nIf we use M to capture the set of all possible models and \u0398 = \u03b8 T \u00d7 \u03b8 R \u00d7 \u03b8 \u03b3 \u00d7 \u03b8 \u00b5 , then model reconciliation operation can be captured as a function E M R h ,M R : 2 \u0398 \u2192 M that takes in a set of model parameters and generates a new version of the model M R h where the set of specified parameters will be set to values from M R . For example,\nM = E M R h ,M R (\u03b8 s1,a T )\nwill be a new model such that \u03b8(M) will be identical to \u03b8(M R h ), except that \u03b8 s1,a TM , will be equal to \u03b8 s1,a T R .\nPractically, the model reconciliation operation corresponds to the robot informing the human about some part of its model. This communication could incur cost and we can capture this by using the cost function C : 2 \u0398 \u2192 R that maps a given set of a threshold to a cost. Now the question we need to ask is whether the agent is trying to explain its policy or if it is trying to explain some behavior (i.e an execution trace). Most of the earlier work that looks at model reconciliation explanation (cf. [Chakraborti et al., 2017;Sreedharan et al., 2018a;2018b]) has looked at sequential plans and has generally ignored this differentiation and treated the problem of explaining plans to be same as that of explaining behavior. In general, a given plan or policy compactly represents a set of possible behaviors and the choice of explaining behavior vs explaining the plans/policies could affect the content of the explanation being given. For example, when explaining policies there is the additional challenge of presenting the entire policy to the user and the explainer may need to justify action choices for extremely unlikely states or contingencies. On the other hand, when explaining a given set of behaviors the explainer needs to only justify their action choices for cases they actually witnessed. For example, when explaining traces from the warehouse scenario, given the small probability of slipping, the robot may never have to mention what to do when it slips, but on the other hand if we are dealing with full policies, the agent may need to talk about the states where the robot is in the slipped positions and they need to get up from that position and move on.\nExplaining policies or plans becomes more relevant when we consider explanatory dialogues where the agent and the user are trying to jointly come to agreement on what policy/plans to follow (eg: decision support systems), while the latter may be more useful when the user is observing some agent operating in an environment.\nWith respect to policies, we assume that the user is presented with the entire policy. A given policy is said to be explicable to the human, if the policy is optimal for the human model. Therefore the goal of the explainer becomes that of ensuring the optimality of the given policy Definition 1. A set of parameters \u03b8 E corresponds to a complete policy explanation for the given robot policy \u03c0 * M R , if the policy is also optimal for E M R h ,M R (\u03b8 E ) and is said to be the minimally complete policy explanation if there exists no other complete explanation \u03b8 E , such that, C(\u03b8 E ) < C(\u03b8 E ) Finding a complete policy explanation is relatively straightforward (the set of all parameters automatically meets this requirement). The more challenging case becomes that of finding the minimal or the cheapest explanations i.e. the minimally complete explanations. Such minimally complete explanations can be calculated by adopting a search strategy similar to [Chakraborti et al., 2017]. The search can start at the human model and try to find the minimal number of parameters that needs to be updated in the human model for the current policy to become optimal. Similar to generating minimally complete explanations, i.e, we can also generate monotonic explanations (i.e explanations where no further information about parameters in the robot model can affect the optimality of the current plan).\nIn the case of policies, we can also describe explicable planning and balancing cost of explanations with that of choosing policies that are inherently explicable, where inexplicablity score (I E ) of a policy \u03c0 is defined as\nI E (\u03c0, M R h ) = |E[V M R h \u03c0 * (s)|s \u223c \u00b5 R h ] \u2212 E[V M R h \u03c0 (s)|s \u223c \u00b5 R h ]\n| Where \u03c0 * is the optimal policy in the human model. Explicable planning thus becomes the problem of choosing Figure 2: Subfigure (a) shows a visualization of a trajectory expected by the user described in the illustrative example, and (b) shows the visualization of a trajectory the user may observe. Subfigure (c), shows the various explanatory messages that could be used in this scenario, note that the messages span information from multiple abstractions of the given task policies that minimize inexplicability score [Kulkarni et al., 2016], while minimizing the potential loss in optimality due to the policy choice (since the most explicable plan may not be an optimal policy). Balanced planning, as studied in [Chakraborti et al., 2019;, proposes going one step further and also takes into account possible savings in inexplicability score that can be achieved by providing explanation (while incurring additional cost of communicating the required explanations).\nFor explaining behavior, we will look at the simplest case, namely the agent needs to explain a set of behaviors that the user has just observed. We will assume that the observer has full observability of the state and is seeing the robot behavior for the first time. In such a setting, a given trace \u03c4 would appear explicable to the user if it could be sampled from their expected MDP policy (i.e a policy optimal in their model) or more generally, i.e P M R h (\u03c4 |\u03c0) > \u03b4, where \u03b4 is some small threshold. 1 Definition 2. A set of parameters \u03b8 E corresponds to a complete behavior explanation for a set of traces\nT = {\u03c4 1 , ...\u03c4 n }, if \u2200\u03c4 \u2208 T, \u2203\u03c0 such that P E M R h ,M R (\u03b8 E ) (\u03c4 |\u03c0) > \u03b4 and \u03c0 is an optimal policy for the model P E M R h ,M R (\u03b8 E ) .\nThe explanation is said to be the minimally complete behavior explanation if there exists no other complete explanation \u03b8 E , such that,\nC(\u03b8 E ) < C(\u03b8 E )\nNote that given the above definition, if \u03b4 is set very high it may not be possible to find a complete explanation, as the trace may genuinely contain low probability transitions. In this work we will assume \u03b4 to be zero.\nWhile model reconciliation could be an important component of either policy or behavior explanation, the applicabil-ity of the model reconciliation explanations on their own for policies is limited by the fact that in all but problems with the smallest state spaces, the user would have trouble going over the entire policy. Thus in these settings, explanatory systems would need to also utilize policy approximation or summarization methods, then allow users the ability to drill down on policy details as required. Since our main goals was to focus on developing approaches that allow us to generate model reconciliation explanations without explicitly defined user models, the rest of the paper will mostly focus on behavior explanation. In Section 8, we will have a brief discussion on how these methods could potentially be extended to policy explanation scenarios.", "n_publication_ref": 6, "n_figure_ref": 1}, {"heading": "Explaining Without Explicit Human Mental Models", "text": "Now we will look at how we can identify cheap complete behavior explanations when the human model is unknown. We will go one step further from identifying not only the parameters that need to be explained, but also capturing the right modality/abstractions to present the information about the parameters. That is, we will no longer assume that the human is using a full MDP model to come up with their decisions. Instead, the robot starts with a set of explanatory messages \u03a8 = {m 1 , m 2 , ..., m n } that can be presented to the user. Where the messages correspond to a set of parameter values (the parameters corresponding to a set of messages {m 1 , ..m k } is denoted as E({m 1 , ..m k })) of the model as captured in some abstraction of this model and has a corresponding cost ( C) associated with it. The abstractions to consider may depend on the specific scenario and the previous information about the intended users (laypeople vs. experts). Some simple possibilities may be to consider qualitative models (say non-deterministic ones instead of stochastic)\nand considering state abstractions the given task. Note that, technically E(\u03a8), need not span the set of all possible model parameters, but could rather be limited to a subset of parameters identified to be relevant to the given problem. One possible way may be to consider variations of explanation techniques like MSE [Khan et al., 2009] to identify set of possible factors that affect the optimality of each action. In Figure 2, the subfigure (c) shows a set of possible explanatory messages for the warehouse domain, that consists of each parameter mapped to some english statement. For models captured using factored representations that use relational or propositional fluents, such statements could be easily generated using templates (cf. [Hayes and Shah, 2017]). Given this setting, we will now make some simplifying assumptions, namely, (1) the order in which the explanatory messages are presented does not matter (2) we have access to a set of observers with similar models and they share this model with the target user (3) the robot is viewing the task at the same level or at a more detailed level of granularity than the user and (4) the user and robot have some shared vocabulary in regards to the task. While assumption ( 1) is easily met since we are mostly dealing with model information and ( 4) is a prerequisite for most explanatory approaches, in section 8 we will discuss how we can possibly relax requirements (2) and (3). Now our goal is to learn a predictive model that is able to predict whether a given user would find a given s, a, s tuple explicable and how the user's perception changes with the given explanatory messages.\nFor example, at the beginning of an episode the user may be presented with the following explanatory messages,\u03a8 = {m 1 = \"Robot slips with probability 0.25 at grey cells\"}, which corresponds to the fact that P (s i |a, s i ) = 0.25, for all states s i where the feature grey cell is true and for all actions a. Now the user will be presented with a sequence of transitions, say (1, 2), right, (2, 2) and asked whether the transition was explicable or not. Then the tuple (1, 2), right, (2, 2) , {m 1 }, l 1 , where l 1 is the label assigned by the user to the transition, becomes input to our learning method.\nThe exact function that we would want to learn would be\nL( s, a, s , {m 1 , ..., m k }) = \uf8f1 \uf8f4 \uf8f2 \uf8f4 \uf8f3 1 if s, a, s \u223c \u03c0 * E M R h ,M R (\u03b8({m1,...,m k }) (s) 0 otherwise\nNote that this is a modified version of the sequential model we introduced in [Zhang et al., 2017] for identifying whether a given plan is explicable or not. Though our methods vary in some significant aspects, namely, (1) we allow for the possibility that the explicability of the actions/traces could be affected by explanations provided by the system; (2) we no longer use labels of high level tasks as a proxy for the explicability of the trace. Instead, we just use a simple binary label on whether the transition is explicable or not; (3) we no longer consider sequence models but rather a much simpler labeling model that maps a single transition to the explicability label. We argue that in cases where the human is markovian on the same set of features as the agent, this rather simpler model suffices.\nIt is also important that our learning approach is more tractable than the ones studied in [Zhang et al., 2017], since in their case to build a balanced dataset (of explicable and inexplicable plans), they would need to uniformly sample through the entire plan space (an extremely hard endeavour with no obvious known approaches), while we stick to traces generated from the optimal policy and only need to randomly generate possible sets of explanatory messages, which is clearly a smaller set.\nOnce we have learned an approximation of the above labeling functionL, the problem of explanation generation for a trace \u03c4 = s 0 , a 0 , s 1 , ..., s n , a n , s n+1 becomes that of finding the subset of \u03a8 that balances the cost of communication with the reduction in the inexplicability of the given trace, i.e arg min\n\u03a8 (C M (\u03a8) + \u03b1 * \u03a3 n i=0 (1 \u2212L( s i , a i , s i+1 ,\u03a8)))\nWhere\u03a8 is a subset of \u03a8 and \u03b1 is some scaling factor that balances the cost of explanation with the number of inexplicable transitions for a given trace.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "Evaluation", "text": "The success of the approach described above would be directly dependent on whether we can learn high accuracy labeling models. Once we have access to such a model, we could be quite confident in our ability to generate useful explanation (provided the user's model is the same as the one the labeler was trained on) and identifying the best explanation becomes a matter of just searching for the required subset of messages that minimizes the objective defined in section 5. So to evaluate the method our focus was on identifying if it was possible to learn high accuracy models. We validated our approach on both simulations and on data collected from users.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "Evaluation on Simulated Data", "text": "For simulations, we used a slightly modified versions of the Taxi domain [Dietterich, 1998] (of size 6*6), the Four rooms domain [Sutton et al., 1999] (of size 9*9) and the warehouse scenario (of size 9*9) described before (implemented using the SimpleRL framework [Abel, 2019]). For each domain, we start with an MDP instance (henceforth referred to as the robot model) and then create a space of possible user models by identifying a set of possible values for each MDP parameter. For example, in the taxi domain the parameters include position of the passengers, their destination, the step cost, discounting etc., for the Four rooms this included the goal locations, locations with negative rewards, discounting, step cost, slip probability, etc., and finally for the warehouse, the position of the box, the position of station #1, the step cost, slipping probabilities and the discounting factors were selected as potential parameters that can be updated. In this setting, we assume that there exists a single explanatory message for each possible parameter.\nFor each individual test, we select a random subset of three parameters and then randomly choose a value for each of these. We then treat this new MDP model as a stand-in for the user model and use it to label traces generated from the original MDP. The traces were generated by choosing a random initial state and then following the optimal policy of the robot until either the terminal state is reached or the trace length reaches a predefined limit. For each trace, a random subset of the explanations was selected and presented to the human. This means updating the MDP parameters to their corresponding values in the robot model only for the parameters specified by the current subset of explanation. Each individual transition was then labeled using this updated MDP. A transition was labeled as inexplicable if the action is not the optimal one in the human model (i.e. Q value is lower) or the next state had a probability of occurring of \u03b4 = 0.\nWe then used this set of labeled transitions to create a training set and test set for a decision tree learner. The input features to the decision tree consist of current state features, (just x and y for Four rooms and the position of the the taxi and passengers for the Taxi domain and for Warehouse it included the position of the robot and the fact whether the agent picked up the box or visited station #1), the index of the action and features capturing the current subset of explanations being considered. In each Warehouse and Four rooms test instance, we collected 900 unique data points as training set and 100 data points as the test set. Due to the complexity of the taxi domain, we generated less data points (since for each different explanation subset we need to solve a new planning problem) and used close to 220 unique points as training data and on average 28 data points as the test set.\nWe then tested on 20 such instances for each domain. Figure 3 plots the average test accuracy for models trained with training sets of varying sizes. As evident from the graph, a simple decision tree seems to be able to easily model the effect of explanations on labeling for these simulated scenarios. We chose a simple learning model to establish the viability of this method, but one could easily see that the use of more sophisticated learning methods and/or more informed features should lead to better results.", "n_publication_ref": 3, "n_figure_ref": 1}, {"heading": "User Studies", "text": "Next, we wanted to establish if we can still learn such simple models when the labels are collected from naive users. Our goal here is not to consider scenarios with possible differences in the user's knowledge, but rather cases where, even in the presence of a set of users with similar backgrounds, their responses to explanations would be too varied to learn useful models. To test this, we used the Warehouse domain as a test bed and collected feedback on how users would view the explicability of traces generated from this domain when presented with explanatory messages detailed in Figure 2.\nFor the study, we recruited 45 master turkers from the Amazon Mechanical Turk. Each participant was provided with the URL to a website (https://goo.gl/Hun3ce) where they could view and label various robot behaviors. We considered a setting where the robot had a full battery, but was picking up a fragile box and thus still needs to visit the station #1. The robot could slip on some cells marked in dark grey with probability 0.25 (slipping here meant the robot picture is tilted to give an impression that it slipped on the cell and didn't prevent the robot from moving to the next cell). To make sure that all the users had similar mental models at the start, they were provided with the following facts, (a) that robot couldn't pass through racks, (b) whenever the robot runs low on battery it needs to get to Station 1, (c) whenever the robot has a green battery sign next to the robot, that means their battery is full and (d) the robot needs to take the shortest route to the goal. Also, they were presented with an example trace in this instructions section and were made to take a small pre-test that allowed them to revise the above facts in various scenarios. After the pre-test, they were shown eight traces from the robot policy sampled according to their probabilities. After the first trace, the user was given an explanation message before each trace, where the message was taken from the seven possible messages (the order of the messages was always randomized).\nFrom the data collected from 45 turkers, we removed data from seven users, based on the fact they didn't find any of the transition in the first trace (i.e the case where no explanation was provided) inexplicable. We imagine this number would go down when we move to expert users or users who are invested in the success of the robot. The data generated for the remaining 38 users were then used to train a decision tree. Since the placement of other objects in the environment were fixed, we were able to use rather simple features for the model like the current position of the robot (x and y), previous position (again x and y), the action, whether they have slipped and finally the explanations given. We found the model to have an average 10-fold cross validation score of 0.935. For a randomly generated train and test split (where the test split was 10% and contained around 7% inexpicable labels) the precision score was 0.9637 and the recall score was 0.9568.\nFurthermore, we could see that the model was able to correctly predict the usefulness of intuitive minimal explanations for the given scenario. For example, it predicted that while the robots decision to visit station #1 would be considered inexeplicable by the user in the absence of any explanation, the user would mark it as explicable when they are explained about the box being fragile and that fragile boxes need to be inspected at station #1. In fact the model predicted that only the message that \"fragile boxes need to be inspected at station #1\" is enough to convince the user about the need for that action (i.e the user could deduce that the box must have been fragile). This shows that such learned models may help us generate cheaper explanations (the above set of explanations is smaller than the corresponding minimal complete behavior explanation for the domain), by taking into account the users ability to correctly predict missing information in simple cases. Another point of interest was that the model predicted all slipping events as explainable even in the absence of any explanations. The cases where the user saw a slip before being told about the possibility of slipping was rare (since there are two explanatory messages related to slipping and the probability of slipping was 0.25). Furthermore when we went over the data, we found that in most such cases, the users did mark it as explainable. This may be because the effect of slipping may not have been that detrimental to the overall plan (it doesn't take you off the current path). It would be interesting to see if this result would be the same in cases where slipping was a more likely event and if it had a more apparent effect on the robot's plan.", "n_publication_ref": 0, "n_figure_ref": 1}, {"heading": "Related Work", "text": "To the best of our knowledge, this work represents the first attempt at learning proxies for user mental models that allows an agent to predict the potential impact of providing explanations as model reconciliation to observers. With that said, there have been works that have looked at the problem of generating explanations in the presence of model uncertainty for human models. In particular, our previous works like [ Sreedharan et al., 2018a;2018b] have looked at cases where the agent has access to a set of potential human models. One drawback of considering a set of possible models is either they would need to have explicit sensing to identify the user model (which could mean asking a large number of questions to the user) or providing a large amount of information to cover the space of all possible models. In our work, the problem of identifying the specifics of the user model is resolved through an offline training process.\nAnother work quite related to the discussion covered in this paper is [Reddy et al., 2018], wherein the authors tried to identify cases where they can learn a potential model for the human's expectation of the task transition dynamics when they do not align with the real world dynamics. Unlike their work, we do not assume that the user can provide traces for the given task, rather they may be able to provide some highlevel feedback on the action (i.e. they may not be able to do or even know the right action but may be able to point out actions or transitions that surprise them). Moreover, their work requires that the user and the robot must have the same reward function, which is again an assumption we do not make. Even if we had followed their technique to learn a potential approximation of the human's transition model for the task, there is no guarantee that the learned representation would be one that makes sense to the human.", "n_publication_ref": 3, "n_figure_ref": 0}, {"heading": "Discussion and Conclusion", "text": "This paper proposes a possible way in which model reconciliation explanation could be applied to cases where the user model is unknown. The method described here is a rather simple and general method to identify information that could potentially affect the user's mental model and produce effects that align with the agent's requirements. There is no requirement here that the messages have to align with actual facts about the world. This again points to the rather troubling similarities between the mechanisms needed to generate useful explanations and lies .\nTwo important assumptions we made throughout the work is that the user only considers the current state (as defined by the robot) to make their decisions and we have access to a model that was learned from interactions to previous users who had similar knowledge level to the current user. Relaxing the first assumption would require us to go beyond learning models that map each transitions to labels. Instead we have to consider sequential labeling models (for example models based on LSTM or CRF) of the type considered in [Zhang et al., 2017] to capture the human's expectations. For example, we considered a simple extension of the warehouse domain where the human believes the robot should visit two locations (i.e the human state contains variables that record whether the user has visited the locations). Even though here the user is considering a more detailed model, we were able to learn labeling models of 80% accuracy by using simple CRFs. As for the second, instead of assuming that all users are of the same type, a more reasonable assumption may be that the users could be clustered into N groups and we could learn a different labeling model for each user type. Now we still have a challenge of identifying the user type of a new user and one way to overcome this would be by adopting a decision-theoretic approach to this problem and modeling it as a POMDP (where user labels become observations and previously learned user models the observation models).\nThe work discussed in this paper only covers explanations that allow the user and the system to reconcile any model difference. This only covers a part of the entire explanatory dialogue. Even if there is no difference in models, the user may still have questions about parts of the policy or may raise alternative policies they think should be followed. This may arise from a difference in inferential abilities and may require providing information that is already part of their deductive closure eg: help them understand the long term consequences of taking some actions. Once you have access to a set of such messages one could use a method similar to the one described in the paper to find the set of helpful ones. Unlike the model reconciliation setting where the messages stand for information about the model, it is not quite clear how one could automatically generate such messages.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "Acknowledgments", "text": "This research is supported in part by the ONR grants N00014-16-1-2892, N00014-18-1-2442, N00014-18-1-2840, the AFOSR grant FA9550-18-1-0067, NASA grant NNX17AD06G and a JP Morgan AI Faculty Research grant .", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "Plan explanations as model reconciliation: Moving beyond explanation as soliloquy", "journal": "", "year": "2017", "authors": "Abel ; David Abel ; Tathagata Chakraborti; Sarath Sreedharan; Yu Zhang; Subbarao Kambhampati ; Tathagata Chakraborti; Sarath Sreedharan; Subbarao Kambhampati"}, {"title": "Improving robot controller transparency through autonomous policy explanation", "journal": "IEEE", "year": "1998", "authors": "Daniele Magazzeni; ; Thomas; G Dietterich; Bradley Hayes; Julie A Shah"}, {"title": "Explicable Robot Planning as Minimizing Distance from Expected Behavior. CoRR, abs", "journal": "", "year": "1611", "authors": "[ Khan"}, {"title": "Nicholay Topin and Manuela Veloso. Generation of policy-level explanations for reinforcement learning", "journal": "Prentice Hall", "year": "1999", "authors": "[ Reddy"}], "figures": [{"figure_label": "3", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 3 :3Figure 3: The test accuracy for increasing sizes of training set.", "figure_data": ""}], "formulas": [], "doi": ""}