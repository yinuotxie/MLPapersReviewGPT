{"title": "NEURAL REPRESENTATION AND GENERATION FOR RNA SECONDARY STRUCTURES", "authors": "Zichao Yan; William L Hamilton; Mathieu Blanchette", "pub_date": "", "abstract": "Our work is concerned with the generation and targeted design of RNA, a type of genetic macromolecule that can adopt complex structures which influence their cellular activities and functions. The design of large scale and complex biological structures spurs dedicated graph-based deep generative modeling techniques, which represents a key but underappreciated aspect of computational drug discovery. In this work, we investigate the principles behind representing and generating different RNA structural modalities, and propose a flexible framework to jointly embed and generate these molecular structures along with their sequence in a meaningful latent space. Equipped with a deep understanding of RNA molecular structures, our most sophisticated encoding and decoding methods operate on the molecular graph as well as the junction tree hierarchy, integrating strong inductive bias about RNA structural regularity and folding mechanism such that high structural validity, stability and diversity of generated RNAs are achieved. Also, we seek to adequately organize the latent space of RNA molecular embeddings with regard to the interaction with proteins, and targeted optimization is used to navigate in this latent space to search for desired novel RNA molecules.", "sections": [{"heading": "INTRODUCTION", "text": "There is an increasing interest in developing deep generative models for biochemical data, especially in the context of generating drug-like molecules. Learning generative models of biochemical molecules can facilitate the development and discovery of novel treatments for various diseases, reducing the lead time for discovering promising new therapies and potentially translating in reduced costs for drug development (Stokes et al., 2020). Indeed, the study of generative models for molecules has become a rich and active subfield within machine learning, with standard benchmarks (Sterling & Irwin, 2015), a set of well-known baseline approaches (G\u00f3mez-Bombarelli et al., 2018;Kusner et al., 2017;Jin et al., 2018), and high-profile cases of real-world impact 1 .\nPrior work in this space has focused primarily on the generation of small molecules (with less than 100 atoms), leaving the development of generative models for larger and more complicated biologics and biosimilar drugs (e.g., RNA and protein peptides) an open area for research. Developing generative models for larger biochemicals is critical in order to expand the frontiers of automated treatment design. More generally, developing effective representation learning for such complex biochemicals will allow machine learning systems to integrate knowledge and interactions involving these biologically-rich structures.\nIn this work, we take a first step towards the development of deep generative models for complex biomolecules, focusing on the representation and generation of RNA structures. RNA plays a crucial role in protein transcription and various regulatory processes within cells which can be influenced by its structure (Crick, 1970;Stefl et al., 2005), and RNA-based therapies are an increasingly active area of research (Pardi et al., 2018;Schlake et al., 2012), making it a natural focus for the development of deep generative models. The key challenge in generating RNA molecules-compared to the generation of small molecules-is that RNA involves a hierarchical, multi-scale structure, including a primary sequential structure based on the sequence of nucleic acids as well as more complex secondary and tertiary structures based on the way that the RNA strand folds onto itself. An effective generative model for RNA must be able to generate sequences that give rise to these more complex emergent structures.\nThere have been prior works on optimizing or designing RNA sequences-using reinforcement learning or blackbox optimization-to generate particular RNA secondary structures (Runge et al., 2019;Churkin et al., 2017). However, these prior works generally focus on optimizing sequences to conform to a specific secondary structure. In contrast, our goal is to define a generative model, which can facilitate the sampling and generation of diverse RNA molecules with meaningful secondary structures, while also providing a novel avenue for targeted RNA design via search over a tractable latent space.\nKey contributions. We propose a series of benchmark tasks and deep generative models for the task of RNA generation, with the goal of facilitating future work on this important and challenging problem. We propose three interrelated benchmark tasks for RNA representation and generation:\n1. Unsupervised generation: Generating stable, valid, and diverse RNAs that exhibit complex secondary structures. 2. Semi-supervised learning: Learning latent representations of RNA structure that correlate with known RNA functional properties. 3. Targeted generation: Generating RNAs that exhibit particular functional properties.\nThese three tasks build upon each other, with the first task only requiring the generation of stable and valid molecules, while the latter two tasks involve representing and generating RNAs that exhibit particular properties. In addition to proposing these novel benchmarks for the field, we introduce and evaluate three generative models for RNA. All three models build upon variational autoencoders (VAEs) (Kingma & Welling, 2014) augmented with normalizing flows (Rezende & Mohamed, 2015;Kingma et al., 2016), and they differ in how they represent the RNA structure. To help readers better understand RNA structures and properties, a self-contained explanation is provided in appendix B.\nThe simplest model (termed LSTMVAE) learns using a string-based representation of RNA structure. The second model (termed GraphVAE) leverages a graph-based representation and graph neural network (GNN) encoder approach (Gilmer et al., 2017). Finally, the most sophisticated model (termed HierVAE) introduces and leverages a novel hierarchical decomposition of the RNA structure. Extensive experiments on our newly proposed benchmarks highlight how the hierarchical approach allows more effective representation and generation of complex RNA structures, while also highlighting important challenges for future work in the area.", "n_publication_ref": 15, "n_figure_ref": 0}, {"heading": "TASK DESCRIPTION", "text": "Given a dataset of RNA molecules, i.e. sequences of nucleotides and corresponding secondary structures, our goals are to: (a) learn to generate structurally stable, diverse, and valid RNA molecules that reflect the distribution in this training dataset; (b) learn latent representations that reflect the functional properties of RNA. A key factor in both these representation and generation processes is that we seek to jointly represent and generate both the primary sequence structure as well as the secondary structure conformation. Together, these two goals lay the foundations for generating novel RNAs that satisfy certain functional properties. To meet these goals, we create two types of benchmark datasets, each one focusing on one aspect of the above mentioned goals:\nUnlabeled and variable-length RNA. The first dataset contains unlabeled RNA with moderate and highly-variable length (32-512 nts), obtained from the human transcriptome (Aken et al., 2016) and through which we focus on the generation aspect of structured RNA and evaluate the validity, stability and diversity of generated RNA molecules. In particular, our goal with this dataset is to jointly generate RNA sequences and secondary structures that are biochemically feasible (i.e., valid), have low free energy (i.e., stable), and are distinct from the training data (i.e., diverse). We will give an extended assessment of the generation aspect under different circumstances, e.g., when constraining the generation procedures with explicit rules.\nLabeled RNA. The second dataset is pulled and processed from a previous study on in vitro RNAprotein interaction, which features labeled RNAs with shorter and uniform length (40 nts) (Cook et al., 2017). With this dataset, our objective is slightly expanded (to include obj. a), so that the latent space is adequately organized and reflective of the interaction with proteins. Therefore, key assessment for the latent space includes AUROC for the classification of protein binding, which is crucial for the design of desired novel RNA molecules.\nEssentially, this creates slight variations in the task formulation, with the first dataset suited to unsupervised learning of a generative model, while the second datasets involves additional supervision (e.g., for a semi-supervised model or targeted generation). Our specific modeling choices, to be introduced in section 3, are invariant to different task formulations, and flexible enough to handle different representations of RNA secondary structures. We refer readers to appendix C for detailed explanation for the dataset and evaluation metrics on the generated molecules and latent embeddings.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "METHODS", "text": "In this section, we introduce three different generative models for RNA. All three models are based upon the variational autoencoder (VAE) framework, involving three key components:\n1. A probabilistic encoder network q \u03c6 (z|x), which generates a distribution over latent states given an input representation of an RNA. We experiment with three different types of input encodings for RNA sequence and secondary structures (see Figure S1: a dot-bracket annotated string, a graph with adjacency matrix representing base-pairings, and a graph augmented with a hierarchical junction tree annotation for the secondary structure.\n2. A probabilistic decoder network p \u03b8 (x|z), which defines a joint distribution over RNA sequences and secondary structures, conditioned on a latent input. As with the encoder network, we design architectures based on a linearized string decoding and a graph-based hierarchical junction-tree decoding approach.\n3. A parameterized prior p \u03c8 (z), which defines a prior distribution over latent states and is learned based on a continuous normalizing flow (CNF) (Chen et al., 2018). shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.\nFor all the approaches we propose, the model is optimized via stochastic gradient descent to minimize the evidence lower bound (ELBO):\nL = \u2212E q \u03c6 (z|x) [p \u03b8 (x|z)] + \u03b2 KL(q \u03c6 (z|x)|p \u03c8 (z))\nwhere \u03b2 is a term to allow KL-annealing over the strength of the prior regularization.\nIn the following sections, we explain our three different instantiations of the encoder (section 3.1), decoder (section 3.2), as well as our procedures to structurally constrain the decoding process using domain knowledge (section 3.3) and our procedures to avoid posterior collapse (section 3.4).", "n_publication_ref": 1, "n_figure_ref": 1}, {"heading": "ENCODING RNA SECONDARY STRUCTURES", "text": "The input to the encoder is a structured RNA molecule, with its sequence given by an ordered array of nucleotides x 1 . . . x L , with x i \u2208 {A, C, G, U }, where L is the length of the sequence, and its secondary structure, either represented as (1) a dot-bracket string S =\u1e8b 1 . . .\u1e8b L with\u1e8b i \u2208 {., (, )};\n(2) or as a graph G with two types of edges -covalent bonds along the RNA backbone, and hydrogen bonds between the base-pairs 2 . We use x uv to denote edge features between nucleotides u and v;\n(3) or as a hypergraph T -a depth-first ordered array of subgraphs\u011c 1 . . .\u011c D with L(\u011c i ) \u2208 {S, H, I, M } indicating the subgraph label, and I(\u011c i ) = {j|j \u2208 {1 . . . L}} indicating the assignment of nucleotides to each subgraph.\nEncoding RNA secondary structure as sequence. First, we obtain a joint encoding over the nucleotide and the dot-bracket annotation, using the joint sequence-structure vocabulary {A, C, G, U } \u00d7 {., (, )}. Then, these one-hot encodings are processed by a stacked bidirectional LSTM (Hochreiter & Schmidhuber, 1997), followed by a multi-head self-attention module (Vaswani et al., 2017) to weigh different positions along the RNA backbone. A global max-pooling is used to aggregate the information into h S , and then we obtain mean \u00b5 S and log variance log \u03c3 S from h S through linear transformations, and draw latent encoding z S from N (\u00b5 S , \u03c3 S ) using the reparameterization trick (Kingma & Welling, 2014).\nLearning graph representation of RNA secondary structure. To encode the graph view G of an RNA secondary structure, we pass rounds of neural messages along the RNA structure, which falls into the framework of Message Passing Neural Network (MPNN) as originally discussed in Gilmer et al. (2017) and similarly motivated by Jin et al. (2018).\nFor much longer RNAs, it is conceptually beneficial to pass more rounds of messages so that a nucleotide may receive information on its broader structural context. However, this may introduce undesired effects such as training instability and over-smoothing issues. Therefor , we combine our MPNN network with gating mechanism, which is collectively referred as the G-MPNN:\nv t\u22121 uv = \u03c3(W g local [x u || x uv ] + W g msg w\u2208N (u) v t\u22121 wu ) (1) v t uv = GRU(v t\u22121 uv , v t\u22121 uv )(2)\nwhere [. . . || . . . ] denotes concatenation, \u03c3 denotes the activation function and GRU indicates the gated recurrent unit (Cho et al., 2014). Then, after T iterations of message passing, the final nucleotide level embedding is given by:\nh u = \u03c3(W g emb [x u || v\u2208N (u) v T vu ]\n). Before pooling the nucleotide level embeddings into the graph level, we pass h 1 . . . h L through a single bidirectional LSTM layer, obtaining\u0125 1 . . .\u0125 L at each step, and h g = max({\u0125 i |i \u2208 1...L}). The latent encoding z G is similarly obtained from h G using the reparameterization trick.\nHierarchical encoding of the RNA hypergraph. To encode the junction tree T of RNA, we employ a type of GRU specifically suited to tree-like structures, which has previously been applied in works such as GGNN (Li et al., 2016) and JTVAE (Jin et al., 2018). We refer to this tree encoding network as T-GRU, and the format of its input is shown in Figure 1.\nOne major distinction between our RNA junction tree and the one used for chemical compounds (Jin et al., 2018) is that an RNA subgraph assumes more variable nucleotide composition such that it is impossible to enumerate based on the observed data. Therefore, we need to dynamically compute the features for each node in an RNA junction tree based on its contained nucleotides, in a hierarchical manner to leverage the nucleotide level embeddings learnt by G-MPNN.\nConsidering a subgraph\u011c i in the junction tree T , we initialize its node feature with:\nx\u011c i = [L(\u011c i ) || max u\u2208I(\u011ci) h u ].\nNotably, max u\u2208\u011ci h u is a max-pooling over all nucleotides assigned to\u011c i , and nucleotide embedding h u comes from G-MPNN. To compute and pass neural messages between adjacent subgraphs in the RNA junction tree T , we use the T-GRU network in Eq.3\nv t\u011c i,\u011cj = T-GRU(x\u011c i , {v t\u22121 G k ,\u011ci |\u011c k \u2208 N (\u011c i )}) (3) h\u011c i = \u03c3(W t emb [x\u011c i || \u011c \u2208N (\u011ci) h\u011c]) (4)\nwith details of T-GRU provided in the appendix D, and compute the embeddings for subgraphs with Eq. 4. Further, we obtain a depth-first traversal of the subgraph embeddings h\u011c 1 . . . h\u011c D which is also the order for hierarchical decoding to be discussed later. This ordered array of embeddings is processed by another bi-directional LSTM , and the final tree level representation h T is again given by the max-pooling over the bi-LSTM outputs. Likewise, latent encoding z T is obtained from h T .", "n_publication_ref": 9, "n_figure_ref": 1}, {"heading": "RNA MOLECULAR GENERATION", "text": "Decoding linearized sequence and structure. In this setting, the decoder simply autoregressively decodes a token at each step, from the joint sequence-structure vocabulary mentioned before in section 3.1, plus one additional symbol to signal the end of decoding. To simplify the design choice, we use a single-layered forward-directional LSTM, and its hidden state is initialized with the latent encoding z, which can be either z S , z G or z T .\nFigure 2: Hierarchical decoding of a structured RNA, involving three types of predictions, that are on the topological level, node level, and nucleotide level. These three types of prediction are interleaved into the procedures of decoding the junction tree structure of RNA and the nucleotide segments.\nHierarchically decoding hypergraph and nucleotide segments. The input to this more sophisticated hierarchical decoder are latent encodings z G which contains order and basic connectivity information of the nucleotides, and z T which contains higher order information about the arrangements of nucleotide branches and their interactions. We give a concise description of the decoding procedures here, along with a detailed algorithm in appendix E. On a high level, we hierarchically decode the tree structure in a depth-first manner, and autoregressively generate a nucleotide segment for each visited tree branch. For these purposes, we interleave three types of prediction (Figure 2).\nDenote the current tree node at decode step t and at the i-th visit as\u011c t,i , whose features include (1) its node label L(\u011c t,i ) and, (2) a summary over the already existing i \u2212 1 nucleotide segments max{h l,j u | u \u2208\u011c t,i and l < t and j < i}, with l denoting the nucleotide is decoded at step l, and j indicating the nucleotide belongs to the j-th branch (this feature is simply zeros when i = 1). Then, its local feature x\u011c t,i is defined as the concatenation of (1) and (2).\nWe make use of a notion called node state: h\u011c t,i , which is obtained by: h\u011c t,i = T-GRU(x\u011c t,i , {v\u011c ,\u011c t,i |\u011c \u2208 N (\u011c t,i )}). Note its similarity to Eq. 3, and h\u011c t,i is used to make:\n\u2022 topological prediction in Figure 2 \n([h\u011c t,i || z T || z G ])\n. The start token is the last nucleotide from the last segment.\nOur hierarchical decoder starts off by predicting the label of the root node using z T , followed by topological prediction on the root node and decoding the first nucleotide segment. The algorithm terminates upon revisiting the root node, topologically predicted to backtrack and finishing the last segment of the root node. The decoded junction tree naturally represents an RNA secondary structure that can be easily transformed to the dot-bracket annotation, and the RNA sequence is simply recovered by connecting nucleotide segments along the depth-first traversal of the tree nodes.", "n_publication_ref": 0, "n_figure_ref": 3}, {"heading": "STRUCTURALLY CONSTRAINED DECODING", "text": "To better regulate the decoding process so that generated RNAs have valid secondary structures, a set of constraints can be added to the decoding procedures at the inference stage. Essentially, a valid RNA secondary structure needs to observe the following rules: (1) base-pairing complementarity, (2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i \u2212 j| > 3;\n(3) each nucleotide can only be paired once, and overlapping pairs are disallowed.\nWe will translate the above rules into specific and applicable constraints, depending on specific decoders. For the sake of space, we only give a broad remark and leave more details in the appendix.\nLinearized decoding constraints. Since the linearized decoder simply proceeds in an autoregressive fashion, constraints can be easily enforced in a way that at each step, a nucleotide with an appropriate structural annotation is sampled by making use of masks and re-normalizing the probabilities. Likewise, a stop token can only sampled when all opening nucleotides have been closed. More details to follow in appendix F.\nHierarchical decoding constraints. The specific set of constraints for hierarchical decoding is discussed in appendix G. Overall, considering the different natures of the three associated types of prediction, each one should require a set of different strategies, which are once again applicable by adding proper masks before sampling. As shown in the algorithm in appendix E, the set of constraints are applied to line 13, 24 and 14 with marked asterisk.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "AVOIDING POSTERIOR COLLAPSE", "text": "As discussed in a line of previous works, VAEs with strong autoregressive decoders are susceptible to posterior collapse, an issue where the decoder simply ignores the latent encoding of the encoder (He et al., 2019). Therefore, to avoid posterior collapsing, we make use of a carefully chosen KL annealing schedule during training to help the encoder adapt its information content in the latent encoding and in coordination with the decoder. This schedule is detailed in section 4. We also learn a parameterized prior as suggested in Chen et al. (2017), but using a CNF instead, following a similar implementation to Yang et al. (2019), with details given in appendix H.\nOur KL annealing schedule is chosen based on empirical observations, as to our knowledge, there has yet to exist any principled methods of selecting such schedule. We have used diagnostic metrics such as mutual information (He et al., 2019) and active units (Burda et al., 2016) along with a validation set to select a proper KL annealing schedule which is to be described later in section 4", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "RESULTS", "text": "We consider three modes of evaluation: (1) unsupervised RNA generation; (2) generation using semi-supervised VAE models and (3) targeted RNA design from an organized latent space. Results are presented below, and relevant hyperparameters can be found in Table S1.\nUnsupervised RNA generation. Here, we evaluate generated RNAs from models trained on the unlabeled RNA dataset for 20 epochs using a KL annealing schedule including 5 epochs of warm-up, followed by gradually increasing the KL annealing term to 3e-3 (for LSTMVAE and GraphVAE), or 2e-3 (for HierVAE). The KL annealing schedule was chosen using a validation set of 1,280 RNAs.\nTable 1 compares the generation capability of different models, from the posterior as well as the prior distribution, and in scenarios such as applying structural constraints to the decoding process or not.\nIt clearly shows that our most advanced model, HierVAE which employs a hierarchical view of the structure in its encoding/decoding aspects, achieves the best performance across different evaluation regimes, generating valid and stable RNAs even when the decoding processed is unconstrained. It is also observed that despite having structural constraints, the validity of our generated RNAs are always slightly below 100%. This can be explained by the threshold hyperparameter which sets the maximum number of steps for topological prediction as well as the maximal length of each nucleotide segment, as shown in Algorithm 1 in appendix E.\nTo further demonstrate the benefits of model training from structural constraints, we sample RNAs from the prior of an untrained HierVAE model. With structural constraints, the validity amounts to 66.34% with an extremely high free energy deviation of 22.613. Without structural constraints, the validity translates to a mere 9.37% and the model can only decode short single stranded RNAs as it lacks the knowledge of constructing more complex structures. This comparison illustrates that model training is essential for obtaining stable RNA folding.\nThe junction tree hierarchy of RNAs developed in our work shares certain modelling similarities with the probabilistic context free grammar (Dowell & Eddy, 2004) used by covariance models (CM) (Eddy & Durbin, 1994). Infernal (Nawrocki & Eddy, 2013) is one of the representative works based on CM, which is capable of sampling RNA secondary structures from a CM built around a consensus secondary structure for a conserved RNA family. However, due to the lack of homologous sequences in our dataset, Infernal is seriously limited and can only sample single stranded RNAs.\nFigure 3 illustrate RNA structures generated using HierVAE from a randomly chosen short path through the latent space. Notably, latent encoding provided by HierVAE translates smoothly in the RNA structure domain: nearby points in the latent space result in highly similar, yet different, structures. The generated structures are particularly stable for short and medium-size RNAs, and slightly less so for longer RNAs with highly complex structures. A side-by-side comparison between generated RNA secondary structures and MFE structures in Figure S3 shows that generated structures can evolve smoothly in the latent space along with their corresponding MFE structures. We also visualize neighborhoods of a Cysteine-carrying transfer RNA and a 5S ribosomal RNA in figure S4 and S5.\nSupervised RNA generation. We then evaluate our generative approaches in a semi-supervised setting using seven RBP binding data sets from RNAcompete-S. First, we compare the efficacy of different representational choices while excluding the generative components, i.e. we jointly train VAE encoders followed by simple MLP classifiers on top of the latent encodings for binary classification on RBP binding.  2. Since our strategy for targeted RNA design makes use of seed molecules in the latent space, we mainly sample RNAs from the posterior distribution of these semi-supervised VAE models. Therefore, we select a KL annealing schedule that tends to retain more information in the latent encodings, i.e. setting maximum \u03b2 to 5e-4 and training 10 epochs.\nResults are promising in that classification AUROC measured by the held-out test set is comparable to the fully supervised classification models in Table S3, and much better compared to models only using fixed and pretrained VAE embeddings as shown in Table S2. Also, RNA structures generated from the posterior distribution, even under the setting of unconstrained and deterministic decoding, have high success rates, very stable conformation and good reconstruction accuracy. Targeted RNA design. We next studied the task of designing RNAs with high RBP binding affinity. Starting from the latent encodings of 10,000 randomly chosen RNA molecules that have negative labels in each RNAcompete-S test set, and use activation maximization to gradually alter the latent encodings so that the predicted binding probability from the embedding classifiers increases. These embedding classifiers have been trained jointly with the VAE models with accuracy reported earlier (Table 2). Then, we use separately trained full classifiers (also earlier shown in Table S3) as proxy of oracles for evaluating the \"ground truth\" probability of RBP binding. Table 3, report the success rate (fraction of RNAs whose \"ground truth\" RBP binding probability was improved), along with the average improvement in binding probabilities. An example of a trajectory of optimized RNAs is shown in Fig. S6.", "n_publication_ref": 3, "n_figure_ref": 4}, {"heading": "RELATED WORK", "text": "Over the years, the field of computational drug discovery has witnessed the emergence of graphcentric approaches. One of the earliest method, proposed in G\u00f3mez-Bombarelli et al. (2018), is defined on the linearized format of molecular structures and represents a family of methods that rely on sequential models to represent and generate SMILES strings of chemical compounds. Later methods have sought to construct more chemical priors into the model, via (1) leveraging graph based representation and generation techniques, (2) enforcing direct chemical constraints to the decoding process, (3) considering a multi-scale view of the molecular structures, or (4) using reinforcement learning to integrate more training signal of the molecular structure and function. As a result, greater success has been achieved by models such as Kusner et al. (2017); ; Jin et al. (2018); You et al. (2018) at generating and searching valid and more useful chemical compounds.\nGraph representation learning is at the heart of these more recent approaches, to help understand the rules governing the formation of these molecular structures, as well as the correspondence between structures and functions. Duvenaud et al. (2015) were among the first to apply GNN to learn molecular fingerprints, and the general neural message passing framework for molecules is proposed in Gilmer et al. (2017), which demonstrate the power of MPNN across various molecular benchmarking tasks. These prior works on molecular MPNN, together with other GNN architectures developed in other areas, such as considering relational edges (Schlichtkrull et al., 2018) and attention (Velickovic et al., 2018), have laid the foundation for the success of these deep generative models.\nDespite the fact that RNA molecules can adopt complex structures, dedicated graph representation learning techniques have been scarce, with some recent works beginning to leverage graph related learning techniques to predict RNA folding (Chen et al., 2020;Singh et al., 2019) and to represent RNA molecular structures (Yan et al., 2020;Oliver et al., 2020). Prior to our work, the design of RNA has mostly focused on the inverse design problem, which is to conditionally generate an RNA sequence whose MFE secondary structure corresponds to an input secondary structure. Therefore, the line of prior works have predominantly relied on sequential techniques, with some representative methods based on reinforcement learning (Runge et al., 2019), or more classically framed as a combinatorial optimization problem and solved with sampling based techniques (Churkin et al., 2017). These prior works are mainly concerned with querying from an energy model with fixed thermodynamic parameters and fixed dynamics of RNA folding, which is in itself limited compared to learning based approaches (Chen et al., 2020;Singh et al., 2019), and are unable to model a joint distribution over RNA sequences and possible folds.", "n_publication_ref": 16, "n_figure_ref": 0}, {"heading": "CONCLUSION AND FUTURE WORKS", "text": "In this work we propose the first graph-based deep generative approach for jointly embedding and generating RNA sequence and structure, along with a series of benchmarking tasks. Our presented work has demonstrated impressive performance at generating diverse, valid and stable RNA secondary structures with useful properties.\nFor future works, there are several important directions to consider. First, it would be beneficial to obtain non-coding RNA families from the RFAM database (Kalvari et al., 2017) which would help our models learn more biologically-meaningful representation indicative of RNA homology and functions, in addition to the evolutionarily conserved RNA structural motifs that would enable the generation of more stable RNA secondary structures. In that context, a detailed comparison to Infernal and other probabilistic context-free grammar models would be meaningful.\nOn the methodological aspect, in light of the recent advances in protein sequences pretraining across a large evolutionary-scale (Rives et al., 2019;Elnaggar et al., 2020), our models for RNAs may similarly benefit by such a procedure with the data collected from RFAM. After the pretraining step, reinforcement learning can be used to finetune the generative component of our model with customizable rewards defined jointly on RNA structural validity, folding stability and functions such as binding to certain proteins.\nOn the evaluation side, it would be of great interest to analyze our models for any potential RNA tertiary structural motifs and to compare them with those deposited in the CaRNAval (Reinharz et al., 2018) or RNA 3D motifs database (Parlea et al., 2016). Our models would also need modifications to allow non-canonical interactions and pseudoknots, which are common in RNA tertiary structures.\nAll in all, the representation, generation and design of structured RNA molecules represent a rich, promising, and challenging area for future research in computational biology and drug discovery, and an opportunity to develop fundamentally new machine learning approaches.", "n_publication_ref": 5, "n_figure_ref": 0}, {"heading": "A ACKNOWLEDGEMENTS", "text": "We would like to thank all members of the Hamilton lab, Blanchette lab, and the four anonymous reviewers for their insightful suggestions. This work was funded by a Genome Quebec/Canada grant to MB and by the Institut de Valorisation des Donn\u00e9es (IAVDO) PhD excellence scholarship to ZY. WLH is supported by a Canada CIFAR AI Chair. We also thank Compute Canada for providing the computational resources.\nB BACKGROUND: RNA STRUCTURE AND KEY PROPERTIES , hence stabilizing the molecule 3 . The set of pairs of interacting nucleotides in an RNA forms its so-called RNA secondary structure. In computational analyses of RNA, it is standard to assume that a secondary structure is nested: if [i, j] and [k, l] form base pairs with i < k, then either l < j (nesting) or k > j (non-overlapping). This enables simple string or planar graph representations (Figure S1 a, b).\nThe nested structure assumption means that secondary structures can be modelled by a probabilistic context free grammar (Dowell & Eddy, 2004), or by the closely related junction tree structure (Figure S1 c) (Sarrazin-Gendron et al., 2020), where each hypernode corresponds to a particular secondary substructure element: (1) stem: consecutive stacked base-pairs locally forming a double-stranded structure;\n(2) hairpin loop : unpaired regions closed by a base-pair;\n(3) internal loop: unpaired regions located between two stems; (4) multiloop: unpaired regions at the junction of at least three stems. Edges link elements that are adjacent in the structure.\nValidity and stability of RNA folding. The notion of free energy of RNA secondary structures can be used to characterize the stability of a particular conformation. Given an RNA sequence, there are combinatorially many valid RNA secondary structures which all need to obey a set of constraints (summarized in section 3.3). However, some structures are more stable than the others by having lower free energy. Therefore, these structures are more likely to exist (hence more useful) in reality due to the stochastic nature of RNA folding. The free energy of an RNA secondary structure can be estimated by an energy-based model with thermodynamic parameters obtained from experiments (Mathews et al., 2004), wherein the minimum free energy (MFE) structure can be predicted, up to a reasonable approximation (Lorenz et al., 2011). 4 ", "n_publication_ref": 4, "n_figure_ref": 2}, {"heading": "C DATASET AND METRICS", "text": "The unlabeled dataset is obtained from the complete human transcriptome which is downloaded from the Ensembl database (Aken et al. (2016); version GRCh38). We slice the transcripts into snippets with length randomly drawn between 32 and 512 nts, and use RNAfold to obtain the MFE structures. We randomly split the dataset into a training set that contains 1,149,859 RNAs, and 20,000 held-out RNAs for evaluating decoding from the posterior distribution. More information on the structural diversity and complexity of this dataset is shown in Figure S2, which should present significant challenges for our algorithms.\nThe labeled dataset is pulled from a previous study on sequence and structural binding preference of RNA binding proteins (RBP), using an in vitro selection protocol called RNAcompete-S (Cook et al., 2017) which generates synthesized RNA sequences bound or unbound to a given RBP. RNAs in this experiment are of uniform length, i.e. 40 nts, and offer a rich abundance of RNA secondary structures compared to its predecessor protocols such as RNAcompete (Ray et al., 2009;2013). Since no benchmark has been ever established since its publication, we randomly sample 500,000 positive sequences bound to an RBP, and the same amount of negative sequences from the pool of unbound sequences, to curate a dataset for each of the seven RBPs investigated in the paper. Then, 80% of all RNAs are randomly selected to the train split, and the rest goes to the test split.\nOur evaluation scheme for the generated RNA secondary structures includes the following metrics:\n\u2022 validity: percentage of generated RNA secondary structures that conform to the structural constraints specified in section 3.3. \u2022 free energy deviation (FE DEV): difference of free energy between the generated RNA secondary structure and the MFE structure of the corresponding sequence, which quantifies the gap of both structures from an energy perspective. A lower FE DEV should indicate higher stability of generated RNAs. \u2022 free energy deviation normalized by length (Normed FE DEV): FE DEV divided by the length of generated RNA, which distributes the contribution of total FE DEV to each base. \u2022 5-mer sequence diversity: entropy of the normalized counts of 5-mer substrings, which directly measures the diversity of RNA sequences, and indirectly for RNA secondary structures when this metric is combined with FE DEV, since monolithic structures of diverse sequences would lead to high FE DEV.", "n_publication_ref": 4, "n_figure_ref": 1}, {"heading": "D TREE ENCODING GRU", "text": "Following Eq.3, T-GRU computes a new message v t\u011c i,\u011cj from\u011c i and\u011c j , based on the features in G i denoted by x\u011c i , as well as neural messages from neighboring subgraphs to\u011c i , i.e. {v t\u22121 G k ,\u011ci |\u011c k \u2208 N (\u011c i )}. The internal structure of T-GRU is equivalent to the tree encoder employed in Jin et al. (2018), which is essentially a neural analogue of the belief propagation algorithm on junction trees. Nevertheless, we write down the message passing formulas of T-GRU here:\ns\u011c i,\u011cj = \u011c k \u2208N (\u011ci) v t\u22121 G k ,\u011ci(S1)\nz\u011c i,\u011cj = \u03c3(W z [x\u011c i || s\u011c i,\u011cj ] + b z ) (S2) r\u011c k ,\u011ci = \u03c3(W r [x\u011c i || v t\u22121 G k ,\u011ci ] + b r ) (S3) v\u011c i,\u011cj = Tanh(W [x\u011c i || \u011c k \u2208N (\u011ci) r\u011c k ,\u011ci \u2022 v t\u22121 G k ,\u011ci ]) (S4) v t\u011c i,\u011cj = (1 \u2212 z\u011c i,\u011cj ) s\u011c i,\u011cj + z\u011c i,\u011cj v\u011c i,\u011cj(S5)\nE ALGORITHM FOR HIERARCHICALLY DECODING STRUCTURED RNA a M TI refers to the threshold which set the maximum allowed number of topological prediction steps; M SI is another threshold to limit the length of each decoded nucleotide segment.", "n_publication_ref": 1, "n_figure_ref": 0}, {"heading": "H DETAILS FOR PARAMETERIZING PRIOR DISTRIBUTION USING NORMALIZING FLOW", "text": "A normalizing flow involves a series of bijective transformation with tractable Jacobian logdeterminant, to map an observed datapoint x \u223c p \u03b8 (x) from a complex distribution to a simpler one, such as the standard normal distribution.\nConsidering the simplified case where we have a single bijective function f \u03b8 : Z \u2192 X to map some simple latent variables z to observed datapoint x, then, using the change of variable theorem, the likelihood of the observed datapoint can be evaluated as:\np \u03b8 (x) = p z (f \u22121 \u03b8 (x))|det \u2202f \u22121 \u03b8 (x) \u2202x | (S6)\nwhere p z (.) denotes some simple base distribution, e.g. N (0; I). Then, it becomes clear the efficiency of this scheme heavily relies on the efficiency of inverting the forward mapping f \u03b8 as well as computing its Jacobian log-determinant.\nIn this project, we use a type of continuous normalizing flow (CNF) which simplifies the above mentioned computation (Chen et al., 2018). Consider a time continuous dynamics f \u03c8 (z(t), t) of some intermediate data representation z(t), and again z(t 0 ) \u223c p z (.), the transformation of variable, along with its inverse mapping, can be expressed as:\nz z(t 1 ) = z(t 0 ) + t1 t0 f \u03c8 (z(t), t)dt (S7) z(t 0 ) = z(t 1 ) + t0 t1 f \u03c8 (z(t), t)dt (S8)\nand the change of probability density can be expressed as:\nlog p \u03c8 (z) = log p z (z(t 0 )) \u2212 t1 t0 tr( \u2202f \u03b8 \u2202z(t) )dt (S9)\nNote that the invertibility issue is no longer a concern under some mild constraints (Chen et al., 2018). Also, Eq. S9 only involves a more light-weight trace operation on the Jacobian rather than evaluating its log-determinant.\nTherefore, we learn a parameterized prior using a CNF, and observe the decomposition of the KL term in the VAE objective:\nKL(q \u03c6 (z|x)|p \u03c8 (z)) = \u2212E z\u223cq \u03c6 (z|x) [p \u03c8 (z)] \u2212 H[q \u03c6 (z|x)](S10)\nTherefore, during training our CNF parameterized with \u03c8 works on the transformation of complex latent encodings z \u223c q \u03c6 (z|x) to some simple z(t 0 ) \u223c N (0; I), with an exact likelihood described by Eq. S9 and integrated into Eq. S10 for the complete training objective. During inference, we simply sample z t0 \u223c N (0; I), and use our CNF to reversely transform it to z \u223c p \u03c8 (.) which should be closer to the approximate posterior. J HYPERPARAMETERS  The optimization takes place in the latent space of HierVAE, starting from the initial encoding of a random RNA molecule in the test set, and at each step altering the latent encoding by using activation maximization on the embedding classifier. The trajectory of generated RNAs is shown in the order of left to right and top to bottom, and the field PRED indicates that the probability of binding, as predicted by another external full classifier on the decoded molecular structure, is overall increasing as the decoded RNA structures smoothly evolving.", "n_publication_ref": 2, "n_figure_ref": 0}, {"heading": "F DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO LINEARIZED DECODING PROCEDURES", "text": "When decoding from the joint vocabulary of sequence and dot-bracket structure ({A, C, G, U } \u00d7 {., (, )}), whenever a nucleotide nuc i with a left bracket is sampled at step i, we append them to a stack, i.e. {(nuc i0 , i 0 ) . . . (nuc i , i)}. Then, at decode step j,\n\u2022 if |i \u2212 j| \u2264 3, a proper mask will be added to the categorical logits of the vocabulary, to avoid sampling any nucleotides with right brackets, which means only an unpaired nucleotide or one that comes with a left bracket can be sampled; \u2022 if |i \u2212 j| > 3, a mask will be applied to make sure that only a nucleotide complementary to nuc i can be sampled with the right bracket. Sampling nucleotides with other forms of structures are allowed.\nAs soon as a nucleotide with a closing right bracket is sampled, we pop out (nuc i , i) from the stack. The special symbol for stop decoding can only be sampled when the stack has become empty.", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "G DETAILS FOR APPLYING RNA STRUCTURAL CONSTRAINTS TO HIERARCHICAL DECODING PROCEDURES", "text": "Additional constraints to be enforced during the hierarchical decoding process to ensure the validity of the decoded RNA secondary structure. Recall in section 3.2 that three types of predictions are involved with the hierarchical decoding, therefore, each type is associated with its own set of rules.\nAll set of rules can be observed by adding proper masks to the categorical logits before sampling, which are detailed below.\nConstraints for making topological prediction, when the current node is\n\u2022 stem node, then the algorithm always expands to a new node upon its first visit, or backtracks to its parent node upon re-visit; \u2022 hairpin node, then the algorithm always backtracks; \u2022 internal loop, then the algorithm acts similarly as for stem node; \u2022 multi-loop, then the algorithm always expands upon first visit and the next re-visit. Further re-visits to the same multi-loop node are not regulated.\nConstraints for predicting new tree node, when the current node is \u2022 stem node, then its child node when exists can be either a hairpin loop, an internal loop, or a multi-loop; \u2022 hairpin node, internal loop or multi-loop, then its child node must be a stem node.\nConstraints for decoding nucleotide segment. Due to the property of non-empty intersection between adjacent subgraphs, the start token for decoding a segment at the current node, is always the last nucleotide decoded at the last node. Therefore, without explicitly mentioning, the algorithm needs to decode at least one new nucleotide at each segment. When the current node is \u2022 stem node, and if it is upon its first visit (i.e. decoding the first segment of a stem), then there is no for constraints. Otherwise, upon its re-visit, the algorithm needs to decode exactly the complementary bases and in the reverse order, according to the first decoded segment; \u2022 hairpin node, then the decoder needs to decode at least four nucleotides before seeing the stop symbol, unless the hairpin is also the root node. \u2022 internal loop node, and if it is upon its first, then constraint is not necessary. Otherwise, upon its revisit, the algorithm needs to decode at least one unpaired nucleotide on condition that the first decoded internal loop segment does not contain any unpaired nucleotides; \u2022 multi-loop node, then there is no need for constraints. HuR 0.880 \u00b1 0.000 0.880 \u00b1 0.000 0.880 \u00b1 0.000 0.888 \u00b1 0.002 PTB 0.900 \u00b1 0.000 0.910 \u00b1 0.000 0.910 \u00b1 0.000 0.910 \u00b1 0.000 QKI 0.820 \u00b1 0.000 0.830 \u00b1 0.000 0.825 \u00b1 0.002 0.830 \u00b1 0.000 Vts1 0.900 \u00b1 0.000 0.908 \u00b1 0.002 0.637 \u00b1 0.079 0.910 \u00b1 0.000 RBMY 0.905 \u00b1 0.002 0.880 \u00b1 0.003 0.802 \u00b1 0.055 0.870 \u00b1 0.002 SF2 0.890 \u00b1 0.000 0.900 \u00b1 0.000 0.900 \u00b1 0.000 0.900 \u00b1 0.000 SLBP 0.777 \u00b1 0.002 0.790 \u00b1 0.000 0.797 \u00b1 0.002 0.797 \u00b1 0.002 ", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "L END-TO-END RNACOMPETE-S CLASSIFIERS", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}, {"heading": "M ALTERNATIVE HIERVAE TRAINING ON RNACOMPETE-S", "text": "", "n_publication_ref": 0, "n_figure_ref": 0}], "references": [{"title": "", "journal": "Nucleic Acids Research", "year": "2016", "authors": "Premanand Bronwen L Aken; Wasiu Achuthan;  Akanni; Friederike M Ridwan Amode; Jyothish Bernsdorff; Konstantinos Bhai; Denise Billis; Carla Carvalho-Silva; Peter Cummins;  Clapham"}, {"title": "Importance Weighted Autoencoders", "journal": "", "year": "2016", "authors": "Yuri Burda; Roger B Grosse; Ruslan Salakhutdinov"}, {"title": "Neural Ordinary Differential Equations", "journal": "", "year": "2018", "authors": "Yulia Tian Qi Chen; Jesse Rubanova; David Bettencourt;  Duvenaud"}, {"title": "Ilya Sutskever, and Pieter Abbeel. Variational Lossy Autoencoder", "journal": "", "year": "2017", "authors": "Xi Chen; P Diederik; Tim Kingma; Yan Salimans; Prafulla Duan; John Dhariwal;  Schulman"}, {"title": "RNA secondary structure prediction by learning unrolled algorithms", "journal": "", "year": "", "authors": "Xinshi Chen; Yu Li; Ramzan Umarov; Xin Gao; Le Song"}, {"title": "Learning phrase representations using RNN encoder-decoder for statistical machine translation", "journal": "", "year": "2014", "authors": "Kyunghyun Cho; Bart Van Merrienboer; Dzmitry Aglar G\u00fcl\u00e7ehre; Fethi Bahdanau; Holger Bougares; Yoshua Schwenk;  Bengio"}, {"title": "Design of RNAs: comparing programs for inverse RNA folding", "journal": "Briefings in Bioinformatics", "year": "2017-01", "authors": "Alexander Churkin; Vladimir Matan Drory Retwitzer; Yann Reinharz; J\u00e9r\u00f4me Ponty; Danny Waldisp\u00fchl;  Barash"}, {"title": "RNAcompete-S: Combined RNA sequence/structure preferences for RNA binding proteins derived from a single-step in vitro selection", "journal": "Methods", "year": "2017", "authors": "K B Cook; S Vembu; K C H Ha; H Zheng; K U Laverty; T R Hughes; D Ray; Q D Morris"}, {"title": "Central Dogma of Molecular Biology", "journal": "Nature", "year": "1970", "authors": "Francis Crick"}, {"title": "Evaluation of several lightweight stochastic context-free grammars for RNA secondary structure prediction", "journal": "BMC Bioinformatics", "year": "2004", "authors": "Robin D Dowell; Sean R Eddy"}, {"title": "Convolutional Networks on Graphs for Learning Molecular Fingerprints", "journal": "", "year": "2015", "authors": "David Duvenaud; Dougal Maclaurin; Jorge Aguilera-Iparraguirre; Rafael G\u00f3mez-Bombarelli; Timothy Hirzel; Al\u00e1n Aspuru-Guzik; Ryan P Adams"}, {"title": "RNA sequence analysis using covariance models", "journal": "Nucleic Acids Research", "year": "1994", "authors": "Sean R Eddy; Richard Durbin"}, {"title": "ProtTrans: Towards Cracking the Language of Life's Code Through Self-Supervised Deep Learning and High Performance Computing", "journal": "bioRxiv", "year": "2020", "authors": "Ahmed Elnaggar; Michael Heinzinger; Christian Dallago; Ghalia Rehawi; Yu Wang; Llion Jones; Tom Gibbs; Tamas Feher; Christoph Angerer; Martin Steinegger; Debsindhu Bhowmik; Burkhard Rost"}, {"title": "Neural Message Passing for Quantum Chemistry", "journal": "", "year": "2017", "authors": "Justin Gilmer; Samuel S Schoenholz; Patrick F Riley; Oriol Vinyals; George E Dahl"}, {"title": "Automatic chemical design using a data-driven continuous representation of molecules", "journal": "ACS central science", "year": "2018", "authors": "Rafael G\u00f3mez-Bombarelli; Jennifer N Wei; David Duvenaud; Jos\u00e9 Miguel Hern\u00e1ndez-Lobato; Benjam\u00edn S\u00e1nchez-Lengeling; Dennis Sheberla; Jorge Aguilera-Iparraguirre; Timothy D Hirzel; P Ryan; Al\u00e1n Adams;  Aspuru-Guzik"}, {"title": "FFJORD: free-form continuous dynamics for scalable reversible generative models", "journal": "", "year": "2019", "authors": "Will Grathwohl; Ricky T Q Chen; Jesse Bettencourt; Ilya Sutskever; David Duvenaud"}, {"title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders", "journal": "", "year": "2019", "authors": "Junxian He; Daniel Spokoyny; Graham Neubig; Taylor Berg-Kirkpatrick"}, {"title": "Long short-term memory", "journal": "Neural Computing", "year": "1997", "authors": "S Hochreiter; J Schmidhuber"}, {"title": "Junction Tree Variational Autoencoder for Molecular Graph Generation", "journal": "", "year": "2018", "authors": "Wengong Jin; Regina Barzilay; Tommi S Jaakkola"}, {"title": "Rfam 13.0: shifting to a genome-centric resource for non-coding RNA families", "journal": "Nucleic Acids Research", "year": "", "authors": "Ioanna Kalvari; Joanna Argasinska; Natalia Quinones-Olvera; Eric P Nawrocki; Elena Rivas; R Sean; Alex Eddy;  Bateman; D Robert; Anton I Finn;  Petrov"}, {"title": "Auto-Encoding Variational Bayes", "journal": "", "year": "2014", "authors": "P Diederik; Max Kingma;  Welling"}, {"title": "Improved variational inference with inverse autoregressive flow", "journal": "", "year": "2016", "authors": "P Durk; Tim Kingma; Rafal Salimans; Xi Jozefowicz; Ilya Chen; Max Sutskever;  Welling"}, {"title": "Grammar Variational Autoencoder", "journal": "", "year": "2017", "authors": "Matt J Kusner; Brooks Paige; Jos\u00e9 Miguel Hern\u00e1ndez-Lobato "}, {"title": "Gated Graph Sequence Neural Networks", "journal": "", "year": "2016", "authors": "Yujia Li; Daniel Tarlow; Marc Brockschmidt; Richard S Zemel"}, {"title": "Constrained Graph Variational Autoencoders for Molecule Design", "journal": "", "year": "2018", "authors": "Qi Liu; Miltiadis Allamanis; Marc Brockschmidt; Alexander L Gaunt"}, {"title": "ViennaRNA Package 2.0", "journal": "Algorithms for Molecular Biology", "year": "2011", "authors": "R Lorenz; S H Bernhart; C Honer Zu Siederdissen; H Tafer; C Flamm; P F Stadler; I L Hofacker"}, {"title": "Incorporating chemical modification constraints into a dynamic programming algorithm for prediction of RNA secondary structure", "journal": "Proceedings of the National Academy of Sciences of the United States of America", "year": "2004", "authors": "H David; Matthew D Mathews; Jessica L Disney; Susan J Childs; Michael Schroeder; Douglas H Zuker;  Turner"}, {"title": "Infernal 1.1: 100-fold faster RNA homology searches", "journal": "Bioinformatics", "year": "", "authors": "Eric P Nawrocki; Sean R Eddy"}, {"title": "Augmented base pairing networks encode RNA-small molecule binding preferences", "journal": "Nucleic Acids Research", "year": "", "authors": "Carlos Oliver; Vincent Mallet; Roman Sarrazin Gendron; Vladimir Reinharz; L William; Nicolas Hamilton; J\u00e9r\u00f4me Moitessier;  Waldisp\u00fchl"}, {"title": "mRNA vaccines -a new era in vaccinology", "journal": "Nature Reviews Drug Discovery", "year": "2018", "authors": "Norbert Pardi; Michael J Hogan; Frederick W Porter; Drew Weissman"}, {"title": "The RNA 3D Motif Atlas: Computational methods for extraction, organization and evaluation of RNA motifs", "journal": "Methods", "year": "2016", "authors": "Lorena G Parlea; Blake A Sweeney; Maryam Hosseini-Asanjan; Craig L Zirbel; Neocles B Leontis"}, {"title": "Rapid and systematic analysis of the RNA recognition specificities of RNAbinding proteins", "journal": "Nature Biotechnology", "year": "2009", "authors": "D Ray; H Kazan; E T Chan; L Pena Castillo; S Chaudhry; S Talukder; B J Blencowe; Q Morris; T R Hughes"}, {"title": "A compendium of RNA-binding motifs for decoding gene regulation", "journal": "Nature", "year": "2013", "authors": "D Ray; H Kazan; K B Cook; M T Weirauch; H S Najafabadi; X Li; S Gueroussov; M Albu; H Zheng; A Yang; H Na; M Irimia; L H Matzat; R K Dale; S A Smith; C A Yarosh; S M Kelly; B Nabet; D Mecenas; W Li; R S Laishram; M Qiao; H D Lipshitz; F Piano; A H Corbett; R P Carstens; B J Frey; R A Anderson; K W Lynch; L O Penalva; E P Lei; A G Fraser; B J Blencowe; Q D Morris; T R Hughes"}, {"title": "On the convergence of adam and beyond", "journal": "", "year": "2018", "authors": "J Sashank; Satyen Reddi; Sanjiv Kale;  Kumar"}, {"title": "Mining for recurrent long-range interactions in RNA structures reveals embedded hierarchies in network families", "journal": "Nucleic Acids Research", "year": "2018", "authors": "Vladimir Reinharz; Antoine Soul\u00e9; Eric Westhof; J\u00e9r\u00f4me Waldisp\u00fchl; Alain Denise"}, {"title": "Variational Inference with Normalizing Flows", "journal": "", "year": "2015", "authors": "Danilo Jimenez Rezende; Shakir Mohamed"}, {"title": "Biological Structure and Function Emerge from Scaling Unsupervised Learning to 250 Million Protein Sequences", "journal": "bioRxiv", "year": "2019", "authors": "Alexander Rives; Joshua Meier; Tom Sercu; Siddharth Goyal; Zeming Lin; Demi Guo; Myle Ott; C Lawrence Zitnick; Jerry Ma; Rob Fergus"}, {"title": "Learning to Design RNA", "journal": "", "year": "2019", "authors": "Frederic Runge; Danny Stoll; Stefan Falkner; Frank Hutter"}, {"title": "Stochastic Sampling of Structural Contexts Improves the Scalability and Accuracy of RNA 3D Module Identification", "journal": "Springer International Publishing", "year": "2020", "authors": "Roman Sarrazin-Gendron; Hua-Ting Yao; Vladimir Reinharz; Carlos G Oliver; Yann Ponty; J\u00e9r\u00f4me Waldisp\u00fchl"}, {"title": "Developing mRNAvaccine technologies", "journal": "RNA Biology", "year": "2012", "authors": "Thomas Schlake; Andreas Thess; Mariola Fotin-Mleczek; Karl-Josef Kallen"}, {"title": "Modeling Relational Data with Graph Convolutional Networks", "journal": "", "year": "2018", "authors": " Michael Sejr; Thomas N Schlichtkrull; Peter Kipf; Rianne Bloem;  Van Den; Ivan Berg; Max Titov;  Welling"}, {"title": "RNA secondary structure prediction using an ensemble of two-dimensional deep neural networks and transfer learning", "journal": "Nature Communications", "year": "2019", "authors": "Jaswinder Singh; Jack Hanson; Kuldip Paliwal; Yaoqi Zhou"}, {"title": "RNA sequence-and shape-dependent recognition by proteins in the ribonucleoprotein particle", "journal": "EMBO reports", "year": "2005", "authors": "Richard Stefl; Lenka Skrisovska; H T Fr\u00e9d\u00e9ric;  Allain"}, {"title": "ZINC 15 -Ligand Discovery for Everyone", "journal": "Journal of Chemical Information and Modeling", "year": "2015", "authors": "Teague Sterling; John J Irwin"}, {"title": "A Deep Learning Approach to Antibiotic Discovery", "journal": "Cell", "year": "2020", "authors": "J M Stokes; K Yang; K Swanson; W Jin; A Cubillos-Ruiz; N M Donghia; C R Macnair; S French; L A Carfrae; Z Bloom-Ackermann; V M Tran; A Chiappino-Pepe; A H Badran; I W Andrews; E J Chory; G M Church; E D Brown; T S Jaakkola; R Barzilay; J J Collins"}, {"title": "Attention is all you need", "journal": "", "year": "2017", "authors": "Ashish Vaswani; Noam Shazeer; Niki Parmar; Jakob Uszkoreit; Llion Jones; Aidan N Gomez; Lukasz Kaiser; Illia Polosukhin"}, {"title": "Graph Attention Networks", "journal": "", "year": "2018", "authors": "Petar Velickovic; Guillem Cucurull; Arantxa Casanova; Adriana Romero; Pietro Li\u00f2; Yoshua Bengio"}, {"title": "Graph neural representational learning of RNA secondary structures for predicting RNA-protein interactions", "journal": "Bioinformatics", "year": "2020", "authors": "Zichao Yan; William L Hamilton; Mathieu Blanchette"}, {"title": "PointFlow: 3D Point Cloud Generation With Continuous Normalizing Flows", "journal": "", "year": "2019", "authors": "Guandao Yang; Xun Huang; Zekun Hao; Ming-Yu Liu; Serge J Belongie; Bharath Hariharan"}, {"title": "Graph Convolutional Policy Network for Goal-Directed Molecular Graph Generation", "journal": "", "year": "2018", "authors": "Jiaxuan You; Bowen Liu; Zhitao Ying; Vijay S Pande; Jure Leskovec"}], "figures": [{"figure_label": "1", "figure_type": "", "figure_id": "fig_0", "figure_caption": "Figure 1 :1Figure 1: Hierarchical encoding. Panel (A)shows the minimum free energy structure (for illustration purpose only) which is represented as a graph. In panel (B), the original molecule is decomposed by grouping structurally adjacent nucleotides into subgraphs, giving rise to a higherorder junction tree representation.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_1", "figure_caption": "(A), to determine if the decoder should expand to a new tree node or backtrack to its parent node, based on MLP topo (h\u011c t,i ); \u2022 tree node prediction in Figure 2 (B), on condition that a new tree node is needed due to a possible topological expansion. This procedure determines the label of the new tree node from the set of {S, H, I, M }, based on MLP node (h\u011c t,i ); \u2022 nucleotide segment decoding in Figure 2 (C), using a single-layered LSTM, whose initial hidden state is MLP dec", "figure_data": ""}, {"figure_label": "3", "figure_type": "", "figure_id": "fig_2", "figure_caption": "Figure 3 :3Figure 3: RNAs generated with structural constraints from HierVAE on a random axis in the latent space (step size: 1e-4), for short (top), medium-length (middle), and long (bottom) RNAs. The Free energy (FE) and its deviation (DEV) from the MFE are given for each structure. which means only the canonical base-pairs and Wobble base-pairs are allowed, i.e. [A-U], [G-C] and [G-U];(2) hairpin loop should have a minimum of three unpaired nucleotides, i.e. for any two paired bases at position i and j, |i \u2212 j| > 3; (3) each nucleotide can only be paired once, and overlapping pairs are disallowed.", "figure_data": ""}, {"figure_label": "S1", "figure_type": "", "figure_id": "fig_3", "figure_caption": "Figure S1 :S1FigureS1: A nested RNA secondary structure can be represented by: (A) dotbracket annotation, where base-pairs corresponding to matching parentheses, or (B) a molecular planar graph with two types of edges, corresponding to consecutive nucleotides (backbone) and basepairing interactions, or (C) a junction tree where node are labeled as stems (S), hairpins (H), internal loops (I), or multiloops (M), and edges correspond to the connections between these elements. All three forms are equivalent.", "figure_data": ""}, {"figure_label": "", "figure_type": "", "figure_id": "fig_4", "figure_caption": "Figure S2: This figure contains information of the unlabeled RNA dataset. (A) The number of hypernodes appears to grow linearly with the length of RNA, and (B) the junction tree height also grows as the length increases but on a more moderate scale. (C) and (D) have shown bar-plots of the number of hypernodes and tree height, indicating that the junction tree of RNA can take on significant depth hence contributing to the diversity and complexity of RNA secondary structures represented in this dataset.", "figure_data": ""}, {"figure_label": "S4", "figure_type": "", "figure_id": "fig_5", "figure_caption": "Figure S4 :S4Figure S4: Neighborhood visualization of tRNA-Cys 6 which is marked by the red bounding box in the center and the walk in the latent space takes place on two random orthogonal axes. Note that actual secondary structure of tRNA-Cys plotted in the figure is different compared to the one deposited online due to the prediction of RNAfold.", "figure_data": ""}, {"figure_label": "1", "figure_type": "table", "figure_id": "tab_0", "figure_caption": "We evaluate RNAs sampled from the posterior distribution: q(z|x), with a held-out test set of 20,000 RNAs. Each molecule is encoded and decoded 5 times. We also evaluate samples from the prior distribution: N (0, I) subject to the transformation of a latent CNF, where we sample 10,000 encodings and each encoding is decoded 10 times. Normed refers to length normalized FE DEV.", "figure_data": "Posterior DecodingPrior DecodingModelValidity\u2191 FE DEV\u2193 Normed\u2193 Diversity\u2191 Validity\u2191 FE DEV\u2193 Normed\u2193 Diversity\u2191Constrained & StochasticLSTMVAE 99.47%18.1970.0616.786 99.50%18.5360.0626.789GraphVAE 99.47%17.2750.0616.790 99.36%18.5340.0656.791HierVAE99.97%8.6780.0356.787 99.86%8.6760.0366.791Unconstrained & StochasticLSTMVAE 62.98%8.7000.0486.791 62.58%9.0600.0496.793GraphVAE 65.79%9.5080.0516.792 63.45%10.1660.0556.794HierVAE94.51%8.2570.0356.787 92.75%7.8970.0376.791"}, {"figure_label": "2", "figure_type": "table", "figure_id": "tab_1", "figure_caption": "Training semi-supervised HierVAE on labeled RNAcompete-S dataset. A test split is used to evaluate the accuracy of embedding classifiers and RNAs decoded from the posterior distribution under two settings: constrained and stochastic (C& S), unconstrained and deterministic (NC&D). RECON ACC refers to reconstruction accuracy which measures the percentage of RNA molecules decoded exactly as the input.", "figure_data": "TestPost C&SPost NC&DDataset AUROC Valid FE DEV RECON ACCValid FE DEV RECON ACCHuR0.884 100%0.42655.85% 99.34%0.26968.73%PTB0.907 100%0.40955.24% 92.07%0.57051.96%QKI0.824 100%0.43955.80% 99.22%0.29666.83%Vts10.900 100%0.53947.96% 98.90%0.36760.60%RBMY0.878 100%0.63448.09% 98.64%0.41961.84%SF20.900 100%0.61644.57% 98.88%0.40957.15%SLBP0.792 100%0.45953.67% 98.60%0.30665.49%"}, {"figure_label": "3", "figure_type": "table", "figure_id": "tab_2", "figure_caption": "", "figure_data": ": Designing novel RNA withhigher chances of RBP binding.Dataset Success ImprovementHuR96.88% 0.561\u00b10.280PTB92.63% 0.561\u00b10.320QKI90.82% 0.326\u00b10.252Vts154.63% 0.197\u00b10.388RBMY 84.33% 0.457\u00b10.395SF298.91% 0.655\u00b10.239SLBP83.98% 0.309\u00b10.297"}, {"figure_label": "", "figure_type": "table", "figure_id": "tab_3", "figure_caption": "Algorithm 1: DFS decode RNA secondary structure1 Given: z T , z G , M TI, M SI a 2 Initialize: stack \u2190 [ ] 3 function decode(z T , z G ) 4 root \u2190 sample(MLP node (z T )) ;", "figure_data": "5root.add incoming message(z T ) ;6stack.push((root, 0)) ;7t \u2190 0 ;8while t \u2264 M TI and stack.size() \u2265 1 do9c node, last nuc \u2190 stack.get last item();10all msg \u2190 {msg | \u2200msg \u2208 c node.get incoming message()} ;11local f ield \u2190 [c node.label() || c node.get segment features()] ;12new msg \u2190 T-GRU(local f ield, all msg) ;// topological prediction;// nucleotide segment prediction14new msg, last nuc, decoded segment, segment f eatures \u2190decode segment(new msg, last nuc, z T , z G , M SI)  *  ;15c node.add decoded segment(decoded segment) ;16c node.add segment f eatures(segment f eatures) ;17if is backtrack = T rue then// backtrack to the parent node22stack.pop() ;23else// predict and expand to new tree node25new node.add incoming message(new msg) ;26new node.add neighbor((c node, last nuc)) ;27stack.push(new node) ;28end29t \u2190 t + 1 ;30end31return root ;"}, {"figure_label": "S1S2", "figure_type": "table", "figure_id": "tab_4", "figure_caption": "Hyperparameters for training VAE and full classifier models. Note that hidden units refer to the dimensionality of encoders and decoders from LSTMVAE, GraphVAE as well as HierVAE models. Dropout is applied to the embedding MLP classifier in case of training semi-supervised VAEs, which contains one hidden layer. Performance of simple MLP classifiers on top of fixed latent embeddings from VAE models, which have been pretrained on the unlabeled RNA dataset as originally shown in Table1.", "figure_data": "O NEIGHBORHOOD VISUALIZATION OF A CYSTEINE-CARRYINGTRANSFER-RNAfor VAE modelslatent dimensionality128hidden units512G-MPNN iterations5T-GRU iterations10learning rate1e-3batch size32optimizerAMSGrad (Reddi et al., 2018)dropout ratio0.2M TI300S TI (hierarchical decoder) 100S TI (linearized decoder)1000for full classifier models(overriding some above hyperparameters)learning rate2e-4epochs200early stopping epochs5K RNACOMPETE-S CLASSIFIERS ON PRETRAINED AND FIXED VAEEMBEDDINGSRBPLSTMVAE GraphVAE HierVAEHuR0.8670.8580.860PTB0.8860.8780.883QKI0.7480.7560.746Vts10.7750.7580.774RBMY0.7340.7250.731SF20.8670.8620.866SLBP0.7490.7370.747"}], "formulas": [], "doi": ""}