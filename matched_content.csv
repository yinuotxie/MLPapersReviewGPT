title,content,review
C-Learning: Learning to Achieve Goals via Recursive Classification,"[TITLE]
C-Learning: Learning to Achieve Goals via Recursive Classification

[ABSTRACT]
We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.

[CAPTIONS]
Figure 1: Testing Hypotheses about Q-learning: (Left) As predicted, Q-values often sum to less than 1. (Right) The performance of Q-learning is sensitive to the relabeling ratio. Our analysis predicts that the optimal relabeling ratio is approximately λ = 1 2 (1 + γ). C-learning (dashed orange) does not require tuning this ratio and outperforms Q-learning, even when the relabeling ratio for Q-learning is optimally chosen.
Figure 2: Predicting the Future: C-learning makes accurate predictions of the expected future state across a range of tasks and discount values. In contrast, learn- ing a 1-step dynamics model and unrolling that model results in high error for large discount values.
Figure 3: Goal-conditioned RL: C-learning is competitive with prior goal-conditioned RL methods across a suite of benchmark tasks, without requiring careful tuning of the relabeling distribution.
Figure 4: Q-learning is sensitive to the relabeling ratio.

[CONTENT]
Section Title: INTRODUCTION
  INTRODUCTION In this paper, we aim to reframe the goal-conditioned reinforcement learning (RL) problem as one of predicting and controlling the future state of the world. This reframing is useful not only because it suggests a new algorithm for goal-conditioned RL, but also because it explains a commonly used heuristic in prior methods, and suggests how to automatically choose an important hyperparameter. The problem of predicting the future amounts to learning a probability density function over future states, agnostic of the time that a future state is reached. The future depends on the actions taken by the policy, so our predictions should depend on the agent's policy. While we could simply witness the future, and fit a density model to the observed states, we will be primarily interested in the following prediction question: Given experience collected from one policy, can we predict what states a different policy will visit? Once we can predict the future states of a different policy, we can control the future by choosing a policy that effects a desired future. While conceptually similar to Q-learning, our perspective is different in that we make no reliance on reward functions. Instead, an agent can solve the prediction problem before being given a reward function, similar to models in model-based RL. Reward functions can require human supervision to construct and evaluate, so a fully autonomous agent can learn to solve this prediction problem before being provided any human supervision, and reuse its predictions to solve many different downstream tasks. Nonetheless, when a reward function is provided, the agent can estimate its expected reward under the predicted future state distribution. This perspective is different from prior approaches. For example, directly fitting a density model to future states only solves the prediction problem in the on-policy setting, precluding us from predicting where a different policy will go. Model-based approaches, which learn an explicit dynamics model, do allow us to predict the future state distribution of different policies, but require a reward function or distance metric to learn goal-reaching policies for controlling the future. Methods based on temporal difference (TD) learning ( Sutton, 1988 ) have been used to predict the future state distribution ( Dayan, 1993 ;  Szepesvari et al., 2014 ;  Barreto et al., 2017 ) and to learn goal-reaching policies ( Kaelbling, 1993 ;  Schaul et al., 2015 ). Section 3 will explain why these approaches do not learn a true Q function in continuous environments with sparse rewards, and it remains unclear what the learned Q function corresponds to. In contrast, our method will estimate a well defined classifier. Since it is unclear how to use Q-learning to estimate such a density, we instead adopt a contrastive approach, learning a classifier to distinguish ""future states"" from random states, akin to  Gutmann & Hyvärinen (2010) . After learning this binary classifier, we apply Bayes' rule to obtain a probability density function for the future state distribution, thus solving our prediction problem. While this initial approach requires on-policy data, we then develop a bootstrapping variant for estimating the future state distribution for different policies. This bootstrapping procedure is the core of our goal- conditioned RL algorithm. The main contribution of our paper is a reframing of goal-conditioned RL as estimating the probabil- ity density over future states. We derive a method for solving this problem, C-learning, which we use to construct a complete algorithm for goal-conditioned RL. Our reframing lends insight into goal- conditioned Q-learning, leading to a hypothesis for the optimal ratio for sampling goals, which we demonstrate empirically. Experiments demonstrate that C-learning more accurately estimates the density over future states, while remaining competitive with recent goal-conditioned RL methods across a suite of simulated robotic tasks.

Section Title: RELATED WORK
  RELATED WORK Common goal-conditioned RL algorithms are based on behavior cloning ( Ghosh et al., 2019 ;  Ding et al., 2019 ; Gupta et al., 2019;  Eysenbach et al., 2020 ;  Lynch et al., 2020 ;  Oh et al., 2018 ;  Sun et al., 2019 ), model-based approaches ( Nair et al., 2020 ;  Ebert et al., 2018 ), Q-learning ( Kaelbling, 1993 ;  Schaul et al., 2015 ;  Pong et al., 2018 ), and semi-parametric planning ( Savinov et al., 2018 ;  Eysenbach et al., 2019 ;  Nasiriany et al., 2019 ;  Chaplot et al., 2020 ). Most prior work on goal- conditioned RL relies on manually-specified reward functions or distance metric, limiting the ap- plicability to high-dimensional tasks. Our method will be most similar to the Q-learning methods, which are applicable to off-policy data. These Q-learning methods often employ hindsight relabel- ing ( Kaelbling, 1993 ;  Andrychowicz et al., 2017 ), whereby experience is modified by changing the commanded goal. New goals are often taken to be a future state or a random state, with the precise ratio being a sensitive hyperparameter. We emphasize that our discussion of goal sampling concerns relabeling previously-collected experience, not on the orthogonal problem of sampling goals for exploration ( Pong et al., 2018 ;  Fang et al., 2019 ;  Pitis et al., 2020 ). Our work is closely related to prior methods that use TD-learning to predict the future state distri- bution, such as successor features ( Dayan, 1993 ;  Barreto et al., 2017 ; 2019;  Szepesvari et al., 2014 ) and generalized value functions ( Sutton & Tanner, 2005 ;  Schaul et al., 2015 ;  Schroecker & Isbell, 2020 ). Our approach bears a resemblance to these prior TD-learning methods, offering insight into why they work and how hyperparameters such as the goal-sampling ratio should be selected. Our approach differs in that it does not require a reward function or manually designed relabeling strate- gies, with the corresponding components being derived from first principles. While prior work on off-policy evaluation ( Liu et al., 2018 ;  Nachum et al., 2019 ) also aims to predict the future state distribution, our work differs is that we describe how to control the future state distribution, leading to goal-conditioned RL algorithm. Our approach is similar to prior work on noise contrastive estimation ( Gutmann & Hyvärinen, 2010 ), mutual-information based representation learning ( Oord et al., 2018 ;  Nachum et al., 2018 ), and vari- ational inference methods ( Bickel et al., 2007 ;  Uehara et al., 2016 ;  Dumoulin et al., 2016 ;  Huszár, 2017 ;  Sønderby et al., 2016 ). Like prior work on the probabilistic perspective on RL ( Kappen, 2005 ;  Todorov, 2008 ;  Theodorou et al., 2010 ;  Ziebart, 2010 ;  Rawlik et al., 2013 ;  Ortega & Braun, 2013 ;  Levine, 2018 ), we treat control as a density estimation problem, but our main contribution is or- thogonal: we propose a method for estimating the future state distribution, which can be used as a subroutine in both standard RL and these probabilistic RL methods.

Section Title: PRELIMINARIES
  PRELIMINARIES We start by introducing notation and prior approaches to goal-conditioned RL. We define a con- trolled Markov process by an initial state distribution p 1 (s 1 ) and dynamics function p(s t+1 | s t , a t ). We control this process by a Markovian policy π θ (a t | s t ) with parameters θ. We use π θ (a t | s t , g) to denote a goal-oriented policy, which is additionally conditioned on a goal g ∈ S. We use s t+ to denote the random variable representing a future observation, defined by the following distribution: Definition 1. The future γ−discounted state density function is where s t+∆ denotes the state exactly ∆ in the future, and constant (1 − γ) ensures that this density function integrates to 1. This density reflects the states that an agent would visit if we collected many infinite-length trajec- tories and weighted states in the near-term future more highly. Equivalently, p(s t+ ) can be seen as the distribution over terminal states we would obtain if we (hypothetically) terminated episodes at a random time step, sampled from a geometric distribution. We need not introduce a reward function to define the problems of predicting and controlling the future. In discrete state spaces, we can convert the problem of estimating the future state distribution into a RL problem by defining a reward function r st+ (s t , a t ) = 1(s t = s t+ ), and terminating the episode when the agent arrives at the goal. The Q-function, which typically represents the expected discounted sum of future rewards, can then be interpreted as a (scaled) probability mass function: However, in continuous state spaces with some stochasticity in the policy or dynamics, the proba- bility that any state exactly matches the goal state is zero. Remark 1. In a stochastic, continuous environment, for any policy π the Q-function for the reward function r st+ = 1(s t = s t+ ) is always zero: Q π (s t , a t , s t+ ) = 0. This Q-function is not useful for predicting or controlling the future state distribution. Fundamen- tally, this problem arises because the relationship between the reward function, the Q function, and the future state distribution in prior work remains unclear. Prior work avoids this issue by manu- ally defining reward functions ( Andrychowicz et al., 2017 ) or distance metrics ( Schaul et al., 2015 ;  Pong et al., 2018 ;  Zhao et al., 2019 ;  Schroecker & Isbell, 2020 ). An alternative is to use hindsight relabeling, changing the commanded goal to be the goal actually reached. This form of hindsight relabeling does not require a reward function, and indeed learns Q-functions that are not zero ( Lin et al., 2019 ). However, taken literally, Q-functions learned in this way must be incorrect: they do not reflect the expected discounted reward. An alternative hypothesis is that these Q-functions reflect probability density functions over future states. However, this also cannot be true: Remark 2. For any MDP with the sparse reward function 1(s t = s t+ ) where the episode terminates upon reaching the goal, Q-learning with hindsight relabeling acquires a Q-function in the range Q π (s t , a t , s t+ ) ∈ [0, 1], but the probability density function p π + (s t+ | s t , a t ) has a range [0, ∞). For example, if the state space is S = [0, 1 2 ], then there must exist some state s t+ such that Q π (s t , a t , s t+1 ) ≤ 1 < p π + (s t+ = s t+ | s t , a t ). See Appendix H for two worked examples. Thus, Q-learning with hindsight relabeling also fails to learn the future state distribution. In fact, it is un- clear what quantity Q-learning with hindsight relabeling optimizes. In the rest of this paper, we will define goal reaching in continuous state spaces in a way that is consistent and admits well-defined solutions (Sec. 4), and then present a practical algorithm for finding these solutions (Sec. 5).

Section Title: FRAMING GOAL CONDITIONED RL AS DENSITY ESTIMATION
  FRAMING GOAL CONDITIONED RL AS DENSITY ESTIMATION This section presents a novel framing of the goal-conditioned RL problem, which resolves the ambi- guity discussed in the previous section. Our main idea is to view goal-conditioned RL as a problem of estimating the density p π + (s t+ | s t , a t ) over future states that a policy π will visit, a problem that Q-learning does not solve (see Section 3). Section 5 will then explain how to use this estimated distribution as the core of a complete goal-conditioned RL algorithm. Definition 2. Given policy π, the future state density estimation problem is to estimate the The next section will show how to estimate f π θ . Once we have found f π θ , we can determine the probability that a future state belongs to a set S t+ by integrating over that set: P(s t+ ∈ S t+ ) = f π θ (s t+ | s t , a t )1(s t+ ∈ S t+ )ds t+ . Appendix A discusses a similar relationship with partially observed goals. There is a close connection between this integral and a goal-conditioned Q-function: We now derive an algorithm (C-learning) for solving the future state density estimation problem (Def. 2). First (Sec. 5.1), we assume that the policy is fixed, and present on-policy and off-policy solutions. Based on these ideas, Section 5.2 builds a complete goal-conditioned RL algorithm for learning an optimal goal-reaching policy. Our algorithm bears a resemblance to Q-learning, and our derivation makes two hypotheses about when and where Q-learning will work best (Sec. 5.3). Rather than estimating the future state density di- rectly, we will estimate it indirectly by learning a classifier. Not only is classification generally an eas- ier problem than density estimation, but also it will allow us to develop an off-policy algorithm in the next section. We will call our approach C-learning. We start by deriving an on-policy Monte Carlo al- gorithm (Monte Carlo C-learning), and then mod- ify it to obtain an off-policy, bootstrapping algorithm (off-policy C-learning). After learning this classifier, we can apply Bayes' rule to convert its binary pre- dictions into future state density estimates. Given a distribution over state action pairs, p(s t , a t ), we de- fine the marginal future state distribution p(s t+ ) = p π + (s t+ | s t , a t )p(s t , a t )ds t da t . The classifier takes as input a state-action pair (s t , a t ) together with another state s t+ , and predicts whether s t+ was sampled from the future state density p π + (s t+ | s t , a t ) (F = 1) or the marginal state density p(s t+ ) (F = 0). The Bayes optimal classifier is Thus, using C π θ (F = 1 | s t , a t , s t+ ) to denote our learned classifier, we can obtain an estimate f π θ (s t+ | s t , a t ) for the future state density function using our classifier's predictions as follows: While our estimated density f θ depends on the marginal density p(s t+ ), our goal-conditioned RL algorithm (Sec. 5.2) will note require estimating this marginal density. In particular, we will learn a policy that chooses the action a t that maximizes this density, but the solution to this maximization problem does not depend on the marginal p(s t+ ). We now present an on-policy approach for learning the classifier, which we call Monte Carlo C- Learning. After sampling a state-action pair (s t , a t ) ∼ p(s t , a t ), we can either sample a future state s (1) t+ ∼ p π + (s t+ | s t , a t ) with a label F = 1, or sample s (0) t+ ∼ p(s t+ ) with a label F = 0. We then train the classifier maximize log likelihood (i.e., the negative cross entropy loss): To sample future states, we note that the density p π + (s t+ | s t , a t ) is a weighted mixture of distribu- tions p(s t+∆ | s t , a t ) indicating the future state exactly ∆ steps in the future: where GEOM is the geometric distribution. Thus, we sample a future state s t+ via ancestral sam- pling: first sample ∆ ∼ GEOM(1 − γ) and then, looking at the trajectory containing (s t , a t ), return the state that is ∆ steps ahead of (s t , a t ). We summarize Monte Carlo C-learning in Alg. 1. While conceptually simple, this algorithm requires on-policy data, as the distribution p π + (s t+ | s t , a t ) depends on the current policy π and the commanded goal. Even if we fixed the policy parameters, we cannot use experience collected when commanding one goal to learn a classifier for another goal. This limitation precludes an important benefit of goal-conditioned learning: the ability to readily share experience across tasks. To lift this limitation, the next section will develop a bootstrapped version of this algorithm that works with off-policy data. We now extend the Monte Carlo algorithm introduced above to work in the off-policy setting, so that we can estimate the future state density for different policies. In the off-policy setting, we are given a dataset of transitions (s t , a t , s t+1 ) and a new policy π, which we will use to generate actions for the next time step, a t+1 ∼ π(a t+1 | s t+1 ). The main challenge is sampling from p π + (s t+ | s t , a t ), which depends on the new policy π. We address this challenge in two steps. First, we note a recursive relationship between the future state density at the current time step and the next time step: p π + (s t+ = s t+ | s t , a t ) future state density at current time step future state density at next time step . (4) We can now rewrite our classification objective in Eq. 3 as This equation is different from the Monte Carlo objective (Eq. 3) because it depends on the new policy, but it still requires sampling from p π + (s t+ | s t+1 , a t+1 ), which also depends on the new policy. Our second step is to observe that we can estimate expectations that use p π + (s t+ | s t , a t ) by sampling from the marginal s t+ ∼ p(s t+ ) and then weighting those samples by an importance weight, which we can estimate using our learned classifier: The second equality is obtained by taking Eq. 2 and dividing both sides by p(s t+ ). In effect, these weights account for the effect of the new policy on the future state density. We can now rewrite our objective by substituting the identity in Eq. 6 for the p(s t+ ) term in the expectation in Eq. 5. The written objective is We use · sg as a reminder that the gradient of an importance-weighted objective should not de- pend on the gradients of the importance weights. Intuitively, this loss says that next states should be labeled as positive examples, states sampled from the marginal should be labeled as negative examples, but reweighted states sampled from the marginal are positive examples.

Section Title: Algorithm summary
  Algorithm summary Alg 2 reviews off policy C-learning, which takes as input a policy and a dataset of transitions. At each iteration, we sample a (s t , a t , s t+1 ) transition from the dataset, a potential future state s t+ ∼ p(s t+ ) and the next action a t+1 ∼ π(a t+1 | s t+1 , s t+ ). We compute the importance weight using the current estimate from the classifier, and then plug the importance weight into the loss from Eq. 3. We then update the classifier using the gradient of this objective. C-learning Bellman Equations. In Appendix D.1, we provide a convergence proof for off-policy C-learning in the tabular setting. Our proof hinges on the fact that the TD C-learning update rule has the same effect as applying the following (unknown) Bellman operator: This equation tells us that C-learning is equivalent to maximizing the reward function r st+ (s t , a t ) = p(s t+1 = s t+ | s t , a t )/p(s t+ ), but does so without having to estimate either the dynamics p(s t+1 | s t , a t ) or the marginal distribution p(s t ).

Section Title: GOAL-CONDITIONED RL VIA C-LEARNING
  GOAL-CONDITIONED RL VIA C-LEARNING We now build a complete algorithm for goal-conditioned RL based on C-learning. We will derive this algorithm in two steps. First, while Section 5.1 shows how to estimate the future state density of a single policy, for goal-conditioned RL we will want to estimate the future state density of a conditional policy, which may be conditioned on many goals. Second, we will discuss how to update a policy using the learned density. To acquire a classifier for a goal-conditioned policy, we need to apply our objective function (Eq. 7) to all policies {π φ (a | s, g) | g ∈ S}. We can do this efficiently by additionally conditioning the classifier and the policy on the commanded goal g ∈ S. However, for learning a goal-reaching policy, we will only need to query the classifier on inputs where s t+ = g. Thus, we only need to learn a classifier conditioned on inputs where s t+ = g, resulting in the following objective: The difference between this objective and the one derived in Section 5.1 (Eq. 7) is that the next action is sampled from a goal-conditioned policy. The density function obtained from this classifier (Eq. 2) represents the future state density of s t+ , given that the policy was commanded to reach goal Now that we can estimate the future state density of a goal-conditioned policy, our second step is to optimize the policy w.r.t. this learned density function. We need to define a reward function that says how good a particular future state density is for reaching a particular goal. While we can use any functional of future state density, a natural choice is the KL divergence between a Dirac density centered at the commanded goal and the future state density of the goal-conditioned policy: Importantly, computing this KL only requires the future state density of the commanded goal. Since p π + (s t+ | s t , a t , g = s t+ ) is a monotone increasing function of the classifier predictions (see Eq. 2), we can write the policy objective in terms of the classifier predictions: If we collect new experience during training, then the marginal distribution p(s t+ ) will change throughout training. While this makes the learning problem for the classifier non-stationary, the learning problem for the policy (whose solution is independent of p(s t+ )) remains stationary.

Section Title: Algorithm Summary
  Algorithm Summary We summarize our approach, which we call goal-conditioned C-learning, in Alg. 3. Given a dataset of transitions, we alternate between estimating the future state density of the goal-conditioned policy and updating the policy to maximize the probability density of reaching the commanded goal. This algorithm is simply to implement by taking a standard actor-critic RL algorithm and changing the loss function for the critic (a few lines of code). In the tabular setting, goal-conditioned C-learning converges to the optimal policy (proof in Appendix D.3).

Section Title: IMPLICATIONS FOR Q-LEARNING AND HINDSIGHT RELABELING
  IMPLICATIONS FOR Q-LEARNING AND HINDSIGHT RELABELING Off-policy C-learning (Alg. 2) bears a resemblance to Q-learning with hindsight relabeling, so we now compare these two algorithms to make hypotheses about Q-learning, which we will test in Section 6. We start by writing the objective for both methods using the cross-entropy loss, CE(·, ·): where C θ = C π θ (F = 1 | s t+1 , a t+1 , s t+ ) is the classifier prediction at the next state and where λ ∈ [0, 1] denotes the relabeling ratio used in Q-learning, corresponding to the fraction of goals sampled from p(s t+ ). There are two differences between these equations, which lead us to make two hypotheses about the performance of Q-learning, which we will test in Section 6. The first difference is how the predicted targets are scaled for random goals, with Q-learning scaling the prediction by γ while C-learning scales the prediction by γ/(γC θ + (1 − C θ )). Since Q-learning uses a smaller scale, we make the following hypothesis: Hypothesis 1. Q-learning will predict smaller future state densities and therefore underestimate the true future state density function. This hypothesis is interesting because it predicts that prior methods based on Q-learning will not learn a proper density function, and therefore fail to solve the future state density estimation prob- lem. The second difference between C-learning and Q-learning is that Q-learning contains a tunable parameter λ, which controls the ratio with which next-states and random states are used as goals. This ratio is equivalent to a weight on the two loss terms, and our experiments will show that Q- learning with hindsight relabeling is sensitive to this parameter. In contrast, C-learning does not re- quire specification of this hyperparameter. Matching the coefficients in the Q-learning loss (Eq. 10) with those in our loss (Eq. 9) (i.e., [1 − λ, λ] ∝ [1 − γ, 1 + γw]), we make the following hypothesis: Hypothesis 2. Q-learning with hindsight relabeling will most accurately solve the future state den- sity estimation problem (Def. 2) when random future states are sampled with probability λ = 1+γ 2 . Prior work has found that this goal sampling ratio is a sensitive hyperparameter ( Andrychowicz et al., 2017 ;  Pong et al., 2018 ;  Zhao et al., 2019 ); this hypothesis is useful because it offers an automatic way to choose the hyperparameter. The next section will experimentally test these hypotheses.

Section Title: EXPERIMENTS
  EXPERIMENTS We aim our experiments at answering the following questions: 1. Do Q-learning and C-learning accurately estimate the future state density (Problem 2)? 2. (Hypothesis 1) Does Q-learning underestimate the future state density function (§ 5.3)? 3. (Hypothesis 2) Is the predicted relabeling ratio λ = (1 + γ)/2 optimal for Q-learning (§ 5.3)? 4. How does C-learning compare with prior goal-conditioned RL methods on benchmark tasks? Do Q-learning and C-learning accurately predict the future? Our first experiment studies how well Q-learning and C-learning solve the future state density estimation problem (Def. 2). We use a continuous version of a gridworld for this task and measure how close the predicted future state density is to the true future state density using a KL divergence. Since this environment is continuous and stochastic, Q-learning without hindsight relabelling learns Q = 0 on this environment. In the on-policy setting, MC C-learning and TD C-learning perform similarly, while the prediction error for Q-learning (with hindsight relabeling) is more than three times worse. In the off-policy setting, TD C-learning is more accurate than Q-learning (with hindsight relabeling), achieving a KL divergence that is 14% lower than that of Q-learning. As expected, TD C-learning performs better than MC C-learning in the off-policy setting. These experiments demonstrate that C-learning yields a more accurate solution to the future state density estimation problem, as compared with Q-learning. See Appendix G.1 for full experimental details and results. Our next experiment studies the ability of C-learning to predict the future in higher- dimensional continuous control tasks. We col- lected a dataset of experience from agents pre- trained to solve three locomotion tasks from OpenAI Gym. We applied C-learning to each dataset, and used the resulting classifier to pre- dict the expected future state. As a baseline, we trained a 1-step dynamics model on this same dataset and unrolled this model autore- gressively to obtain a prediction for the ex- pected future state. Varying the discount factor, we compared each method on Walker2d-v2 in  Fig. 2  and the other tasks in Appendix Fig. 7. The 1-step dynamics model is accurate over short horizons but performance degrades for larger values of γ, likely due to prediction errors accumulating over time. In contrast, the predictions obtained by MC C-learning and TD C-learning remain accurate for large values of γ. Appendix G.2 contains for experimental details; Appendix I and the project website contain more visualizations.

Section Title: Testing our hypotheses about Q-learning
  Testing our hypotheses about Q-learning We now test two hypotheses made in Section 5.3. The first hypothesis is that Q-learning will underestimate the future state density function. To test this hypothesis, we compute the sum over the predicted future state density function, st+ p π + (s t+ = s t+ | s t , a t ), which in theory should equal one. We compared the predictions from MC C-learning and Q-learning using on-policy data (details in Appendix G.1). As shown in Fig. 1a, the predictions from C-learning summed to 1, but the predictions from Q-learning consistently summed to less than one, especially for large values of λ. However, our next experiment shows that Q-learning works best when using large values of λ, suggesting that successful hyperparame- ters for Q-learning are ones for which Q-learning does not learn a proper density function. Our second hypothesis is that Q-learning will perform best when the relabeling ratio is chosen to be λ = (1 + γ)/2. Fig. 1b shows the results from this experiment. The performance of Q-learning is highly sensitive to the relabeling ratio: values of λ that are too large or too small result in Q- learning performing poorly, worse than simply predicting a uniform distribution. Second, not only does the optimal choice of λ increase with γ, but our theoretical hypothesis of λ = (1 − γ)/2 almost exactly predicts the optimal value of λ. Our third observation is that C-learning, which uses a 50-50 sampling ratio, consistently does better than Q-learning, even for the best choice of λ. These experiments support our hypothesis for the choice of relabeling ratio while reaffirming that our principled approach to future state density estimation obtains a more accurate solution.

Section Title: Goal-Conditioned RL for continuous control tasks
  Goal-Conditioned RL for continuous control tasks Our last set of experiments apply goal- conditioned C-learning (Alg. 3) to benchmark continuous control tasks from prior work, shown in  Fig. 3 . These tasks range in difficulty from the 6-dimensional Sawyer Reach task to the 45- dimensional Pen task (see Appendix G). The aim of these experiments is to show that C-learning is competitive with prior goal-conditioned RL methods, without requiring careful tuning of the goal sampling ratio. We compare C-learning with a number of prior methods based on Q-learning, which differ in how goals are sampled during training: TD3 ( Fujimoto et al., 2018 ) does no relabeling,  Lin et al. (2019)  uses 50% next state goals and 50% random goals, and HER ( Andrychowicz et al., 2017 ) uses final state relabeling (we compare against both 100% and 50% relabeling). None of these methods require a reward function or distance function for training; for evaluation, we use the L2 metric between the commanded goal and the terminal state (the average distance to goal and minimum distance to goal show the same trends). As shown in Fig. 3, C-learning is competitive with the best of these baselines across all tasks, and substantially better than all baselines on the Sawyer manipulation tasks. These manipulation tasks are more complex than the others because they require indirect manipulation of objects in the environment. Visualizing the learned policies, we observe that C-learning has discovered regrasping and fine-grained adjustment behaviors, behav- iors that typically require complex reward functions to learn ( Popov et al., 2017 ). 2 On the Sawyer Push and Sawyer Drawer tasks, we found that a hybrid of TD C-learning and MC C-learning per- formed better than standard C-learning. This variant, which is analogous to an ""n-step"" version of C-learning, is simple to implement and is described in Appendix E. In summary, C-learning per- forms as well as prior methods on simpler tasks and better on complex tasks, does not depend on a sensitive hyperparameter (the goal sampling ratio), and maximizes a well-defined objective function. Our analysis predicts the optimal relabeling ratio.

Section Title: Predicting the goal sampling ratio for goal conditioned RL
  Predicting the goal sampling ratio for goal conditioned RL While C-learning prescribes a precise method for sampling goals, prior hind- sight relabeling methods are sensitive to these parameters. To visualize this, we varied the goal sampling ratio used by  Lin et al. (2019)  on the maze2d-umaze-v0 task from  Fu et al. (2020) . As shown in  Fig. 4 , properly choosing this ratio can result in a 50% decrease in final distance. Additionally, our hypothesis that the optimal goal sampling ratio is λ = (1 − γ)/2 accurately predicts the best value for this ratio.

Section Title: CONCLUSION
  CONCLUSION A goal-oriented agent should be able to predict and control the future state of its environment. In this paper, we used this idea to reformulate the standard goal-conditioned RL problem as one of estimating and optimizing the future state density function. We showed that Q-learning does not directly solve this problem in (stochastic) environments with continuous states, and hindsight relabeling produces, at best, a mediocre solution for an unclear objective function. In contrast, C- learning yields more accurate solutions. Moreover, our analysis makes two hypotheses about when and where hindsight relabeling will most effectively solve this problem, both of which are validated in our experiments. Our experiments also demonstrate that C-learning scales to high-dimensional continuous controls tasks, where performance is competitive with state-of-the-art goal conditioned RL methods while offering an automatic and principled mechanism for hindsight relabeling.
","[Significance and novelty]
<Use of classification for estimating future density function> The use of classification as a tool for estimating the future density function is considered novel and insightful, offering a new perspective on addressing goal-conditioned reinforcement learning problems.
<Indirect approach for predicting future state distribution> The method's use of an indirect approach to predict future state distribution, utilizing a binary prediction and Bayesian transfer technique, presents a unique and conceptually novel approach to the problem.

[Potential reasons for acceptance]
<Solid theory and experiments> The paper's theoretical framework and experimental results are robust and convincing, showcasing comparable performance to prior Q-learning-based methods without in-depth hyperparameter tuning.

[Potential reasons for rejection]
<Notation and clarity issues> Several reviewers expressed concerns about the clarity of the manuscript and its use of notation, indicating potential issues with understanding key concepts and derivations.
<Limited improvement in performance> Despite competitive results, the paper's performance could be considered limited in terms of substantially surpassing existing baselines, which may raise questions about its practical impact.

[Suggestions for improvement]
<Clarity and notation refinement> The authors should consider revising and refining the paper's notation to address confusion and enhance clarity for readers, particularly in the derivation of key equations.
<Further exploration of performance improvement> Exploring avenues for significantly improving performance beyond the competitive results, as suggested by one of the reviewers, could strengthen the practical impact of the proposed method.

"
Pre-Training by Completing Point Clouds,"[TITLE]
Pre-Training by Completing Point Clouds

[ABSTRACT]
There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.

[CAPTIONS]
Figure 1: OcCo consists of two steps: (a) occlusion o(·) of a point cloud P based on a random camera view-point into a partial point cloudP, and (b) a model c(·) that completes the occluded point cloud P so thatP ≈ P. We demonstrate that the completion model c(·) can be used as initialization for downstream tasks, leading to faster training and better generalization over existing methods.
Figure 2: Examples of self-occluded objects generated by our method.
Figure 3: Visualisation on the learned features and embeddings of OcCo-initialised encoders. Above half illustrates the location of learned features in the architecture of PointNet (Qi et al., 2017a).
Figure 4: Number of (unique) detected object parts in the feature maps of random, Jigsaw and OcCo-initialised PointNet. Digit in the bracket is the number of parts in that object category.
Figure 5: Learning curves of random, Jigsaw and OcCo , '10%' is the portion of used training data
Table 1: Adjusted mutual information (AMI) under transformations. We reported the mean and std over 10 runs. 'J', 'T', 'R' stand for jittering, translation and rotation respectively.
Table 2: Few-shot classification accuracy
Table 3: Comparison between OcCo , Jigsaw and Rand initialization on 3D object recognition benchmarks. After confirming the scores from (Qi et al., 2017a; Wang et al., 2019b; Uy et al., 2019; Sauder & Sievers, 2019) are reproducible, we reported the mean and standard error over three runs.
Table 4: Overall point prediction accuracy (mAcc) and mean intersection of union (mIoU) on ShapeNetPart. We reported the mean and standard error based on three runs.
Table 5: Overall point prediction accuracy (mAcc) and mean class intersection of union (mIoU) on the S3DIS averaged across 6-cv-fold over three runs. OcCo encoders are pre-trained on ModelNet40.

[CONTENT]
Section Title: INTRODUCTION
  INTRODUCTION Point clouds are a natural representation of 3D objects. Recently, there has been a flurry of exciting new point cloud models in areas such as segmentation ( Landrieu & Simonovsky, 2018 ;  Yang et al., 2019a ;  Hu et al., 2020a ) and object detection ( Zhou & Tuzel, 2018 ;  Lang et al., 2019 ;  Wang et al., 2020b ). Current 3D sensing modalities (i.e., 3D scanners, stereo cameras, lidars) have enabled the creation of large repositories of point cloud data ( Rusu & Cousins, 2011 ;  Hackel et al., 2017 ). However, annotating point clouds is challenging as: (1) Point cloud data can be sparse and at low resolutions, making the identity of points ambiguous; (2) Datasets that are not sparse can easily reach hundreds of millions of points (e.g., small dense point clouds for object classification ( Zhou & Neumann, 2013 ) and large vast point clouds for 3D reconstruction ( Zolanvari et al., 2019 )); (3) Labelling individual points or drawing 3D bounding boxes are both more complex and time- consuming compared with annotating 2D images ( Wang et al., 2019a ). Since most methods require dense supervision, the lack of annotated point cloud data impedes the development of novel models. On the other hand, because of the rapid development of 3D sensors, unlabelled point cloud datasets are abundant. Recent work has developed unsupervised pre-training methods to learn initialization for point cloud models. These are based on designing novel generative adversarial networks (GANs) ( Wu et al., 2016 ;  Han et al., 2019 ;  Achlioptas et al., 2018 ) and autoencoders ( Hassani & Haley, 2019 ;  Li et al., 2018a ;  Yang et al., 2018 ). However, completely unsupervised pre-training methods have been recently outperformed by the self-supervised pre-training techniques of ( Sauder & Sievers, 2019 ) and ( Alliegro et al., 2020 ). Both methods work by first voxelizing point clouds, then splitting each axis into k parts, yielding k 3 voxels. Then, voxels are randomly permuted, and a model is trained to rearrange the permuted voxels back to their original positions. The intuition is that such a model learns the spatial configuration of objects and scenes. However, such random permutation destroys all spatial information that the model could have used to predict the final object point cloud. Our insight is that partial point-cloud masking is a good candidate for pre-training in point-clouds because of two reasons: (1) The pre-trained model requires spatial and semantic understanding of the input point clouds to be able to reconstruct masked shapes. (2) Mask-based completion tasks have become the de facto standard for learning pre-trained representations in natural language processing (NLP) ( Mikolov et al., 2013 ;  Devlin et al., 2018 ;  Peters et al., 2018 ). Different from random permutations, masking respects the spatial constraints that are naturally encoded in point clouds of real-world objects and scenes. Given this insight, we propose Occlusion Completion (OcCo) a self-supervised pre-training method that consists of (a) a mechanism to generate occluded point clouds, and (b) a completion task to reconstruct the occluded point cloud. Specifically, in (a) point clouds are generated by determining what part of objects would be occluded if the underlying object was observed from a particular view-point. In fact, many point clouds generated from a fixed 3D sensor will have occlusions exactly like this. Given an occluded point cloud, the goal of the completion task (b) is to learn a model that accurately reconstructs the missing parts of the point cloud. For a model to perform this task well, it needs to learn to encode localized structural information, based on the context and geometry of partial objects. This is something that is useful for any point cloud model to know, even if used only for classification or segmentation. We demonstrate that the weights learned by our pre-training method on a single unsupervised dataset can be used as initialization for models in downstream tasks (e.g., object classification, part and semantic segmentation) to improve them, even on completely different datasets. Specifically our pre-training technique: (i) leads to improved generalization over prior baselines on the downstream tasks of object classification, object part and scene semantic segmentation; (ii) speeds up model convergence, in some cases, by up to 5×; (iii) maintains improvements as the size of the labelled downstream dataset decreases; (iv) can be used for a variety of state-of-the-art point cloud models.

Section Title: OCCLUSION COMPLETION
  OCCLUSION COMPLETION We now introduce Occlusion Completion (OcCo). Our approach is shown in  Figure 1 . Our main insight is that by continually occluding point clouds and learning a model c(·) to complete them, the weights of the completion model can be used as initialization for downstream tasks (e.g., classification, segmentation) , speeding up training and improving generalization over other initialization techniques. Throughout we assume point clouds P are sets of points in 3D Euclidean space, P = {p 1 , p 2 , ..., p n }, where each point p i is a vector of coordinates (x i , y i , z i ) and features (e.g. color and normal). We begin by describing the components that make up our occlusion mapping o(·). Then we detail how to learn a completion model c(·), giving pseudocode and the architectural details in appendix. Finally we discuss the criteria on validating the effectiveness of a pre-training model for 3D point clouds.

Section Title: GENERATING OCCLUSIONS
  GENERATING OCCLUSIONS We first describe a randomized occlusion mapping o : P → P (where P is the space of all point clouds) from a full point cloud P to an occluded point cloudP. We will do so by determining which points are occluded when the point cloud is viewed from a particular camera position. This requires three steps: (1) A projection of the point cloud (in a world reference frame) into the coordinates of a camera reference frame; (2) Determining which points are occluded based on the camera view-points; (3) Mapping the points back from the camera reference frame to the world reference frame. Viewing the point cloud from a camera. A camera defines a projection from a 3D world reference frame into a distinctive 3D camera reference frame. It does so by specifying a camera model and a camera view-point from which the projection occurs. The simplest camera model is the pinhole Under review as a conference paper at ICLR 2021 camera, and view-point projection for it is given by a simple linear equation: In the above, (x, y, z) are the original point cloud coordinates, the matrix including r and t entries is the concatenation of a 3D rotation matrix with a 3D translation vector, and the final matrix to the left is the camera intrinsic matrix (f specifies the camera focal length, γ is the skewness between the x and y axes in the camera, and w, h are the width and height of the camera image). Given these, the final coordinates (x cam , y cam , z cam ) are the positions of the point in the camera reference frame. We will refer to the intrinsic matrix as K and the rotation/translation matrix as [R|t].

Section Title: Determining occluded points
  Determining occluded points We can think of the point (x cam , y cam , z cam ) in multiple ways: (a) as a 3D point in the camera reference frame, (b) as a 2D pixel with coordinates (f x cam /z cam , f y cam /z cam ) with a depth of z cam . In this way, some 2D points resulting from the projection may be occluded by others if they have the same pixel coordinates, but appear at a larger depth. To determine which points are occluded, we first use Delaunay triangulation to reconstruct a polygon mesh from the points, and remove the points which belong to the hidden surfaces that are determined via z-buffering.

Section Title: Mapping back from camera frame to world frame
  Mapping back from camera frame to world frame Once occluded points are removed, we re- project the point cloud to the original world reference frame, via the following linear transformation: Our randomized occlusion mapping o(·) is constructed as follows. Fix an initial point cloud P. Given a camera intrinsics matrix K, sample rotation/translation matrices [[R 1 |t 1 ], . . . , [R V |t V ]], where V is the number of views. For each view v ∈ [V ], project P into the camera frame of that view-point using eq. (1), find occluded points and remove them, then map the rest back to the world reference using eq. (2). This yields the final occluded world frame point cloud for view-point v:P v .

Section Title: THE COMPLETION TASK
  THE COMPLETION TASK Given an occluded point cloudP produced by o(·), the goal of the completion task is to learn a completion mapping c : P → P fromP to a completed point cloud P. We say that a completion mapping is accurate w.r.t. loss (·, ·) if EP ∼o(P) (c(P), P) → 0. The structure of the completion model c(·) is an ""encoder-decoder"" network ( Dai et al., 2017b ;  Yuan et al., 2018 ;  Tchapmi et al., 2019 ;  Wang et al., 2020a ). The encoder maps an occluded point cloud to a vector, and the decoder reconstructs the full shape. After pre-training, the encoder weights can be used as initialization for downstream tasks. In appendix we gives pseudocode for OcCo and describes the architectures.

Section Title: EXPERIMENTS
  EXPERIMENTS

Section Title: PRE-TRAINING AND DOWNSTREAM TRAINING DETAILS
  PRE-TRAINING AND DOWNSTREAM TRAINING DETAILS We evaluate how OcCo improves the learning and generalization of a number of classification and segmentation tasks. Here we describe the details of training in each setting.

Section Title: OcCo pre-training
  OcCo pre-training For all experiments, we will use a single pre-training dataset based on Mod- elNet40 ( Wu et al., 2015 ). It includes 12,311 synthesized objects from 40 object categories, di- vided into 9,843 training objects and 2,468 testing objects. To construct the pre-training dataset, we generate occluded point clouds based on the training objects with a fixed camera intrinsics {f = 1000, γ = 0, ω = 1600, h = 1200}, 10 random selected viewpoints and zero translation.  Figure 2  shows examples of the resulting occluded point clouds. Given these, we train an ""encoder- decoder"" style completion model c(·). For encoders, similar to prior completion models ( Tchapmi et al., 2019 ;  Wang et al., 2020a ;  Wen et al., 2020a ), we consider PointNet (Qi et al., 2017a), PCN ( Yuan et al., 2018 ) and DGCNN ( Wang et al., 2019b ). These networks encode an occluded point cloud into a 1024-dimensional vector. We adapted the folding-based decoder from ( Yuan et al., 2018 ) to complete the point clouds in a two-stage procedure. We use the Chamfer Distance (CD) as our loss function (·, ·). We use Adam ( Kingma & Ba, 2015 ) with an initial learning rate of 1e-4, decayed by 0.7 every 10 epochs to a minimum value of 1e-6, for a total of 50 epochs. We use a batch size of 32 and set the momentum in the batch normalisation to be 0.9.

Section Title: Few-shot learning
  Few-shot learning We use ModelNet40 and Syndey10 ( De Deuge et al., 2013 ) for ""K-way N - shot"" learning. During training, K classes are randomly selected and for each class we sample N random samples, then the model is tested on the same K classes . As in  Sharma & Kaul (2020) , we represent each object with 100 points. We use the same training settings as used in the next paragraph.

Section Title: Object classification
  Object classification We use three 3D object recognition benchmarks: ModelNet40, Scan- Net10 ( Qin et al., 2019 ) and ScanObjectNN ( Uy et al., 2019 ); we describe them in the appendix. All objects are represented with 1024 points. We use the same training settings as the original works. Concretely, for PCN and PointNet, we use the Adam optimizer with an initial learning rate 1e-3, decayed by 0.7 every 20 epochs to a minimum value of 1e-5. For DGCNN, we use the SGD optimizer with a momentum of 0.9 and a weight decay of 1e-4. The learning rate starts from 0.1 and then reduces using cosine annealing  Loshchilov & Hutter (2017)  with a minimum value of 1e-3. We use dropout  Srivastava et al. (2014)  in the fully connected layers before the softmax output layer. The dropout rate of PointNet and PCN is set to 0.7, and is 0.5 for DGCNN. For all three models, we train them for 200 epochs with a batch size of 32. We report the results based on three runs.

Section Title: Part segmentation
  Part segmentation We use the ShapeNetPart ( Armeni et al., 2016 ) benchmark for object part segmentation. This dataset contains 16,881 objects from 16 categories, and has 50 parts in total. Each object is represented with 2048 points, and we use the same training settings as the original work.

Section Title: Semantic segmentation
  Semantic segmentation We use the S3DIS benchmark ( Armeni et al., 2016 ) for semantic indoor scene segmentation. It contains 3D scans collected via Matterport scanners in 6 different places, encompassing 271 rooms. Each point, described by a 9-dimensional vector (including coordinates, RGB values and normalised location), is labeled as one of 13 semantic categories (e.g. chair, table and floor). We use the same preprocessing procedures and training settings as the original work.

Section Title: WHAT IS LEARNED FROM OCCO PRE-TRAINING
  WHAT IS LEARNED FROM OCCO PRE-TRAINING Alongside OcCo's ability to improve learning tasks we analyze the properties of the pre-trained representation itself. Here we describe the approaches we use for such analyses.

Section Title: Visualisation of learned features
  Visualisation of learned features Feature visualisation ( Olah et al., 2017 ) is widely used to qualitatively understand the role of a convolutional neural network unit. It links highly activated parts of a CNN channel with human concepts which have semantic meaning. Ideally the pre-training process learns disentangled features that are useful to distinguish different parts of an object or a scene. These learned features will be beneficial to not only few-shot learning, but also object recognition and part and scene segmentation.

Section Title: Detection of semantic concepts
  Detection of semantic concepts To quantitatively analyse the learned features of pre-training, we adapt network dissection ( Bau et al., 2017 ; 2020) to determine the number of concept detectors in a pre-trained point cloud feature encoder. Specifically, for the k-th channel, we first create a binary activation mask M k based on highly activated point subsets. Since point cloud encoders usually learn each point feature either independently or via neighborhood aggregation, the feature maps usually do not change in the vertical direction (see  Figure 3 ). Therefore we can skip the retrieval step and directly quantify the alignment between an activation mask M k and the n-th concept mask C n (i.e., object parts) via mean intersection of union (mIoU) over a collection of point clouds D P : mIoU (k,n) = E P∼D P |M k (P) ∩ C n (P)| |M k (P) ∪ C n (P)| (3) where | · | is the set cardinality. mIoU (k,n) can be interpreted as how well unit k detects concept c. Structural invariance/equivariance under SO(3) transformation. Pre-training should learn a representation that is robust under under rigid SO(3) transformations (i.e., rotation, translation, permutation). Although a single representation might vary after transformation, the cluster structures should be preserved. We use adjusted mutual information (AMI) ( Nguyen et al., 2009 ) based on the clustering Ω and the ground truth label C, which prevents the score from monotonically increasing when the number of clusters increases, AMI(Ω, C) = E P∼D P I(Ω; C) − E[I(Ω; C)] (H(Ω) + H(C))/2 − E[I(Ω; C)] (4) where Ω is the clustering determined by the learned embeddings Enc(·) and unsupervised clustering methods such as K-means. I(Ω; C) = k j P (w k ∩ c j ) log P (w k ∩cj ) P (w k )P (cj ) denotes the mutual information, H(·) is the entropy. AMI has a maximal of 1 when two partitions are identical, and reaches a minimal of 0 if two clusters are total uncorrelated. It is calculated as: Once we finish the pre-training on ModelNet40, we first analyse the learned features and embeddings of the OcCo PointNet via the tests or probes described above. Specifically, we examine the learned concepts of the pre-trained encoders on ShapeNetPart. We assign activation mask M k with points that have top 20% highest values in the k-th unit of the feature, and the n-th concept mask C n is derived from the ground-truth annotations of the n-th object parts. We ignore the object parts which have less than 100 points. We call k-th channel a detector of concept n when mIoU (k,n) > 0.5. We analyze the learned embeddings of the pre-trained encoders on ShapeNet10 and ScanObjectNN. We cluster the learned embeddings Enc(P) into Ω with K-means and calculate the AMI w.r.t labels. Since the encoders are permutation invariant, here we consider rotation, translation and jittering.

Section Title: COMPLETION RESULTS AND PROBE TESTS
  COMPLETION RESULTS AND PROBE TESTS

Section Title: Visualisation of learned features
  Visualisation of learned features In  Figure 3 , we first visualize the features learned by OcCo PointNet on the objects from test split of ModelNet40. We visualize each learned feature by coloring the points according to their channel values. We find that, in early stage the encoder is able to learn low-level geometric primitives, i.e., planes, cylinders and cones, while later the network recognises more complex shapes like wings, leafs and upper bodies. We use t-SNE on the embeddings of OcCo encoders based on ShapeNet10, distinguishable clusters are formed for different categories.

Section Title: Number of concept detectors
  Number of concept detectors In  Figure 4 , we sketch the number of detected parts based on random, Jigsaw and OcCo (trained for 10 epochs and 50 epochs)-initialised PointNet. We find that, while keeping the previously learned concepts, OcCo helps the encoder progressively detect more object parts as the training proceeds. We show that OcCo have outperformed prior methods in terms of total detected parts (numbers in legends). We provide visualisations in the appendix. Invariance/Equivariance under SO(3) transformation. We compare Jigsaw and OcCo - initialised PointNet encoder with two hand-crafted point cloud global descriptors: viewpoint feature histogram (VFH) ( Rusu et al., 2010 ) and M2DP ( He et al., 2016 ) in  Table 1 . Each point cloud is represented as a vector, and we use K-means for clustering, where K is set as the number of categories. We show that OcCo pre-training helps the networks to learn better embeddings of point cloud objects, especially when they are occluded and with outlier points (ScanObjectNN).

Section Title: FEW-SHOT LEARNING
  FEW-SHOT LEARNING We use the same setting and train/test split as ( Sharma & Kaul, 2020 ) (cTree), and report the mean and standard deviation across on 10 runs. The top half of the table reports results for eight randomly initialized point cloud models, while the bottom-half reports results on two models across three pre-training methods. We bold the best results (and those whose standard deviation overlaps the mean of the best result). It is worth mentioning ( Sharma & Kaul, 2020 ) pre-trained the encoders on both datasets before fine tuning, while we only pre-trained once on ModelNet40. The results show that models pre-trained with OcCo either outperform or have standard deviations that overlap with the best method in 7 out of 8 settings.

Section Title: OBJECT CLASSIFICATION RESULTS
  OBJECT CLASSIFICATION RESULTS We now compare OcCo against prior initialization approaches on object classification tasks.  Table 3  compares OcCo-initialization to random (Rand) and ( Sauder & Sievers, 2019 )'s (Jigsaw) initialization on various object classification datasets among different encoders. ""MN40"", ""ScN10"" and ""SO15"" stand for ModelNet40, ScanNet10 and ScanObjectNN respectively. Recall that OcCo-initialization is pre-trained only on occlusions generated from the train split of ModelNet40. We color blue the best results for each encoder and bold in black the overall best result (and those whose standard deviation overlaps the mean of the best result) for each dataset. We show that OcCo-initialized models outperform all baselines. These results demonstrate that the OcCo-initialized models have strong transfer capabilities on out-of-domain datasets. We make more comparisons in the appendix.

Section Title: OBJECT PART SEGMENTATION RESULTS
  OBJECT PART SEGMENTATION RESULTS   Table 4  compares OcCo-initialization to random and ( Sauder & Sievers, 2019 )'s (Jigsaw) initialization on object part segmentation task. We show that OcCo-initialized models outperform or match others in terms of accuracy and IoU in all three encoders, demonstrating representations derived from completing occluded ModelNet40 improves the performance of part segmentation.

Section Title: SEMANTIC SEGMENTATION
  SEMANTIC SEGMENTATION Here we compare random, Jigsaw and OcCo initialization on semantic segmentation task. We follow the same design of PointNet and DGCNN, use a k-fold train-test procedure as in ( Armeni et al., 2016 ). The results are reported in  Table 5 . OcCo-initialized models outperform random and jigsaw- initialized ones, demonstrating that the pre-trained representations derived from completing occluded ModelNet40 brings improvements on segmenting indoor scenes which consist of occluded objects.

Section Title: LEARNING CURVES
  LEARNING CURVES We plot the learning curves for classification and segmentation tasks in  Figure 5 . We observe that the models with OcCo initialization converge faster to better test accuracy than the random and sometimes Jigsaw-initialized models. For example, on ModelNet40 with a PCN encoder, the OcCo-initialized model takes around 10 epochs to converge, while the randomly initialized model takes around 50 epochs. Similarly, for ScanObjectNN with DGCNN encoder, the OcCo-initialized model converges around 20 epochs and to a better test accuracy than the random and Jigsaw-initialized model.

Section Title: RELATED WORK
  RELATED WORK

Section Title: DEEP MODELS FOR POINT CLOUDS
  DEEP MODELS FOR POINT CLOUDS Work on deep models for point clouds can largely be divided into three different structural approaches: (a) pointwise-based networks, (b) convolution-based networks, and (c) graph-based networks. We call the networks that independently process each point, before aggregating these point representations: pointwise-based networks (Qi et al., 2017a;b;  Joseph-Rivlin et al., 2019 ;  Duan et al., 2019 ;  Zhao et al., 2019 ;  Yang et al., 2019c ;  Lin et al., 2019 ). One well-known method, PointNet, devises a novel neural network that is designed to respect the permutation invariance of point clouds. Each point is independently fed into a multi-layer perceptron, then outputs are aggregated using a permutation- invariant function (e.g., max-pooling) to obtain a global point cloud representation. Another class of methods are convolution-based networks ( Hua et al., 2018 ;  Su et al., 2018 ;  Li et al., 2018b ;  Atzmon et al., 2018 ;  Landrieu & Simonovsky, 2018 ;  Hermosilla et al., 2018 ;  Groh et al., 2018 ;  Rao et al., 2019 ). These works map point clouds to regular grid structures and extend the classic convolution operator to handle these grid structures. A representative model, PCNN ( Atzmon et al., 2018 ), defines two operators, extension and restriction, for mapping point cloud functions to volumetric functions and vise versa. The third class of models is graph-based networks ( Simonovsky & Komodakis, 2017 ;  Wang et al., 2019b ;  Shen et al., 2018 ;  Wang et al., 2018 ;  Zhang & Rabbat, 2018 ;  Chen et al., 2019 ). These networks regard each point as a vertex of a graph and generate edges based on spatial information and node similarities. A popular method is DGCNN ( Wang et al., 2019b ), which introduces a new operation, EdgeConv, to aggregate local features and a graph update module to learn dynamic graph relations from layer to layer. NRS ( Cao et al., 2020 ) uses a neural random subspace method based on the encoded embeddings to further improve the model performance.

Section Title: PRE-TRAINING FOR POINT CLOUDS
  PRE-TRAINING FOR POINT CLOUDS Pre-training models on unlabelled data are gaining popularity recently due to its success on a wide range of tasks, such as natural language understanding ( Mikolov et al., 2013 ;  Devlin et al., 2018 ), object detection ( He et al., 2020 ;  Chen et al., 2020 ) and graph representations ( Hu et al., 2020c ; d ). The representations learned from these pre-trained models can be used as a good initializer in downstream tasks, where task-specific annotated samples are scarce. The three most common pre-training objectives for point clouds are based on: (i) generative adversarial networks (GAN), (ii) autoencoders, and (iii) spatial relation ( Sauder & Sievers, 2019 ;  Sharma & Kaul, 2020 ). However, GANs for point clouds are limited to non-point-set inputs, i.e., voxelized representations ( Wu et al., 2016 ), 2D depth images of point clouds ( Han et al., 2019 ), and latent representations from autoencoders ( Achlioptas et al., 2018 ), as sampling point sets from a neural network is non-trivial. Thus these GAN approaches cannot leverage the natural order-invariance of point-sets. Autoencoders ( Yang et al., 2018 ;  Li et al., 2018a ;  Hassani & Haley, 2019 ;  Shi et al., 2020 ) learn to encode point clouds into a latent space before reconstructing these point clouds from their latent representation. Similar to these methods, generative models based on normalizing flow ( Yang et al., 2019b ) and approximate convex decomposition ( Gadelha et al., 2020 ) have been shown effective for the unsupervised learning on point clouds. However, both GAN and autoencoder-based pre-training methods have been recently outperformed on downstream tasks by the pre-training technique of  Sauder & Sievers (2019)  or  Sharma & Kaul (2020)  in few-shot setting. These methods are based on spatial relation reconstruction, which aims to reconstruct points clouds given rearranged point clouds as input. To this end,  Sauder & Sievers (2019)  equally split the 3D space into k 3 voxels, rearrange k 3 voxels and train a model to predict the original voxel label for each point. However, these random permutations destroy all spatial information that the model could have used to predict the true point cloud. Inspired by cover-trees ( Beygelzimer et al., 2006 ),  Sharma & Kaul (2020)  utilised ball covers for hierarchical partitioning of points. They then train a model to classify each point to their assigned clusters. However, the selection of the ball centroids is somewhat random and they need to pre-train from scratch for each fine-tuning task. Instead, our method creates spatially realistic occlusions that a completion model learns to reconstruct. As such, this model learns how to naturally encode 3D object shape and contextual information. Recently there is a new method called PointContrast ( Xie et al., 2020b ) which mainly uses contrastive learning for pre-training indoor segmentation models. Our method is more general and transferable compared with theirs. Point cloud completion ( Yuan et al., 2018 ) has received attentions in recent years. Most works aim at achieving a lower reconstruction loss by incorporating 1) a better encoder ( Xie et al., 2020a ;  Huang et al., 2020 ), 2) a better decoder ( Tchapmi et al., 2019 ;  Wen et al., 2020b ); 3) cascaded refinement ( Wang et al., 2020a ) and 4) multi-viewed consistency ( Hu et al., 2020b ). Completing 3D shapes for model initialisation has been considered before.  Schönberger et al. (2018)  used scene completion ( Song et al., 2017 ;  Dai et al., 2020 ;  Hou et al., 2020 ) as an auxiliary task to initialise 3D voxel descriptors for visual localisation. They generated nearly-complete and partial voxelised scenes based on depth images and trained a variational autoencoder for completion. They have showed that the pre-trained encoder is more robust under different viewpoints and weather conditions. We adapt this idea to pre-training for point clouds. We have shown that our initialisation is better than random and prior methods in terms of 1) object understanding; 2) invariance under transformations; and 3) downstream task performance.

Section Title: DISCUSSION
  DISCUSSION In this work, we have demonstrated that why and how the Occlusion Completion (OcCo) learns the representations on point clouds that are more transformation invariant, more accurate in few-shot learning, and in various classification and segmentation fine tuning tasks, compared to prior work. In future, it would be interesting to design a completion model that is explicitly aware the view-point of the occlusion. A model like this would likely converge even quicker, and require fewer parameters, as this knowledge could act as a stronger inductive bias during learning. In general, we advocate for structuring deep models using graphical constraints as an inductive bias to improve learning.
  We noticed that the random initialised/pre-trained model in ( Sauder & Sievers, 2019 ) (mIoU=40.3/41.2) did not achieve the similar results as the original DGCNN (mIoU=56.1). They consider a transductive setting which is not directly comparable to ours, so here we stick to the supervised setting and report our reproduced scores.
","[Significance and novelty]
<Pre-training on point cloud completion> The paper proposes using point cloud completion as a pre-training step for point cloud processing methods, which is considered a novel idea within the field of 3D recognition. However, some reviewers argue that the novelty of this contribution may be limited due to its similarity to techniques used in other fields such as NLP and computer vision.

[Potential reasons for acceptance]
<Experimental results> The experimental results demonstrate the effectiveness of the pre-training on point cloud completion in improving various downstream applications. Reviewers note that the results show promising performance improvements, indicating the potential significance of the proposed approach.
<Clear presentation> Reviewers appreciate the clarity and presentation of the paper, noting that the authors effectively describe the pre-training method and experimental setup, which contributes to the paper's potential acceptance.

[Potential reasons for rejection]
<Limited novelty> Some reviewers express concerns about the limited novelty of the proposed approach, suggesting that similar techniques have been used in other domains such as NLP and computer vision. They question the significant methodological advancement provided by the paper's contribution.
<Weak experimental results> The experimental results are considered modest by some reviewers, who raise concerns about the significance of the performance gains and the lack of information on run-to-run variance, indicating potential weaknesses in the experimental evaluation.

[Suggestions for improvement]
<In-depth analysis of method effectiveness> Reviewers suggest providing a deeper analysis of why the pre-training on point cloud completion is effective, including visualizing the learned features and addressing why this particular task was chosen over other potential pre-training strategies.
<Additional experimental details> Including crucial information needed to understand the experimental results, such as details on training the networks for different tasks, significance of improvements over baselines, and addressing run-to-run variance, is recommended to strengthen the paper's experimental evaluation.

"
PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning,"[TITLE]
PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning

[ABSTRACT]
Imitation learning is a natural way for a human to describe a task to an agent, and it can be combined with reinforcement learning to enable the agent to solve that task through exploration. However, traditional methods which combine imitation learning and reinforcement learning require a very large amount of interaction data to learn each new task, even when bootstrapping from a demonstration. One solution to this is to use meta reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. In this work, we introduce a new method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). Dual inference strategies allow PERIL to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. In contrast to pure imitation learning, our approach is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties. By exploiting the flexibility of meta-RL, we show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards.

[CAPTIONS]
Figure 1: Overview of our proposed method. We obtain a set of demonstrations of an unseen task and we adapt to it through efficient demonstration-conditioned exploration.
Figure 2: Meta-training framework. Environment contains a set of tasks T ∼ p(T ), each with its own ground truth task descriptor b. Samples generated by the policy overwrite the encoder buffer X T for the current task T . All samples are added to the replay buffer B T . concatenates context from the demonstration and X T to produce a hybrid context c E . Red dotted line copies collected transitions into the demonstration buffer if these result in higher rewards than the demonstrations.
Figure 3: Snapshots of the 5 different task families used. We present two (top and bottom) distinct tasks for each task family: Reach2D, Stick2D, Peg2D, Key2D, Reach3D (left to right). A robotic agent must navigate in order to find the unseen goal. In Stick2D and Peg2D the agent must insert the effector. In Key2D the agent must also rotate the interactive (blue) handle to an arbitrary angle.
Figure 4: Test-task performance vs. number of collected transitions during meta-training. By ex- ploiting hybrid inference, PERIL consistently achieves better performance with respect to other baselines. Auxiliary systems are particularly useful in complex tasks which require efficient explo- ration. All results are 3-point averaged.
Figure 5: Test-task performance vs. number of collected transitions during meta-training (left and middle). PERIL-based methods are capable of generalising within task families. Latent task dis- tributions p(z) during adaptation (right) demonstrate MetaIL (top) is incapable of clustering tasks within different task families. In contrast, PERIL (bottom) can discriminate them.
Figure 6: TSNE plot of collected z on adaptation. Stick2D is held out of meta-training and PERIL interpolates dynamics from Key2D and Stick2D.
Figure 7: Effect on k.
Table 1: Mean adaptation rate (K-shot) along all task families. PERIL baselines demon- strate superior adaptation to unseen tasks both in adaptation speed and performance and can endure zero-shot learning.

[CONTENT]
Section Title: INTRODUCTION
  INTRODUCTION Reinforcement Learning (RL) and Imitation Learning (IL) are two popular approaches for teaching an agent, such as a robot, a new task. However, in their standard form, both require a very large amount of data to learn: exploration in the case of RL, and demonstrations in the case of IL. In recent years, meta-RL and meta-IL have emerged as promising solutions to this, by leveraging a meta-training dataset of tasks to learn representations which can quickly adapt to this new data. However, both these methods have their own limitations. Meta-RL typically requires hand-crafted, shaped reward functions to describe each new task, which is tedious and not practical for non- experts. A more natural way to describe a task is to provide demonstrations, as with meta-IL. But after adaptation, these methods cannot continue to improve the policy in the way that RL methods can, and are restricted by the similarity between the new task and the meta-training dataset. A third limitation, which both methods can suffer from, is that defining a low-dimensional representation of the environment for efficient learning (as opposed to learning directly from high-dimensional images), requires hand-crafting this representation. As with rewards, this is not practical for non- experts, but more importantly, it does not allow generalisation across different task families, since each task family would require its own unique representation and dimensionality. In this work, we propose a new method, PERIL, which addresses all three of these limitations, in a hybrid framework that combines the merits of both RL and IL. Our method allows for tasks to be defined using demonstrations only as with IL, but upon adaptation to the demonstrations, it also allows for continual policy improvement through further exploration of the task. Furthermore, we define the state representation using only the agent's internal sensors, such as position encoders of a robot arm, and through interaction we implicitly recover the state of external environment, such as the poses of objects which the robot is interacting with. Overall, this framework allows for learning of new tasks without requiring any expert knowledge in the human teacher. PERIL operates by implicitly representing a task by a latent vector, which is predicted during learn- ing of a new task, through two means. First, during meta-training we encourage high mutual infor- mation between the demonstration data and the latent space, which during testing forms a prior for the latent space and represents the agent's task belief from demonstrations alone. Second, we allow further exploration to continually update the latent space, by conditioning on the robot's states and actions during exploration of this new task. We model the latent space via an encoder, from which posterior sampling can be done to encode the agent's current task belief. In essence, the encoder aims to learn an embedding which can simultaneously (i) infer the task intent, and (ii) output a policy which can solve the inferred task. During meta-training, PERIL is optimised end-to-end, by simul- taneously learning both a policy and the embedding function upon which the policy is conditioned. In our experiments we find PERIL achieves exceptional adaptation rates and is capable of exploiting demonstrations to efficiently explore unseen tasks. Through structured exploration, PERIL outper- forms other Meta-RL and Meta-IL baselines and is capable of zero-shot learning. We show how our method is capable of multi-family meta-learning as well as out-of-family meta-learning by clus- tering distinct meta-trained latent space representations. As an extension, we also show how to use privileged information during training to create an auxiliary loss for training the embedding function, which helps to form a stronger relationship between the latent space and the true under- lying state which defines the task. Supplementary videos are available at our anonymous webpage https://sites.google.com/view/peril-iclr-2021

Section Title: RELATED WORK
  RELATED WORK Meta-RL was conceptualised as an RNN-based task. Developed by  Wang (2016)  and Duan et al., the authors use RNNs to feed a history of transitions into the model such that the policy can internalise the dynamics. On another line,  Finn et al. (2017)  developed a learning-to-learn strategy, model agnostic meta learning (MAML), which meta-learns an initialisation that adapts the parameters of the policy network and fine tunes it during meta-testing. Although promising results in simple goal- finding tasks, MAML-based methods fail to produce efficient stochastic exploration policies and adapt to complex tasks ( Gupta et al., 2018 ). Meta-learning robust exploration strategies is key in order to improve sample efficiency and allow fast adaptation at test time. In light of this, context- based RL was developed with the aim of reducing the uncertainty of newly explored tasks. These map transitions τ collected from an unseen task into a latent space z via an encoder q φ (z|τ ), such that the conditioned policy π θ (τ |z) can efficiently solve said task ( Rakelly et al.; Wang & Zhou, 2020 ). An underlining benefit of decoupling task encoding from the policy is that it disentangles task inference from reward maximisation, whilst gradient-based and RNN-based meta-RL policies do this internally. An important remark regarding training conditions of the discussed meta-RL methods is that they are typically meta-trained using dense reward functions ( Rakelly et al.; Wang, 2016 ;  Wang & Zhou, 2020 ). These dense reward functions provide information-rich contexts of the unseen task. Considering that the ultimate goal is to allow agents to solve new tasks in the real world, adaptation during test time must be robust to sparse reward feedback ( Schoettler et al., 2020 ). In the context of RL, incorporating demonstrations has proven successful in aiding exploration strategies, stabilising learning and increasing sample efficiency ( Vecerik et al., 2017 ). Learning expressive policies from a set of demonstrations requires a vast amount of expert trajectories ( Ross et al. ), particularly in high dimensional state-action spaces ( Rajeswaran et al. ). In contrast, Meta-IL can be implemented in meta-RL by conditioning the agent with expert trajectories. On the an- other hand, Zhou et al. propose a MAML-based meta-IL approach which averages objective across demonstrations. The latter learns to adapt at test-time by receiving binary rewards. A similar strat- egy was also developed by and Mendonca et al.. The caveats of these approaches remain that of traditional IL: (i) Imitating expert trajectories hinders the policy from doing better than the demon- strations; (ii) Cloning behaviours reduces flexibility and generalisation capacity.

Section Title: PROBLEM STATEMENT
  PROBLEM STATEMENT We assume access to a set of tasks T ∈ p(T ), with each task represented as a partially observable Markov decision process (POMDP). Observations o are incomplete and only include measurements of the agent's internal state. For example, a peg-in-hole task would include the robot's (and therefore the peg's) pose in the observation, but not the hole's pose. The full state s contains both the robot's internal state, and the state of the external environment. Our method involves an agent inferring z, a descriptor of a task which provides the information required to solve that task, such as a represen- tation of s. Each task is then defined as T = {p(o 0 |z), p(o , z |o, a, z), r(o, a|z)}, with an unknown initial observation distribution p(o 0 |z), transition distribution p(o , z |o, a, z), and reward function r(o, a|z), where a is the action taken by the agent. By leveraging task beliefs, PERIL exploits en- riched observational spaces (s ∼ {o ∩ z}) with the objective of closing said POMDP into a stable MDP form. With the aim of supporting continuous task inference through interaction, we construct z as a probabilistic embedding conditioned on a set of recently collected transitions, referred to as the context c. We define a contexton c T t = (o t , a t , r t , o t+1 ) as a transition collected on task T at time-step t such that the context c T 0:t (what we call c) denotes the set of accumulated contextons. During meta-testing, the agent then attempts to find the true posterior p(z|c) over the task belief z ∼ p(z|c), by conditioning on c. Demonstrations significantly reduce the search space in exploration whilst providing a natural means of communicating the task. Thus, access to expert trajectories provides information-rich context which can be exploited to pre-condition the policy via a prior over z. Subsequently, online adaptation through RL can further disambiguate z. In our approach, we leverage dual meta-learning objectives to perform hybrid adaptation to new tasks, with conditioning on both demonstration contexts and exploratory contexts, where we define a demonstration as a trajectory of T observations and actions Primal Inference: The agent observes a set of k = 1, ..., K demonstrations from an expert set D demo := {d k } to form a context prior c demo = {d k } K k .

Section Title: Exploratory Adaptation
  Exploratory Adaptation The agent explores with initialised context c ← − c demo and adapts with sampled trajectories τ such that at time-step t, a hybrid context is formed c ← − c demo ∩ τ 0:t . Primal task inference aims to provide long-term inference on a task belief: meta-trained inference from previously solved tasks. Using posterior sampling, exploratory adaptation provides a proxy for short-term memory: as the context is updated with recent transitions, structured exploration can adjust primal inference by reasoning about the uncertainty of z, where c := {c demo ∩(τ T e , ..., τ ∞T e )}.

Section Title: PROBABILISTIC EMBEDDINGS FOR META-RL
  PROBABILISTIC EMBEDDINGS FOR META-RL As we do not have access to the posterior p(z|c), we use variational inference methods to produce an approximation q φ (z|c). Through generative processes, we sample z ∼ q φ (z|c) and optimise φ by maximising a meta-objective conditioned on z. Variational Encoders (VEs) are capable of producing latent distributions which optimise an arbitrary task-dependent objective G(T |·). We average the expectancy over a set of tasks from p(T ) to formulate a meta-objective which rewards fast adaptation to unseen tasks (Eq. 1), where β controls the constraint for mutual information between the inferred variable z and context c. This information bottleneck filters down redundant information and compresses the z into a generalisable form. We set the prior p(z) to a unit Gaussian. Traditional meta-RL methods leverage RNN-based inference systems to produce latent features from a history of recently collected transitions. The problem with this approach is that learning from en- tire trajectories leads to massive variances which hinder the learning process ( Duan et al.; Humplik et al., 2019 ). In theory recurrence is not strictly required, since any MDP is defined by a distribu- tion of sequentially invariant transitions. In light of this, we exploit task encoders as suggested by Rakelly et al., where q(z|c) is modelled as a product of Gaussian Factors. Each factor ψ φ (z|c n ) parameterised by φ, is independently computed: We build on top of probabilistic meta-RL methods derived by Rakelly et al.. Decoupling task in- ference from task completion allows efficient off-policy objectives to be optimised. Moreover, con- ditioning a policy with diverse task descriptors aims to generalise behaviours amongst different dynamics and rewards. This is generally not the case when employing MAML-based or RNN-based meta-training kernels to ""learn how to learn"" new tasks, since policy gradients modify the policy pa- rameters online which, in turn, can result in highly unstable and inefficient adaptation to new tasks, requiring up to hundreds of sample trajectories in unseen environments. To that end, we employ meta-adapted maximum entropy RL, where a proxy for the policy π(a|s) is defined by including the task belief z as part of the observational space π θ (a|o, z), resulting in task- conditioned critic L T critic and actor L T actor losses for the the Q-function Q θ (·) and the policy π θ (·) respectively (see A.1 for more details). As VEs are a form of generative processes, G(T |·) can be defined with the aim of recovering transition dynamics or reconstructing state spaces ( Humplik et al., 2019 ). Instead, we optimise q φ (z|c) in a model-free manner as proposed in Rakelly et al.. Here, parameters φ can be optimised to maximise expected discounted rewards under a policy π θ (·|z) through reconstruction of Q θ (·, z). We fold L T critic as a proxy for the task-dependent goal G(T |z):

Section Title: CONDITIONING ON DEMONSTRATIONS
  CONDITIONING ON DEMONSTRATIONS By leveraging meta-learning, we suggest that the agent can learn to exploit the overlapping statistical information from within heterogeneous demonstrations belonging to different tasks, with the aim of inferring task embeddings z which can help improve exploration during test-time adaptation. In order to link demonstrations into the probabilistic meta-RL framework, we formulate an objective based on mutual information I(z; τ ) between the demonstration trajectories and the latent space distribution p(z) (Eq. 3). This objective is similar to that used in Yu et al. with the exception that we adapt this to meta-imitation learning and off-policy RL instead of recovering reward functions. As direct access to the posterior p(z|τ ) is not available, I(z; τ ) is intractable. However, in order to adjust for this, we leverage a variational approximation to p(z|τ ) using our task belief encoder q φ (z|τ ). We assume access to a distribution of expert demonstrations p π E (τ |z). Additionally, we include further desiderata over the mutual information objective based on distributional matching:

Section Title: Learning from demonstrations
  Learning from demonstrations Matching generated trajectories to those sampled from expert trajectories: min θ E p(z) [D KL (p π E (τ |z)||p θ (τ |z))]. This secondary objective can be considered as trying to match the distribution of trajectories generated by the agent's policy conditioned on z, to those from the expert policy (similar to BC). Since they share the same marginal distribution p(z), matching these distributions also encourages matching of the conditionals p π E (z|τ ) and p(z|τ ). Linking variational posterior. min θ E p θ (τ ) [D KL (p(z|τ )||q(z|τ ))]. Encourage q φ (z|τ ) to approx- imate the true posterior p(z|τ ) such that, given a new demonstration, the encoder properly infers the task. This objective acts as a regulator as it reduces over-fitting by penalising variance: optimal distributional mismatch is reached when the encoder produces constant information-less represen- tations of z. The distributional matching objective can therefore be defined as in (4). After several manipulation steps (see section A.1 for more details), we deconstruct this objective into a differentiable form and define two separate loss components L T bc (Eq. 5) and L T inf o (Eq. 6), where π b is the behavioural policy which collected the transitions (truncated importance sampling). As L T inf o conditions the encoder, we aggregate it to the task-dependent goal G(T |z) (7). Under review as a conference paper at ICLR 2021

Section Title: AUXILIARY SYSTEMS
  AUXILIARY SYSTEMS An inherent problem is that the agent generates predictions Q θ (o, a, z) based on unsupervised esti- mates z ∼ q φ (z|c T ). Whilst this is theoretically generalisable to any MDP, it brings large variances and instabilities during training because the agent must simultaneously (i) distinguish one task from another, and (ii) use the task beliefs to solve that task. A plausible solution to mitigate these instabil- ities is to exploit privileged information (a brief task descriptor such as the position and orientation of a door) during training, allowing the encoder q φ (z|c T ) to produce task beliefs z which provide succinct descriptors of T . Although this information is not available during testing, it could be made available, for example, if training is performed in a full-state simulation, or in the real-world but with extra environment instrumentation. In theory, a perfect task descriptor is that which provides suffi- cient information to close the POMDP. We model ground truth task descriptorb for T as a vector b ∈ R v where v is the dimension of the task descriptor. During training we wish to condition the encoder to produce latent spaces z ∼ q φ (z|c T ) which, when mapped into the dimensions of b, can produce approximations ofb. We use a VAE d λ (b|z) parameterised by λ as our auxiliary module, which produces vectors b µ , b σ ∈ R v and optimise it via a log-likelihood maximisation objective (8). Inference b ∼ d λ (b|z) from the trajectories in c T is decoupled from RL as gradients do not pass through the actor or the critic. This stabilises the encoder by having a fixed supervised target, and allows off-policy training. Leveraging auxiliary objectives for meta-RL was coined by  Humplik et al. (2019) , where the authors condition task inference solely on L T aux to train RNN-based poli- cies. However, in PERIL we use simple descriptors such that these solely condition the encoder to converge towards a suggested direction. Thus, we extend G(T |z) with the auxiliary loss to produce the ultimate meta-objective (9). Details of the final computational graph is presented in Figure 8. By keeping the critic loss L T critic in G(T |z), we give freedom for the encoder to perform unsupervised learning so that it can structure representations of z which represent different dynamics and rewards: if we solely base task inference on L T aux , a task such as screwing a bolt would output the same task belief as a task of inserting a peg, even though the tasks are fundamentally different.

Section Title: IMPLEMENTATION
  IMPLEMENTATION Meta-training requires sampling from a set of transitions (o t , a t , r t , o t+1 ) T t=0 corresponding to a dis- tribution of tasks T ∈ p(T ) and optimising the meta-objective. We use task-specific replay buffers B T to store collected transitions from which we can randomly sample during training. Following primal inference objective outlined in section 3.1, task inference is initially conditioned via con- textons retrieved from expert demonstrations. Consequently, we store a set of k demonstrations generated by an expert policy on task-conditioned demonstration buffers D T . This expert policy may consist of a human user or, in our case, a pre-trained SAC agent with access to the full state of the environment. Meta-training algorithms are presented in Algorithms 1 and its process is illus- trated in  Figure 2 . Further details of the meta-training and meta-testing processes in section A.2. We augment D T with imperfect trajectories which lead to demo-like (and increased) sum of rewards. This allows PERIL to: (i) be robust to imperfect demonstrations and surpass performance from the expert; (ii) combat the overfitting nature of BC from a narrow set of k demonstrations. The agent must also exploit posterior sampling to update its task belief in order to accomplish continuous adap- tation objectives. To this end, we use exploratory data to update the task belief during adaptation and set additional task-dependent encoder buffers X T which only stores recently collected contextons. This mitigates distributional shift between off-policy training and on-policy adaptation.

Section Title: EXPERIMENTS
  EXPERIMENTS We leverage probabilistic meta-RL methods to find values for z which reconstruct unobserved rep- resentations of a new task. To that end, we focus on validating PERIL in environments where the agent must explore in order to understand the dynamics, whilst also utilising sparse rewards to identify the task intent. Furthermore, neural networks, are particularly susceptible to abrupt discon- tinuities. Thus, it is of particular interest to leverage meta-training in situations as such. We devise tasks which involve contact-rich interactions and boundaries, factors which are considered to require a higher level of perception of the environment ( Zhu et al., 2020 ). More details on A.3. The different task families tested in this study are illustrated in  Figure 3 . Within a task family, one single task is defined by the configuration of the environment, such as the pose of the object or goal. When leveraging privileged information, this pose is used as the task descriptor. For each task, demonstrations are provided from different robot initial poses, and the agent is then required to perform that task, from any new robot initial pose. We compare our proposed methods to other meta-RL and IL baselines. To ensure fairness in all tests, we used the same number of parame- ters during training. Specifically, MetaIL (Yu et al.), extends PEARL by conditioning exploration on demonstrations. We also use Noisy-BC as a baseline where the agent clones a demonstration with additional noise (20% of p(o 0 |·)) . We also consider the case where privileged information is unavailable, thus compare these baselines to both PERIL and PERIL-A. In PERIL we omit L aux .

Section Title: PERFORMANCE
  PERFORMANCE We evaluate the performance of each method over 5 task families ( Figure 4 ) using k = 3 demon- strations per task. Since meta-learning requires trajectory-based adaptation, we record the averaged trajectory return after 3 policy roll-outs. The results indicate that, in contrast to other baselines, meta-learning without demonstrations through sparse reward feedback is particularly ineffective (PEARL). On the other hand, PERIL-based methods significantly outperform MetaIL and Noisy- BC, especially in tasks from Peg2D or Reach2D/3D where the agent must explore efficiently. It is in these cases where the auxiliary module contributes greatest. On the other hand, in task families such as Stick2D and Key2D, a few contact-rich interactions with the environment can quickly help decipher the task. In this case, goal-oriented auxiliary targets are not as helpful.

Section Title: INCREASING GENERALISATION
  INCREASING GENERALISATION Most meta-RL research involves adaptation to tasks within a single task family ( Rakelly et al.; Duan et al.; Yu et al. ). But in practice, we seek agents capable of interpolating from a diverse set of dynamics. To test the representational power behind PERIL we evaluate training and adaptation per- formance in multi-task family settings ( Figure 5 ). Here, we train a single agent along each task from from within the 2D task families. TSNE plots of the latent space distribution created during meta- testing reveals PERIL-based methods produce structured task beliefs during hybrid inference of new tasks. Moreover, this verifies PERIL's long-term memory contribution. Last, it showcases PERIL performs structured exploration at the multi-task level, allowing the agent to efficiently switch from one macro policy to another and robustly inherit a diverse set of behaviours. Despite the effec- tiveness of privileged information in complex exploration tasks in single task family adaptation, auxiliary systems do not generally contribute as much during meta-multi-tasking. We believe that task inference is substantially more convoluted in the multi-task family case as discerning dynam- ics and intent requires more complex embeddings in the latent space than the position of the goal. We also test generalisation by adapting PERIL in out-of-task distributions ( Figure 6 ), where PERIL interpolates from within meta-trained latent space representations to adapt to unseen dynamics. Metrics during adaptation of unseen tasks reveal PERIL is substantially more efficient in exploration with respect to other baselines, and is capable of zero-shot learning ( Table 1 ), where the context from the demonstration is sufficient to condition the agent to adapt to an unseen task. Note we record the K-shot as the average kth adaptation trajectory until successful completion of the new task.

Section Title: ALTERATIONS ON K
  ALTERATIONS ON K We study the inherent meta-IL dependency over the number of per-task expert demonstrations k. We find that increasing the number of demonstrations per task slightly improves sample efficiency but does not alter the asymptotic capacity of PERIL ( Figure 7 ). Improvement is small since, by augmenting the demonstrations online, PERIL is capable of self-inducing ad- ditional demos which improves sample efficiency.

Section Title: CONCLUSION
  CONCLUSION We have introduced PERIL, a new method for meta imitation and meta reinforcement learning which builds a representation of a new task by conditioning on both demonstrations, and further explo- ration. This provides a framework where new tasks can be defined naturally by a non-expert, without requiring reward shaping or state-space engineering. Experiments across a range of tasks show our method is able to adapt not only to novel instances within a task family, but also to entirely novel task families, whilst doing so with superior data efficiency to a range of baselines. This provides the foundation for further theoretical extensions of this framework, such as learning from imperfect demonstrations, as well as further applications, such as real-world contact-rich robot manipulation.

Section Title: A.1 MATHEMATICAL EXTENSIONS
  A.1 MATHEMATICAL EXTENSIONS

Section Title: Probabilistic Kernel
  Probabilistic Kernel We adopt a MaxEnt RL formulation to define a meta-objective for task T ∈ p(T ) (10), where H(·|o) represents the observational entropy, and the temperature α controls the weighting term for the entropy-based reward. Optimising the objective defined in (10) is possible by using the SAC soft policy iteration approach. The difference to standard SAC is that in our case, the observation is augmented by z resulting in task-conditioned critic (11) and actor (12) losses for the the Q-function Q θ (·) and the policy π θ (·) respectively. V corresponds to the frozen target value function (no gradients stored in the forward pass). Notice we use the term B T to represent the distribution of transitions in task T , which is modelled by a replay buffer. Moreover, the over-line operator in z denotes that gradients are detached. You may seek further details on the derivation of actor and critic losses in Appendix B of (Haarnoja et al.). Where Z θ (o) is the normalising partition function, which is intractable yet has no effect on the com- putation of the gradients. In the computation of L T actor , the gradients for z are detached, allowing the policy loss to exploit the representation of the MDP passed on by the critic without conditioning the inference network q φ (z|c). Observe in 11 we allow gradients from the inference process q : C φ − → Z to pass onto the computation of Q θ (o, a, z) such that meta-optimisation of the latent variable z is conditioned on reconstructing the Q-function.

Section Title: Conditioning on Demonstrations
  Conditioning on Demonstrations We minimise the conditioned mutual information term by enduring the following deconstruction process. In order to optimise the two terms in (13), we define L bc and L inf o as the leftmost and rightmost distribution expectancies respectively. In this definition, the expectancies are not differentiable and requires parametric manipulation. The first step is to adapted this objective into a meta-RL form, by approximating the conditional p(τ |z) with the policy p θ (τ |z). We define the first term L bc (θ) as the conditional behavioural cloning loss.
","[Significance and novelty]
<Combines meta-RL and imitation learning> The paper introduces a method that combines meta-RL and imitation learning to efficiently adapt to new tasks, which is a significant and novel contribution to the field.

[Potential reasons for acceptance]
<Strong experimental results> The paper presents strong experimental results showcasing the method's outperformance of baselines, which adds weight to the significance of the findings.
<Addresses important problems in adaptive RL> The paper addresses several important problems in adaptive RL, including generalizing to complex tasks and using demonstrations to avoid costly random exploration.

[Potential reasons for rejection]
<Complexity and lack of clarity> The method is quite complex, consisting of many different parts and loss terms, which may impact the overall clarity and comprehensibility of the paper.
<Inconsistencies and unpolished notation> The notation and mathematical formulation in the paper are not polished enough, with inconsistencies, variable name clashes, and some parts of the objective function not properly introduced and explained.

[Suggestions for improvement]
<Simplify the approach and clarify necessity of components> The paper could benefit from a clear analysis of the necessity of each component and potential simplification of the method, including ablations and additional baselines to understand the roles of different parts.
<Polish notation and mathematical formulation> It's important to address inconsistencies and variable name clashes, and to carefully explain and introduce all parts of the objective function to enhance the overall clarity and readability of the paper.
<Address problem formulation and solution alignment> The paper should align the problem formulation with the proposed solution, ensuring consistency in the understanding of the underlying concepts and methods.

"
"Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples","[TITLE]
Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples

[ABSTRACT]
We present DeClaW, a system for detecting, classifying,  and warning of adversarial inputs presented to a classification neural network. In contrast to current state-of-the-art methods that, given an input, detect whether an input is clean or adversarial,  we  aim  to  also  identify  the  types  of adversarial attack (e.g., PGD, Carlini-Wagner or clean). To achieve this, we extract statistical profiles, which we term as anomaly feature vectors, from a set of latent features. Preliminary findings suggest that AFVs can help distinguish among several types of adversarial attacks (e.g.,  PGD versus Carlini-Wagner) with close to 93% accuracy on the CIFAR-10 dataset. The results open the door to using AFV-based methods for exploring not only adversarial attack detection but also classification of the attack type and then design of attack-specific mitigation strategies.

[CAPTIONS]
Figure 1: Histograms of (standarized) layer input values for one NATURAL sample and a adversarial versions from FGM and PGD20 of the same CIFAR10 sample. Attack perturbations tend to be small, but do induce potential dif- ferences when carefully analyzed.
Figure 2: DeClaW subclassing extends sampling hook into the target pretrained model at a batch normalization layer that collects latent feature statistics on the dataset. Another pass generates AFVs for natural and adversarial samples.
Figure 3: DeClaW's second stage classifier takes an image's AFV as input and outputs attack type (or clean). Dropout layers are used after each hidden layer for regularization.
Figure 5: Insight into the discrimination power provided by anomaly detection features. The gure shows nine histogram plots corresponding to nine dierent DeClaW features evaluated across the same set of 5000 samples. On each plot, the blue shade represents the distribution of feature values for natural samples - each other color represents the distribution of feature values for the application of a given attack method to the same 5000 samples.
Figure 4: Each anomaly feature helps provide partial discrimination among attacks and clean examples. When used in combination, they can help towards classifying attack types and whether an example is clean.
Figure 5: Confusion matrix for the detection of adversarial examples where CLEAN bit is set only if no ATTACK class is detected and ATTACK bit is set otherwise.
Figure 6: Declaw's classification accuracy among attacks after attack methods are clustered based on large FPR and FNR. Appendix (Fig. 9) is an expanded figure.
Table 1: Attack classes used in the baseline experiment. In the non-clustered attack mode, 12 classes were used. In the clustered attack classification mode, those 12 classes were grouped into 6 cluster groups based on a transitive closure against either FPR or FNR > 25%. Attack classes that are too difficult to discriminate between each other (and thus have high inter-class FPR or FNR) are then assigned to the same cluster. This clustering-by-FPR/FNR threshold technique is discussed in Section A.3.
Table 2: DeClaW's command line hyper-parameters.
Table 3: Best performing parameters and their resulting accuracy metrics (F1 values for natural and across classes as well as average observed accuracy for detection and clas- sification mode over a grid parameter evaluation. Attack classes were grouped into clusters based on a proxy to sim- ilarity, the corresponding F N R and F P R rates between two attack classes. A transitive closure reduced a dozen attack classes into six. Using this criteria, all models are in a plateau, whose maximal point is 94.2% for classification Acc C+A k and 90.8% for detection Acc)C|A.
Table 4: The basic anomaly detection building block fea- tures based on region-based aggregations.
Table 5: Twenty five additional features are used to identify whether a given feature value f r (l i ) from Table 4 represent extrema events with respect to the overall distribution and percentiles of feature values obtained for normative natural samples.
Table 6: P-values resulting from the application of several statistical tests are used as features. These tests compare a random small subsample of the population of 40960 ob- served layer input values against a random small subsample of the population of 40960 normative layer input values.

[CONTENT]
Section Title: Introduction
  Introduction While deep neural networks (DNNs) help provide image classification, object detection, and speech recognition in autonomous driving, medical, and other domains, they re- main vulnerable to adversarial examples. In this paper, we focus on the detection problem - determining whether an input is adversarial and even attempting to determine its type (e.g., PGD vs. Carlini-Wagner), so that potentially in the future, attack-specific countermeasures can be taken. Existing detection techniques include training a second- stage classifier ( Grosse et al., 2017 ;  Gong et al., 2017 ;  Met- zen et al., 2017 ; Aigrain and Detyniecki, 2019), detecting statistical properties ( Bhagoji et al., 2017 ;  Hendrycks and Gimpel, 2016 ;  Li and Li, 2017 ;  Crecchi et al., 2019 ; Pang Accepted by the ICML 2021 workshop on A Blessing in Disguise: ), and running statistical tests ( Grosse et al., 2017 ;  Feinman et al., 2017 ;  Roth et al., 2019 ). In contrast to our work, these approaches are focused on detection of an attack rather than determining the attack type. It is an open ques- tion as to whether attacks can be distinguished sufficiently. If the answer is yes, that may help provide more insight into how attacks differ, leading to more robust classifiers, or attack-specific mitigation strategies. To explore the question, our hypothesis is that it is possi- ble to do a fine-grained distributional characterization of latent variables at an intermediate hidden layer across natu- ral input samples. Our hope is that adversarial samples are outliers with respect to the that characterization in different ways. Unfortunately, finding differences between natural and different types of adversarial inputs is not straightfor- ward.  Fig. 1  shows histograms of (standarized) layer input values for a NATURAL/CLEAN sample as well as for two (first-stage) white-box attacks (Fast Gradient Sign (FGM) and L ∞ Projected Gradient Descent (PGD) with 20 steps and = 8/255. over a CIFAR10 pretrained model. Each histogram shows frequency counts for z-scores of observed values. While these distributions exhibit some differences, the differences are minute, making discrimination among them challenging. We propose an approach called DeCLaW (Detecting, Clas- sifying, and Warning of adversarial examples) that aims to overcome this challenge by constructing a set of anomaly features, in the form of an anomaly feature vector (AFV), from an input image that better distinguishes among clean and adversarial images as well as several of the common at- tack types. The key insight is to collect and look at statistical values in outlier areas in  Figure 1  (shown with a magnifying glass) and try to characterize their distributions for different types of attacks, generating a collection of anomaly fea- tures that form the input image's AFV. We find that, given a model to be protected, each anomaly feature in the vector induces some separation between different types of attacks from clean examples. While overlaps exist when looking at one anomaly feature, across multiple anomaly features, we find good success in distinguishing among attack types. DeClaW uses a second stage neural network that is trained on AFV values rather than latent features. Given an image, Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples its AFV is computed from the hidden layer values in the target classifier and then fed to the second stage neural net- work for analysis of the attack type. As a detector, using 11 different popular attack methods from the IBM Adver- sarial Robustness Toolbox (ART), DeClaW achieves 96% accuracy for CIFAR10 on adversarial examples and 93% accuracy on clean examples. Notably, AFV size is small compared to input pixel val- ues or the latent variables, resulting in a simpler second stage classifier. E.g., the current implementation of De- ClaW uses only 176 anomaly detection features in contrast to approximately 16K latent features used by Metzen et al.'s adversarial example detector [18]. As an attack-type classifier, after grouping similar attacks into clusters, DeClaW achieves an overall 93.5% classifi- cation accuracy for CIFAR10. To our knowledge, we are the first to produce good attack classification. For example, whereas the work in ( Metzen et al., 2017 ) reports detection rate on three attack methods (i.e., FGM, DeepFool, BIM), it does not discriminate between those underlying attacks. DeClaW is able to discriminate among these attacks with a high success rate.

Section Title: Our Approach
  Our Approach DeClaW is both an attack labeling classifier as well as a binary attack detector. The fundamental idea is to augment a pretrained model with a sampling hook at a chosen layer to capture unperturbed data that can be analyzed and sta- tistically characterized. That is then used to generate what we term as an anomaly feature vector when presented a perturbed input of a given type (e.g., adversarial attack gen- erated by PGD). DeClaW's AFV generation sampling hook as shown in  Fig. 2 . The AFVs from the perturbed inputs as well as natural inputs of different classes are used to train a second stage neural network ( Fig. 3 ) that can help distinguish among perturbed inputs and clean inputs and also help identify the type of perturbed input (e.g., a PGD attack versus a DeepFool attack). As we discuss later, some attack types may be difficult to distinguish reliably. When that is the case, those attack types are clustered together using a clustering algorithm and treated as one composite class. Below, we describe the key concepts of the scheme. One advantage of our approach is that the the size of AFV is designed to be independent of the latent feature vector size and is typically orders of magnitude smaller than the typical size of the feature vector from which it is derived. Thus, training and classification of an input using the second stage classifier of DeClaW is efficient. We elaborate on this in Section 3 (Experiments).

Section Title: Anomaly Feature Vectors
  Anomaly Feature Vectors An anomaly detection defense can be conceptualized as testing whether a sample is anomalous or not by check- ing whether statistics for the given sample are within the expected limits of the non-anomalous (natural) inputs. How- ever, because the goal of sophisticated attacks is to resemble as much as possible a natural input, a number of coarse- grain A/B statistical tests simply lack the resolution to reliably discern anomalies. We rely on various histogram comparative techniques, subsampling projections, aggrega- tion predicates and distance metrics to account for shape and area differences between the histogram of expected layer in- puts and the histogram for observed layer inputs associated with an evaluated input image.

Section Title: Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples
  Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples DeClaW's neural network trains on a small and compact number of anomaly-detection distribution characterization features as opposed to on pixels or their derivative latent variables. The basic DeClaW second stage neural network uses 176 anomaly detection features - a significantly smaller number than the number of features used by Metzen et al.'s adversarial example detector ( Metzen et al., 2017 ). In Met- zen et al.'s detector, the preferred position reported is at stage AD(2), the second residual block, where the inputs have a shape of 32 × 32 × 16 ≈ 16K which is then fed to the detector convolution network. Moreover, the num- ber of AFV features is independent of the input size since AFV features are essentially distributional characterization features. DeClaW trains fast, converging in few epochs.  Fig. 4  depicts three histogram plots corresponding to differ- ent DeClaW anomaly features evaluated across the same set of 5000 samples. On each plot, the blue shade represents the distribution of feature values for natural samples - each other color represents the distribution of feature values for the application of a given attack method to the same 5000 samples. Each anomaly feature is generated by applying a subsampling projection that zooms in a particular range of standardized scores (z-scores) and then by collecting ag- gregation metrics over the selected values (e.g., frequency counts of the selected z-scores). Each plot illustrates the discriminating power that such region-based aggregation feature induce. For example, the first plot shows a histogram for the values taken by an AFV feature that counts how many standardized scores are observed with value less than a minimum tolerance (i.e., the LEFT tail of the histogram). We rely on four subsampling projections or regions: the LEFT tail (that is, low z-scores), the CENTER (that is, non- anomalous z-scores), the RIGHT tail (that is, high z-scores), and the OVERALL histogram - and ratios of these. Dif- ferent aggregation predicates (e.g., count, sum, mean, ratio, etc) are used to aggregate over the subsampling projection. A total of 12 different shades are shown corresponding to 11 attack methods that our second stage detector seeks to detect - plus the natural/clean samples shown in a blue shade. Intuitively, whereas the detection goal is to separate the blue shade from other shades, the classification goal is to find combinations of features that allow to separate each shade from the others. Some features (e.g., Z.SUM()) induce lin- ear separatability between natural samples and attack classes - hence, DeClaW's high detection accuracy and low FPR as well as FNR - whereas other features induce partial separatability between attack classes (hence DeClaW's high classification accuracy). For example, the above-mentioned Z.SUM() feature measures the sum of absolute standard- ized scores of layer-input values observed for a given sam- ple and empirically discriminates well among AFVs from CLEAN/NATURAL (i.e. blue shade) from AFVs from any adversarially perturbed samples (i.e., all others).

Section Title: Experiments
  Experiments We examine DeClaW classification performance with re- spect to one dozen different classes: one class 0 for Nat- ural samples and 11 attack classes A 1 · · · A 11 for attack- perturbed samples obtained from the corresponding applica- tion of 11 different evasion methods. Each of attack class A 1 · · · A 11 was generated by applying the attack function of the IBM Adversarial Toolkit over the same subset of natural samples. Default parameters were used in the attacks. The Appendix lists the attack classes.  Table 1 . We also examine DeClaW detection performance with respect to the Class 0 above for Natural samples and the agglomeration of the above 11 classes aboves into one class representing attack- perturbed samples by any type of attack method. For most of the results in this paper, unless noted otherwise as in the case of clustering of attack classes that are too similar to discriminate, we focused on 11 different attack classes plus 1 class for the natural samples. For the CIFAR-10 dataset, we used all the 60000 natural samples to train and test the second stage classifier. Using a subset of natural CIFAR-10 samples C 6 , we first inferred the reference temperature of the dataset of the natural sam- ples. For each attack class A k , we collected the resulting anomaly-detection feature vector produced by our hook for the same C 6 natural samples. In total, we generated a to- tal of 60000 anomaly detection feature vectors from the CIFAR-10 natural samples and a total of 102000 attack sam- ples across the 11 attack classes. About 7.3% = 8000 110000 of attacks failed to be successfully computed within our time bounds and we omitted those from consideration. We used a 70% to 30% split between train and test. More details of experimental setup are in the Appendix. We use a 32-layer Wide Residual Network ( Zagoruyko and Komodakis, 2016 ) as the classifier for CIFAR-10, with a sampling hook placed before the batch normalization stage of the model's pipeline (details in the Appendix). As a de- tector, for CIFAR-10, this network's accuracy on clean test data is 93% and 96% on adversarial data (see  Fig. 5 ). This compares well to state-of-the-art baselines. For instance, Metzen et al. report 91.3% accuracy on clean data and 77% to 100% on adversarial data that used same permitted distor- tion during attacks as the detector was trained on ( Metzen et al., 2017 ). Aigran et al. achieve only 55.7% false-positive rate when evaluated against FGSM attacks with a 95% true positive rate (Aigrain and Detyniecki, 2019). Moreover, DeClaW (see  Fig. 6 ) achieves high classification accuracy among attack methods, with similar attacks clus- tered. For instance, on adversarial versions of test data, it had almost perfect ability to distinguish among PGD and type of attack method. The attack classes are itemized in  Table 1 . For most of the results in this paper, unless noted otherwise as in the case of clustering of attack classes that are too similar to discriminate, we focused on 11 dierent attack classes plus 1 class for the natural samples. For the CIFAR-10 dataset, we used all the 60000 natural samples to train and test the second stage classier. Using a subset of natural CIFAR-10 samples ⇠ 6 , we rst inferred the reference temperature of the dataset of the natural samples. 1 For each attack class : , we collected the resulting anomaly-detection feature vector produced by our hook for the same ⇠ 6 natural samples. In total, we generated a total of 60000 anomaly detection feature vectors from the CIFAR- 10 natural samples and a total of 102000 attack samples across the 11 attack classes. 2 We used a 70% to 30% split between train and test - resulting in second stage datasets as follows: • training with 42000 and testing with 18000 anomaly detec- tion feature vectors for natural class ⇠ and • training with 72200 and testing with 29790 anomaly detec- tion feature vectors across attacked classes 1 . . . 11 . 1 In the Future Work section, we identify issues with generating a reference temperature of a dataset such as the presence of concept drift or multimodality. 2 About 7.3% = 8000 110000 of attacks failed to be successfully computed within time bounds. We performed the experiments using 12 classes: • one class ⇠ of anomaly detection feature vectors from nat- ural samples (divided into subsets ⇠ 1 . . . ⇠ 6 , each of 10000 samples). • and 11 classes : , each resulting from the application of a given attack class : over the same (10000 sample size) subset ⇠ 6 of the natural samples. DeClaW provides user control of various basic hyper-parameters. These parameters are shown in  Table 2 . We use a 32-layer Wide Residual Network [23] as the classier for CIFAR-10. The network has been trained for 80 epochs with stochastic gradient descent and momentum on 10000 data points from the train set. The momentum term was set to 0.9 and the initial learning rate was set to 0.1, reduced to 0.01 after 50 epochs, and further reduced to 0.1 after 70 epochs. After each epoch, the network's performance on the validation data (the remaining 5000 data points from the train set) was determined. The network with maximal performance on the validation data was used in the sub- sequent experiments (with all tunable weights being xed). This network's accuracy on non-adversarial test data is 91.3%. We attach an adversary detection subnetwork (called ""detector"" below) to the ResNet. The detector is a convolutional neural network using Carlini-Wagner attack images and close to 90% accuracy in distinguishing PGD attacks from non-PGD attacks. F1- scores, which can be computed from the confusion matrix, ranged from 90%-99% for the class groups shown. To our knowledge, we are the first to report such classification accuracy levels across different attack methods.

Section Title: Limitations
  Limitations Our approach shows promise on CIFAR-10 dataset in distin- guishing among several types of attacks. But the approach should be tried on additional datasets to assess generalizabil- ity. Preliminary results on CIFAR-100 dataset are promising and given in the Appendix under Additional Results. The approach also needs to be evaluated against adversarial attacks where the adversary has knowledge of the DeClaW pipeline. DeClaw pipeline, as it stands, is also not adver- sarially trained. We do note that in corporate environments, the DeClaW pipeline need not be made visible to an adver- sary, nor its results need to be directly visible, since it is primarily used for error detection and error classification rather than as the mainstream classifier (e.g., YouTube or Facebook could use an attack detector/classifier inspired by DeClaW on the backend for analysis without exposing that to end-users.) Furthermore, DeClaW pipeline could be adversarially trained. We note that for an adversary to recreate the DeClaW pipeline, they require both the training data and whitebox access to the target model so as to create AFVs - a higher threshold than just whitebox access.

Section Title: Conclusion
  Conclusion We present DeClaW, a system to detect, classify and warn of adversarial examples. We achieve high detection accuracy on CIFAR-10 (96% accuracy on adversarial data and 93% on clean data) while being the first to our knowledge to classify the type of attack. We are able to distinguish among many of the 11 different attacks on CIFAR-10 with F1- scores ranging from 90%-99%.

Section Title: Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples
  Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples

Section Title: A. Appendix
  A. Appendix

Section Title: A.1. Attack Classes
  A.1. Attack Classes   Table 1  shows the attack classes that we trained and evalu- ated DeClaw on.

Section Title: A.2. Experimental Setup
  A.2. Experimental Setup For the CIFAR-10 dataset, we used all the 60000 natural samples to train and test the second stage classifier. Using a subset of natural CIFAR-10 samples C 6 , we first inferred the reference temperature of the dataset of the natural sam- ples. 1 For each attack class A k , we collected the resulting anomaly-detection feature vector produced by our hook for the same C 6 natural samples. In total, we generated a to- tal of 60000 anomaly detection feature vectors from the CIFAR-10 natural samples and a total of 102000 attack sam- ples across the 11 attack classes. 2 We used a 70% to 30% split between train and test. - resulting in second stage datasets as follows: • training with 42000 and testing with 18000 anomaly detection feature vectors for natural class C and • training with 72200 and testing with 29790 anomaly detection feature vectors across attacked classes A 1 . . . A 11 . We performed the experiments using 12 classes: • one class C of anomaly detection feature vectors from natural samples (divided into subsets C 1 . . . C 6 , each of 10000 samples). • and 11 classes A k , each resulting from the application of a given attack class A k over the same (10000 sample size) subset C 6 of the natural samples. DeClaW provides user control of various basic hyper- parameters. These parameters are shown in  Table 2 . We use a 32-layer Wide Residual Network ( Zagoruyko and Komodakis, 2016 ) as the classifier for CIFAR-10. The network has been trained for 80 epochs with stochastic gradient descent and momentum on 10000 data points from the train set. The momentum term was set to 0.9 and the initial learning rate was set to 0.1, reduced to 0.01 after 50 epochs, and further reduced to 0.1 after 70 epochs. After each epoch, the network's performance on the validation data (the remaining 5000 data points from the train set) was determined. The network with maximal performance on the validation data was used in the subsequent experiments (with all tunable weights being fixed). We attach an adversary detection subnetwork (called ""de- tector"" below) to the ResNet. The detector is a convolu- tional neural network using batch normalization ( Ioffe and Szegedy, 2015 ) and rectified linear units. At this time, we focused on placing our sampling hook into the batch nor- malization stage of the original model's (e.g., CIFAR-10) pipeline.

Section Title: A.3. Clustering of similar attack types
  A.3. Clustering of similar attack types DeClaW learns a statistical envelope for each attack class. However, as some attacks are more sophisticated derivative variants of other attacks, the resulting envelope may be similar. Attacks are essentially perturbations optimized to reduce visible artifacts against the natural inputs. It is therefore to be expected that given the set of selected distributional features selected to identify those perturbations, some of these attacks would produce distributional profiles too close to each other to clearly differentiate between them. The re- sult is that macro-averaged metrics across such confounded classes introduces losses in recall, precision, and F1 val- ues that distort the perceived quality of the detector and classifier. Higher detection and classification accuracy can be obtained by treating such clustered classes as one meta attack class. For example, we found that a set of twenty attack classes clustered into ten different similarity clusters of different sizes. Similarly, in this paper, our twelve attack classifica- tion classes were found to cluster into just the six different classification clusters identified in  Table 1 . Our proposed solution is to cluster similar attacks. To do so, we use a transitive closure with respect to ranked FPR and FNR (being above a given tolerance (e.g., 10%), currently favoring FNR when conflicts arise. For example, given a normalized confusion matrix, the decision whether to cluster to attack classes A i , A j is solely decided on the 2x2 pairwise confusion matrix between those two attack classes - i.e., Now, we threshold the above matrix, for f pr and f nr val- ues greater than some threshold t ≥ 0.2, we focus solely on large attack classification error. For example, given M (A i , A j ) = ( 1 a b 1 ) where a, b could either be 0 or 1. If any is 1, attack classes a, b are clustered as one. As a con- vention, we assign the parent class as the one with the lowest classification label. The same principle applies for larger number of classes, except that a transitive closure over large classification error is used to drive the merge. As the number of classes is small, this process is done by hand, however, a union-join() or dfs() algorithm can be used to implement the transitive closure for the case when the number of classes to be defended is extremely large or it is desirable that such procedure is automated.  Table 3  compares detection and classification accuracy metrics when using the clustered (6 attack clusters) mode. After clustering attacks that were found to be difficult to distinguish based on AFVs, reducing the number of classification labels from 12 to 6, the aver- age classification accuracy increased by 21% while average detection accuracy remained the same.

Section Title: A.4. Feature Generation Strategy
  A.4. Feature Generation Strategy Rather than training our second stage network with pix- els as features, we trained the second stage classifier using anomaly-detection features. For each input sample x i , we examined the resulting input tensor l i presented to the sam- pling hook layer. That is, l i represents the layer inputs to the final batch normalization layer shown in  Table 2 .

Section Title: Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples
  Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples Given that our CIFAR-10 input tensor l i to the batch nor- malization layer contains 640 channels of 8 × 8 features, our sampling hook observes flattened vectors v i contain- ing 40960 values. We compare statistical features of these vectors derived from subsampling projections from it and comparing the outlier distribution against normative mean, variance, and count references. By examining the observed features location relative to channel-wise means µ c and stan- dard deviations σ c , we identify outliers that contribute to an image being clean or adversarial, and if it's adversarial, which type of attack it is. In total, we consider eight categories of features: region- based, extrema-based, histogram counts, histogram bin com- parisons, probability tests, Wasserstein distances, PCA/LDA dimensionality reduction features, and nearest neighbor fea- tures.

Section Title: A.5. Basic Building Blocks for Anomaly Features
  A.5. Basic Building Blocks for Anomaly Features The input tensor values l i have a shape of [1, 640, 8, 8] - meaning each sample has 640 channels of 64 values each. By considering each tensor l i as a vector in R 40960 of ran- dom variables, we estimate means and variances within each of the 640 channels as well as across all the 640 chan- nels of l i . For convenience, let L = len(l i ), - that is, L = 640 × 8 × 8 = 40960 for CIFAR10. We first estimate the baseline channel-wise means and stan- dard deviations, notated as µ c and σ c for a channel c. For a given input tensor l i and its values in channel c notated as l i,c , we define its mean µ c (l i ) = li,c L and standard deviation σ c (l i ) = (li,c−µc(li)) 2 L . Then, similar to the process in the original batch normalization paper ( Ioffe and Szegedy, 2015 ), we compute running means and standard variances for each channel with an exponential smoother to compute our baseline means. For a channel c, let µ base,c re- fer to the baseline mean and let σ base,c refer to the baseline standard deviation. We hypothesize that using aggregation schemes to accumu- late outliers would yield stronger anomaly detection signals. To this end, we set lo and hi watermarks with respect to distribution of values in a channel c. Trivially, these water- marks are (as is commonly done) implemented as σ-based control limits - specifically, lo c = µ base,c − σ base,c and hi c = µ base,c + σ base,c . Given lo and hi control limits or watermarks for each chan- nel, we create three indicator vectors S lo , S mi , S hi for all l i that places each feature into one of those three disjoint sets. Let v i refer to a flattened vector of l i and z i refer to a z-scored version of v i . For any given index k, and channel c corresponding to the channel that v i [k] belongs to, the value of S lo [k] = 1 if v i [k] ≤ µ base,c − σ base,c and is 0 otherwise. Likewise, the value of S hi [k] = 1 if v i [k] ≥ µ base,c + σ base,c and is 0 otherwise. The value of S mi [k] = 1 if µ base,c − σ base,c < v i [k] < µ base,c + σ base,c and is 0 otherwise. Finally, we let an overall outlier vector S ov be a vector where its value is 1 where either S lo or S hi is 1 and 0 otherwise. Using the above 4 indicator vectors, we generate 2 basic aggregation-based anomaly features for each vector S x : For any input tensor l i , these 8 scores are computed and then used as building blocks to generate features used by DeClaW. Some features also compare against values from a reference dataset of representative samples. Let D N refer to such a dataset, formed from 10k randomly chosen values from the natural samples in the training set.

Section Title: A.6. Region-based Features
  A.6. Region-based Features With the building block and aggregatation features described in Section A.5, we specify region-based features. These region-based features count and measure the signal of out- liers in the left and right regions of the distribution curve. There are 25 features derived from the C(S x ) and Z(S x ) building blocks as described in  Table 4 .

Section Title: A.7. Extreme Value Features
  A.7. Extreme Value Features The above region-based features in Section A.6 deal with central and fat portions of the tails of the distribution of l i values. Next, we develop features that isolate extrema tail values of the distribution of l i values. Let R = 25 be the number of region-based features above. For each feature f r , ∀r ∈ [1, 25], we note individual feature extrema values. For a given f r , let φ lo (f r ) and φ hi (f r ) be extrema percentile thresholds (e.g., lo = 10% and hi = 90%). That is, 10% of feature values f r (l N i ) from the population of samples l N i from D N are less than φ lo=10% (f r ). Similarly, 90% of feature values f r (l N i ) are less than φ hi=90% (f r ). Finally, we define indicator flags e r (l i ) that identify, for l i , whether or not the feature value f r (l i ) belongs to the normative population of f r values. For each feature value f r (l i ) of every layer input l i , values within the range [(φ lo (f r ), φ hi (f r )) will be considered nor- mative values and values outside will be flagged as outliers. After computing these R boolean flags for every layer input values l i , we compute a normative score as simply: We also add a normalized score, which divides the normative score by the number of region-based features R. A total of 25 extra features (see  Table 5 ) are thus generated to flag unusually anomalous feature values.

Section Title: A.8. Basic and Comparative Histogram Features
  A.8. Basic and Comparative Histogram Features In addition to the above R region-based anomaly features, to train the neural network to understand fine-grained differ- ences between the estimated pdf of observed layer inputs l i and 10k randomly chosen normative layer inputs l N i from the training set in D N , histogram bincount features were Feature Region-based name specification normative score: used over the standarized values z i of l i , where z i is com- puted by subtracting the mean from l i and dividing by the standard deviation. Until now, each of the features above represent features evaluated solely with respect to the observed layer inputs l i . Next, we develop a set of comparative (logical/boolean, differential, ratio)-based features, that seek to train the neu- ral network to discern the observed distribution of l i values against the distribution of normative l N T ∈ R L layer input values - extracted by element-wise averaging across the D normative natural samples in D N : That is, the average l N T input layer values for natural samples at the preselected batch normalization layer is a vector of size L which is the element-wise arithmetic mean of every l N i from the normative set of D N .

Section Title: Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples
  Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples Given any vector of l i values of size L = 40960, a his- togram of B = 23 bins is computed using the range [−3, 3], resulting in intervals of size q = 6σ B . The resulting B = 23 bincounts (h 01 (l * i ) · · · h B (l * i )) represent 23 additional fea- tures. 3 Moreover, just as we generate B = 23 histogram bin- count features (h 01 (l i ) · · · h B (l i )) for observed layer in- put values l i , we also compute the histogram bincounts (h 01 (l N i ) · · · h B (l N i )) for the normative layer input values. Then, using these two sets of B = 23 histogram bincounts (i.e., observed bincounts against normative bincounts), we compute features that magnify dissimilarity (specifically, square difference as well as relative error) between observed bincounts for a sample and the reference normative bin- counts from natural samples. In all, the following three sets of B = 23 additional features are added: • (B = 23 features f 51 · · · f 74 :) the OBSERVED bin- counts for observed layer values l i : h 01 (l i ) · · · h B (l i ), • (B = 23 features f 75 · · · f 98 :) the SQUARE DIF- FERENCE between observed and normative bincounts: • (B = 23 features f 99 · · · f 122 :) the RELATIVE ERROR between observed and normative bincounts: • and (3 features f 123 · · · f 125 :) the sum, mean, and vari- ance of the RELATIVE ERROR above. The fine-grain targeting scope of our binning features is illustrated in  Fig. 7  by juxtaposing along the three previously discussed coarse-grain aggregation regions.

Section Title: A.9. Comparative A/B Probability Test Features
  A.9. Comparative A/B Probability Test Features A couple of A/B tests are used to compare the size L = 40960 populations of observed l i layer values from norma- tive layer values. We examine whether observed values are normally distributed, whether observed values have a similar mean to that of normative values, whether channel means of observed values are similar to the means of chan- nels in normative values, whether observed values have the same distribution as normative values, and whether observed values have a similar variance to that of normative values. These features are described in  Table 6 .

Section Title: A.10. Wasserstein Distance Features
  A.10. Wasserstein Distance Features Given the bin-counts from Section A.8, the earth moving distances (i.e., Wasserstein distances) compute how much work is needed to make look alike the skyline of a histogram H 1 to the skyline of a reference histogram H 0 . Here, H 1 represents the histogram for the distribution of observed layer input values for a given sample and H 0 represents the histogram for the distribution of normative layer in- put values - that is, extracted from natural samples. Our Wassertein histogram bin-count comparison features, built using features f 99 through f 122 , compare: • (H 1 ) Left Tail from Observed Distribution for the layer inputs of a given sample against (H 0 ) Left Tail of the distribution across all natural samples. • (H 1 ) Right Tail from Observed Distribution for the layer inputs of a given sample against (H 0 ) Right Tail of the distribution across all natural samples. • (H 1 ) Center Tail from Observed Distribution for the layer inputs of a given sample against (H 0 ) Center Tail of the distribution across all natural samples. This hypothesis tests whether the shape of the pdf carries anomaly detection value.

Section Title: A.11. Dimensionality Reduction Features
  A.11. Dimensionality Reduction Features We generate PCA(ndim=2) and LDA(ndim=2) features to test whether separability features improve classification ac- curacy. Fig. 8 shows a xy-plot of AFVs using the first two LDA features. It illustrates the spatial clustering patterns across samples of different attack methods and whether ad- ditional separatability features ought to be introduced and for which attacks.

Section Title: A.12. Nearest Neighbor Classification Support Features
  A.12. Nearest Neighbor Classification Support Features Given a subsample of generated AFVs for both natural sam- ples and samples perturbed by various attack methods, we trained a Radius Nearest Neighbor classifier (RNN). Then,

Section Title: Annex Figures
  Annex Figures   Table 1:   Table 1: Attack classes used in the baseline experiment. In the non-clustered attack mode, 12 classes were used. In the clustered attack classification mode, those 12 classes were grouped into 6 cluster groups based on a transitive closure against either FPR or FNR > 25%. Attack classes that are too difficult to discriminate between each other (and thus have high inter-class FPR or FNR) are then assigned to the same cluster. This clustering-by-FPR/FNR threshold technique is discussed in Section A.3.           Table 2:   Table 2: DeClaW's command line hyper-parameters.           Table 3:   Table 3: Best performing parameters and their resulting accuracy metrics (F1 values for natural and across classes as well as average observed accuracy for detection and clas- sification mode over a grid parameter evaluation. Attack classes were grouped into clusters based on a proxy to sim- ilarity, the corresponding F N R and F P R rates between two attack classes. A transitive closure reduced a dozen attack classes into six. Using this criteria, all models are in a plateau, whose maximal point is 94.2% for classification Acc C+A k and 90.8% for detection Acc)C|A.           Table 4:   Table 4: The basic anomaly detection building block fea- tures based on region-based aggregations.           Table 5:   Table 5: Twenty five additional features are used to identify whether a given feature value f r (l i ) from Table 4 represent extrema events with respect to the overall distribution and percentiles of feature values obtained for normative natural samples.           fig_7 Figure 7:   Figure 7: To compare the observed distribution z i values against the expected distribution of z N i values histograms are used. We use 23 bins and construct features that measure discrepancies between observed and expected bincounts as well as symmetry between selected regions R x and R y of the compated distributions.       Table 6:   Table 6: P-values resulting from the application of several statistical tests are used as features. These tests compare a random small subsample of the population of 40960 ob- served layer input values against a random small subsample of the population of 40960 normative layer input values.        
","[Significance and novelty]
<DNN-based attack-type classifier> The proposed method of using a DNN-based attack-type classifier to classify clean images and adversarial examples is a significant contribution to the field of image classification and security.
<Anomaly feature vector (AFV) on CIFAR10 dataset> The use of anomaly feature vectors on the CIFAR10 dataset for classifying different attack types adds novelty to the approach.

[Potential reasons for acceptance]
<Well-organized paper> The well-organized structure of the paper and the illustration of statistical differences between attack algorithms contribute to its potential acceptance.
<High accuracies in classifying attack types> The achievement of high accuracies in classifying different attack types is a strong point in favor of acceptance.

[Potential reasons for rejection]
<Clarity of attack type classification> The lack of clarity in the meaning of classifying different attack types might be a potential reason for rejection.
<Unreliable results based on clustering algorithm> The reliance on a clustering algorithm for defining composite classes, leading to potentially unreliable results, could be a reason for rejection.

[Suggestions for improvement]
<Clarify attack type classification> The authors should focus on providing a clearer explanation of the meaning and implications of classifying different attack types.
<Reduce reliance on additional clustering algorithms> To improve the reliability of the results, the paper should aim to reduce reliance on additional clustering algorithms for defining composite classes.

"
