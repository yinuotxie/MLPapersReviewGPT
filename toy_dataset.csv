title,url,abstract,authors,review_title,review,rating
Pre-Training by Completing Point Clouds,https://openreview.net/pdf?id=jPSYH47QSZL,"There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.","Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,Matt Kusner",A good one,"This paper proposes a better pre-trained prior for a variety of downstream applications in point cloud analysis. The workflow of the pre-training mechanism is to first 1) generate occluded points that result from view occlusion and then 2) optimize the encoder to learn how to complete the occluded points from the partial point cloud. In downstream applications, the obtained encoder will be used as the initial weights in the network training. Empirical experiments have shown that such a pre-train mechanism can improve initialization over prior baselines and benefit a variety of tasks even with a large domain gap.

Pros:
1. The experimental results have shown a steady improvement in performance by using the proposed pre-training approach in different encoder architectures and different downstream applications. That provides strong support for validating the effectiveness of the proposed approach.
2. I also like the result that the initialization is only pre-trained on the occlusions generated from the ModelNet40 but still work in another dataset. And yet, the pre-training is done in a self-supervised manner. This is a great plus for this approach as it indicates that it could be a general-purpose booster for a wide range of applications without spending too much effort in collecting special-purpose dataset for pre-training. 
3. The paper is well written and presented.

Cons:
1. The improvement, as shown in the statistics, is very incremental in most cases. I understand it is difficult to achieve better results on well-established benchmarks, but it somehow indicates the improvement is limited.
2. Though the paper already stated some nice explanation of the idea behind this approach, I would appreciate it if a more in-depth analysis of why such a pre-training mechanism could work is provided. Specifically, I would like more analysis of why such a pre-training method can adapt to different datasets? What are the common features that OcCo captures across different datasets? 
Some visualization similar to Figure 3 would be helpful.

---- Final Rating ----

The authors' response has resolved my concerns. I would keep my positive rating.","7: Good paper, accept"
Pre-Training by Completing Point Clouds,https://openreview.net/pdf?id=jPSYH47QSZL,"There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.","Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,Matt Kusner","paper shows promising results using point cloud completion tasks for pre-training representations, method is clearly described; however, highly related work not discussed that limits novelty of the proposed OcCo task, experimental setup not fully clear, making it hard to understand the results","The paper considers the problem of training networks for point cloud processing through a point cloud completion task. Given a point cloud, it is rendered from a set of viewpoints and for each viewpoint the set of visible points is determine. A network is then trained to generate the full point cloud from the partially observed point cloud for a given view. Here, an encoder-decoder architecture is used, where the encoder corresponds to the network that should be pre-trained. Experimental results show that the proposed method outperforms two baselines for three tasks (object classification, object part segmentation, and semantic segmentation), when using less training data, and that the pre-training on the occlusion task leads to faster convergence.

On the positive side, the occlusion completion (OcCo) task is clearly described and it should be fairly straightforward for a researcher to setup this task. The task requires no human annotation and is thus suitable for pre-training from large datasets captured in uncontrolled settings. The experiments cover a wide range of tasks and settings and show that the OcCo pre-training strategy outperforms random initialization and the approach from Saunders & Sievers. Here, the simplicity of the OcCo task coupled with its performance is clearly a major strength of the paper. In particular, Fig. 4 shows that the networks pre-trained on the OcCo task tend to converge much faster compared to the baselines.

On the negative side, I feel that the paper oversells the novelty of the OcCo task. The OcCo task is a variation of the (semantic) scene completion task that asks to complete a partial observation of a scene / object and is receiving attention in the computer vision community. Recent examples include [Dai et al.,  SG-NN: Sparse Generative Neural Networks for Self-Supervised Scene Completion of RGB-D Scans, CVPR 2020] and [Hou et al., RevealNet: Seeing Behind Objects in RGB-D Scans, CVPR 2020], with older works including [Firman et al., Structured prediction of unobserved voxels from a single depth image, CVPR 2016]. [Schönberger et al., Semantic Visual Localization, CVPR 2018] use (semantic) scene completion as a proxy task to train a 3D descriptors for 3D-3D matching between models. Given a voxelized partial observation of a scene, they train an encoder-decoder architecture to predict the complete volume (potentially also predicting semantic labels for each voxel). The embedding in the latent space are then used as 3D descriptors (i.e., the decoder part of the network is not needed at test time). In other words, they use the OcCo task for training their networks. They show that the learned representation generalizes between datasets and sensor modalities (training on 3D data obtained from stereo images, tested on LiDAR data). Given this result, I see limited novelty in using the OcCo task for pre-training point cloud networks and it does not seem very surprising that pre-training on the OcCo task should result in meaningful representations.

My second main point of criticism is the level of detail of the experimental evaluation. While the experiments cover a wide range of tasks and settings, I feel that crucial information needed to understand the results are missing:
1) Is the same dataset (ModelNet40) used to pre-train on the OcCo task also used to pre-train the JigSaw approach from Sauder & Sievers? Unfortunately, no details on the latter are provided in the main paper (or I was not able to find them), making it hard to understand how meaningful the comparison is.
2) There are no details on how the networks are trained for the different tasks, e.g., for how many epochs are the network trained for the task at hand?, do the networks converge for all pre-training strategies?
3) How significant are the improvements over the two baselines. For most considered settings, the improvements seem rather small, e.g., often less than 1 point compared to the Rand baseline in Tab. 2 and 3. Is this a meaningful improvment? Or would simply using a different random seed for training explain such a difference? Given that the paper claims that ""These results demonstrate that the OcCo-initialized models have strong transfer capabilities on out-of-domain datasets"" and that ""OcCo-initialized models achieve superior results compared to the randomly-initialized models"", this is an important question to answer.
I think there is enough space in the paper to include this information. Specifically, I do not think that Alg. 1 is necessary in the main paper (but would be good to have as an appendix) as the text and Fig. 1 already describe the approach in sufficient detail. Similarly, the z-Buffer algorithm is a classic computer graphics technique that is covered in basic lectures and does not need to be discussed in detail (e.g., see [Pittaluga et al., Revealing Scenes by Inverting Structure from Motion Reconstructions, CVPR 2019] briefly mentioning z-Buffering and the use of Delaunay triangulation for determining visibility). I think this space could be spend on providing more details.

In the current form, I do not think the paper is ready for publication as the paper, in my opinion, overclaims its contributions, misses relevant work (see also below), and misses crucial details necessary to understand the experimental results. As such, I am currently recommending to reject the paper. I believe that these issues can be addressed, but I would base my final recommendation based on the authors' feedback.

Here are additional detailed comments:
* In Sec. 2.1, I do not understand the comment ""Our goal is to learn a randomized occlusion mapping o : P → P (where P is the space of all point clouds) from a full point cloud P to an occluded point cloud P"". As far as I can tell, the mapping is not learned but follows a fixed pipeline.
* I don't understand how Eq. 2 ""most closely approximates the inverse of eq. (1)"". Eq. 2 is the inverse of Eq. 1. The only approximation that I could see if the 2D projection coordinates are rounded to the nearest integer.
* The introduction teases with the statement ""Current 3D sensing modalities (i.e., 3D scanners, stereo cameras, lidars) have enabled the creation of large repositories of point cloud data (Rusu & Cousins, 2011; Hackel et al., 2017)."" However, only synthetic data is used for pre-training, which is a bit disappointing. I am not convinced that the synthetic datasets ""are qualitatively similar to point clouds in datasets where points are collected via 3D imaging devices such as handheld scanners (Dai et al., 2017a; Armeni et al., 2016) and lidar (Geiger et al., 2012)."" Based on my experience, handheld scanners (RGB-D cameras or (multi-view) stereo cameras) produce much more noisy measurements with outliers while lidar sensors, especially for autonomous vehicles, typically produce sparser point clouds.
* Sec. 3.2 states that ""We observe that, in early stage the encoder is able to learn low-level geometric primitives, i.e., planes, cylinders and cones, while later the network recognises more complex shapes like wings, leafs and upper bodies (non-rigid)."" I am not sure how I see this in Fig. 3 since I don't know what the color-coding signifies.
* Looking at Fig. 3, I am not sure whether the statement that ""clearly separable clusters are formed for different object classes"" is true. There seems to be quite some overlap between classes, but this is also a bit hard to tell given the small size of the figure.
* [Yang et al., PointFlow: 3D Point Cloud Generation with Continuous Normalizing Flows, ICCV 2019] and [Gadelha et al., Label-Efficient Learning on Point Clouds using Approximate Convex Decompositions, ECCV 2020]  both propose generative models for point clouds. Both of them show that they can be used for unsupervised representation learning and they show competitive results for the task of only training an SVM classifier on top of the learned representation for ModelNet. Both should be discussed in the related work.

### After rebuttal phase ###
The comments by the authors and the revised version of the paper successfully address my concerns. I thus recommend to accept the paper.","7: Good paper, accept"
Pre-Training by Completing Point Clouds,https://openreview.net/pdf?id=jPSYH47QSZL,"There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.","Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,Matt Kusner",Limited novelty and weak improvements,"The authors propose completing an occluded point cloud as a pretraining step for point cloud processing methods. Multiple occlusions are generated for the network to complete by simulating a camera perspective. 

Pros: 
- First work analyzing this specific pretraining for point clouds

Cons:
- Weak novelty
- Limited experimental reliability

Overall, the novelty of the paper seems rather weak. The only novel contribution is the idea of point cloud completion as a pretraining task. This idea is rather simple and similar techniques are well known and used in other fields such as NLP. Hence, it does not represent a significant methodological advancement.
Nevertheless, the paper would still be interesting if it showed extraordinary results in this particular field of application. Unfortunately, the experimental results seem weak as well. They show modest gains with respect to existing techniques and they lack any information on run-to-run variance. This makes it impossible to understand if the gains that are shown are statistically significant or just lucky runs with careful parameter tuning. ",4: Ok but not good enough - rejection
Pre-Training by Completing Point Clouds,https://openreview.net/pdf?id=jPSYH47QSZL,"There has recently been a flurry of exciting advances in deep learning models on point clouds. However, these advances have been hampered by the difficulty of creating labelled point cloud datasets: sparse point clouds often have unclear label identities for certain points, while dense point clouds are time-consuming to annotate. Inspired by mask-based pre-training in the natural language processing community, we propose a pre-training mechanism based point clouds completion. It works by masking occluded points that result from observations at different camera views. It then optimizes a completion model that learns how to reconstruct the occluded points, given the partial point cloud. In this way, our method learns a pre-trained representation that can identify the visual constraints inherently embedded in real-world point clouds. We call our method Occlusion Completion (OcCo). We demonstrate that OcCo learns representations that improve the semantic understandings as well as generalization on downstream tasks over prior methods, transfer to different datasets, reduce training time and improve label efficiency.","Hanchen Wang,Qi Liu,Xiangyu Yue,Joan Lasenby,Matt Kusner","Since the idea itself is simple enough, the reviewer would not argue too much about the technical contribution of this paper, but concerns more about the analytic contributions. The reviewer’s concerns lie as follows.","The idea of this paper is simple but fascinating. Actually there are many studies concerning the task of point cloud completion, but using it as the initialization approach to improve the other tasks is quiet novel. The experimental results seem solid and quantitatively prove the effectiveness of the OcCo-initialization.

Since the idea itself is simple enough, the reviewer would not argue too much about the technical contribution of this paper, but concerns more about the analytic contributions. The reviewer’s concerns are as follows.
1. In addition to verify the effectiveness of OcCo-initialization, more analysis on why this simple idea can take effect should also be given. For example, the author should go deeper to explain why OcCo-initialized PointNet can outperform the random initialized PointNet (e.g. by visualizing the learned features of the two kinds of PointNet, like Figure 3).
2. The idea of OcCo-initialization can be concluded as some kind of task oriented initialization approach. Considering the simplicity of this idea, similar initialization strategy can be formulated, such as pre-training network on segmentation task and apply them on classification task. So why the author only chooses the completion task as the initialization strategy, or if task oriented initialization can be considered as a universal strategy in point cloud processing?
3. Although the experimental results look solid, the reviewer still concerns if the proposed OcCo-initialization can achieve the SOTA results or close enough to the current SOTA. For example, PointCNN can achieve much better segmentation results compared to the methods in Table 3. So if the OcCo-initialization can still succeed in the PointCNN which has better ability of learning point cloud features?
4. The author is advised to clarify the necessity of Sec 2.1, which is the main part of the model description. In reviewer’s opinion, a method to generate partial point cloud from single view is essentially not a technical contribution for this paper.
",5: Marginally below acceptance threshold
GamePad: A Learning Environment for Theorem Proving,https://openreview.net/pdf?id=r1xwKoR9Y7,"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.","Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya Sutskever",An intriguing integration of ML and automated theorem proving,"Summary: This paper mixes automated theorem proving with machine learning models. The final goal, of course, is to be able to train a model that works in conjunction with an automated theorem proving system to efficiently prove theorems, and, ideally, in a way that resembles the way humans prove theorems. This is a distant goal, and the authors instead focus on several tractable tasks that are required for future progress in this direction. They start by integrating the Coq theorem proving environment with ML frameworks, allowing for the creation of models that perform various tasks related to theorem proving. In particular, they focus on two tasks. One is to estimate how many steps are left to complete the proof given a current proof state. The other is to determine what is a good choice of next step. Finally, they also consider issues surrounding representations of the various data structures involved in proofs (i.e., the proof tree, variables, etc.). They test various models on a synthetic nearly trivial logical expression proof, along with a more complicated (and meaningful real world) group theory result.

Strengths: This is a very important area. Automated theorem proving has a potentially very significant impact, and being able to take advantage of some of the recent successes in ML would be excellent. The main environment proposed here, integrating PyTorch with Coq could potentially be a very useful platform for future research in this area. The paper exposes many interesting questions, and I generally think we need more exploratory papers that open up an area (as opposed to seeking to finalize existing areas) 

Weaknesses: The paper is pretty tough to understand without a lot of background in all of the existing theorem proving work (which might be fine for a conference in this area, but for this venue it would be nice to be more self-contained). The organization could also use some work, since it's often tough to figure out what the authors actually did. The experimental results seem very preliminary---although it's hard to say, as there is no easy way to compare the results to anything else out there. In general a lot of details seem missing.

Verdict: The authors admit this is a preliminary work, and I agree with that. The paper certainly introduces many more questions than it answers. However, I think that in this case it's a good thing, and this type of paper has the potential to inspire a lot of new and exciting research, so I voted for acceptance.

Comments and questions:

- As mentioned, a lot of the terminology is introduced very quickly and could stand to be more self-contained, i.e., ""tactics"" could be defined as being simple transformations that are applied to a current proof state to obtain another proof state, and each language has a library of tactics available.

- Probably the major contribution of the work is the integration of the CoQ and Pytorch, so a bit more content describing how the Python data structures that wrap around Coq structures would be interesting here.

- I didn't really understand one of the major contributions: the embedding function for the M_i conditioned on the environment. How does the sampled Gaussian vector work here? In general this section is pretty confusing, it would be great to include a schematic to show how the different levels of embeddings for different structures work here.

- How does the real-world dataset work? Does the dataset contain one automated proof of the entire theorem, or several different proofs (ultimately produced by different user choices)? Are you measuring accuracy on the proofs of individual lemmas?","7: Good paper, accept"
GamePad: A Learning Environment for Theorem Proving,https://openreview.net/pdf?id=r1xwKoR9Y7,"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.","Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya Sutskever",Nice exposition on the opportunities and the issues in using machine learning for interactive theorem proving in Coq,"In the paper, the authors describe how machine learning techniques can be used to help build proofs in the widely-used interactive theorem prover Coq. They do so by explaining the experience with their system called GamePad, which converts various proof-related objects of Coq to python data structures so that python-based machine learning tools can be applied to those data structures. 

Although the word, GamePad, appears in the title of the paper, the paper focuses mostly on how Coq works, which aspects of proving in Coq can be aided by machine learning techniques, and what challenges they experienced when using machine learning techniques to the Feit-Thompson data set, an impressive big Coq proof of a famous theorem in group theory. I wasn't impressed by the GamePad tool, which seems to be just a translator of Coq internal data structures to python data structures. But I liked the authors' general exposition about the opportunities and the issues in using machine learning techniques to theorem proving in Coq. They explain that one key difficulty of the tactic prediction problem is the need to synthesize a term parameter to a tactic. They also point out the issue of choosing the granularity of tactic when approaching this problem. 

I give positive score mainly because some other audience in ICLR may learn about a new problem domain and get excited about it by interacting with the authors of the paper.

Here are some minor comments.

* Caption of Figure 1: its goal that the statement ===> its goal the statement

* p3: function K ===> function M

* p5: In the interpreter-inspired embedding of a dependent product, are you drawing a real number from a normal distribution for every occurrence of v in the proof once, and using the drawn real number for the occurrence whenever the occurrence is referred to later? Or when it is referred to later, are you drawing a new real number?

* p6: How did you generate the training data for the experiment reported in Section 6?","7: Good paper, accept"
GamePad: A Learning Environment for Theorem Proving,https://openreview.net/pdf?id=r1xwKoR9Y7,"In this paper, we introduce a system called GamePad that can be used to explore the application of machine learning methods to theorem proving in the Coq proof assistant. Interactive theorem provers such as Coq enable users to construct machine-checkable proofs in a step-by-step manner. Hence, they provide an opportunity to explore theorem proving with human supervision. We use GamePad to synthesize proofs for a simple algebraic rewrite problem and train baseline models for a formalization of the Feit-Thompson theorem. We address position evaluation (i.e., predict the number of proof steps left) and tactic prediction (i.e., predict the next proof step) tasks, which arise naturally in tactic-based theorem proving.","Daniel Huang,Prafulla Dhariwal,Dawn Song,Ilya Sutskever","Interesting direction, but too preliminary","The submission describes a system for applying machine learning to interactive theorem proving. The paper focuses on two tasks: tactic prediction (e.g. attempting a proof by induction) and position evaluation (the number of  remaining steps required for a proof). Experiments show that a neural model outperforms an SVM on both tasks, using proof states sampled from a proof of the Feit-Thompson theorem as a dataset. It's great to see work on applying neural networks to symbolic reasoning. The paper is clearly written, and provides helpful background on interactive theorem proving.

The main weakness of the paper is the limited experiments, which only really show that neural methods outperform an SVM (with only a high level description of the features) - and only on the proof of a single theorem. The paper doesn't explore relevant interesting questions, such as whether the model is helpful for guiding humans or machines in making proofs, or perhaps if the approach can be used to find more human-understandable proofs than those found without training on human data. What are the trade-offs in learning from human proofs instead of automated proofs?

Overall, the paper explores an interesting direction, but I think the current experiments are too preliminary for acceptance.


",4: Ok but not good enough - rejection
Generalisation and the Geometry of Class Separability,https://openreview.net/pdf?id=4NtqESjOIAz,"Recent results in deep learning show that considering only the capacity of machines does not adequately explain the generalisation performance we can observe. We propose that by considering the geometry of the data we can better explain generalisation achieved in deep learning. In particular we show that in classification the separability of the data can explain how good generalisation can be achieved in high dimensions. Further we show that layers within a CNNs sequentially increase the linear separability of data, and that the information these layers retain or discard can help explain why these models generalise.","Dominic Belcher,Adam Prugel-Bennett,Srinandan Dasmahapatra",Linear separability in layers.,"This work investigates how linearly separable the features in a network are as a function of network depth.  I think looking at linear separability is a bit of a naive probe to apply in this case.

Instead of taking the linear classifier accuracy as a measure of the linear separability, why not instead interpret the linear classifiers likelihood as a variational lower bound on the mutual information between the labels and the layers representation?  You could then go beyond linear and measure how much information (or a variational lower bound there of) of how much information is present at each layer of the network, which would likely be of more interest to the crowd of this workshop.

There is related work that I think goes a bit further, including this older work: https://arxiv.org/abs/1905.00414 and its more recent followup (which just came out so this paper can't have hoped to have referenced, but I include it anyway because the authors should find it interesting: https://arxiv.org/abs/2010.15327

Overall, I think this is find for the workshop, but the work would probably need more indepth experiments if it were to be expanded into a full paper.",6: Marginally above acceptance threshold
Generalisation and the Geometry of Class Separability,https://openreview.net/pdf?id=4NtqESjOIAz,"Recent results in deep learning show that considering only the capacity of machines does not adequately explain the generalisation performance we can observe. We propose that by considering the geometry of the data we can better explain generalisation achieved in deep learning. In particular we show that in classification the separability of the data can explain how good generalisation can be achieved in high dimensions. Further we show that layers within a CNNs sequentially increase the linear separability of data, and that the information these layers retain or discard can help explain why these models generalise.","Dominic Belcher,Adam Prugel-Bennett,Srinandan Dasmahapatra",Review,"This paper attempts to explain the generalization in a deep network using separability of the features at various layers. The paper is motivated via a perceptron classifying a Gaussian distribution.

Overall, I think the experimental evidence in the paper is not rigorous enough to draw the conclusions that the authors wish to draw. The paper will benefit from more careful experiments, I am listing down some suggestions below.

What is the connection between the result in Section 2 with those in Section 3? What is the variance on the plots in Fig. 2?
On lines 103-110, max-pooling should reduce variability because it collapses spatial variability in the input. I think there is confounding occurring here due to the presence of batch-norm which (at least in theory) de-correlates the features. De-correlated features are easier to separate using a linear classifier. Further, the features of well-performing deep networks are not easily separable, e.g., https://arxiv.org/abs/1902.01889.",6: Marginally above acceptance threshold
"Using Anomaly Feature Vectors for Detecting, Classifying and Warning of Outlier Adversarial Examples",https://openreview.net/pdf?id=XDo0go2IJgT,"We present DeClaW, a system for detecting, classifying,  and warning of adversarial inputs presented to a classification neural network. In contrast to current state-of-the-art methods that, given an input, detect whether an input is clean or adversarial,  we  aim  to  also  identify  the  types  of adversarial attack (e.g., PGD, Carlini-Wagner or clean). To achieve this, we extract statistical profiles, which we term as anomaly feature vectors, from a set of latent features. Preliminary findings suggest that AFVs can help distinguish among several types of adversarial attacks (e.g.,  PGD versus Carlini-Wagner) with close to 93% accuracy on the CIFAR-10 dataset. The results open the door to using AFV-based methods for exploring not only adversarial attack detection but also classification of the attack type and then design of attack-specific mitigation strategies.","Nelson Manohar-Alers,Ryan Feng,Sahib Singh,Jiguo Song,Atul Prakash",A method to classify clean image and adversarial examples crafted by different attack types,"This paper proposed a DNN-based attack-type classifier based on anomaly feature vector (AFV) on the CIFAR10 dataset. The authors augmented a pre-trained model with a sampling hook at a chosen layer to generate AFV of clean or adversarial examples. The AFVs from the perturbed inputs as well as natural inputs of different classes are used to train the classifier. The results show high accuracies both in detecting clean examples and classifying adversarial examples crafted by different attack algorithms.

Strengths:
1. This paper is well organized. The authors illustrated the statistical differences between different attack algorithms. They as well achieved high accuracies in classifying different attack types.

Weaknesses:
1. The meaning of classifying different attack types is not clear.
2. In this paper, the attack types are first clustered by a clustering algorithm where the attack types in one cluster are treated as one composite class, while the accuracy reported in the paper is based on these composite classes. Therefore, the result is unreliable because it depends on the additional clustering algorithm. For example, a naïve classifier could have a high classification accuracy if the classes are defined by a naïve clustering algorithm.
",Accept
PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning,https://openreview.net/pdf?id=BIIwfP55pp,"Imitation learning is a natural way for a human to describe a task to an agent, and it can be combined with reinforcement learning to enable the agent to solve that task through exploration. However, traditional methods which combine imitation learning and reinforcement learning require a very large amount of interaction data to learn each new task, even when bootstrapping from a demonstration. One solution to this is to use meta reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. In this work, we introduce a new method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). Dual inference strategies allow PERIL to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. In contrast to pure imitation learning, our approach is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties. By exploiting the flexibility of meta-RL, we show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards.","Alvaro Prat,Edward Johns",Review,"Summary: This work seeks to efficiently learn new tasks by combining meta-RL and imitation learning (IL). Such a combination is a natural thing to try, as both lines of work improve sample complexity of learning a new task: meta-RL by leveraging experience on prior related tasks, and IL by leveraging demonstrations. Demonstrations also form a natural way of specifying a new task to the agent.

This paper extends an existing meta-RL approach (PEARL) to additionally leverage demonstrations. This leads to strong results on a set of 2D problems, as well as a 3D reaching task.

Specifically, PEARL is a Thompson-Sampling approach, consisting of two main components: (i) a learned posterior distribution $q(z \mid c)$, which encodes a distribution over latent $z$’s reflecting the task conditioned on previously observed states, rewards, and actions $c = s_0, a_0, r_0, …$; and (ii) a context-conditioned policy $\pi(a \mid s, z)$, which is used for both exploration to infer the task, by producing more trajectories for the context $c$, and for solving the task, once uncertainty over $z$ is low. Specifically, this paper modifies PEARL in the following ways:
- The context-conditioned policy is trained with an objective similar to behavior cloning to produce trajectories that match the demonstrations, in addition to learning from normal reward signal.
- The posterior $q(z \mid c)$ is trained to produce $z$’s that encode information about the task, given demonstrations for $c$.

Strengths:
- This paper studies an important problem: how can we quickly learn new tasks? For many real-world RL tasks, we want policies that can quickly adapt to new tasks without retraining from scratch. This paper observes that prior approaches have drawbacks: IL on its own can be data-hungry, requiring additional roll-outs or many demonstrations; and meta-RL can be challenging with sparse rewards. Therefore, combining the two is a natural and promising direction to investigate.
- The experimental results are generally quite encouraging. PERIL substantially outperforms the baselines, and can even generalize to a fairly wide distribution of 2D tasks (i.e., the same policy can learn to simultaneously do reaching, peg-placing, and key-rotating tasks, while existing works typically learn a narrower task distribution).

Weaknesses:
- Fairly strong assumption. This paper assumes that an expert distribution $p_{\pi_E}(\tau \mid z)$ over trajectories conditioned on the learned latent $z$ is available. This seems to be a fairly restrictive assumption, since $z$ is learned by PERIL, and therefore, it seems unreasonable for an expert policy to also be able to condition on $z$. Instead, it would be nice if we could relax this to only condition on e.g., the observation $o$.
- Clarity. While the high-level approach is clear, many of the details are confusing and unclear, which makes it challenging to evaluate this approach. I list the main points of confusion below.
    - The problem statement defines the task in terms of $z$, which is confusing, because $z$ should be part of the approach, rather than part of the setting. In particular, it’s unclear what it means for the dynamics model to condition on $z$. It seems like this may be mixing the learned latent $z$ with the true state $s$? More generally, the problem statement (Section 3.1) mixes the approach with the problem setting, which makes it confusing to understand what is a constraint due to the setting, and what’s a design choice for the approach.
    - The principled way to optimize Eq (3) is to maximize variational lower bound (Barber & Agakov, 2003), by substituting the posterior $p(z \mid \tau)$ with an arbitrary function $q(z \mid \tau)$. This appears to be what the paper is doing, but the current phrasing is pretty unclear. In particular, it’s unclear to me how $\mathcal{L}_\text{info}(z)$ is optimized / defined. How is $p(z \mid \tau)$ defined? It’s clear how you can do this in the case where the task descriptor is available, e.g., in Section 3.4, but in general, it’s unclear what the learned $z$ should be. Is this from leveraging the latent space of the expert SAC agent? What is $\pi_b$ in Equation 6?
    - The notation for the task-dependent objective $G(\tau)$ seems unnecessary and serves to distract — in particular, it’s not initially clear why we need this and not just maximizing the expected discounted rewards. I would suggest removing this notation, and just saying at the end of the approach: “overall, we minimize the following loss: $\mathcal{L}_\text{critic}(z) + \mathcal{L}_\text{info}(z) + \mathcal{L}_\text{aux}(z) + \ldots$.
    - There are quite a few undefined loss functions in line 12 of Algorithm 1, in particular $\mathcal{L}_\text{mi}$ and $\mathcal{L}_\text{D_KL}$.
- Related work. This paper generally seems to lack appropriate citations in several key places.
    - In the introduction, several key areas seem to require citations (e.g., citations for meta-IL, posterior sampling with meta-RL should cite PEARL, claims that meta-RL requires shaped rewards / claims that meta-IL cannot adapt afterwards).
    - The following references seem highly relevant to related works section on exploration in meta-RL: [1], [2], [3].
- Experiments.
    - Why is the behavior cloning baseline with noisy demonstrations? It seems like the fair comparison should be BC w/o noise.
    - This paper claims that PERIL is capable of exploring beyond demonstrations, but the tasks that this paper evaluates on don’t seem to require much sophisticated exploration. Substantiating these claims seems to require evaluation on tasks requiring more exploration.

I am initially recommending rejection, due to the aforementioned weaknesses. I believe that the related work and clarity could be improved during the rebuttal period, which would help me raise my score, although I find the strong assumption to be a fairly serious weakness.

Additional comments:
- Ill-formatted citations. Many of the citations are missing the year, e.g., Zhou et al., Ross et al., Mendonca et al., Duan et al., Yu et al.
- Contextons —> contexts?
- Variational Autoencoders are inconsistently abbreviated as VE and VAE. Seems like it should follow the standard of using VAE.

References:
[1] VariBAD: A Very Good Method for Bayes-Adaptive Deep RL via Meta-Learning. Luisa Zintgraf, Kyriacos Shiarlis, Maximilian Igl, Sebastian Schulze, Yarin Gal, Katja Hofmann, Shimon Whiteson. Oct. 2019. ICLR 2020. https://arxiv.org/abs/1910.08348.

[2] Explore then Execute: Adapting without Rewards via Factorized Meta-Reinforcement Learning. Evan Zheran Liu, Aditi Raghunathan, Percy Liang, Chelsea Finn. June 2020. ICML LifelongML Workshop 2020. https://openreview.net/forum?id=La1QuucFt8-.

[3] Environment Probing Interaction Policies. Wenxuan Zhou, Lerrel Pinto, Abhinav Gupta. July 2019. ICLR 2019. https://arxiv.org/abs/1907.11740.",4: Ok but not good enough - rejection
PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning,https://openreview.net/pdf?id=BIIwfP55pp,"Imitation learning is a natural way for a human to describe a task to an agent, and it can be combined with reinforcement learning to enable the agent to solve that task through exploration. However, traditional methods which combine imitation learning and reinforcement learning require a very large amount of interaction data to learn each new task, even when bootstrapping from a demonstration. One solution to this is to use meta reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. In this work, we introduce a new method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). Dual inference strategies allow PERIL to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. In contrast to pure imitation learning, our approach is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties. By exploiting the flexibility of meta-RL, we show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards.","Alvaro Prat,Edward Johns",A very ambitious work which falls short on the science,"## Summary of the Work
The work propose a method which allows us to synthesize meta-RL and meta-IL, by pre-training and conditioning a context-based off-policy meta-RL algorithm on imitation data. Strongly inspired by PEARL (Rakelly, et al 2019) and meta-IL (Yu, et al 2020), this method outperforms previous methods by varying margins on a range of newly-introduced 2D and 3D robotics tasks. The work introduces several new design elements and losses to this family of methods, and the experiments do not make clear which ones are responsible for the increased performance. Additionally, it's not clear the included experiments can fully substantiate the long list of claims provided by the authors, not the least of which is that their method performs zero-shot adaptation to new tasks.

## Pros and Cons

### Pros
* Addresses several important problems in adaptive RL
 - generalizing to complex tasks and outside-of-distribution tasks
 - using demonstration data to avoid costly random exploration
 - fine-tuning policies acquired with meta-RL/meta-IL
* Hyperparameters and reward functions are well-documented
* The visualizations are clear and helpful
* Overall the work is well-organized

### Cons
* Makes many claims about the method which are difficult to fully substantiate in such a short paper
* Treatment of prior work is limited to only a few methods from the past few years, and does not acknowledge many prior works in context-based meta-RL/MTRL. Relationship to the very-similar prior works PEARL and meta-IL is unclear.
* Experiments section makes it very difficult to compare presented results to prior work
* Lack of ablations make it unclear which (of many) new design decisions are responsible for the method's performance
* Experiments don't appear to make a credible simulation of demonstration data which might be encountered in the real world
* Claims of applicability to robotics necessitates comparisons with robotics benchmarks (See MetaWorld and/or RLBench).
* Presented method is very complex
* Experiments lack multiple seeds and any statistical significance tests
* Terminology and notation often veers far from previous works and conventions, making it difficult to parse. Sometimes writing is ambiguous or unclear.

## Evaluation
### Quality
3/5
The presented work and experiments are high-quality in their motivation, implementation, and usually their presentation. I think the quality of this work suffers when we consider how well it positions itself with respect to prior work (not well), and the level of detail with which it explores and substantiates each of its claims (of which there are many, making it impossible to address any of them fully). The method section is extensive and recapitulates in detail many concepts from RL, variational inference, IL, etc. It can probably be more sparse to make room for a better treatment of prior work and more experiments/analysis of the work's claims.

### Clarity
2/5
This work suffers from significant clarity issues, mostly around use of language (e.g. zero-shot learning vs few-shot learning), non-standard notation and terminology (e.g. ""primal inference,"" ""privileged information,"" etc.) and the use of equations which are not really necessary to demonstrate a point, 

### Originality
3/5
While original in nature (for instance, introducing privileged information, considering fine-tuning and off-distribution tasks, using demonstrations for pre-training, etc.), the work does not make it easy for the reader to divine how its contributions build on significantly-similar previous works which are highly-cited. This does it and the reader a disservice. 

### Significance
2/5
If all claims in the work are well-substantiated, it can be a very significant work, but I don't believe they are substantiated. For instance, the introduction mentions low-dimensional observation spaces as a limitation of current meta-RL/IL work, but the experiments don't seem to contain any use of high-dimensional (e.g. image) observation spaces. It's not clear how the claim of zero-shot learning is supported.

### Misc Editorial Comments and Reviewer's Notes

#### Claims
* Addresses three limitations of current meta-RL/IL
 - shaped reward functions
 - constrained low-dimensional action/observation spaces
 - requires hand-defining low-dimensional observation/action spaces
* A hybrid framework which combines the merits of RL and IL
  - tasks defined only using demonstrations
  - unlike other (meta)-IL algorithms, allows for improvement of the policy after adaptation
* Uses only proprioceptive actions from the agent, and implicitly recovers the external environment state
* Allows for learning new tasks without requiring any expert knowledge in the human teacher
* Achieves ""exceptional"" adaptation rates and is capable of exploiting demonstrations for efficient exploration
* Outperforms other meta-RL and meta-IL baselines
* Is capable of zero-shot learning
* Is capable of multi-family meta-learning and out-of-family meta-learning through clustering in the latent space
* Shows how we can use privileged information (during training) to create an auxiliary loss for training the embedding function, allowing us to recover the ""true underlying state""

#### Mechanisms
* Represents a task by a latent vector, which is the belief state of the task given a demonstration
* Meta-training encourages high mutual information between the demonstration data and latent space
* After adaptation, the agent can explore and update the latent space

#### 1. Introduction
* ""hand-crafted, shaped reward functions..."" nothing in the meta-RL formulation requires shaped reward functions, as opposed to a sparse ones which are easier to craft. Granted, on-policy meta-RL algorithms are challenges by sparse reward settings in a similar fashion that on-policy RL algorithms are challenged by sparse reward settings, but this is a property of RL in general and not just meta-RL. [2] is a meta-RL method which can cope with sparse rewards, and is extensively-cited in this work.

* ""defining a low-dimensional.."" this is not a limitation per se of meta-RL or meta-IL -- there's nothing in their formulation which necessitates meta-RL operating on low-dimensional state as opposed to images, though it is certainly a design challenge. See [1]

* ""different task families"" Perhaps I have misunderstood the authors, but this seem orthogonal to the purpose of the work and of meta-RL. It is not immediately clear what the authors mean by ""task families."" While there is certainly work on cross-domain transfer in IL and RL, adapting policies to different action and/or observation spaces is not a typical goal of meta-IL and meta-RL algorithms, so it seems strange to level this critique. 

[see ""claims"" above]

#### 2. Related Work
* Meta-learning and meta-RL far predates Wang and Duan. Please see [3,4, etc.]
* Modern work on context-based meta-RL and adaptive RL predates PEARL. Please see [5, 6, 7] which all perform variational inference on trajectories to generate a latent context, which can then be used for adaptation

#### 3. Method
* The proposed approach seems hardly different than PEARL[2] with the following changes. The reviewer may have missed something, but given close relationship between these methods, please make crystal clear for readers the differences between this method and the substantially-similar PEARL method.
  - Pre-training uses demonstration data rather than RL episodes
  - This work studies what happens if you continue training after adaptation
  - Introduces an auxiliary loss which allows conditioning on privileged information
* 3.2: ""Traditional meta-RL methods leverage RNN-based"" - This is hardly true in any universal sense. Previous meta-RL method have used RNNs [8], variational inference [2], autoregressive models (attention)[9], hierarchy [10], exploration policies [11], etc. to implicitly model the latent task space.
* 3.3: The included equations don't seem to add much to the paper's story and seem to recapitulate well-known results from RL, variational inference literature, or cited work.
 * 3.5: The use of a SAC expert trained on full-state versions of the environment is not a faithful simulation of expert data, which will be significantly noisier and lower-entropy than a SAC expert, and also is unlikely to be optimal according to any RL loss. I think that this reduces the IL aspects work to a form of offline RL where the offline data source is a SAC policy, and the results demonstrate that the method can reconstruct the privileged information which was available to the SAC agent. The authors note that they augment the demonstration data with ""imperfect demonstrations"", but are silent about how this is achieved (and it must be done with great care).

#### 4. Experiments
* 4.1: Though it is ambiguous from the text, these experiments seem to either present 1 experiment per method, or the average of 3 experiments per method. This is unfortunately not enough data to make a statistically meaningful comparison of the performance, especially considering the small performance differences involved. Please see [12] for a handy guide on how to compare performance. In short, you will likely need 10 seeds for each experiment and should conduct a statistical test to ensure your differences are real. Please include a 95% confidence interval in your plots for the benefit of readers.
* 4.1: These experiments are meaningful and helpful, but it's also important to readers that they can verify you have reasonable implementations of the comparison methods. This necessitates providing some results for some of the environments used in Yu, et al and/or Rakelly, et al. How is the reader to know your implementation or hyperparameters are fairly representing the comparison methods?
* 4.2: I think the reader would benefit from seeing t-SNE plots from the comparison methods as well. The presented plots look very similar to t-SNE plots generated by plotting samples from a PEARL posterior.
* 4.2: It is very unclear what the authors mean by ""zero-shot"" learning. By my estimation, this method always requires some samples of the target environment to attain the presented performance, making it squarely a few-shot domain.
* This work introduces many new design elements on top of PEARL and Yu et al, and it's unclear which of them are responsible for the observed performance. Please include ablations which compare your method's performance without each new design element, to demonstrate the impact of each.

[1] https://arxiv.org/pdf/2006.07262.pdf
[2] http://proceedings.mlr.press/v97/rakelly19a/rakelly19a.pdf
[3] http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.34.1796
[4] https://www.sciencedirect.com/science/article/abs/pii/S0893608002002289
[5] https://openreview.net/pdf/29c35690ff52463c84d9456ab511e4c944ddbea4.pdf
[6] https://arxiv.org/pdf/1809.10253
[7] https://arxiv.org/abs/1806.02813
[8] https://arxiv.org/abs/1611.02779
[9] https://arxiv.org/abs/1707.03141
[10] https://arxiv.org/abs/1710.09767
[11] https://arxiv.org/abs/1802.07245
[12] https://arxiv.org/abs/1904.06979",3: Clear rejection
PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning,https://openreview.net/pdf?id=BIIwfP55pp,"Imitation learning is a natural way for a human to describe a task to an agent, and it can be combined with reinforcement learning to enable the agent to solve that task through exploration. However, traditional methods which combine imitation learning and reinforcement learning require a very large amount of interaction data to learn each new task, even when bootstrapping from a demonstration. One solution to this is to use meta reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. In this work, we introduce a new method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). Dual inference strategies allow PERIL to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. In contrast to pure imitation learning, our approach is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties. By exploiting the flexibility of meta-RL, we show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards.","Alvaro Prat,Edward Johns","Writing is not clear and needs some work, Connection to prior work needs extra attention, An additional baseline is needed","## Summary of work
This work proposes PERIL, a method for combined Meta Imitation Learning and Meta Reinforcement Learning using context-based meta-learning. Given a set of demonstrations, a latent variable representing the desired task is inferred, and trajectories are generated conditioned on the inferred latent variable.  The data from the expert demonstrations and trajectories are used for meta-learning updates.

## Review
Key comments below are included in bold text.

* Related Work
  * I don't agree that meta-learning was conceptualized as an RNN-based task. There are many variants of meta-learning, RNN-based, MAML (optimization based), Encoder-based (Neural Processes), etc.
  * It seems that ""Scalable Meta-IRL Through Context-Conditional Policies"" (Ghasemipour et al. 2019) and maybe ""Robust Imitation of Diverse Behaviors"" (Wang et al., 2017), are closely related context conditional meta learning methods (similar to Yu et al. which you cited). They may merit citation.
  * Bottom of page 2: Why do they have the traditional caveats? They are using rewards, so they should be able to do better.
* Section 3
  * Section 3.1: A number of works which also combine Meta-IL & Meta-RL use the setup you use as well (Primal Inference + Exploratory Adaptation). I think you should acknowledge this and provide citations (such as Mendonca et al., Zhou et al., Gupta et al. which you cited in other sections).
  * Section 3.2: What is a Variational Encoder? This is not a standard term.
  * Section 3.2: Please clarify how the KL relates to the mutual information. While this may be a simple connection, I think it should be explained better.
  * __Section 3.2: Where is equation 1 coming from? It looks like the Evidence Lower Bound but $\mathcal{G}(\mathcal{T}|z)$ is not specified and could be a non-likelihood objective function. Please clarify.__
  * __Section 3.2: You mention that you are using an encoder in the same manner as Rakelly et al., but you do not actually describe the method in your manuscript and how you perform context aggregation. Please include a complete description in the main manuscript.__
  * __Section 3.2: First paragraph of page 4 requires significantly better clarification. Specifically the sentences ""This is generally not ... in unseen environments."" It is not clear why you are saying your context-based approach is better.__
* __Section 3.3:__
  * __While you do acknowledge Yu et al. in the beginning of this section, you understate the extent to which this is similar to their mutual information objective. Equations 3&4, ""Learning from demonstrations"", and ""Linking variational posterior"" are effectively identical to Yu et al.'s with different variable names. You should clarify and attribute credit to their work in a more clear manner.__
  * __I do acknowledge that you have tried to address the intractability of $\mathcal{L}\_{info}$ in a different manner than Yu et al., but the main manuscript would benefit from a brief explanation for why you took this route (even though you have derivations in the appendix)__
  * __You need to explain why $\mathcal{L}\_{BC}$ is not included in your objective. From looking around in your appendix, I think this is because you are estimating the $p(z)$ marginal with the expectation distribution as in equation 5, so you don't take the gradient with respect to $q\_\phi$. This still needs explanation in my opinion.__
  * __Section 3.4: First sentence, what do you mean by unsupervised? You are using the critic to train the model the generates $z$. Please elaborate.__
  * __Section 3.4: Sentence before equation 9, doesn't this depend on what information is included in the vector $b$?__
  * __General Question: What is the reinforcement learning algorithm you are using?__
  * __Section 3.5: I don't understand what you mean by $\mathcal{D}^{\mathcal{T}}$?__
* Section 4
  * Please explain what the auxiliary informations are for each task.
  * __Baselines__
    * __As far as I can tell, you have not compared to any prior method that combines Meta-IL and Meta-RL (such as Mendonca et al. or Zhou et al. which you cite in your work). I think this is a big flaw of your experimental section. Including this would demonstrate whether your latent-based meta-learning approach is better or not from prior works with used other types of meta-learning, which should be your main value proposition. I think latent-based might work better than the MAML-based as in some prior work, but you have not provided any experiments to support this.__
    * __Why are you comparing to noisy BC and not normal BC (also from your description of noisy BC is not clear what the method is)? Also please clarify what BC loss you are using (e.g. mean-squared error loss, maximum likelihood with learned variances, or whatever you used).__
    * __I think it would be valuable to also include one setting where only the auxiliary loss + critic loss (no mutual info) is used to see the the value of the additional mutual information losses.__
  * Section 4.1: How many train/test tasks in each task family. 
  * __Section 4.1: You mention sparse rewards here, but in the appendix there is information about ""dense rewards during meta-training"" for the critic. I don't quite understand what is happening here. Are you using sparse or dense rewards? And is every baseline method that uses rewards use the same type of reward? Please clarify.__
  * __Section 4.2: Do you mean that you are training a single agent on all tasks simultaneously? How are you doing this when in the observation/action spaces are different? Do all these have the same observation/action space?__
  * __Section 4.2: Last sentence of paragraph 1 needs elaboration. I don't think you have provided any explanation for these experiments, for example how you are setting up experiments for adapting to unseen dynamics.__
  * __Section 4.2: Table 1, how is 0 possible? Also, you haven't provided explanation for how these values are computed.__
",4: Ok but not good enough - rejection
PERIL: Probabilistic Embeddings for hybrid Meta-Reinforcement and Imitation Learning,https://openreview.net/pdf?id=BIIwfP55pp,"Imitation learning is a natural way for a human to describe a task to an agent, and it can be combined with reinforcement learning to enable the agent to solve that task through exploration. However, traditional methods which combine imitation learning and reinforcement learning require a very large amount of interaction data to learn each new task, even when bootstrapping from a demonstration. One solution to this is to use meta reinforcement learning (meta-RL) to enable an agent to quickly adapt to new tasks at test time. In this work, we introduce a new method to combine imitation learning with meta reinforcement learning, Probabilistic Embeddings for hybrid meta-Reinforcement and Imitation Learning (PERIL). Dual inference strategies allow PERIL to precondition exploration policies on demonstrations, which greatly improves adaptation rates in unseen tasks. In contrast to pure imitation learning, our approach is capable of exploring beyond the demonstration, making it robust to task alterations and uncertainties. By exploiting the flexibility of meta-RL, we show how PERIL is capable of interpolating from within previously learnt dynamics to adapt to unseen tasks, as well as unseen task families, within a set of meta-RL benchmarks under sparse rewards.","Alvaro Prat,Edward Johns",review,"Summary:

This paper introduces PERIL, a meta RL method that combines demonstration trajectories and trajectories collected by the policy, in order to adapt to a new task. To this end, the authors combine ideas from metaRL (specifically from PEARL (Rakelly et al. 2019) and Humplik et al (2019)) where a set encoder is used to encode trajectories to a latent vector describing the task, with imitation learning techniques by (a) training this encoder also with demonstrations (b) initialising the latent vector at test time by feeding demonstrations through the encoder, and (c) having additional losses inspired by metaIL techniques. The motivation is that using demonstrations allows us to learn tasks that are difficult otherwise, for example because the rewards (at test time) are sparse. 

Overall impression:

I like the idea of using demonstrations for metaRL when tasks are sparse. Many metaRL methods do not work well in sparse reward tasks, and using expert demonstrations is a nice way of guiding the agent towards behaviour that can solve the task. Empirically, the proposed method PERIL outperforms the baselines PEARL and MetaIL, so that is promising. The authors provide analysis of the latent space which nicely illustrates what the method has learned. However, PERIL is quite complex since it consists of many different parts and loss terms (six if I counted correctly), and it needs demonstrations + interactions + (sparse) reward signals at test time. I found it hard to keep track of everything and make sense of how these parts fit together. From both the text and the empirical results, it is not clear to me why all the parts are necessary / what they do, and I am left wondering if a simpler approach would work as well. The notation and mathematical formulation in the paper is not polished enough (there are inconsistencies, variable name clashes, some parts of the objective function not properly introduced and explained) which added to my confusion. Therefore, even though the idea seems promising, I think the paper is not quite ready for publication.

Questions:
- In the introduction you say that MetaIL methods have the drawback that ""after adaptation, they cannot continue to improve the policy in the way that RL methods can"". You say that you method PERIL ""allows for continual policy improvement through further exploration of the task"". I have a few questions about this.
 - Since only the latent embedding is updated, doesn't PERIL also suffer from the fact that the policy cannot be improved in the way that RL methods can (but instead, all adaptation is that within the limits of task inference)? 
 - Why is additional exploration at test time even necessary, if we have expert rollouts and the policy itself isn't actually updated (the only thing that's adapted at test time is the latent embedding)? If all the demos + trajectories are used for is task inference, then shouldn't the expert demos always be sufficient?
 - You motivate your approach by saying that at test time, it is useful if the expert does not have to provide a shaped reward. However, you do make use of a shaped reward during training - this is a limitation that should be discussed in the paper. In addition, you still need (sparse) rewards at test time. Are those really necessary, given that you have a demonstration of the task? Did you test PERIL without those sparse reward inputs to the encoder?
- Table 1, how was the agent trained? Was it with number of adaptation trajectories k>0? If so, what if you would train the agent with k=0? On the other hand, can you get good zero-shot adaptation performance by just increasing the number of demonstrations? 
- You say $d_\lambda$ is a VAE, but if I understand your setup correctly then $d_\lambda$ is only the decoder of a VAE right? And Eq 8 is the reconstruction loss? Which also means it's not technically a VAE, because in encodes and decodes different things (encodes trajectories, decodes task descriptors - Humplik et al. 2019 describe this as an information bottleneck). Shouldn't there also be a KL term somewhere here?
- Where is $L_{bc}$ used? It's not part of Eq (7), but I also can't find it anywhere else except in Algo 1 and Fig 2. And what about $L_{mi}$, it's only in Algo 1 but nowhere else? Fig 2 has $L_{KL}$, where is that from? It would really help my understanding of piecing everything together if there was one single equation somewhere, that includes all loss terms. For each loss term, I as the reader want to clearly understand where it comes from, and why it is necessary (see suggestions for additional baselines/ablations below).

Suggestions / Feedback:
- The problem formulation and the proposed solution don't match. In your problem setting you say you're in a general POMDP where the true state may be partially observed, but in your algorithm you rely on the fact that it's a POMDP only w.r.t. the task (i.e., reward and transition function) and *not* w.r.t. the environment state. That's an important difference! To explain that in more detail: in the introduction and problem setting you say $z$ models the true underlying state $s$ which can change at any moment: your transition function is $p(o',z'|o,a,z)$ where $o$ are observations. However, the entire formulation in your algorithm relies on the fact that $z$ does _not_ change over time, but instead describes a fixed task. That's also what PEARL does, which is what your formulation is based on. I think there's two ways to resolve this: (A) Either change the problem setting such that $z$ is fixed throughout time and define the transition function as $p(s'|s,a,z)$ where the environment state $s$ is now fully observable, or (B) change the algorithm to actually model a belief over a latent $z$ that can change over time. Option (A) is probably an easier fix, but then you might also have to change some of the environments (if I understand correctly, in Key2D the state of the handle is unobserved and can change the unobserved environment state).
- Section 3.1, I would add explicitly what the objective of the policy is (both in writing and in a mathematical expression). You aim to maximise the return of a policy that conditions on $K_d$ demonstrations, and which has interacted with the environment for $K_r$ rollouts (changing the notation here to make the distinction clear). From there it is easy for the reader to see what happens if you set $K_d=0$ (you get something more similar to PEARL), and what happens when you set $K_r=0$ (which is the zero-shot case). It's good to contrast this for the reader, and explain / show empirically why and when $K_d>0$ and $K_r>0$ is necessary. (See my comment on baselines below.) 
- To understand PERIL better, I would suggest to add a few baselines. 
 - PEARL with a pre-initialised buffer that contains the demonstrations. The encoder and policy will be trained as normal, but there's additional data coming from the buffer that contains expert trajectories. Since PEARL uses an off-policy algorithm it is possible to train the policy with this data. I think this is an important comparison, because it's a very simple way to incorporate demonstrations into PEARL and it would be good to understand if/when/why this works/doesn't work.
 - In addition to the above, use the demonstrations at test time to initialise the context in PEARL. This is very close to the setting in PERIL, except that some parts of the objective function are missing ($L_{info}$, $L_{aux}$, $L_{bc}$, $L_{mi}$ - I think).  This would give insight into whether those additional losses are truly necessary (currently you only have ablations on $L_{aux}$).
 - Zero-Shot PERIL. There is some analysis of this in Table 1, but I think it would still be helpful to add this baseline. Does it work well for within-task-distribution adaptation (Fig 4) and not so well for settings that require more generalisation (Fig 5)? What if we just throw in more demonstrations, is that sufficient to do zero-shot adaptation or do we really need the policy rollouts? I think this is a central question that should be very clearly answered in the paper. Table 1 is a good start but this analysis can be expanded.
 - Humplik et al. (2019), but with additional demonstration data to train the encoder/decoder. Again, this is the simplest way to incorporate the demonstration data into this method without explicitly making use of it at test time. This comparison would tell us something about why the demonstrations are necessary - are they necessary during training but not at test time, or the other way around, or are they necessary both during training and testing?
 - Not sure I got everything, there's still $L_{bc}$ and $L_{mi}$ which I'm not entirely sure where they come from and if they are necessary. But basically, I think it's really important to analyse which parts are necessary - and make the method as simple as possible if you find some parts are not necessary.

Smaller comments (didn't influence my score):
- There's a clash between using the variable k/K for the demonstrations (e.g., Sec 3.1 ""primal inference"", Sec 4.1 first sentence, Fig 7), and for the number of policy rollouts (e.g., algorithm 1 line 5, Table 1). This is confusing, so I strongly suggest using two distinct variable names (something like $k_d$ and $k_r$ also works).
- Similarly it would help if you use two separate notations for the trajectories $\tau$ that come from the demonstrations, and the ones that come from the policy. Throughout Section 3 I don't always understand which one of the two you are talking about.
- Your references need fixing. Some of them are without a year, and some say technical report even though they were published at a conference (e.g. Finn et al., Rakelly et al.). Your ""Wang"" reference for RL2 also seems wrong (first sentence in related work)? It should be Jane Wang et al. ""Learning to reinforcement learn"". The way I always get my bibtex entries is via scholar.google.com: search for the paper there; click on the ""cite"" button under it and then ""BibTeX"" (Double check whether the paper was published somewhere though, google scholar often only puts the arXiv link then actually the paper was published somewhere. Sometimes the authors also put the correct bibtex comment on their homepage/github with the code).
- Fig 1 left, there's a typo: ""learn to lean"" -> ""learn to learn""
- For your experiments, I would call PERIL-A only ""PERIL"" (since this is your full method, including all losses), and then call the *ablations* different, so for example PERIL-noAux when you remove the auxiliary loss.
- All figures should have some form of indication of the error/std/confidence interval (using shaded regions around the mean for example).
- Sec 4.2, explain what the multi-task family setting is and why it is challenging. 
",4: Ok but not good enough - rejection
C-Learning: Learning to Achieve Goals via Recursive Classification,https://openreview.net/pdf?id=tc5qisoB-C,"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.","Benjamin Eysenbach,Ruslan Salakhutdinov,Sergey Levine","Good idea, but confusing manuscripts.","The authors propose a new algorithm, called C-learning, which tackles goal-conditioned reinforcement learning problems. Specifically, the algorithm converts the future density estimation problem, which goal-conditioned Q learning is inherently performing, to a classification problem. The experiments showed that the modification allows a more precise density estimation than Q-learning, and in turn, a good final policy.

Overall, I like the general idea to use classification as a tool for estimating the future density function. Especially, the idea is valuable in that it allows a better understanding of prior Q-learning based approaches in choosing a sensitive hyperparameter. However, the manuscript can be enhanced much by adapting more precise notations and adding more explanations on equations:
* $p$ is highly overloaded; it is used to represent future conditional state density function and marginal state density function for both on-policy and off-policy, and transition dynamics.
* Also, it would be important to notate $\pi$ in most of the parts, including $p$, $Q$, and $C$ (unless it is very obvious). Especially, the current notation is very confusing when the off-policy algorithm is introduced.
* Related to this concern, I am not fully convinced of the off-line algorithm due to the marginal future state distribution $p(s_{t+})$. Doesn’t it supposed to be $p^{\pi_{\text{eval}}}(s_{t+})$, and therefore, does the marginal distribution also need to be adjusted as $p(s_{t+}|s_{t+1},a_{t+1})$?
* It would be also helpful if the full derivation for Equation (6) is included in the main manuscript.

-- After rebuttal

I've read the authors' feedbacks and other reviewers' comments. My major concern was the clarity of the manuscript as other reviewers mentioned, and I believe the concern has been resolved during the rebuttal period. I adjusted my ratings representing that.",6: Marginally above acceptance threshold
C-Learning: Learning to Achieve Goals via Recursive Classification,https://openreview.net/pdf?id=tc5qisoB-C,"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.","Benjamin Eysenbach,Ruslan Salakhutdinov,Sergey Levine",well written and clear,"Summary:
This work presents a goal-conditioned RL, which estimates probability density using a classifier. 

Strengths:
+ The problem is well explained, the logical structure seems adequate. 
+ The paper is well written and clear. 
+ The approach technically sounds and mathematically well-formulated.

Weaknesses:
- Although the reported evaluation results are competitive to baselines, it would have been even stronger if the performance is substantially improved. Do you have any insight on how better results can be achieved?

 ","8: Top 50% of accepted papers, clear accept"
C-Learning: Learning to Achieve Goals via Recursive Classification,https://openreview.net/pdf?id=tc5qisoB-C,"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.","Benjamin Eysenbach,Ruslan Salakhutdinov,Sergey Levine",Interesting and relevant to UOM paper but relevance and connection is not discussed. ,"This paper studies a problem of predicting future state distribution in an MDP. The approach taken is an indirect approach which first predicts whether an observation comes from the future and transfers this binary prediction via Bayes rule to predictions over future states. 

Conceptually the paper's appears novel but a previous similar work was very relevant. In UOM paper, Definition 1 is almost exactly the same as the discounted state occupancy function defined the UOM paper, just an extra normalization constant (1-\gamma). Apparently at this core the idea is not novel. However, the paper's development of casting the prediction problem into two stages (first binary prediction) and using Bayesian to transfer into the future state distribution is novel. The connection of UOM paper is interesting but not discussed unfortunately. In the UOM paper, the authors there appear to focus on reward-less MDPs, where you can generate/compute the value function given a reward function on the fly. Here the paper focuses on more on the estimation of the the discounted state occupancy function, -- this is my interpretation.   

Paragraph ""In discrete state spaces"":
introducing this reward: again this is just another interpretation of the discounted state occupancy function in UOM (equation 2). 

Remark 1:
I don't understand the point of Remark 1. do you mean for a continuous-state problems, the prob of reaching some particular state is zero? Isn't that that obvious? What is the point here?

Hypothesis 1 and 2: I don't know whether they make sense. The paper relies on the experiments to try to make sense of it. I'm not sure this is a sound approach. Why testing these hypotheses is interesting at the first place? At least this isn't clear from the paper. 


UOM paper (not cited):
https://papers.nips.cc/paper/5590-universal-option-models

I've read the authors' feedbacks and other reviewers' comments. R5's main concerns are the clarity and the motivation of Bayessian classifier and off-policy learning. That should have been resolved from authors' feedbacks. 
","7: Good paper, accept"
C-Learning: Learning to Achieve Goals via Recursive Classification,https://openreview.net/pdf?id=tc5qisoB-C,"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.","Benjamin Eysenbach,Ruslan Salakhutdinov,Sergey Levine",Building probability density of reaching a future observation via contrastive classification,"This paper explores learning a classifier to predict if a given state/observation will be reached in the future from the current state and action pair. Using this classifier, the paper is able to create a probability density function that could be conditioned on reaching a goal state. This idea is interesting and could leverage large scale self-supervised learning to build the future state probability density function. I have some concerns which are listed below.

1. How does the method work on low dimensional action spaces where the probability of reaching a future state from current state and action is usually higher?

2. How does the method work on high dimensional tasks that have discrete-continuous dynamics due to multiple contacts, such as the pen task. The probability of reaching a future state from the current state + action is very low and the experiments show only training on a human demonstrated dataset. Can the policy be used to control the hand to reach a target pen position very different from the human demonstration?","7: Good paper, accept"
C-Learning: Learning to Achieve Goals via Recursive Classification,https://openreview.net/pdf?id=tc5qisoB-C,"We study the problem of predicting and controlling the future state distribution of an autonomous agent. This problem, which can be viewed as a reframing of goal-conditioned reinforcement learning (RL), is centered around learning a conditional probability density function over future states. Instead of directly estimating this density function, we indirectly estimate this density function by training a classifier to predict whether an observation comes from the future. Via Bayes' rule, predictions from our classifier can be transformed into predictions over future states. Importantly, an off-policy variant of our algorithm allows us to predict the future state distribution of a new policy, without collecting new experience. This variant allows us to optimize functionals of a policy's future state distribution, such as the density of reaching a particular goal state. While conceptually similar to Q-learning, our work lays a principled foundation for goal-conditioned RL as density estimation, providing justification for goal-conditioned methods used in prior work. This foundation makes hypotheses about Q-learning, including the optimal goal-sampling ratio, which we confirm experimentally. Moreover, our proposed method is competitive with prior goal-conditioned RL methods.","Benjamin Eysenbach,Ruslan Salakhutdinov,Sergey Levine",Official Blind Review #5,"[summary]
This paper studies to predict future state density function by using an indirectly method via classification. The main idea is to sample the future state from two sources: 1) from replay buffer, 2) the actual next state in the trajectory (in off policy setting we only need the next state) and then use a classifier to distinguish them. By Bayesian rule we can recalculate the conditional density function by the ratio of the classifier. The paper compare this method with several baseline and find that they can predict the conditional density function very close to reality.

[originality]
I really like the idea and the method seems very interesting and novel to me. However, the main concern is the motivation of the classifier. It seems to me that we can classify the true $s_{t,+}$ with any source of distribution of $p(s_{+})$. For example, in your Algorithm 1 or 2, we can replace $p(s+)$ with arbitrary distribution (even the distribution we create), then the whole derivation still hold. I believe it is reasonable to try to distinguish $p(s+)$ with $p(s+|s_t, a_t)$, but did you compare with any other prior distribution of $p(s+)$?

[clarity and theoretical soundness]
My major concern of the paper is the clarity which I will explain in the sequel.
- Notation.
 The main notation of the future state density function is abused the notation of $p$. I think in Definition 1 the right hand side you are defining a new condition density function, not a new future state. So I would recommend to write it as:
$$p_{+}(s'|s,a) = (1-\gamma)\sum_{t=0}^{\infty}\gamma^{t}p_{t}(s'|s,a),$$
where $p_t(s'|s,a)$ is the distribution if $s'$ occur $t$ step after the occurrence of the $s,a$ pair.
Many notations in the main text has this kind of problem where it is hard to distinguish the future distribution with the original distribution.
- Main message in section 5 is not clear.
Algorithm box 1 is confusing. It seems to me that we have double terms of $s_{t,+}$, and they are all used in the loss function (F=1 and F=0). If I understand correctly, we should put $s_{t,+,0} \sim p(s_{t,+}$ (again I don't like this notation, p can be any other meaning) and $s_{t,+,1} \gets s_{t+\delta}$, and the loss function for $F=0$ uses $s_{t,+,0}$ and $F=1$ uses $s_{t,+,1}$. 
And without looking at the paragraph in details, it is hard to tell what is $F$ at the first glance. I feel like you can rewrite this section a little bit by motivating the reader why we consider using a classifier.
-Minor.
  1. Remark 1 should add some assumption, otherwise we can always think tabular case is a special case of in the continuous state space, where the probability mass function is not 0.
  2. Eq.(1) should be $p(F=1|s_t, a_t, s_{t,+}) = $XXX, missing a $s_{t,+}$ in left hand side. Similar for the next line.
  3.The sentence after Eq.(2), why we can get rid of estimating the marginal density? Not clear and no explanation.
  4. Eq.(4) missing an expectation for $E_{s_{t+1}, a_{t+1}}$.
  5. Eq.(5), we should put $F(\theta, \pi) = $ in the equation not a line above.
  6. Eq.(5), the subscript of expectation is not standard (compared to Eq.(3)).
  7. Unclear reason why we want to stop gradient in Eq.(7) and Algorithm 3. Is that stabilized the learning process?

[related work]
In the first line of the page 2, I think ""contrastive approach"" is not accurate. I didn't see any contrastive objective in the paper.
I feel like the density based methods in off-policy evaluation(e.g. Liu et. al. 2018 and DICE family (e.g. Nachum et. al. 2019)) is more relevant to value based method, where we can set the reward function as the indicator function and estimate the average reward of the policy $\pi$. Their methods are also based on a recursive objective, I think a comparison between that will be beneficial to further clarify your contribution.

[reference]
1. Qiang Liu, Lihong Li, Ziyang Tang, Dengyong Zhou. Breaking the Curse of Horizon: Infinite-Horizon Off-Policy Estimation
2. Ofir Nachum, Yinlam Chow, Bo Dai, Lihong Li. DualDICE: Behavior-Agnostic Estimation of Discounted Stationary Distribution Corrections
",4: Ok but not good enough - rejection
Unsupervised Learning of Node Embeddings by Detecting Communities,https://openreview.net/pdf?id=Byl3K2VtwB,"We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide interesting insights into the graph structure, so that the separate node clustering step of existing methods is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.","Chi Thang Duong,Dung Hoang,Truong Giang Le Ba,Thanh Le Cong,Hongzhi Yin,Matthias Weidlich,Quoc Viet Hung Nguyen,Karl Aberer",Official Blind Review #3,"This paper is reporting an unsupervised approach to learn node embeddings and communities simultaneously by minimizing the mincut loss function. This approach programs the data through encoder to generate membership likehood matrix H, and then generates corresponding membership matrix P using node selection. The matrix P and adjacency matrix are coupled to generate community adjacency matrix to minimize the cutting. However, despite such attractive points, the novelty and strength of this study is not outstanding enough for publication in ICLR. Details comments are as follows,
1. Similar work such as the methods of transferring matrix E to H then to P have already been published , which reduce the novelty of this study.
2. Though there is a schematic, the learning embedding process is still not described clearly. More details of the algorithms need to be discussed to such as how to get the node embedding matrix E.",3: Weak Reject
Unsupervised Learning of Node Embeddings by Detecting Communities,https://openreview.net/pdf?id=Byl3K2VtwB,"We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide interesting insights into the graph structure, so that the separate node clustering step of existing methods is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.","Chi Thang Duong,Dung Hoang,Truong Giang Le Ba,Thanh Le Cong,Hongzhi Yin,Matthias Weidlich,Quoc Viet Hung Nguyen,Karl Aberer",Official Blind Review #3,"Although this paper seems to only combine existing techniques in community detection and node embedding into a co-train process. The idea is simple and easy understood and the paper is well-written. Theoretical analysis is provided for the approximation error for the sampling strategy. However, major concerns are:

1. Experimental results show that co-training node embedding and community detection can improve the performance for node classification. The improvements may result from the assumption that papers with the same class label are associated with the same community in the citation graph. However, in the dataset, there are many cases that there are not dense connections among the same labeled papers. The authors should check the correlation between the detected communities and the original paper labels.

2. No comparison with other community-preserving node embedding methods, such as ""Community Preserving Network Embedding"" in AAAI17

3. Since this paper aims to combine community detection and node embedding process, a set of baseline should be considered. For example, if considering the downstream node classification of node embedding as an evaluation task, then how about the performance of the following two-step method. We can first detect communities based on the node features then do graph node embedding by considering the communities' membership and node features together (e.g. simply concatenating both community membership features and node features).

4. Efficiency and scalability evaluations are needed. Spectral clustering has a scalability issue when meeting big graphs. Since the spectral process is also applied in the proposed method, efficiency and scalability evaluations are encouraged to provide, especially for big graphs which are not covered in the selected datasets in this paper.

5. In Sec 5.3 and Fig 2, it's mentioned that trends of the three datasets are different. For the increasing trend, how about the performance for an extreme case where all nodes are considered in one batch. On the other hand, adding more nodes in one minibatch could provide more information, but why there exists a decreasing trend? Though the authors provide a reason in Sec 5.3, it's better to analyze the reason directly from the datasets.",3: Weak Reject
Unsupervised Learning of Node Embeddings by Detecting Communities,https://openreview.net/pdf?id=Byl3K2VtwB,"We present Deep MinCut (DMC), an unsupervised approach to learn node embeddings for graph-structured data. It derives node representations based on their membership in communities. As such, the embeddings directly provide interesting insights into the graph structure, so that the separate node clustering step of existing methods is no longer needed. DMC learns both, node embeddings and communities, simultaneously by minimizing the mincut loss, which captures the number of connections between communities. Striving for high scalability, we also propose a training process for DMC based on minibatches. We provide empirical evidence that the communities learned by DMC are meaningful and that the node embeddings are competitive in different node classification benchmarks.","Chi Thang Duong,Dung Hoang,Truong Giang Le Ba,Thanh Le Cong,Hongzhi Yin,Matthias Weidlich,Quoc Viet Hung Nguyen,Karl Aberer",Official Blind Review #1,"This work proposes a neural netowrk approach to minimize mincutloss, thus achieving embedding nodes and find communities at the same time. However, it is difficult for me to understand the paper and I feel that it is not clearly written.

1. The algorithm is not written in a box as in Algorithm 1. At first I thought algorithm 1 is the main method, but only after reading it I realized that it is one step of the algorithm. I would appreciate it if the complete algorithm (including input, output, parameters) can be summerized clearly.

2. I am confused about the claim ""spectral approach underperforms significantly on the bibliographic datasets as it only uses the structure information in the graph."" I thought the input of all methods are the adjencency matrix A.

3. In table 2 and table 4, why does the paper compare different methods with different measures? Is it possible to compare all methods using all measures?",3: Weak Reject
